<p><strong>Lesson 13</strong></p>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=9s">00:00:10</a> <a href="http://fast.ai/">Fast.ai</a> student accepted into Google Brain Residency program</p>
</li>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=6m30s">00:06:30</a> Cyclical Learning Rates for Training Neural Networks (another student’s paper)<br>
&amp; updates on Style Transfer, GAN, and Mean Shift Clustering research papers</p>
</li>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=13m45s">00:13:45</a> Tiramisu: combining Mean Shitft Clustering and Approximate Nearest Neighbors</p>
</li>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=22m15s">00:22:15</a> Facebook AI Similarity Search (FAISS)</p>
</li>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=28m15s">00:28:15</a> The BiLSTM Hegemony</p>
</li>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=35m">00:35:00</a> Implementing the BiLSTM, and Grammar as a Foreign Language (research)</p>
</li>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=45m30s">00:45:30</a> Reminder on how RNN’s work from Lesson #5 (Part 1)</p>
</li>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=47m20s">00:47:20</a> Why Attentional Models use “such” a simple architecture<br>
&amp; “Tacotron: a Fully End-To-End Text-To-Speech Synthesis Model” (research)</p>
</li>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=50m15s">00:50:15</a> Continuing on Spelling_bee_RNN notebook (Attention Model), from Lesson 12</p>
</li>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=58m40s">00:58:40</a> Building the Attention Layer and the ‘attention_wrapper.py’ walk-through</p>
</li>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=1h15m40s">01:15:40</a> Impressive student’s experiment with different mathematical technique on Style Transfer</p>
</li>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=1h18m">01:18:00</a> Translate English into French, with Pytorch</p>
</li>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=1h31m20s">01:31:20</a> Translate English into French: using Keras to prepare the data<br>
Note: Pytorch latest version now supports Broadcasting</p>
</li>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=1h38m50s">01:38:50</a> Writing and running the ‘Train &amp; Test’ code with Pytorch</p>
</li>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=1h44m">01:44:00</a> NLP Programming Tutorial, by Graham Neubig (NAIST)</p>
</li>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=1h48m25s">01:48:25</a> Question: “Could we translate Chinese to English with that technique ?”<br>
&amp; new technique: Neural Machine Translation of Rare Words with Subword Units (Research)</p>
</li>
<li>
<p><a href="https://youtu.be/-lx2shfA-5s?t=1h54m45s">01:54:45</a> Leaving Translation aside and moving to Image Segmentation,<br>
with the “The 100 layers Tiramisu: Fully Convolutional DenseNets” (research)<br>
and “Densely Connected Convolutional Networks” (research)</p>
</li>



