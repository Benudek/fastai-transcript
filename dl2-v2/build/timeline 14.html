<p><strong>Lesson 14</strong></p>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=1m25s">00:01:25</a> Time-Series and Structured Data<br>
&amp; “Patient Mortality Risk Predictions in Pediatric Intensive Care, using RNN’s” (research)</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=7m30s">00:07:30</a> Time-Series with Rossmann Store Sales (Kaggle)<br>
&amp; 3rd place solution with “a very uncool NN ;-)”.</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=18m">00:18:00</a> Implementing the Rossman solution with Keras + TensorFlow + Pandas + Sklearn<br>
Building Tables &amp; Exploratory Data Analysis (EDA)</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=27m15s">00:27:15</a> Digress: categorical variable encodings and “Vtreat for R”</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=30m15s">00:30:15</a> Back to Rossmann solution<br>
&amp; “Python for Data Analysis” (book)</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=36m30s">00:36:30</a> What Jeremy does everytime he sees a ‘date’ in a structured ML model<br>
&amp; other tips</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=43m">00:43:00</a> Dealing with duration of special events (holidays, promotions) in Time-Series</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=52m">00:52:00</a> Using ‘inplace=True’ in .drop(), &amp; a look at our final ‘feature engineering’ results</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=53m40s">00:53:40</a> Starting to feed our NN<br>
&amp; using ‘pickle.dump()’ for storage encodings</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=1h45s">01:00:45</a> “Their big mistake” and how they could have won #1</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=1h5m30s">01:05:30</a> Splitting into Training and Test, but not randomly</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=1h8m20s">01:08:20</a> Why they modified their Sales Target with ‘np.log()/max_log_y’</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=1h11m20s">01:11:20</a> A look at our basic model</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=1h14m45s">01:14:45</a> Training our model and questions</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=1h16m45s">01:16:45</a> Running the same model with XGBoost</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=1h20m10s">01:20:10</a> “The really, really, really weird things here !”<br>
&amp; end of the Rossmann competition;-)</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=1h26m30s">01:26:30</a> Taxi Trajectory Prediction (Kaggle) with “another uncool NN” winner</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=1h38m">01:38:00</a> “Start with a Conv layer and pass it to an RNN” question and research</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=1h42m40s">01:42:40</a> The 100-layers Tiramisu: Fully Convolutional Densenets, for Image Segmentation (Lesson 13 cont.)</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=1h58m">01:58:00</a> Building and training the Tiramisu model</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=2h2m50s">02:02:50</a> ENet and LINKNet models: better than the Tiramisu ?</p>
</li>
<li>
<p><a href="https://youtu.be/1-NYPQw5THU?t=2h4m">02:04:00</a> Part 2: conclusion and next steps</p>
</li>
</ul>


