1
00:00:00,709 --> 00:00:06,750
so welcome to lesson 10 or is somebody

2
00:00:04,440 --> 00:00:09,089
on the forum described at lesson 10 not

3
00:00:06,750 --> 00:00:11,609
seven which is probably a clearer way to

4
00:00:09,089 --> 00:00:18,240
think about this we're going to be

5
00:00:11,609 --> 00:00:21,809
talking about NLP before we do let's do

6
00:00:18,239 --> 00:00:23,879
a quick review of last week because last

7
00:00:21,809 --> 00:00:25,528
week you know there's quite a few people

8
00:00:23,879 --> 00:00:27,509
who have kind of flown here to San

9
00:00:25,528 --> 00:00:28,679
Francisco for this inverse of course I'm

10
00:00:27,510 --> 00:00:31,170
seeing them pretty much every day

11
00:00:28,679 --> 00:00:32,820
they're working full time on this and

12
00:00:31,170 --> 00:00:34,380
quite a few of them are still struggling

13
00:00:32,820 --> 00:00:36,000
to understand the material from last

14
00:00:34,380 --> 00:00:38,700
week right so if you're finding it

15
00:00:36,000 --> 00:00:40,289
difficult that's that's fine one of the

16
00:00:38,700 --> 00:00:42,960
reasons I kind of put it up there up

17
00:00:40,289 --> 00:00:45,269
front is so that we've got something to

18
00:00:42,960 --> 00:00:48,859
cogitate about and think about gradually

19
00:00:45,270 --> 00:00:51,329
work towards for the by lesson 14 mod 7

20
00:00:48,859 --> 00:00:54,269
your you'll get a second crack at it

21
00:00:51,329 --> 00:00:56,189
okay but it's it's world work that

22
00:00:54,270 --> 00:00:57,180
there's so many pieces and so hopefully

23
00:00:56,189 --> 00:00:59,250
you can keep developing a better

24
00:00:57,179 --> 00:01:01,140
understanding to understand the pieces

25
00:00:59,250 --> 00:01:03,359
you'll need to understand you know the

26
00:01:01,140 --> 00:01:06,629
shapes of convolutional layer outputs

27
00:01:03,359 --> 00:01:09,118
and receptive fields and loss functions

28
00:01:06,629 --> 00:01:10,379
and and everything right so it's all

29
00:01:09,118 --> 00:01:13,228
stuff that you're going to understand

30
00:01:10,379 --> 00:01:15,509
need to understand for all of your deep

31
00:01:13,228 --> 00:01:16,978
learning studies anyway right so

32
00:01:15,509 --> 00:01:18,930
everything you do to develop an

33
00:01:16,978 --> 00:01:22,530
understanding of last week's lesson is

34
00:01:18,930 --> 00:01:24,540
going to help you everything else so one

35
00:01:22,530 --> 00:01:25,920
key thing I wanted to mention is we

36
00:01:24,540 --> 00:01:27,270
started out with something which is

37
00:01:25,920 --> 00:01:29,310
really pretty simple which is single

38
00:01:27,269 --> 00:01:32,039
person classifier a single object class

39
00:01:29,310 --> 00:01:34,650
player single object bounding box

40
00:01:32,040 --> 00:01:37,490
without a classifier and then single

41
00:01:34,650 --> 00:01:40,680
object classifier and darling box and

42
00:01:37,489 --> 00:01:45,658
anybody who's hopefully spent some time

43
00:01:40,680 --> 00:01:47,909
studying since lesson 8 mod seven has

44
00:01:45,659 --> 00:01:50,430
got to the point where they understand

45
00:01:47,909 --> 00:01:53,159
this bit right now the reason I mention

46
00:01:50,430 --> 00:01:56,368
this is because the bit where we go to

47
00:01:53,159 --> 00:01:59,070
multiple objects is actually almost

48
00:01:56,368 --> 00:02:01,140
identical to this except we first have

49
00:01:59,069 --> 00:02:03,959
to solve the matching problem we ended

50
00:02:01,140 --> 00:02:06,439
up creating far more activations than we

51
00:02:03,959 --> 00:02:09,209
need for our number of bounding boxes

52
00:02:06,438 --> 00:02:10,788
ground truth bounding boxes and so we

53
00:02:09,209 --> 00:02:12,650
match each ground truth object

54
00:02:10,788 --> 00:02:14,958
to a subset of those activations right

55
00:02:12,650 --> 00:02:16,310
and once we've done that the loss

56
00:02:14,959 --> 00:02:19,039
function that we then do to which

57
00:02:16,310 --> 00:02:21,348
matched pair is almost identical to this

58
00:02:19,039 --> 00:02:26,889
loss function right so if you're feeling

59
00:02:21,348 --> 00:02:29,929
stuck go back to lesson eight right and

60
00:02:26,889 --> 00:02:31,908
make sure you understand the data set

61
00:02:29,930 --> 00:02:35,000
the data loader and most importantly the

62
00:02:31,908 --> 00:02:39,399
loss function from the end of lesson

63
00:02:35,000 --> 00:02:39,400
eight or the start of lesson nine okay

64
00:02:39,459 --> 00:02:43,549
okay

65
00:02:40,639 --> 00:02:45,950
so once we've got this thing which can

66
00:02:43,549 --> 00:02:48,560
predict the class and bounding box for

67
00:02:45,949 --> 00:02:52,188
one object we went to multiple objects

68
00:02:48,560 --> 00:02:53,900
by just creating more activations we had

69
00:02:52,188 --> 00:02:55,250
to then deal with the matching problem

70
00:02:53,900 --> 00:02:57,379
having done with the dealt with a

71
00:02:55,250 --> 00:03:01,098
matching problem we then basically moved

72
00:02:57,378 --> 00:03:04,098
each of those anchor boxes in and out a

73
00:03:01,098 --> 00:03:05,449
little bit and around a little bit so

74
00:03:04,098 --> 00:03:08,688
they tried to line up with a particular

75
00:03:05,449 --> 00:03:11,839
ground truth objects and we talked about

76
00:03:08,688 --> 00:03:14,180
how we took advantage of the

77
00:03:11,840 --> 00:03:18,049
convolutional nature of the network to

78
00:03:14,180 --> 00:03:21,049
to try to have activations that had a

79
00:03:18,049 --> 00:03:22,430
receptive field that was similar to the

80
00:03:21,049 --> 00:03:25,849
ground truth object we were predicting

81
00:03:22,430 --> 00:03:29,449
and khloÃ© sultan provided this fantastic

82
00:03:25,848 --> 00:03:30,589
picture I guess for her own notes but

83
00:03:29,449 --> 00:03:34,159
she shared it with everybody which is

84
00:03:30,590 --> 00:03:36,799
lovely to talk about like what is SSD

85
00:03:34,158 --> 00:03:39,228
mode he had to forward do line by line

86
00:03:36,799 --> 00:03:41,239
and I apparently wanted to show this to

87
00:03:39,229 --> 00:03:42,889
help you with your revision but I also

88
00:03:41,239 --> 00:03:46,039
can't you wanted to show through this

89
00:03:42,889 --> 00:03:48,560
show this to kind of say doing this kind

90
00:03:46,039 --> 00:03:50,719
of stuff is very useful for you to do

91
00:03:48,560 --> 00:03:52,250
like walk through and in whatever way

92
00:03:50,719 --> 00:03:53,930
helps you make sure you understand

93
00:03:52,250 --> 00:03:56,299
something and you can see what Chloe's

94
00:03:53,930 --> 00:04:01,250
done here is she's focused particularly

95
00:03:56,299 --> 00:04:04,639
on the dimensions of the tensor at each

96
00:04:01,250 --> 00:04:06,378
point and the in the in the path has be

97
00:04:04,639 --> 00:04:08,329
kind of gradually down sampling using

98
00:04:06,378 --> 00:04:10,728
these tragic convolutions making sure

99
00:04:08,329 --> 00:04:13,280
she understands why those grid sizes

100
00:04:10,729 --> 00:04:16,939
happen and then understanding how the

101
00:04:13,280 --> 00:04:19,608
outputs come out of those okay and so

102
00:04:16,939 --> 00:04:23,538
one thing you might be wondering is well

103
00:04:19,608 --> 00:04:24,469
how did Chloe calculate these numbers so

104
00:04:23,538 --> 00:04:26,050
I don't know

105
00:04:24,470 --> 00:04:28,070
the answer I have spoken to her but

106
00:04:26,050 --> 00:04:29,600
obviously one approach would be like

107
00:04:28,069 --> 00:04:30,860
from first principles just thinking

108
00:04:29,600 --> 00:04:31,910
through it but then you want to know

109
00:04:30,860 --> 00:04:35,030
what am i right

110
00:04:31,910 --> 00:04:38,030
right and so this is where you've got to

111
00:04:35,029 --> 00:04:40,189
remember this pdb dot set trace idea

112
00:04:38,029 --> 00:04:43,399
right so I just went in just before

113
00:04:40,189 --> 00:04:46,370
class and went into SSD multi-head dot

114
00:04:43,399 --> 00:04:49,669
forward and entered pdb dot set trace

115
00:04:46,370 --> 00:04:51,949
and then I ran a single batch right and

116
00:04:49,670 --> 00:04:54,319
so at that so I put the trace at the end

117
00:04:51,949 --> 00:05:00,439
and then I could just print out the size

118
00:04:54,319 --> 00:05:03,529
of all of these guys right so which by

119
00:05:00,439 --> 00:05:08,240
the way reminds me last week there may

120
00:05:03,529 --> 00:05:16,909
have been a point where I said 21 plus 4

121
00:05:08,240 --> 00:05:22,639
equals 26 which is not true in most

122
00:05:16,910 --> 00:05:24,439
universes and like by the way when I

123
00:05:22,639 --> 00:05:26,029
code I do that stuff like that's the

124
00:05:24,439 --> 00:05:28,730
kind of thing I do all the time so

125
00:05:26,029 --> 00:05:30,439
that's why we have debuggers and know

126
00:05:28,730 --> 00:05:32,210
how to check things and do things in

127
00:05:30,439 --> 00:05:34,069
small little bits along the way anyway

128
00:05:32,209 --> 00:05:35,750
so this idea of putting a debugger

129
00:05:34,069 --> 00:05:37,430
inside your forward function and

130
00:05:35,750 --> 00:05:40,910
printing out the sizes is something

131
00:05:37,430 --> 00:05:44,870
which is damn super or you could just

132
00:05:40,910 --> 00:05:46,010
put a print statement here as well so I

133
00:05:44,870 --> 00:05:48,350
actually don't know if that's how Khloe

134
00:05:46,009 --> 00:05:51,379
figured it out but that's how I would if

135
00:05:48,350 --> 00:05:53,660
I was her and then we talked about

136
00:05:51,379 --> 00:05:55,639
increasing K which is the number of

137
00:05:53,660 --> 00:05:57,200
anchor boxes for each convolutional grid

138
00:05:55,639 --> 00:05:59,930
cell which we can do with different

139
00:05:57,199 --> 00:06:03,829
zooms and different aspect ratios and so

140
00:05:59,930 --> 00:06:06,340
that gives us a plethora of activations

141
00:06:03,829 --> 00:06:10,279
and therefore predicted bounding boxes

142
00:06:06,339 --> 00:06:15,099
which we then went down to a small

143
00:06:10,279 --> 00:06:17,239
number using non maximum suppression and

144
00:06:15,100 --> 00:06:18,740
I'll try and remember to put a link

145
00:06:17,240 --> 00:06:20,660
there's a really interesting paper that

146
00:06:18,740 --> 00:06:22,490
one of our students told me about that I

147
00:06:20,660 --> 00:06:24,890
hadn't heard about which is attempting

148
00:06:22,490 --> 00:06:27,410
to like you know I've mentioned on

149
00:06:24,889 --> 00:06:30,319
maximum suppression it's like kind of

150
00:06:27,410 --> 00:06:31,970
hacky kind of leads totally heuristic

151
00:06:30,319 --> 00:06:35,029
you know didn't even talk about the code

152
00:06:31,970 --> 00:06:36,590
because it seems kind of hideous so

153
00:06:35,029 --> 00:06:37,609
somebody actually came up with a paper

154
00:06:36,589 --> 00:06:40,429
recently

155
00:06:37,610 --> 00:06:43,939
attempts to do an end-to-end ComNet to

156
00:06:40,430 --> 00:06:46,430
replace that NMS piece so I'll I'll put

157
00:06:43,939 --> 00:06:49,069
that paper up nobody's created a torch a

158
00:06:46,430 --> 00:06:52,038
pipe torch implementation yeah so be an

159
00:06:49,069 --> 00:06:56,598
interesting project if anyone wanted to

160
00:06:52,038 --> 00:06:59,180
try that um one thing I've noticed in

161
00:06:56,598 --> 00:07:02,870
our study groups during the week is not

162
00:06:59,180 --> 00:07:05,120
enough people reading papers the the

163
00:07:02,870 --> 00:07:08,658
what we are doing in class now is

164
00:07:05,120 --> 00:07:11,689
implementing papers the papers are the

165
00:07:08,658 --> 00:07:13,218
real ground truth right and I think you

166
00:07:11,689 --> 00:07:14,569
know from talking to people a lot of the

167
00:07:13,218 --> 00:07:15,978
reason people aren't reading papers is

168
00:07:14,569 --> 00:07:18,169
because a lot of people don't think

169
00:07:15,978 --> 00:07:19,310
they're capable of reading papers they

170
00:07:18,168 --> 00:07:22,758
don't think they're the kind of people

171
00:07:19,310 --> 00:07:25,339
that read papers but you are you're here

172
00:07:22,759 --> 00:07:28,189
right and like we started looking at a

173
00:07:25,339 --> 00:07:29,810
paper last week and we read the words

174
00:07:28,189 --> 00:07:33,889
that were in English and we largely

175
00:07:29,810 --> 00:07:35,689
understood them right so it's like if

176
00:07:33,889 --> 00:07:40,819
you actually look through this picture

177
00:07:35,689 --> 00:07:43,370
from SSD carefully you'll realize SSD

178
00:07:40,819 --> 00:07:45,770
multi-head but forward is not doing the

179
00:07:43,370 --> 00:07:49,098
same as this and then you might think oh

180
00:07:45,769 --> 00:07:51,680
well I wonder if this is better you know

181
00:07:49,098 --> 00:07:53,209
and my answer is probably right because

182
00:07:51,680 --> 00:07:56,658
that's his team all he had up forward

183
00:07:53,209 --> 00:07:58,579
was like the first thing I tried just to

184
00:07:56,658 --> 00:08:00,889
get something out there you know but you

185
00:07:58,579 --> 00:08:02,778
know that there are between this and the

186
00:08:00,889 --> 00:08:05,718
yellow vert and three paper and stuff

187
00:08:02,778 --> 00:08:06,860
they're probably much better ways one

188
00:08:05,718 --> 00:08:09,709
thing you'll notice in particular is

189
00:08:06,860 --> 00:08:13,729
they use a smaller K but they have a lot

190
00:08:09,709 --> 00:08:15,978
more sets of grids one by one three by

191
00:08:13,728 --> 00:08:20,180
three five by five ten by ten 19 by 19

192
00:08:15,978 --> 00:08:22,818
and 38 by 38 8700 per class right so a

193
00:08:20,180 --> 00:08:24,610
lot more then the we hats they've been

194
00:08:22,819 --> 00:08:28,639
interesting thing to experiment with

195
00:08:24,610 --> 00:08:30,979
another thing I noticed is that where

196
00:08:28,639 --> 00:08:32,778
else we had four by four two by two one

197
00:08:30,978 --> 00:08:35,838
by one which means there's a lot of

198
00:08:32,778 --> 00:08:38,179
overlap like every set fits within every

199
00:08:35,839 --> 00:08:40,789
other set in this case where you've got

200
00:08:38,179 --> 00:08:42,739
one three five there's that you don't

201
00:08:40,788 --> 00:08:44,179
have that overlap right so it might

202
00:08:42,740 --> 00:08:46,159
actually make it easier to learn so

203
00:08:44,179 --> 00:08:48,919
there's lots of interesting things you

204
00:08:46,159 --> 00:08:49,909
can play with based on stuff that's out

205
00:08:48,919 --> 00:08:51,169
you know either trying to make it closer

206
00:08:49,909 --> 00:08:52,669
to the paper or

207
00:08:51,169 --> 00:08:54,729
think about other things you could try

208
00:08:52,669 --> 00:08:57,199
that aren't in the paper or whatever

209
00:08:54,730 --> 00:09:01,159
perhaps most important thing I would

210
00:08:57,200 --> 00:09:03,890
recommend is to put the code and the

211
00:09:01,159 --> 00:09:06,078
equations next to each other yes Rachel

212
00:09:03,889 --> 00:09:07,819
there was a question of whether you

213
00:09:06,078 --> 00:09:09,289
could speak about the used cyclic

214
00:09:07,820 --> 00:09:14,510
learning rate argument and the fit

215
00:09:09,289 --> 00:09:18,019
function we broke it there so put the

216
00:09:14,509 --> 00:09:21,649
code and the equations from the paper

217
00:09:18,019 --> 00:09:25,129
next to each other and draw in one of

218
00:09:21,649 --> 00:09:28,129
two groups you are either a code person

219
00:09:25,129 --> 00:09:30,708
like me who's not that happy about math

220
00:09:28,129 --> 00:09:32,689
in which case I start with a code and

221
00:09:30,708 --> 00:09:35,328
then I look at the math and I learn

222
00:09:32,690 --> 00:09:37,570
about how the math master the code and

223
00:09:35,328 --> 00:09:41,328
end up eventually understanding the math

224
00:09:37,570 --> 00:09:44,480
all your PhD in stochastic differential

225
00:09:41,328 --> 00:09:48,199
equations like Rachel whatever that

226
00:09:44,480 --> 00:09:50,990
means in which case you can look at the

227
00:09:48,200 --> 00:09:53,180
math and then learn about how the code

228
00:09:50,990 --> 00:09:55,190
implements the math right then either

229
00:09:53,179 --> 00:09:56,838
way unless you're one of those rare

230
00:09:55,190 --> 00:10:00,170
people who is equally comfortable in

231
00:09:56,839 --> 00:10:04,430
either world you'll learn about one or

232
00:10:00,169 --> 00:10:05,360
the other now learning about code is

233
00:10:04,429 --> 00:10:06,889
pretty easy because there's

234
00:10:05,360 --> 00:10:09,169
documentation and we know how to it's

235
00:10:06,889 --> 00:10:11,059
indexed up and so forth sometimes

236
00:10:09,169 --> 00:10:13,789
learning the math is hard because the

237
00:10:11,059 --> 00:10:15,799
notation might seem how to look up but

238
00:10:13,789 --> 00:10:18,620
there's actually a lot of resources for

239
00:10:15,799 --> 00:10:21,799
example list of mathematical symbols on

240
00:10:18,620 --> 00:10:24,799
Wikipedia is amazingly great it has

241
00:10:21,799 --> 00:10:26,929
examples of the explanations of what

242
00:10:24,799 --> 00:10:30,859
they mean and tells you what to search

243
00:10:26,929 --> 00:10:33,198
for to find out more about it really

244
00:10:30,860 --> 00:10:36,230
terrific and if you google for math

245
00:10:33,198 --> 00:10:40,219
notation cheat sheet you'll find more of

246
00:10:36,230 --> 00:10:43,220
these kinds of terrific resources ok so

247
00:10:40,220 --> 00:10:45,589
over time you do need to learn the

248
00:10:43,220 --> 00:10:47,839
notation but as you'll see from the

249
00:10:45,589 --> 00:10:49,940
Wikipedia page there's not actually that

250
00:10:47,839 --> 00:10:52,100
much division right obviously there's a

251
00:10:49,940 --> 00:10:54,350
lot of concepts behind it but once you

252
00:10:52,100 --> 00:10:56,720
know the notation you can then quickly

253
00:10:54,350 --> 00:10:59,420
look at the concept as it pertains to a

254
00:10:56,720 --> 00:11:02,420
particular thing you're studying nobody

255
00:10:59,419 --> 00:11:04,819
learns all of math and then starts

256
00:11:02,419 --> 00:11:07,370
learning machine learning right

257
00:11:04,820 --> 00:11:10,220
everybody even top researchers I know

258
00:11:07,370 --> 00:11:12,110
when they're reading a new paper will

259
00:11:10,220 --> 00:11:13,430
very often come to bits of math they

260
00:11:12,110 --> 00:11:17,649
haven't seen before and they'll have to

261
00:11:13,429 --> 00:11:17,649
go away and what that that bit of math

262
00:11:17,830 --> 00:11:23,810
another thing you should try doing is to

263
00:11:20,330 --> 00:11:26,360
recreate things that you see in the

264
00:11:23,809 --> 00:11:30,379
papers right so here was the key most

265
00:11:26,360 --> 00:11:33,769
important figure one from the focal loss

266
00:11:30,379 --> 00:11:38,120
paper the retina paper so recreate it

267
00:11:33,769 --> 00:11:40,039
right and like very often I put these

268
00:11:38,120 --> 00:11:41,929
challenges up on the forums right so

269
00:11:40,039 --> 00:11:43,339
like keep an eye on the lessons register

270
00:11:41,929 --> 00:11:45,379
either put on the forums and so I put

271
00:11:43,340 --> 00:11:47,240
this challenge up there and within about

272
00:11:45,379 --> 00:11:49,700
three three minutes serrata had said

273
00:11:47,240 --> 00:11:53,299
done it in Microsoft Excel

274
00:11:49,700 --> 00:11:55,190
naturally along with actually a lot more

275
00:11:53,299 --> 00:11:56,839
information within the original paper

276
00:11:55,190 --> 00:11:59,210
the nice thing here is that she was

277
00:11:56,840 --> 00:12:02,330
actually able to draw a line showing at

278
00:11:59,210 --> 00:12:04,910
a point five ground truth probability

279
00:12:02,330 --> 00:12:08,000
what's the loss for different amounts of

280
00:12:04,909 --> 00:12:08,659
gamma I'm just kind of cool and if you

281
00:12:08,000 --> 00:12:16,100
want to cheat

282
00:12:08,659 --> 00:12:18,799
she's also provided Python code to I did

283
00:12:16,100 --> 00:12:20,629
discover a reminder bug in my code last

284
00:12:18,799 --> 00:12:22,990
week the way that I was flattening out

285
00:12:20,629 --> 00:12:26,059
the convolutional activations did not

286
00:12:22,990 --> 00:12:27,980
line up with how I was using them in the

287
00:12:26,059 --> 00:12:29,089
loss function and fixing that actually

288
00:12:27,980 --> 00:12:31,399
made it quite a bit better so my

289
00:12:29,090 --> 00:12:32,930
motorbikes and cows and stuff we're

290
00:12:31,399 --> 00:12:34,159
actually in the right place

291
00:12:32,929 --> 00:12:37,159
so when you go back to that notebook

292
00:12:34,159 --> 00:12:40,579
you'll see it's a little less bad than

293
00:12:37,159 --> 00:12:45,679
it was okay

294
00:12:40,580 --> 00:12:51,080
so there's some there's some quick

295
00:12:45,679 --> 00:12:52,939
coverage of what's gone before yes quick

296
00:12:51,080 --> 00:12:56,600
question are you gonna put the

297
00:12:52,940 --> 00:13:00,350
PowerPoint on github I'll put a subset

298
00:12:56,600 --> 00:13:02,060
of it on okay and then secondly usually

299
00:13:00,350 --> 00:13:04,279
when we down sample we increase the

300
00:13:02,059 --> 00:13:07,429
number of filters or depth when we're

301
00:13:04,279 --> 00:13:10,269
doing sampling from 77 to 44 why are we

302
00:13:07,429 --> 00:13:13,639
decreasing the number from 512 to 256

303
00:13:10,269 --> 00:13:18,639
why not decrease dimension and SSD head

304
00:13:13,639 --> 00:13:18,639
is it performance-related 77

305
00:13:20,339 --> 00:13:33,189
well seven by seven two four by four I

306
00:13:23,889 --> 00:13:35,889
guess they've got star it's it's it's

307
00:13:33,188 --> 00:13:37,899
because well my treats because that's

308
00:13:35,889 --> 00:13:40,928
kind of what the papers tend to do is is

309
00:13:37,899 --> 00:13:43,600
we've got a number of well we have a

310
00:13:40,928 --> 00:13:46,418
number of our paths and we kind of want

311
00:13:43,600 --> 00:13:47,819
each one to be the same so we don't want

312
00:13:46,418 --> 00:13:53,078
each one to have a different number of

313
00:13:47,818 --> 00:13:54,969
filters and also this is what the the

314
00:13:53,078 --> 00:13:57,849
papers did so I was trying to match up

315
00:13:54,970 --> 00:14:00,040
with that and having these 256 it's it's

316
00:13:57,850 --> 00:14:02,889
a different concept because we're taking

317
00:14:00,039 --> 00:14:06,248
advantage of not just the last layer but

318
00:14:02,889 --> 00:14:10,589
the layers before that as well life's

319
00:14:06,249 --> 00:14:15,129
easier if we make them more consistent

320
00:14:10,589 --> 00:14:18,249
okay so we're now going to move to NLP

321
00:14:15,129 --> 00:14:24,369
and so let me kind of lay out where

322
00:14:18,249 --> 00:14:26,769
we're going here we we've seen a couple

323
00:14:24,369 --> 00:14:28,749
of times now this idea of lab taking a

324
00:14:26,769 --> 00:14:30,480
pre trained model in fact we've seen it

325
00:14:28,749 --> 00:14:33,308
in every lesson take a pre trained model

326
00:14:30,480 --> 00:14:35,168
whip off some stuff on the top replace

327
00:14:33,308 --> 00:14:42,578
it with some new stuff get it to do

328
00:14:35,168 --> 00:14:44,739
something similar right and so what

329
00:14:42,578 --> 00:14:48,039
we're going to do so and so we've kind

330
00:14:44,739 --> 00:14:52,179
of dived in a little bit deeper to that

331
00:14:48,039 --> 00:14:55,629
to say like okay with convolute a dot

332
00:14:52,178 --> 00:14:57,488
pre-trained it had like a standard way

333
00:14:55,629 --> 00:14:59,639
of like sticking stuff on the top which

334
00:14:57,489 --> 00:15:02,048
does a particular thing which with some

335
00:14:59,639 --> 00:15:05,019
classification okay and then we learn

336
00:15:02,048 --> 00:15:07,838
actually we have we can stick any PI

337
00:15:05,019 --> 00:15:09,428
torch module we like on the end and have

338
00:15:07,839 --> 00:15:14,579
it do anything we like with a in a

339
00:15:09,428 --> 00:15:16,720
custom here and so suddenly you discover

340
00:15:14,578 --> 00:15:21,899
wow there's there's some really

341
00:15:16,720 --> 00:15:25,470
interesting things we can do in fact

342
00:15:21,899 --> 00:15:25,470
that reminds me

343
00:15:32,360 --> 00:15:39,589
reminds me young Lou said well what if

344
00:15:38,089 --> 00:15:41,989
we did a different kind of custom in

345
00:15:39,589 --> 00:15:44,540
here and so the different custom hit was

346
00:15:41,989 --> 00:15:48,199
well let's take the original pictures

347
00:15:44,539 --> 00:15:51,558
and rotate them and then make our

348
00:15:48,198 --> 00:15:53,568
dependent variable the op you know the

349
00:15:51,558 --> 00:15:57,558
opposite of that rotation basically and

350
00:15:53,568 --> 00:15:59,328
see if it can learn to unrotated and

351
00:15:57,558 --> 00:16:01,850
this is like a super useful thing

352
00:15:59,328 --> 00:16:04,399
obviously in fact I think Google photos

353
00:16:01,850 --> 00:16:06,829
nowadays has this option that it'll

354
00:16:04,399 --> 00:16:09,948
actually automatically rotate your

355
00:16:06,828 --> 00:16:13,399
photos for you but the cool thing is as

356
00:16:09,948 --> 00:16:15,649
young lord shows here you can build that

357
00:16:13,399 --> 00:16:17,448
Network right now by doing exactly the

358
00:16:15,649 --> 00:16:20,989
same as our previous lesson but your

359
00:16:17,448 --> 00:16:22,548
custom had is one that spits out a

360
00:16:20,989 --> 00:16:25,879
single number which is how much to

361
00:16:22,548 --> 00:16:28,188
rotate by and your data set has a

362
00:16:25,879 --> 00:16:30,709
dependent variable which is how much did

363
00:16:28,188 --> 00:16:33,048
you rotate though yeah so like you

364
00:16:30,708 --> 00:16:36,798
suddenly realized with this idea of a

365
00:16:33,048 --> 00:16:41,989
backbone plus a custom head you can do

366
00:16:36,798 --> 00:16:43,188
almost anything you can think about so

367
00:16:41,989 --> 00:16:45,410
today we're going to look at the same

368
00:16:43,188 --> 00:16:50,238
idea and say like okay well how does

369
00:16:45,409 --> 00:16:53,118
that apply to NLP and then in the next

370
00:16:50,239 --> 00:16:57,439
lesson we're going to go further and say

371
00:16:53,119 --> 00:17:00,079
like well if NLP in computer vision kind

372
00:16:57,438 --> 00:17:02,269
of lets you do the same basic ideas how

373
00:17:00,078 --> 00:17:04,638
do we combine the two and we're going to

374
00:17:02,269 --> 00:17:09,288
learn about a model that can actually

375
00:17:04,638 --> 00:17:13,548
learn to find word structures from

376
00:17:09,288 --> 00:17:16,638
images or images from word structures or

377
00:17:13,548 --> 00:17:19,158
images from images and that will form

378
00:17:16,638 --> 00:17:21,528
the basis if you wanted to go further of

379
00:17:19,159 --> 00:17:23,539
doing things like going from an image to

380
00:17:21,528 --> 00:17:26,720
a sentence that's called image

381
00:17:23,538 --> 00:17:28,970
captioning for going from a sentence to

382
00:17:26,720 --> 00:17:35,089
an image which will kind of have to do a

383
00:17:28,970 --> 00:17:37,639
phrased image and so from there you know

384
00:17:35,089 --> 00:17:39,240
we've got to go deeper then into

385
00:17:37,638 --> 00:17:40,889
computer vision to think

386
00:17:39,240 --> 00:17:43,410
like okay what other kinds of things we

387
00:17:40,890 --> 00:17:45,720
can do with this idea of a pre-trained

388
00:17:43,410 --> 00:17:46,950
network plus a custom head and so we'll

389
00:17:45,720 --> 00:17:49,350
look at various kinds of image

390
00:17:46,950 --> 00:17:52,170
enhancement like increasing the

391
00:17:49,349 --> 00:17:55,949
resolution of a low-res photo to guess

392
00:17:52,170 --> 00:17:58,800
what was missing or adding artistic

393
00:17:55,950 --> 00:18:01,910
filters on top of photos or changing

394
00:17:58,799 --> 00:18:05,009
photos of horses into photos of zebras

395
00:18:01,910 --> 00:18:06,800
and stuff like that and then finally

396
00:18:05,009 --> 00:18:11,910
that's gonna bring us all the way back

397
00:18:06,799 --> 00:18:13,079
to bounding boxes again and so to get

398
00:18:11,910 --> 00:18:15,000
there we've got a first of all that

399
00:18:13,079 --> 00:18:16,558
about segmentation which is not just

400
00:18:15,000 --> 00:18:18,750
learnt figuring out where a bounding

401
00:18:16,558 --> 00:18:21,539
boxes but figuring out what every single

402
00:18:18,750 --> 00:18:23,910
pixel in an image is part of so this

403
00:18:21,539 --> 00:18:26,099
pixel is part of a person this pixel is

404
00:18:23,910 --> 00:18:28,500
part of a car and then we're going to

405
00:18:26,099 --> 00:18:31,259
use that idea particularly an idea court

406
00:18:28,500 --> 00:18:33,420
unit it turns out that this idea from

407
00:18:31,259 --> 00:18:35,579
unit we can apply to bounding boxes

408
00:18:33,420 --> 00:18:37,320
where it's called feature pyramids

409
00:18:35,579 --> 00:18:40,369
everything has to have a different name

410
00:18:37,319 --> 00:18:43,169
in every slightly different area and

411
00:18:40,369 --> 00:18:45,808
we'll use that to hopefully get some

412
00:18:43,170 --> 00:18:48,300
better better results and really good

413
00:18:45,808 --> 00:18:51,869
results actually it bounding boxes so

414
00:18:48,299 --> 00:18:52,919
that's kind of our path from here um so

415
00:18:51,869 --> 00:18:54,808
it's all gonna kind of build on each

416
00:18:52,920 --> 00:19:01,110
other that take us into lots of

417
00:18:54,808 --> 00:19:04,200
different areas now for NLP last part we

418
00:19:01,109 --> 00:19:07,609
relied on a pretty great library called

419
00:19:04,200 --> 00:19:11,160
torch text but as pretty great as it was

420
00:19:07,609 --> 00:19:16,019
I've since then found the limitations of

421
00:19:11,160 --> 00:19:18,450
it too problematic to keep using it as a

422
00:19:16,019 --> 00:19:20,910
lot of you complained on the forums it's

423
00:19:18,450 --> 00:19:22,799
pretty damn slow partly that that's

424
00:19:20,910 --> 00:19:26,370
because it's doing parallel not doing

425
00:19:22,799 --> 00:19:29,490
parallel processing and partly it's

426
00:19:26,369 --> 00:19:31,259
because it doesn't remember what you did

427
00:19:29,490 --> 00:19:35,880
last time and it does it all over again

428
00:19:31,259 --> 00:19:38,308
from scratch and then it's kind of hard

429
00:19:35,880 --> 00:19:39,420
to do fairly simple things like a lot of

430
00:19:38,308 --> 00:19:41,789
you were trying to get into the toxic

431
00:19:39,420 --> 00:19:44,789
comment competition on cattle which was

432
00:19:41,789 --> 00:19:47,039
a multi-label problem and trying to do

433
00:19:44,789 --> 00:19:49,069
that with torch text I eventually got it

434
00:19:47,039 --> 00:19:51,119
working but it took me like a week of

435
00:19:49,069 --> 00:19:53,970
hacking away

436
00:19:51,119 --> 00:19:55,409
which is kind of ridiculous so to fix

437
00:19:53,970 --> 00:19:57,870
all these problems we've created a new

438
00:19:55,410 --> 00:20:01,500
library called fast AI blood test

439
00:19:57,869 --> 00:20:04,739
fast AI dot text is a replacement for

440
00:20:01,500 --> 00:20:08,339
the combination of torch text and fast

441
00:20:04,740 --> 00:20:11,150
AI NLP yeah so don't use fast I don't

442
00:20:08,339 --> 00:20:14,819
NLP anymore okay that's like that's

443
00:20:11,150 --> 00:20:17,340
obsolete it's it's slower it's more

444
00:20:14,819 --> 00:20:20,399
confusing it's less good in every way

445
00:20:17,339 --> 00:20:22,619
right but there's a lot of overlaps okay

446
00:20:20,400 --> 00:20:24,330
like intentionally a lot of the classes

447
00:20:22,619 --> 00:20:26,549
have the same names all of the functions

448
00:20:24,329 --> 00:20:33,899
have the same names but this is the non

449
00:20:26,549 --> 00:20:36,899
torch text okay okay so we're going to

450
00:20:33,900 --> 00:20:38,550
work with IMDB again alright so for

451
00:20:36,900 --> 00:20:41,540
those of you have forgotten go back and

452
00:20:38,549 --> 00:20:41,539
check out lesson 4

453
00:20:41,660 --> 00:20:46,950
that basically this is a data set of

454
00:20:44,250 --> 00:20:49,109
movie reviews and you remember we used

455
00:20:46,950 --> 00:20:51,090
it to find out whether we might enjoy

456
00:20:49,109 --> 00:20:55,189
some be getting or not

457
00:20:51,089 --> 00:20:55,189
and we thought probably my kind of thing

458
00:20:55,519 --> 00:21:02,160
so we're going to use the same data set

459
00:20:58,140 --> 00:21:04,830
and by default it it calls itself a co

460
00:21:02,160 --> 00:21:10,440
IMDB so this is just that the raw data

461
00:21:04,829 --> 00:21:13,109
set that you can download and as you can

462
00:21:10,440 --> 00:21:15,420
see I'm doing from fast AI dot text

463
00:21:13,109 --> 00:21:19,099
import star there's no torch text and

464
00:21:15,420 --> 00:21:19,100
I'm not using faster I don't an LP

465
00:21:20,240 --> 00:21:24,329
I'm going to use path Lea but as per

466
00:21:22,680 --> 00:21:28,470
usual we're going to learn about what

467
00:21:24,329 --> 00:21:33,480
these tags are later so you might

468
00:21:28,470 --> 00:21:37,650
remember the basic paths for NLP is that

469
00:21:33,480 --> 00:21:41,309
we have to take sentences and turn them

470
00:21:37,650 --> 00:21:46,250
into numbers and there's a couple of

471
00:21:41,309 --> 00:21:50,129
steps to get there okay so at the moment

472
00:21:46,250 --> 00:21:52,440
somewhat intentionally fast AI dot text

473
00:21:50,130 --> 00:21:55,640
doesn't provide that many helper

474
00:21:52,440 --> 00:21:58,019
functions it's really designed more to

475
00:21:55,640 --> 00:22:00,420
let you handle things in a fairly

476
00:21:58,019 --> 00:22:03,200
flexible way right so as you can see

477
00:22:00,420 --> 00:22:04,909
here I wrote something called get texts

478
00:22:03,200 --> 00:22:08,778
which goes

479
00:22:04,909 --> 00:22:10,999
each thing in classes and these are the

480
00:22:08,778 --> 00:22:13,999
three classes that they have in IMDB

481
00:22:10,999 --> 00:22:16,069
negative positive and then there's

482
00:22:13,999 --> 00:22:17,389
another folder unsupervised that stuff

483
00:22:16,069 --> 00:22:19,069
they haven't gotten around to labeling

484
00:22:17,388 --> 00:22:22,398
yet so I'm just going to call that class

485
00:22:19,069 --> 00:22:26,210
now and so I just go through each one of

486
00:22:22,398 --> 00:22:30,288
those classes and then I just find every

487
00:22:26,210 --> 00:22:32,538
file in that folder with that name and I

488
00:22:30,288 --> 00:22:34,700
open it up and read it and check it into

489
00:22:32,538 --> 00:22:37,339
the end of this all right

490
00:22:34,700 --> 00:22:40,639
okay and as you can see with path lab

491
00:22:37,339 --> 00:22:42,978
it's super easy to grab stuff and pull

492
00:22:40,638 --> 00:22:48,319
it in and then the label is just

493
00:22:42,979 --> 00:22:49,999
whatever class or not - so right so I'll

494
00:22:48,319 --> 00:22:54,259
go ahead and I'll do that for the Train

495
00:22:49,999 --> 00:22:56,989
bit and I'll do that for the test fit so

496
00:22:54,259 --> 00:22:59,298
there's 70,000 and Tre and 25,000 in

497
00:22:56,989 --> 00:23:00,950
test talk about 50,000 if the Train ones

498
00:22:59,298 --> 00:23:02,269
are unsupervised we won't actually be

499
00:23:00,950 --> 00:23:05,210
able to use them when we get to the

500
00:23:02,269 --> 00:23:08,778
classification piece okay so I actually

501
00:23:05,210 --> 00:23:11,690
find this much easier than the kind of

502
00:23:08,778 --> 00:23:13,159
torch text approach of having lots of

503
00:23:11,690 --> 00:23:15,830
layers and wrappers and stuff because in

504
00:23:13,159 --> 00:23:18,700
the end reading text files is not it's

505
00:23:15,829 --> 00:23:18,699
not that hard okay

506
00:23:19,940 --> 00:23:25,479
one thing that's always good idea is to

507
00:23:21,950 --> 00:23:25,479
sort things randomly

508
00:23:26,769 --> 00:23:31,009
it's useful to know this simple trick

509
00:23:29,089 --> 00:23:32,238
for sorting things randomly particularly

510
00:23:31,009 --> 00:23:33,649
when you've got multiple things you have

511
00:23:32,239 --> 00:23:36,829
to sort the same way in this case you've

512
00:23:33,648 --> 00:23:40,368
got labels and texts NP dot random block

513
00:23:36,829 --> 00:23:44,089
permutation if you give it an integer it

514
00:23:40,368 --> 00:23:45,858
gives you a random list from 0 up to and

515
00:23:44,089 --> 00:23:49,519
not including the number you give it in

516
00:23:45,858 --> 00:23:53,598
some random order and so you can then

517
00:23:49,519 --> 00:23:56,118
just pass that in as a indexer right to

518
00:23:53,598 --> 00:23:57,858
give you a list that's sorted in that

519
00:23:56,118 --> 00:24:01,038
random order so in this case it's going

520
00:23:57,858 --> 00:24:04,519
to sort train texts and train labels in

521
00:24:01,038 --> 00:24:08,479
the same random way okay so that's a

522
00:24:04,519 --> 00:24:11,509
useful little idiom to use so now I've

523
00:24:08,479 --> 00:24:13,190
got my texts and my labels sorted I can

524
00:24:11,509 --> 00:24:15,979
go ahead and create a data frame from

525
00:24:13,190 --> 00:24:18,779
them why am I doing this well the reason

526
00:24:15,979 --> 00:24:23,700
I'm doing this is because there is

527
00:24:18,779 --> 00:24:28,609
a somewhat standard approach starting to

528
00:24:23,700 --> 00:24:31,680
appear for text classification datasets

529
00:24:28,609 --> 00:24:38,159
which is to have your training set as a

530
00:24:31,680 --> 00:24:43,470
CSV file with the with the labels first

531
00:24:38,160 --> 00:24:45,810
and the text of the NLP documents second

532
00:24:43,470 --> 00:24:48,480
you know trained on CSV and a test or

533
00:24:45,809 --> 00:24:51,029
CSB so it basically looks like this you

534
00:24:48,480 --> 00:24:54,029
have your labels and your texts and then

535
00:24:51,029 --> 00:24:57,990
a file called classes text which just

536
00:24:54,029 --> 00:25:01,109
lists the classes I think somewhat

537
00:24:57,990 --> 00:25:04,829
standard is just in a reasonably recent

538
00:25:01,109 --> 00:25:07,769
academic paper yarn lakorn and a team of

539
00:25:04,829 --> 00:25:09,659
researchers looked at quite a few data

540
00:25:07,769 --> 00:25:13,410
sets and they use this format for all of

541
00:25:09,660 --> 00:25:18,000
them and so that's what I've started

542
00:25:13,410 --> 00:25:20,430
using as well for my recent paper so

543
00:25:18,000 --> 00:25:25,259
what I've done is you'll find that this

544
00:25:20,430 --> 00:25:27,600
notebook if you put your data into this

545
00:25:25,259 --> 00:25:30,410
format the whole notebook will work

546
00:25:27,599 --> 00:25:33,629
every time and so far than having a

547
00:25:30,410 --> 00:25:35,700
thousand different classes or formats

548
00:25:33,630 --> 00:25:37,740
and readers and writers or whatever have

549
00:25:35,700 --> 00:25:39,960
you said let's just pick a standard

550
00:25:37,740 --> 00:25:41,250
format and your job your coders you can

551
00:25:39,960 --> 00:25:45,110
do it perfectly well is to put it in

552
00:25:41,250 --> 00:25:48,809
that format which is the CSV file okay

553
00:25:45,109 --> 00:25:53,009
the CSV files have no header okay

554
00:25:48,809 --> 00:25:55,200
by default all right now you'll notice

555
00:25:53,009 --> 00:25:57,109
at the start here that I had two

556
00:25:55,200 --> 00:25:59,880
different paths one was the

557
00:25:57,109 --> 00:26:03,000
classification path one was the language

558
00:25:59,880 --> 00:26:09,000
model of path in NLP you'll see LM all

559
00:26:03,000 --> 00:26:11,299
the time L M means language model so the

560
00:26:09,000 --> 00:26:14,490
classification path is going to contain

561
00:26:11,299 --> 00:26:17,250
the information that we're going to use

562
00:26:14,490 --> 00:26:19,349
to create a sentiment analysis model the

563
00:26:17,250 --> 00:26:20,819
language model path is going to contain

564
00:26:19,349 --> 00:26:22,649
the information we need to create a

565
00:26:20,819 --> 00:26:24,839
language model so they're a little bit

566
00:26:22,650 --> 00:26:29,580
different one thing that's different is

567
00:26:24,839 --> 00:26:32,369
that when we create the train dot CSV in

568
00:26:29,579 --> 00:26:35,569
the classification path we

569
00:26:32,369 --> 00:26:39,439
move everything that has a label of two

570
00:26:35,569 --> 00:26:41,879
because label of two is unsupervised

571
00:26:39,440 --> 00:26:46,920
okay so we remove the unsupervised data

572
00:26:41,880 --> 00:26:48,030
from the classifier we cannot use it so

573
00:26:46,920 --> 00:26:51,480
that means that this is going to have

574
00:26:48,029 --> 00:26:53,509
actually 25,000 positive 25,000 negative

575
00:26:51,480 --> 00:26:56,730
and the second difference is the labels

576
00:26:53,509 --> 00:26:59,819
for the classification path the labels

577
00:26:56,730 --> 00:27:02,880
are the actual labels but for the

578
00:26:59,819 --> 00:27:04,799
language model there are no labels so we

579
00:27:02,880 --> 00:27:06,540
just use a bunch of zeros and that just

580
00:27:04,799 --> 00:27:09,329
makes it a little bit easier because we

581
00:27:06,539 --> 00:27:13,289
can use a consistent data frame format

582
00:27:09,329 --> 00:27:18,000
or CSV format okay now the language

583
00:27:13,289 --> 00:27:19,710
model we can create our own validation

584
00:27:18,000 --> 00:27:22,920
set and so you've probably come across

585
00:27:19,710 --> 00:27:24,840
by now SK learn but model selection dot

586
00:27:22,920 --> 00:27:28,050
train test split which is a really

587
00:27:24,839 --> 00:27:30,149
simple little function that grabs a data

588
00:27:28,049 --> 00:27:31,700
set and randomly splits it into a

589
00:27:30,150 --> 00:27:34,080
training set and the validation set

590
00:27:31,700 --> 00:27:36,090
according to whatever proportion you

591
00:27:34,079 --> 00:27:39,210
specify and so in this case I

592
00:27:36,089 --> 00:27:41,279
concatenate my classification training

593
00:27:39,210 --> 00:27:43,380
and validation together so that's got to

594
00:27:41,279 --> 00:27:45,089
be a hundred thousand altogether split

595
00:27:43,380 --> 00:27:47,850
it by ten percent and so now I've got

596
00:27:45,089 --> 00:27:51,209
90,000 training ten thousand validation

597
00:27:47,849 --> 00:27:52,049
for my language model so I'll go ahead

598
00:27:51,210 --> 00:27:56,630
and save that

599
00:27:52,049 --> 00:27:59,220
so that's my basic you know get the data

600
00:27:56,630 --> 00:28:04,860
in a standard format for my language

601
00:27:59,220 --> 00:28:09,420
model in my classifier right so the next

602
00:28:04,859 --> 00:28:12,178
thing we need to do is tokenization so

603
00:28:09,420 --> 00:28:13,950
tokenization means at this stage we've

604
00:28:12,179 --> 00:28:16,530
got for a document for a movie review

605
00:28:13,950 --> 00:28:18,000
we've got a big long string and we

606
00:28:16,529 --> 00:28:22,190
wanted to pretend it into a list of

607
00:28:18,000 --> 00:28:26,579
tokens which are kind of a list of words

608
00:28:22,190 --> 00:28:29,670
but not quite right for example don't we

609
00:28:26,579 --> 00:28:33,419
want to be door hmm you probably want

610
00:28:29,670 --> 00:28:36,000
full stop to be a token and so forth

611
00:28:33,420 --> 00:28:39,058
right so tokenization is something that

612
00:28:36,000 --> 00:28:42,269
we passed off to a terrific library

613
00:28:39,058 --> 00:28:44,849
called spacey partly terrific because in

614
00:28:42,269 --> 00:28:45,460
australian wrote it and partly terrific

615
00:28:44,849 --> 00:28:50,619
because it's

616
00:28:45,460 --> 00:28:52,720
good at what it does we put a bit of

617
00:28:50,619 --> 00:28:54,569
stuff on top of Spacey but the vast

618
00:28:52,720 --> 00:28:58,720
majority the works being done by Spacey

619
00:28:54,569 --> 00:29:01,509
before we pass it to Spacey I've written

620
00:28:58,720 --> 00:29:04,420
this simple fix-up function which is

621
00:29:01,509 --> 00:29:05,650
basically each time I don't put a

622
00:29:04,420 --> 00:29:07,990
different data set and I've looked at

623
00:29:05,650 --> 00:29:11,350
about a dozen in building this everyone

624
00:29:07,990 --> 00:29:15,880
had different weird things that need to

625
00:29:11,349 --> 00:29:19,779
be replaced so here all the ones I've

626
00:29:15,880 --> 00:29:23,650
come up with so far hopefully this will

627
00:29:19,779 --> 00:29:26,410
help you out as well so I HTML and

628
00:29:23,650 --> 00:29:28,810
escape all the entities and then there's

629
00:29:26,410 --> 00:29:30,880
a bunch more things I replace have a

630
00:29:28,809 --> 00:29:32,349
look at the result of running this on

631
00:29:30,880 --> 00:29:35,380
text that you put in and make sure

632
00:29:32,349 --> 00:29:37,179
there's not more weird tokens in there

633
00:29:35,380 --> 00:29:40,050
it's amazing how many weird things

634
00:29:37,180 --> 00:29:40,049
people do to test

635
00:29:40,200 --> 00:29:45,880
so basically I've got this function

636
00:29:42,220 --> 00:29:49,630
called get all which is going to go

637
00:29:45,880 --> 00:29:51,250
ahead and call get texts and texts is

638
00:29:49,630 --> 00:29:54,100
going to go ahead and do a few things

639
00:29:51,250 --> 00:29:57,970
one of which is to apply that fix up

640
00:29:54,099 --> 00:29:59,049
that we just mentioned so let's kind of

641
00:29:57,970 --> 00:30:01,180
look through this because there's some

642
00:29:59,049 --> 00:30:04,960
interesting things to point out so I got

643
00:30:01,180 --> 00:30:07,870
to use pandas to open our train CSV from

644
00:30:04,960 --> 00:30:09,220
the language model path but I'm passing

645
00:30:07,869 --> 00:30:15,369
in an extra parameter you may not have

646
00:30:09,220 --> 00:30:16,900
seen before called chat sighs Python and

647
00:30:15,369 --> 00:30:20,739
pandas can both be pretty inefficient

648
00:30:16,900 --> 00:30:26,530
when it comes to storing and using text

649
00:30:20,740 --> 00:30:30,630
data and so you'll see that very few

650
00:30:26,529 --> 00:30:34,329
people in NLP are working with large

651
00:30:30,630 --> 00:30:36,730
corpuses and I think part of the reason

652
00:30:34,329 --> 00:30:37,899
is that traditional tools have just made

653
00:30:36,730 --> 00:30:43,150
it really difficult you ran out of

654
00:30:37,900 --> 00:30:44,880
memory all the time so this process I'm

655
00:30:43,150 --> 00:30:47,519
showing you today I have used on

656
00:30:44,880 --> 00:30:50,170
corpuses of over a billion words

657
00:30:47,519 --> 00:30:52,359
successfully using this exact code right

658
00:30:50,170 --> 00:30:54,130
and so one of the simple tricks is to

659
00:30:52,359 --> 00:30:56,740
use this thing called Chuck size with

660
00:30:54,130 --> 00:30:59,620
pandas what that means is that pandas to

661
00:30:56,740 --> 00:31:02,730
not return a data frame but it returns

662
00:30:59,619 --> 00:31:06,869
an iterator that we can iterate through

663
00:31:02,730 --> 00:31:13,509
chunks of a data frame and so that's why

664
00:31:06,869 --> 00:31:15,879
I don't say truck train equals get texts

665
00:31:13,509 --> 00:31:18,220
because here is a finger texts but

666
00:31:15,880 --> 00:31:20,920
instead I call get all which loops

667
00:31:18,220 --> 00:31:22,509
through the data frame but actually what

668
00:31:20,920 --> 00:31:25,120
it's really doing is its looping through

669
00:31:22,509 --> 00:31:27,940
chunks with the data frame so each of

670
00:31:25,119 --> 00:31:31,899
those chunks is basically a data frame

671
00:31:27,940 --> 00:31:34,360
representing a subset of the data when

672
00:31:31,900 --> 00:31:36,580
I'm working with NLP data many times I

673
00:31:34,359 --> 00:31:37,509
come across data with foreign text or

674
00:31:36,579 --> 00:31:39,549
characters

675
00:31:37,509 --> 00:31:42,220
is it better to discard them or keep

676
00:31:39,549 --> 00:31:45,308
them no no definitely keep them and this

677
00:31:42,220 --> 00:31:48,700
whole process is is Unicode and I've

678
00:31:45,308 --> 00:31:50,678
actually used this on Chinese text this

679
00:31:48,700 --> 00:31:56,080
is designed to work on pretty much

680
00:31:50,679 --> 00:31:58,360
anything yeah yeah in general most of

681
00:31:56,079 --> 00:32:01,960
the time it's not a good idea to remove

682
00:31:58,359 --> 00:32:03,189
anything like old-fashioned NLP

683
00:32:01,960 --> 00:32:05,440
approaches tend to do all this like

684
00:32:03,190 --> 00:32:07,360
limit ization and all these kind of

685
00:32:05,440 --> 00:32:08,980
normalization steps to kind of get rid

686
00:32:07,359 --> 00:32:10,899
of you know that lower case everything

687
00:32:08,980 --> 00:32:13,870
blah blah blah but you know that's

688
00:32:10,900 --> 00:32:15,220
throwing away information which you

689
00:32:13,869 --> 00:32:16,989
don't know ahead of time whether it's

690
00:32:15,220 --> 00:32:21,730
useful or not so don't throw away

691
00:32:16,990 --> 00:32:23,740
information ok so we go through each

692
00:32:21,730 --> 00:32:27,130
chunk each of which is a data frame and

693
00:32:23,740 --> 00:32:29,910
we call get texts get texts is going to

694
00:32:27,130 --> 00:32:36,900
grab the labels and makes them into ants

695
00:32:29,910 --> 00:32:38,980
it's going to grab then the the texts

696
00:32:36,900 --> 00:32:41,920
and I'll point out a couple of things

697
00:32:38,980 --> 00:32:44,230
the first is that before we include the

698
00:32:41,920 --> 00:32:47,170
text we have this beginning of stream

699
00:32:44,230 --> 00:32:50,319
token which you might remember we used

700
00:32:47,170 --> 00:32:52,179
way back up here there's nothing special

701
00:32:50,319 --> 00:32:53,589
about these particular strings of

702
00:32:52,179 --> 00:32:56,769
letters they're just once I figured

703
00:32:53,589 --> 00:32:59,799
don't appear in normal text it's very

704
00:32:56,769 --> 00:33:03,339
often so every text is going to start

705
00:32:59,799 --> 00:33:05,980
with X POS now why is that because it's

706
00:33:03,339 --> 00:33:09,069
often really useful for your model to

707
00:33:05,980 --> 00:33:10,118
know when a new text is starting for

708
00:33:09,069 --> 00:33:12,368
example if it's a language

709
00:33:10,118 --> 00:33:14,499
model right we are going to concatenate

710
00:33:12,368 --> 00:33:15,878
all the text together and so it'd be

711
00:33:14,499 --> 00:33:17,798
really helpful for it to know all this

712
00:33:15,878 --> 00:33:20,438
articles finished and a new one started

713
00:33:17,798 --> 00:33:26,229
so I should probably like forget some of

714
00:33:20,439 --> 00:33:29,379
their context yeah Devo is quite often

715
00:33:26,229 --> 00:33:30,939
texts have multiple fields like a title

716
00:33:29,378 --> 00:33:33,398
and abstract and then the main document

717
00:33:30,939 --> 00:33:35,499
and so by the same token I've got this

718
00:33:33,398 --> 00:33:39,098
thing here which alerts us actually have

719
00:33:35,499 --> 00:33:40,868
multiple fields in our CSV okay so this

720
00:33:39,098 --> 00:33:42,788
really this process is designed to be

721
00:33:40,868 --> 00:33:45,009
very flexible and again at the start of

722
00:33:42,788 --> 00:33:47,378
each one we put a special field starts

723
00:33:45,009 --> 00:33:49,899
here token followed by the number of the

724
00:33:47,378 --> 00:33:52,868
field that's starting here for as many

725
00:33:49,898 --> 00:33:55,418
fields as we have okay then we apply our

726
00:33:52,868 --> 00:33:59,978
fix up to it and then most importantly

727
00:33:55,419 --> 00:34:05,528
we tokenize it and we tokenize it by

728
00:33:59,979 --> 00:34:08,588
doing a process or multiprocessor multi

729
00:34:05,528 --> 00:34:11,739
processing I guess I should say and so

730
00:34:08,588 --> 00:34:13,748
tokenizing tends to be pretty slow but

731
00:34:11,739 --> 00:34:15,278
we've all got multiple cores and our

732
00:34:13,748 --> 00:34:17,949
machines now and some of the better

733
00:34:15,278 --> 00:34:20,528
machines on AWS and stuff can have

734
00:34:17,949 --> 00:34:27,428
dozens of course here on a university

735
00:34:20,528 --> 00:34:30,730
computer we've got 56 cause so Spacey is

736
00:34:27,429 --> 00:34:32,648
not very amenable to multi processing

737
00:34:30,730 --> 00:34:34,480
but I finally figured out how to get it

738
00:34:32,648 --> 00:34:36,940
to work and the good news is it's all

739
00:34:34,480 --> 00:34:38,798
wrapped up in this one function now and

740
00:34:36,940 --> 00:34:43,119
so all you need to pass to that function

741
00:34:38,798 --> 00:34:44,528
is a list of things to tokenize which

742
00:34:43,119 --> 00:34:47,649
each part of that list will be tokenized

743
00:34:44,528 --> 00:34:49,329
on a different core and so i've also

744
00:34:47,648 --> 00:34:52,568
created this function called partition

745
00:34:49,329 --> 00:34:54,519
by cause which takes a list and splits

746
00:34:52,568 --> 00:34:56,440
it into sub lists the number of sub

747
00:34:54,518 --> 00:35:00,939
lists is the number of cores that you

748
00:34:56,440 --> 00:35:04,358
have in your computer ok so on on my

749
00:35:00,940 --> 00:35:08,079
machine without more-- processing this

750
00:35:04,358 --> 00:35:09,818
takes about an hour and a half and with

751
00:35:08,079 --> 00:35:10,210
more processing it takes about two

752
00:35:09,818 --> 00:35:13,179
minutes

753
00:35:10,210 --> 00:35:15,759
alright so it's a really handy thing to

754
00:35:13,179 --> 00:35:17,469
have and now that this codes here you

755
00:35:15,759 --> 00:35:20,349
know feel free to look inside it and

756
00:35:17,469 --> 00:35:23,169
take advantage of it for your own stuff

757
00:35:20,349 --> 00:35:24,010
right remember we all have multiple

758
00:35:23,170 --> 00:35:26,440
processes

759
00:35:24,010 --> 00:35:28,570
protocol cause even in our laptops and

760
00:35:26,440 --> 00:35:31,329
very few things in place that and take

761
00:35:28,570 --> 00:35:35,349
advantage of it unless you make a bit of

762
00:35:31,329 --> 00:35:37,060
an effort to make it work so there's a

763
00:35:35,349 --> 00:35:39,640
couple of tricks to get things working

764
00:35:37,059 --> 00:35:43,029
quickly and reliably as it runs it

765
00:35:39,639 --> 00:35:46,059
prints out how it's going and so here's

766
00:35:43,030 --> 00:35:48,640
the result at the end right

767
00:35:46,059 --> 00:35:50,980
beginning of stream token beginning of

768
00:35:48,639 --> 00:35:54,029
field number one token here's the

769
00:35:50,980 --> 00:35:56,260
tokenized text you'll see that the

770
00:35:54,030 --> 00:36:01,839
punctuation is on the whole now a

771
00:35:56,260 --> 00:36:03,490
separate token you'll see there's a few

772
00:36:01,838 --> 00:36:08,469
interesting little things one is this

773
00:36:03,489 --> 00:36:10,449
what's this t up t up MGM for MGM

774
00:36:08,469 --> 00:36:14,709
obviously was originally capitalized

775
00:36:10,449 --> 00:36:16,449
right but the interesting thing is that

776
00:36:14,710 --> 00:36:19,269
normally people are the lowercase

777
00:36:16,449 --> 00:36:23,399
everything or they leave the case as is

778
00:36:19,269 --> 00:36:27,670
now if you leave the case as is then

779
00:36:23,400 --> 00:36:30,818
screw you or caps and screw you lower

780
00:36:27,670 --> 00:36:32,260
case are two totally different sets of

781
00:36:30,818 --> 00:36:35,039
tokens that have to be learned from

782
00:36:32,260 --> 00:36:36,970
scratch or if you love a case them all

783
00:36:35,039 --> 00:36:42,250
then there's no difference at all

784
00:36:36,969 --> 00:36:45,549
between screw or you and right so how do

785
00:36:42,250 --> 00:36:48,809
you fix this so that you both get a

786
00:36:45,550 --> 00:36:51,730
semantic impact of like I'm shouting now

787
00:36:48,809 --> 00:36:53,710
right but not have every single word

788
00:36:51,730 --> 00:36:56,409
have to learn the shouted version versus

789
00:36:53,710 --> 00:36:57,880
the normal version and so the idea I

790
00:36:56,409 --> 00:36:59,858
came up with and I'm sure other people

791
00:36:57,880 --> 00:37:02,680
have done this too is to come up with a

792
00:36:59,858 --> 00:37:06,670
unique token to mean the next thing is

793
00:37:02,679 --> 00:37:08,259
all uppercase so then I lowercase it so

794
00:37:06,670 --> 00:37:10,180
now whatever used to be up case it's now

795
00:37:08,260 --> 00:37:12,640
lowercase it's just one token and then

796
00:37:10,179 --> 00:37:15,219
we can learn the semantic meaning of all

797
00:37:12,639 --> 00:37:17,529
uppercase and so I've done a similar

798
00:37:15,219 --> 00:37:20,019
thing if you've got like 29 exclamation

799
00:37:17,530 --> 00:37:22,900
marks in a row we don't learn a separate

800
00:37:20,019 --> 00:37:25,300
token for 29 exclamation marks instead I

801
00:37:22,900 --> 00:37:27,849
put in a special token for the next

802
00:37:25,300 --> 00:37:30,190
thing repeats lots of times and then I

803
00:37:27,849 --> 00:37:32,410
put the number 29 and then I put the

804
00:37:30,190 --> 00:37:34,240
explanation mark okay and so there's a

805
00:37:32,409 --> 00:37:36,309
few little tricks like that and if

806
00:37:34,239 --> 00:37:36,889
you're interested in LP have a look at

807
00:37:36,309 --> 00:37:39,139
the code

808
00:37:36,889 --> 00:37:40,819
tokenizer for these little tricks that

809
00:37:39,139 --> 00:37:46,400
I've added in because some of them are

810
00:37:40,820 --> 00:37:48,740
kind of fun okay so the nice thing with

811
00:37:46,400 --> 00:37:52,579
doing things this way is we can now just

812
00:37:48,739 --> 00:37:55,009
NP dot save that and load it back up

813
00:37:52,579 --> 00:37:57,529
later like we don't have to recalculate

814
00:37:55,010 --> 00:37:59,450
all this stuff each time like we tend to

815
00:37:57,530 --> 00:38:01,060
have to do with with torch text or a lot

816
00:37:59,449 --> 00:38:03,489
of other libraries

817
00:38:01,059 --> 00:38:06,049
okay so we've now got it

818
00:38:03,489 --> 00:38:08,989
tokenized the next thing we need to do

819
00:38:06,050 --> 00:38:11,870
is to turn it into numbers which we call

820
00:38:08,989 --> 00:38:13,969
numerical Eisinger and the way we

821
00:38:11,869 --> 00:38:16,099
numeric lies it is very simple we make a

822
00:38:13,969 --> 00:38:18,230
list of all the words that appear in

823
00:38:16,099 --> 00:38:21,829
some order and then we replace every

824
00:38:18,230 --> 00:38:24,440
word with its index into that list the

825
00:38:21,829 --> 00:38:28,210
list of all the words that appear or all

826
00:38:24,440 --> 00:38:30,800
the tokens that we call the vocabulary

827
00:38:28,210 --> 00:38:33,170
so here's an example of some of the

828
00:38:30,800 --> 00:38:34,880
vocabulary the counter class in in

829
00:38:33,170 --> 00:38:38,690
plaque that is very handy for this it

830
00:38:34,880 --> 00:38:42,200
basically gives us a list of unique

831
00:38:38,690 --> 00:38:44,179
items and their counts okay so here are

832
00:38:42,199 --> 00:38:47,149
the 25 most common and things in the

833
00:38:44,179 --> 00:38:49,449
vocabulary you can see there are things

834
00:38:47,150 --> 00:38:52,190
like apostrophe s and double quote and

835
00:38:49,449 --> 00:38:55,849
end of paragraph and also stuff like

836
00:38:52,190 --> 00:38:59,000
that now generally speaking we don't

837
00:38:55,849 --> 00:39:01,519
want every unique token in our

838
00:38:59,000 --> 00:39:04,579
vocabulary if it doesn't appear at least

839
00:39:01,519 --> 00:39:06,530
two times then might just be a spelling

840
00:39:04,579 --> 00:39:07,909
mistake or a word I mean we can't learn

841
00:39:06,530 --> 00:39:11,000
anything about it if it doesn't appear

842
00:39:07,909 --> 00:39:12,230
that often also the stuff that we're

843
00:39:11,000 --> 00:39:15,739
going to be learning about at least so

844
00:39:12,230 --> 00:39:17,409
far in this part gets a bit clunky once

845
00:39:15,739 --> 00:39:21,019
you've got a vocabulary bigger than

846
00:39:17,409 --> 00:39:22,730
60-thousand time permitting we may look

847
00:39:21,019 --> 00:39:25,009
at some work I've been doing recently on

848
00:39:22,730 --> 00:39:26,719
handling larger vocabularies otherwise

849
00:39:25,010 --> 00:39:29,060
that might have to come in a in a future

850
00:39:26,719 --> 00:39:31,039
class okay but actually for

851
00:39:29,059 --> 00:39:32,630
classification I've discovered that

852
00:39:31,039 --> 00:39:34,880
doing more than about 60,000 words

853
00:39:32,630 --> 00:39:37,250
doesn't seem to help anyway so we're

854
00:39:34,880 --> 00:39:39,349
going to limit our vocabulary to 60,000

855
00:39:37,250 --> 00:39:41,829
words things that appear at least twice

856
00:39:39,349 --> 00:39:45,409
and so here's a simple way to do that

857
00:39:41,829 --> 00:39:48,019
use that dot most common pass in the

858
00:39:45,409 --> 00:39:50,108
mass vocab size that'll sort it by the

859
00:39:48,019 --> 00:39:53,139
frequency by the way

860
00:39:50,108 --> 00:39:55,179
and if it appears less often than a

861
00:39:53,139 --> 00:39:59,379
minimum frequency then don't bother at

862
00:39:55,179 --> 00:40:01,719
all okay so that gives us Ida s that's

863
00:39:59,380 --> 00:40:03,970
the the same name that torch text used

864
00:40:01,719 --> 00:40:07,598
remember it means that into string so

865
00:40:03,969 --> 00:40:10,239
this is just the list of tokens unique

866
00:40:07,599 --> 00:40:13,450
tokens in the vocab I'm going to insert

867
00:40:10,239 --> 00:40:16,479
two more tokens a token for unknown

868
00:40:13,449 --> 00:40:20,589
evoke a bite of your unknown and vocab

869
00:40:16,480 --> 00:40:23,170
item for padding okay then we can create

870
00:40:20,590 --> 00:40:26,519
the dictionary which goes in the

871
00:40:23,170 --> 00:40:29,230
opposite direction so string to end and

872
00:40:26,519 --> 00:40:31,269
that won't cover everything because we

873
00:40:29,230 --> 00:40:34,420
intentionally truncated it down to

874
00:40:31,269 --> 00:40:35,500
60,000 words right and so if we come

875
00:40:34,420 --> 00:40:37,210
across something that's not in the

876
00:40:35,500 --> 00:40:39,519
dictionary we want to replace it with

877
00:40:37,210 --> 00:40:42,490
zero for unknown and so we can use a

878
00:40:39,519 --> 00:40:46,539
default dick for that with a lambda

879
00:40:42,489 --> 00:40:48,009
function that always returns Europe okay

880
00:40:46,539 --> 00:40:49,539
so you can see all these things were

881
00:40:48,010 --> 00:40:52,900
using the kind of kick coming coming

882
00:40:49,539 --> 00:40:55,269
back up so now that we've got our s to a

883
00:40:52,900 --> 00:40:59,700
dictionary defined we can then just call

884
00:40:55,269 --> 00:41:02,170
that for every word for every sentence

885
00:40:59,699 --> 00:41:07,239
right and so there's an Americanized

886
00:41:02,170 --> 00:41:09,220
version and there it is okay and so of

887
00:41:07,239 --> 00:41:12,159
course the nice thing is again we can

888
00:41:09,219 --> 00:41:14,169
save that step as well that's so each

889
00:41:12,159 --> 00:41:16,929
time we get to another step we can save

890
00:41:14,170 --> 00:41:19,300
it and these are not very big files

891
00:41:16,929 --> 00:41:24,149
compared to what you used to with with

892
00:41:19,300 --> 00:41:28,720
images text is generally pretty small

893
00:41:24,150 --> 00:41:31,030
very important to also save that

894
00:41:28,719 --> 00:41:34,989
vocabulary but because the this this

895
00:41:31,030 --> 00:41:37,089
list of numbers means nothing unless you

896
00:41:34,989 --> 00:41:39,879
know what each number refers to and

897
00:41:37,088 --> 00:41:41,849
that's what I to restaurants you okay so

898
00:41:39,880 --> 00:41:46,900
you save those three things and then

899
00:41:41,849 --> 00:41:50,490
later on you can load them back up so

900
00:41:46,900 --> 00:41:55,119
now our vocab size is 60,000 and term

901
00:41:50,489 --> 00:42:01,339
and training language model has 90,000

902
00:41:55,119 --> 00:42:03,769
documents you know okay so

903
00:42:01,340 --> 00:42:05,240
that's the pre-processing you do we can

904
00:42:03,769 --> 00:42:07,070
probably rap a little bit more of that

905
00:42:05,239 --> 00:42:08,750
in little utility functions if we want

906
00:42:07,070 --> 00:42:11,990
to but it's all pretty straightforward

907
00:42:08,750 --> 00:42:14,059
and basically that exact code will work

908
00:42:11,989 --> 00:42:21,789
for any data set you have once you've

909
00:42:14,059 --> 00:42:25,699
got it in that CSV format so here is a

910
00:42:21,789 --> 00:42:31,969
kind of a new insight that's not new at

911
00:42:25,699 --> 00:42:34,659
all which is that we'd like to pre train

912
00:42:31,969 --> 00:42:39,289
something like we know from lesson four

913
00:42:34,659 --> 00:42:41,989
that if we pre train our classifier by

914
00:42:39,289 --> 00:42:44,630
first creating a language model and then

915
00:42:41,989 --> 00:42:46,009
fine-tuning that as a classifier that

916
00:42:44,630 --> 00:42:47,869
was helpful do you remember it actually

917
00:42:46,010 --> 00:42:50,930
got us a new start at the out result we

918
00:42:47,869 --> 00:42:54,880
got the best IMDb classifier result that

919
00:42:50,929 --> 00:42:57,409
had ever been published but quite a bit

920
00:42:54,880 --> 00:43:02,150
well we're not going that far enough

921
00:42:57,409 --> 00:43:08,599
though right because IMDB movie reviews

922
00:43:02,150 --> 00:43:11,750
are not that different to any other

923
00:43:08,599 --> 00:43:13,969
English document you know compared to

924
00:43:11,750 --> 00:43:18,190
how different they are to a random

925
00:43:13,969 --> 00:43:22,579
string or even to a Chinese document so

926
00:43:18,190 --> 00:43:25,220
just like imagenet allowed us to train

927
00:43:22,579 --> 00:43:27,230
things that recognized stuff that kind

928
00:43:25,219 --> 00:43:28,699
of looks like pictures and we could use

929
00:43:27,230 --> 00:43:30,699
it on stuff that was nothing to do with

930
00:43:28,699 --> 00:43:33,169
image net like satellite images

931
00:43:30,699 --> 00:43:35,899
why don't we train a language model

932
00:43:33,170 --> 00:43:39,349
that's just like good at English and

933
00:43:35,900 --> 00:43:44,809
then fine tune it to be good at major

934
00:43:39,349 --> 00:43:48,819
reviews so this basic insight Leadbeater

935
00:43:44,809 --> 00:43:54,590
try building a language model on

936
00:43:48,820 --> 00:43:58,160
Wikipedia so my friend Steven marady has

937
00:43:54,590 --> 00:44:01,519
already processed Wikipedia found a

938
00:43:58,159 --> 00:44:04,449
subset of nearly you know the most of it

939
00:44:01,519 --> 00:44:07,070
but throwing away the stupid little

940
00:44:04,449 --> 00:44:10,579
articles so most of the bigger articles

941
00:44:07,070 --> 00:44:13,519
and he calls that wiki text 103 so I

942
00:44:10,579 --> 00:44:15,019
grabbed wiki text 103 and I trained a

943
00:44:13,519 --> 00:44:16,849
language model honor

944
00:44:15,019 --> 00:44:18,469
and we and I used exactly the same

945
00:44:16,849 --> 00:44:21,500
approach I'm about to show you for

946
00:44:18,469 --> 00:44:23,659
trading an IMDB language model but

947
00:44:21,500 --> 00:44:28,280
instead I trained a wikitext 103

948
00:44:23,659 --> 00:44:30,409
language model and then I saved it and

949
00:44:28,280 --> 00:44:34,130
I've made it available for anybody who

950
00:44:30,409 --> 00:44:37,489
wants to use it at this URL so this is

951
00:44:34,130 --> 00:44:40,160
not a URL for wikitext 103 the documents

952
00:44:37,489 --> 00:44:43,489
this is the wiki text 103 the language

953
00:44:40,159 --> 00:44:48,279
model and so the idea now is let's train

954
00:44:43,489 --> 00:44:53,359
an IMDB language model which starts with

955
00:44:48,280 --> 00:44:57,350
these words there hopefully to you folks

956
00:44:53,360 --> 00:44:58,940
this is a extremely obvious extremely

957
00:44:57,349 --> 00:45:00,949
non-controversial idea because it's

958
00:44:58,940 --> 00:45:07,970
basically what we've done in nearly

959
00:45:00,949 --> 00:45:11,269
every class so far but when I first when

960
00:45:07,969 --> 00:45:14,839
I first mentioned this to two people in

961
00:45:11,269 --> 00:45:18,619
the NLP community I guess like June July

962
00:45:14,840 --> 00:45:20,780
of last year there couldn't have been

963
00:45:18,619 --> 00:45:22,909
less interest you know I asked on

964
00:45:20,780 --> 00:45:25,340
Twitter where a lot of the top thread of

965
00:45:22,909 --> 00:45:27,829
research is kind of people that I follow

966
00:45:25,340 --> 00:45:29,539
and they'll be back I was like hey what

967
00:45:27,829 --> 00:45:32,779
if we like pre-trained like a general

968
00:45:29,539 --> 00:45:35,509
language model like no old language is

969
00:45:32,780 --> 00:45:36,950
different you know you can do that or

970
00:45:35,510 --> 00:45:40,280
like I don't know why you would bother

971
00:45:36,949 --> 00:45:43,339
anyway I've talked to people at

972
00:45:40,280 --> 00:45:45,019
conferences and they're like pretty sure

973
00:45:43,340 --> 00:45:49,220
people have tried that and it stupid

974
00:45:45,019 --> 00:45:55,369
like there was just this like I don't

975
00:45:49,219 --> 00:45:59,719
know this weird like frank past and you

976
00:45:55,369 --> 00:46:02,239
know I guess because I am arrogant in

977
00:45:59,719 --> 00:46:04,189
obstreperous I ignored them even though

978
00:46:02,239 --> 00:46:07,309
they know much more about NLP than I do

979
00:46:04,190 --> 00:46:13,400
and just trade it anyway and let me show

980
00:46:07,309 --> 00:46:14,539
you what happened so here's how we do it

981
00:46:13,400 --> 00:46:17,990
right

982
00:46:14,539 --> 00:46:20,989
grab the wiki text models right and if

983
00:46:17,989 --> 00:46:22,879
you use double you get minus R it'll

984
00:46:20,989 --> 00:46:26,088
actually recursively grab the whole

985
00:46:22,880 --> 00:46:28,280
directory it's got a few things or not

986
00:46:26,088 --> 00:46:30,529
we need to make sure that our language

987
00:46:28,280 --> 00:46:33,019
model has exactly the same embedding

988
00:46:30,530 --> 00:46:35,990
size number of hidden and number of

989
00:46:33,019 --> 00:46:41,449
layers as my wiki text one did otherwise

990
00:46:35,989 --> 00:46:43,279
you can't load the way it's in here so

991
00:46:41,449 --> 00:46:45,199
here's our pre-trained path here's our

992
00:46:43,280 --> 00:46:48,890
preteen language model path let's go

993
00:46:45,199 --> 00:46:52,039
ahead and torch dot load in those

994
00:46:48,889 --> 00:46:56,389
weights from the forward wickety text

995
00:46:52,039 --> 00:46:58,369
103 model okay we don't normally use

996
00:46:56,389 --> 00:47:01,568
torch dot load but that's that's that's

997
00:46:58,369 --> 00:47:05,088
the you know the pi torch way of

998
00:47:01,568 --> 00:47:08,058
grabbing a file and it basically gives

999
00:47:05,088 --> 00:47:10,880
you a dictionary containing the name of

1000
00:47:08,059 --> 00:47:15,019
the layer and a tensor of those weights

1001
00:47:10,880 --> 00:47:17,809
or an array of those words now here's

1002
00:47:15,019 --> 00:47:19,940
the problem that wiki text language

1003
00:47:17,809 --> 00:47:22,670
model was built with a certain

1004
00:47:19,940 --> 00:47:25,159
vocabulary which was not the same as

1005
00:47:22,670 --> 00:47:28,400
this one was built on right so my number

1006
00:47:25,159 --> 00:47:31,129
40 was not the same as wiki text 103

1007
00:47:28,400 --> 00:47:31,789
models number 40 so we need to map one

1008
00:47:31,130 --> 00:47:34,940
to the other

1009
00:47:31,789 --> 00:47:39,108
okay that's very very simple because

1010
00:47:34,940 --> 00:47:41,889
luckily I saved the I to s for the wiki

1011
00:47:39,108 --> 00:47:46,219
text okay right so here's the list of

1012
00:47:41,889 --> 00:47:48,828
what each word is when I trained the

1013
00:47:46,219 --> 00:47:51,500
wiki text 103 model and so we can do the

1014
00:47:48,829 --> 00:47:53,930
same default dict trick to map it in

1015
00:47:51,500 --> 00:47:56,449
Reverse right and I'm going to use minus

1016
00:47:53,929 --> 00:47:58,639
1 to mean that it's not in the wiki text

1017
00:47:56,449 --> 00:48:02,298
dictionary and so now I can just say

1018
00:47:58,639 --> 00:48:06,558
okay my new set of weights is just a

1019
00:48:02,298 --> 00:48:08,239
whole bunch of zeros with vocab sighs by

1020
00:48:06,559 --> 00:48:10,819
embedding size so we're going to create

1021
00:48:08,239 --> 00:48:13,399
an embedding matrix I'm then going to go

1022
00:48:10,818 --> 00:48:17,179
through every one of the words in my

1023
00:48:13,400 --> 00:48:21,139
IMDB vocab room okay I am going to look

1024
00:48:17,179 --> 00:48:23,899
it up in s - I - so a string to int for

1025
00:48:21,139 --> 00:48:27,078
the wiki text 103 vocabulary and see if

1026
00:48:23,900 --> 00:48:29,088
it's words there okay and if that is

1027
00:48:27,079 --> 00:48:31,309
word there then I'm not going to get

1028
00:48:29,088 --> 00:48:33,048
this minus one right so I will be

1029
00:48:31,309 --> 00:48:35,030
greater than or equal to zero so in that

1030
00:48:33,048 --> 00:48:37,309
case I will just set that row of the

1031
00:48:35,030 --> 00:48:37,920
embedding matrix two to the weight that

1032
00:48:37,309 --> 00:48:43,200
I just looked

1033
00:48:37,920 --> 00:48:45,900
that which was stored inside this this

1034
00:48:43,199 --> 00:48:47,578
named element and so these these names

1035
00:48:45,900 --> 00:48:49,250
you can look at you can just look at

1036
00:48:47,579 --> 00:48:51,990
this dictionary and it's pretty obvious

1037
00:48:49,250 --> 00:48:53,699
what each name corresponds to it looks

1038
00:48:51,989 --> 00:48:55,739
very similar to the names that you gave

1039
00:48:53,699 --> 00:48:58,469
it when you set up your module so here

1040
00:48:55,739 --> 00:49:02,219
are the encoder weights

1041
00:48:58,469 --> 00:49:05,730
okay so I grabbed it from the encoder

1042
00:49:02,219 --> 00:49:08,939
weights if I don't find it then I will

1043
00:49:05,730 --> 00:49:12,510
use the row mean in other words here is

1044
00:49:08,940 --> 00:49:14,309
the average imbedding weight across all

1045
00:49:12,510 --> 00:49:17,849
of the wikitext what are three things

1046
00:49:14,309 --> 00:49:19,798
okay so that's pretty simple so I'm

1047
00:49:17,849 --> 00:49:22,740
going to end up with an embedding matrix

1048
00:49:19,798 --> 00:49:24,719
for every word that's in both my vocab

1049
00:49:22,739 --> 00:49:27,169
room for IMDB and the week in text 103

1050
00:49:24,719 --> 00:49:30,209
vocab I will use the wiki text 103

1051
00:49:27,170 --> 00:49:31,950
embedding matrix weights for anything

1052
00:49:30,210 --> 00:49:34,349
else I will just use whatever was the

1053
00:49:31,949 --> 00:49:36,480
average weight from the wiki text 103

1054
00:49:34,349 --> 00:49:38,280
embedding matrix okay and then I'll go

1055
00:49:36,480 --> 00:49:42,679
ahead and I will replace the encoder

1056
00:49:38,280 --> 00:49:44,730
weights with that turn into a tensor

1057
00:49:42,679 --> 00:49:46,798
we haven't talked much about weight

1058
00:49:44,730 --> 00:49:49,949
tying we might do so later but basically

1059
00:49:46,798 --> 00:49:54,690
the decoder so the thing that turns the

1060
00:49:49,949 --> 00:49:56,818
final prediction back into a word uses

1061
00:49:54,690 --> 00:50:00,119
exactly the same weights so I pop it

1062
00:49:56,818 --> 00:50:01,679
there as well and then there's a bit of

1063
00:50:00,119 --> 00:50:03,480
a weird thing with how we do embedding

1064
00:50:01,679 --> 00:50:05,519
dropout that ends up with a whole

1065
00:50:03,480 --> 00:50:07,380
separate copy of them for a reason that

1066
00:50:05,519 --> 00:50:09,210
doesn't matter much anyway so we just

1067
00:50:07,380 --> 00:50:12,630
popped the weights back where they need

1068
00:50:09,210 --> 00:50:15,960
to go so this is now something that a

1069
00:50:12,630 --> 00:50:19,858
dictionary we can now or a set of torch

1070
00:50:15,960 --> 00:50:22,818
state which we can load in so let's go

1071
00:50:19,858 --> 00:50:25,619
ahead and create our language model okay

1072
00:50:22,818 --> 00:50:27,000
and so the basic approach we're going to

1073
00:50:25,619 --> 00:50:28,289
use and I'm going to look at this in

1074
00:50:27,000 --> 00:50:30,929
more detail in a moment but the basic

1075
00:50:28,289 --> 00:50:36,088
approach are going to use is I'm going

1076
00:50:30,929 --> 00:50:41,489
to concatenate all of the documents

1077
00:50:36,088 --> 00:50:44,250
together into a single a single list of

1078
00:50:41,489 --> 00:50:47,759
tokens of length twenty four point nine

1079
00:50:44,250 --> 00:50:51,840
nine eight million okay so that's going

1080
00:50:47,760 --> 00:50:54,660
to be what I pass in as my trainings

1081
00:50:51,840 --> 00:50:56,160
so the language model right we basically

1082
00:50:54,659 --> 00:50:58,259
just take all our documents and just

1083
00:50:56,159 --> 00:50:59,639
concatenate them back to that okay and

1084
00:50:58,260 --> 00:51:02,220
we're going to be continuously trying to

1085
00:50:59,639 --> 00:51:04,230
predict what's the next word after these

1086
00:51:02,219 --> 00:51:06,809
words what's the next word after these

1087
00:51:04,230 --> 00:51:09,119
words have a look at these details in a

1088
00:51:06,809 --> 00:51:11,039
moment I'm going to set up a whole bunch

1089
00:51:09,119 --> 00:51:12,569
of drop out look at that in detail

1090
00:51:11,039 --> 00:51:15,389
moment

1091
00:51:12,570 --> 00:51:17,580
once we've got a model data object we

1092
00:51:15,389 --> 00:51:19,579
can then grab the model from it so

1093
00:51:17,579 --> 00:51:24,299
that's going to give us a learner okay

1094
00:51:19,579 --> 00:51:29,239
and then as per usual we can call lo not

1095
00:51:24,300 --> 00:51:32,760
fit so we first of all as per usual just

1096
00:51:29,239 --> 00:51:34,859
do a single epoch on the last layer just

1097
00:51:32,760 --> 00:51:36,990
to get that okay and the way I've set it

1098
00:51:34,860 --> 00:51:39,090
up is the last layer is actually the

1099
00:51:36,989 --> 00:51:40,289
embedding words because that's obviously

1100
00:51:39,090 --> 00:51:41,640
the thing that's going to be the most

1101
00:51:40,289 --> 00:51:44,099
wrong because like a lot of those

1102
00:51:41,639 --> 00:51:45,869
embedding weights didn't even exist in

1103
00:51:44,099 --> 00:51:47,940
the vocab so we're just gonna train a

1104
00:51:45,869 --> 00:51:51,239
single epoch of just the inventing

1105
00:51:47,940 --> 00:51:54,960
weights and then we'll start doing a few

1106
00:51:51,239 --> 00:51:59,009
epochs of the full model and so how is

1107
00:51:54,960 --> 00:52:02,940
that looking well here's lesson four

1108
00:51:59,010 --> 00:52:10,590
which was our academic world's best ever

1109
00:52:02,940 --> 00:52:15,960
result and after 14 he parks we had a

1110
00:52:10,590 --> 00:52:18,300
four point two three loss here after one

1111
00:52:15,960 --> 00:52:19,440
epoch we have a four point one two loss

1112
00:52:18,300 --> 00:52:24,180
right

1113
00:52:19,440 --> 00:52:25,380
so by pre-training on wikitext 103 in

1114
00:52:24,179 --> 00:52:27,419
fact let's go and have a look

1115
00:52:25,380 --> 00:52:28,890
we kept training and training at a

1116
00:52:27,420 --> 00:52:31,829
different rate eventually we got to four

1117
00:52:28,889 --> 00:52:34,349
point one six so by pre training on

1118
00:52:31,829 --> 00:52:37,920
wikitext 103 we have a better loss after

1119
00:52:34,349 --> 00:52:41,269
one Ã©poque than the best loss we got for

1120
00:52:37,920 --> 00:52:45,210
the language model otherwise yes Rachel

1121
00:52:41,269 --> 00:52:48,090
what is the wikitext 103 model is it a

1122
00:52:45,210 --> 00:52:51,090
double UDL STM again yeah and we're

1123
00:52:48,090 --> 00:52:54,120
about to dig into that it's the way I

1124
00:52:51,090 --> 00:52:57,360
trained it was literally the same lines

1125
00:52:54,119 --> 00:52:59,069
of code that you see here but without

1126
00:52:57,360 --> 00:53:03,090
pre-training it only takes one two three

1127
00:52:59,070 --> 00:53:05,880
okay so let's take a 10-minute break

1128
00:53:03,090 --> 00:53:07,670
come back at 7:40 and

1129
00:53:05,880 --> 00:53:12,170
dig in and have a look at these models

1130
00:53:07,670 --> 00:53:14,909
okay welcome back before we go back into

1131
00:53:12,170 --> 00:53:19,079
language models and NLP classifiers a

1132
00:53:14,909 --> 00:53:20,969
quick discussion about something pretty

1133
00:53:19,079 --> 00:53:23,789
new at the moment which is the faster I

1134
00:53:20,969 --> 00:53:26,609
dock project so the goal of a fast IO

1135
00:53:23,789 --> 00:53:30,210
dock project is to create documentation

1136
00:53:26,610 --> 00:53:32,430
that makes readers say wow that's the

1137
00:53:30,210 --> 00:53:35,970
most fantastic document documentation

1138
00:53:32,429 --> 00:53:38,849
I've ever read and so we have some

1139
00:53:35,969 --> 00:53:42,239
specific ideas about how to do that but

1140
00:53:38,849 --> 00:53:45,420
it's the same kind of idea of like top

1141
00:53:42,239 --> 00:53:48,209
down you know thoughtful take full

1142
00:53:45,420 --> 00:53:50,309
advantage of the medium approach you

1143
00:53:48,210 --> 00:53:54,269
know interactive experimental code first

1144
00:53:50,309 --> 00:53:56,369
that we're all familiar with if you're

1145
00:53:54,269 --> 00:54:00,840
interested in getting involved

1146
00:53:56,369 --> 00:54:03,989
the basic approach you can see in in the

1147
00:54:00,840 --> 00:54:06,870
docs directory so this is the this is

1148
00:54:03,989 --> 00:54:09,719
the readme in the docs directory

1149
00:54:06,869 --> 00:54:14,130
in there there is a amongst other things

1150
00:54:09,719 --> 00:54:17,459
a transforms template dot a doc what the

1151
00:54:14,130 --> 00:54:19,579
hell is a doc hey Doc is asciidoc how

1152
00:54:17,460 --> 00:54:24,630
many people here have come across sq doc

1153
00:54:19,579 --> 00:54:26,250
that's awesome ASCII table is people are

1154
00:54:24,630 --> 00:54:27,930
laughing because there's one hand up and

1155
00:54:26,250 --> 00:54:30,570
it's somebody who was in our study group

1156
00:54:27,929 --> 00:54:33,029
today who talked to me about ask you dog

1157
00:54:30,570 --> 00:54:35,789
ask you doc is the most amazing project

1158
00:54:33,030 --> 00:54:38,490
it's like markdown but it's like what

1159
00:54:35,789 --> 00:54:41,730
markdown needs to be to create actual

1160
00:54:38,489 --> 00:54:44,189
books and like a lot of actual books are

1161
00:54:41,730 --> 00:54:46,530
written in ASCII doc and so it's as easy

1162
00:54:44,190 --> 00:54:48,780
to use this back down about there's way

1163
00:54:46,530 --> 00:54:51,030
more cool stuff you can do with it in

1164
00:54:48,780 --> 00:54:53,190
fact here is an ASCII doc file here

1165
00:54:51,030 --> 00:54:55,890
right and as you'll see it looks very

1166
00:54:53,190 --> 00:55:02,639
normal there's headings right and this

1167
00:54:55,889 --> 00:55:05,059
is pre formatted text and there's yeah

1168
00:55:02,639 --> 00:55:09,569
there's lists and whatever else it looks

1169
00:55:05,059 --> 00:55:13,559
pretty standard and actually I'll show

1170
00:55:09,570 --> 00:55:16,380
you a more complete sq doc thing more

1171
00:55:13,559 --> 00:55:18,119
standard after doc thing but you can do

1172
00:55:16,380 --> 00:55:20,579
stuff like say put a table of contents

1173
00:55:18,119 --> 00:55:23,519
here please

1174
00:55:20,579 --> 00:55:28,019
you can say : : means put a definition

1175
00:55:23,519 --> 00:55:30,090
list here please plus means this is a

1176
00:55:28,019 --> 00:55:31,710
continuation of the previous list item

1177
00:55:30,090 --> 00:55:35,250
and so there's just like little things

1178
00:55:31,710 --> 00:55:37,650
that you can do which are super handy or

1179
00:55:35,250 --> 00:55:40,019
like put this thing make it slightly

1180
00:55:37,650 --> 00:55:44,309
smaller than everything else so it's

1181
00:55:40,019 --> 00:55:50,070
like turbocharged markdown and so this

1182
00:55:44,309 --> 00:55:52,259
asciidoc creates this HTML and I didn't

1183
00:55:50,070 --> 00:55:53,910
add any CSS or do anything myself like

1184
00:55:52,260 --> 00:55:56,160
we literally started this project like

1185
00:55:53,909 --> 00:55:59,099
four hours ago so this is like just an

1186
00:55:56,159 --> 00:56:03,210
example basically right and so you can

1187
00:55:59,099 --> 00:56:06,299
see we've got a table of contents we can

1188
00:56:03,210 --> 00:56:08,400
jump straight to here we've got a cross

1189
00:56:06,300 --> 00:56:10,830
reference we can click on to jump

1190
00:56:08,400 --> 00:56:12,750
straight to the cross reference each

1191
00:56:10,829 --> 00:56:15,329
method kind of comes along with its

1192
00:56:12,750 --> 00:56:18,269
details and so on and so forth right and

1193
00:56:15,329 --> 00:56:22,319
to make things even easier rather than

1194
00:56:18,269 --> 00:56:24,150
having to know that this is meant to be

1195
00:56:22,320 --> 00:56:26,100
like the argument list is meant to be

1196
00:56:24,150 --> 00:56:28,320
smaller than the main part or how do you

1197
00:56:26,099 --> 00:56:31,889
create a cross reference or how you

1198
00:56:28,320 --> 00:56:33,960
meant to format the arguments to the

1199
00:56:31,889 --> 00:56:36,589
method name and list out each one of its

1200
00:56:33,960 --> 00:56:39,570
arguments we've created a special

1201
00:56:36,590 --> 00:56:41,280
template where you can just write

1202
00:56:39,570 --> 00:56:43,890
various stuff in curly brackets like

1203
00:56:41,280 --> 00:56:46,200
please put the arguments here and here

1204
00:56:43,889 --> 00:56:48,150
is an example of one argument and here

1205
00:56:46,199 --> 00:56:51,449
is a cross reference and here is a

1206
00:56:48,150 --> 00:56:54,240
method and so forth so we're in the

1207
00:56:51,449 --> 00:56:55,710
process of documenting the documentation

1208
00:56:54,239 --> 00:56:57,509
template that there's basically like

1209
00:56:55,710 --> 00:56:59,670
five or six of these little curly braket

1210
00:56:57,510 --> 00:57:02,100
things you'll need to learn but for you

1211
00:56:59,670 --> 00:57:03,990
to create the documentation of class or

1212
00:57:02,099 --> 00:57:06,960
a method you can just copy one that's

1213
00:57:03,989 --> 00:57:09,868
already there basically and so the idea

1214
00:57:06,960 --> 00:57:11,579
is we're going to have like it'll almost

1215
00:57:09,869 --> 00:57:15,090
be like a book you know there'll be

1216
00:57:11,579 --> 00:57:19,409
tables and pictures little videos

1217
00:57:15,090 --> 00:57:22,050
segments and hyperloop throughout and

1218
00:57:19,409 --> 00:57:25,079
all that stuff you might be wondering

1219
00:57:22,050 --> 00:57:26,280
what about docstrings but actually I

1220
00:57:25,079 --> 00:57:29,190
don't know if you've noticed but if you

1221
00:57:26,280 --> 00:57:31,080
look at the Python standard library and

1222
00:57:29,190 --> 00:57:33,070
look at the doc string for example for

1223
00:57:31,079 --> 00:57:35,949
reg X compile it's

1224
00:57:33,070 --> 00:57:38,800
single-line nearly every dock string in

1225
00:57:35,949 --> 00:57:40,179
Python is a single line and Python then

1226
00:57:38,800 --> 00:57:42,519
does exactly this they have done a

1227
00:57:40,179 --> 00:57:44,949
website containing the documentation

1228
00:57:42,519 --> 00:57:46,269
that says like hey this is what regular

1229
00:57:44,949 --> 00:57:47,919
expressions are and this is what you

1230
00:57:46,269 --> 00:57:49,989
need to know about them and if you want

1231
00:57:47,920 --> 00:57:51,159
them to grow fast to unity his compile

1232
00:57:49,989 --> 00:57:54,250
and here's what some information about

1233
00:57:51,159 --> 00:57:56,170
compile and his examples it's not in the

1234
00:57:54,250 --> 00:57:59,739
dog stream and that's why we're doing it

1235
00:57:56,170 --> 00:58:03,579
as well our dog strings will be one line

1236
00:57:59,739 --> 00:58:06,069
unless you need like two sometimes it's

1237
00:58:03,579 --> 00:58:07,559
going to be very similar to Python but

1238
00:58:06,070 --> 00:58:11,320
even better

1239
00:58:07,559 --> 00:58:13,480
so everybody is welcome to help

1240
00:58:11,320 --> 00:58:14,950
contribute to the documentation and

1241
00:58:13,480 --> 00:58:17,860
hopefully by the time if you're watching

1242
00:58:14,949 --> 00:58:20,379
this on the MOOC it'll be reasonably

1243
00:58:17,860 --> 00:58:23,970
fleshed out and we'll try to keep a list

1244
00:58:20,380 --> 00:58:23,970
of things to do

1245
00:58:24,000 --> 00:58:32,769
all right so I'm going to do one first

1246
00:58:30,599 --> 00:58:36,699
so one question that came up in the

1247
00:58:32,769 --> 00:58:39,519
break was how does this compare to word

1248
00:58:36,699 --> 00:58:41,079
to vet and this is actually a great

1249
00:58:39,519 --> 00:58:43,599
thing for you to spend time thinking

1250
00:58:41,079 --> 00:58:45,400
about during the week is how does this

1251
00:58:43,599 --> 00:58:46,900
compare to work lÃ©vesque I'll give you

1252
00:58:45,400 --> 00:58:49,630
the summary now but it's a very

1253
00:58:46,900 --> 00:58:51,579
important conceptual difference the main

1254
00:58:49,630 --> 00:58:55,240
conceptual difference is what is word to

1255
00:58:51,579 --> 00:58:59,139
vet word Tyvek is a single embedding

1256
00:58:55,239 --> 00:59:03,519
matrix each word has a vector and that's

1257
00:58:59,139 --> 00:59:05,440
it so in other words it's a single it's

1258
00:59:03,519 --> 00:59:08,230
a single layer from a pre-trained model

1259
00:59:05,440 --> 00:59:11,079
and specifically that layer is the input

1260
00:59:08,230 --> 00:59:14,889
layer and also specifically that

1261
00:59:11,079 --> 00:59:17,769
pre-trained model is a linear model okay

1262
00:59:14,889 --> 00:59:20,500
that is free trained on something called

1263
00:59:17,769 --> 00:59:22,960
a co-occurrence matrix so we have no

1264
00:59:20,500 --> 00:59:24,820
particular reason to believe that this

1265
00:59:22,960 --> 00:59:27,250
model has learned anything much about

1266
00:59:24,820 --> 00:59:28,630
the English language or that it has any

1267
00:59:27,250 --> 00:59:33,360
particular capabilities because it's

1268
00:59:28,630 --> 00:59:38,200
just a single linear layer and that's it

1269
00:59:33,360 --> 00:59:43,660
so what's this wikitext 103 model it's a

1270
00:59:38,199 --> 00:59:46,269
language model and it has a four hundred

1271
00:59:43,659 --> 00:59:51,659
dimensional embedding matrix

1272
00:59:46,269 --> 00:59:55,360
three hidden layers with 1,150

1273
00:59:51,659 --> 00:59:59,049
activations per layer and regularization

1274
00:59:55,360 --> 01:00:00,849
and all that stuff tied input output

1275
00:59:59,050 --> 01:00:05,950
matrix equator sees it's basically a

1276
01:00:00,849 --> 01:00:08,139
state-of-the-art AWD ASTM so like what's

1277
01:00:05,949 --> 01:00:10,659
the difference between a single layer of

1278
01:00:08,139 --> 01:00:15,989
a single linear model versus a three

1279
01:00:10,659 --> 01:00:18,690
layer recurrent neural network

1280
01:00:15,989 --> 01:00:21,579
everything you know they're they're very

1281
01:00:18,690 --> 01:00:23,980
different levels of capability and so

1282
01:00:21,579 --> 01:00:27,309
you'll see when you try using a pre

1283
01:00:23,980 --> 01:00:29,710
trained language model this is a what

1284
01:00:27,309 --> 01:00:32,429
Vic layer you'll get very very different

1285
01:00:29,710 --> 01:00:35,289
results to the vast majority of tasks

1286
01:00:32,429 --> 01:00:37,239
what if the numpy array does not fit in

1287
01:00:35,289 --> 01:00:39,250
memory is it possible to write a PI

1288
01:00:37,239 --> 01:00:43,539
torch data loader directly from a large

1289
01:00:39,250 --> 01:00:44,829
CSV file it almost certainly won't come

1290
01:00:43,539 --> 01:00:49,150
up so I'm not going to spend time on it

1291
01:00:44,829 --> 01:00:50,889
like these things are tiny these they're

1292
01:00:49,150 --> 01:00:52,090
just hints think about how many

1293
01:00:50,889 --> 01:00:53,559
Institute would need to run out of

1294
01:00:52,090 --> 01:00:55,269
memory that's not gonna happen

1295
01:00:53,559 --> 01:00:58,090
they don't have a kitten's jpg memory

1296
01:00:55,269 --> 01:01:03,039
just in your memory so I've actually

1297
01:00:58,090 --> 01:01:05,410
done a another Wikipedia model which I

1298
01:01:03,039 --> 01:01:07,809
called Giga wiki which was on all of

1299
01:01:05,409 --> 01:01:09,940
Wikipedia and even that easily fits in

1300
01:01:07,809 --> 01:01:11,259
there and the reason I'm not using it is

1301
01:01:09,940 --> 01:01:13,990
because it turned out not to really help

1302
01:01:11,260 --> 01:01:17,020
very much business wiki text 103 but you

1303
01:01:13,989 --> 01:01:19,059
know I've built a bigger model than

1304
01:01:17,019 --> 01:01:21,820
anybody else I've found in the academic

1305
01:01:19,059 --> 01:01:24,909
literature pretty much and it fits in

1306
01:01:21,820 --> 01:01:26,860
memory on a single machine what is the

1307
01:01:24,909 --> 01:01:29,589
idea behind averaging the weights of

1308
01:01:26,860 --> 01:01:31,750
embeddings they're going to be set to

1309
01:01:29,590 --> 01:01:34,720
something you know like there are words

1310
01:01:31,750 --> 01:01:36,519
that weren't there so other options is

1311
01:01:34,719 --> 01:01:38,019
we could leave them a zero but that

1312
01:01:36,519 --> 01:01:40,509
seems like a very extreme thing to do

1313
01:01:38,019 --> 01:01:44,619
like zero is a very extreme number why

1314
01:01:40,510 --> 01:01:46,750
would it be zero we could set it equal

1315
01:01:44,619 --> 01:01:48,579
to some random numbers but if so what

1316
01:01:46,750 --> 01:01:49,869
would be the mean and standard deviation

1317
01:01:48,579 --> 01:01:53,110
of those random numbers or should they

1318
01:01:49,869 --> 01:01:55,119
be uniform if we just average the rest

1319
01:01:53,110 --> 01:01:57,370
of the you know the embeddings then we

1320
01:01:55,119 --> 01:01:58,269
have something that's reasonable scale

1321
01:01:57,369 --> 01:02:00,639
I just declare

1322
01:01:58,269 --> 01:02:02,289
this is how you're initializing words

1323
01:02:00,639 --> 01:02:05,109
that didn't appear in the training yeah

1324
01:02:02,289 --> 01:02:07,000
that's right and then I think you've

1325
01:02:05,110 --> 01:02:08,890
pretty much kind of just answered this

1326
01:02:07,000 --> 01:02:10,719
one but someone had asked if there's a

1327
01:02:08,889 --> 01:02:13,659
specific advantage to creating our own

1328
01:02:10,719 --> 01:02:15,459
pre-trained embedding over using glob or

1329
01:02:13,659 --> 01:02:16,779
word - back yeah I think I know we're

1330
01:02:15,460 --> 01:02:20,789
not creating a preacher and embedding

1331
01:02:16,780 --> 01:02:24,490
we're putting a pre training model okay

1332
01:02:20,789 --> 01:02:26,170
so um let's talk a little bit more with

1333
01:02:24,489 --> 01:02:27,669
this is a ton of stuff we've seen before

1334
01:02:26,170 --> 01:02:29,170
but it's changed a little bit it's

1335
01:02:27,670 --> 01:02:31,030
actually a lot easier than it was in

1336
01:02:29,170 --> 01:02:35,789
part one but I want to go a little bit

1337
01:02:31,030 --> 01:02:35,790
deeper into the language model loader

1338
01:02:36,300 --> 01:02:40,300
okay so this is the language model

1339
01:02:38,889 --> 01:02:42,909
loader and I really hope that by now

1340
01:02:40,300 --> 01:02:45,340
you've learned in your editor or IDE how

1341
01:02:42,909 --> 01:02:49,269
to jump to symbols okay yeah I don't

1342
01:02:45,340 --> 01:02:51,340
want it to be a burden for you to find

1343
01:02:49,269 --> 01:02:53,050
out what the source code of language

1344
01:02:51,340 --> 01:02:55,000
model loader is alright and if it's

1345
01:02:53,050 --> 01:02:56,950
still a burden please you know go back

1346
01:02:55,000 --> 01:03:00,070
and try and learn those keyboard

1347
01:02:56,949 --> 01:03:01,569
shortcuts in vs code you know like if

1348
01:03:00,070 --> 01:03:03,760
your editor doesn't make it easy

1349
01:03:01,570 --> 01:03:05,320
don't use that editor anymore okay

1350
01:03:03,760 --> 01:03:10,840
there's lots of good free editors that

1351
01:03:05,320 --> 01:03:12,690
make this easy okay so so here's the

1352
01:03:10,840 --> 01:03:16,900
source code for language model loader

1353
01:03:12,690 --> 01:03:22,470
and yeah it's it's it's interesting to

1354
01:03:16,900 --> 01:03:26,110
notice that it it's not doing anything

1355
01:03:22,469 --> 01:03:30,759
particularly tricky it's not deriving

1356
01:03:26,110 --> 01:03:32,559
from anything at all right what makes it

1357
01:03:30,760 --> 01:03:34,600
something that's capable being a data

1358
01:03:32,559 --> 01:03:39,509
loader is that it's something you can

1359
01:03:34,599 --> 01:03:39,509
iterate over okay and so specifically

1360
01:03:40,409 --> 01:03:44,559
okay

1361
01:03:41,590 --> 01:03:46,870
so specifically here's the fit function

1362
01:03:44,559 --> 01:03:48,130
inside foster yo model this is like

1363
01:03:46,869 --> 01:03:50,769
whatever where everything ends up

1364
01:03:48,130 --> 01:03:53,079
eventually which goes to each epoch and

1365
01:03:50,769 --> 01:03:55,059
then it creates an iterator from the

1366
01:03:53,079 --> 01:03:55,809
data loader and then just does a for

1367
01:03:55,059 --> 01:03:57,369
loop through it

1368
01:03:55,809 --> 01:03:59,259
alright so anything you can do a for

1369
01:03:57,369 --> 01:04:02,559
loop through can be a data loader

1370
01:03:59,260 --> 01:04:07,090
specifically it needs to return tuples

1371
01:04:02,559 --> 01:04:10,049
of mini-batches independent and

1372
01:04:07,090 --> 01:04:11,890
dependent variable for mini batches so

1373
01:04:10,050 --> 01:04:15,789
anything with a

1374
01:04:11,889 --> 01:04:19,659
Dunda in a method is something that can

1375
01:04:15,789 --> 01:04:21,339
act as an iterator and yield is a neat

1376
01:04:19,659 --> 01:04:22,509
little Python keywords you probably

1377
01:04:21,338 --> 01:04:22,960
should learn about if you don't already

1378
01:04:22,509 --> 01:04:25,358
know it

1379
01:04:22,960 --> 01:04:27,429
but it basically spits out a thing and

1380
01:04:25,358 --> 01:04:29,759
wets for you to ask for another thing

1381
01:04:27,429 --> 01:04:35,259
normally in like a for loop or something

1382
01:04:29,759 --> 01:04:37,420
so in this case we start by initializing

1383
01:04:35,259 --> 01:04:39,960
the language model passing it in the

1384
01:04:37,420 --> 01:04:43,480
numbers so this is the numerical eyes

1385
01:04:39,960 --> 01:04:46,179
big big long list of all of our

1386
01:04:43,480 --> 01:04:48,759
documents concatenated together and the

1387
01:04:46,179 --> 01:04:52,239
first thing we do is to batch of fudge

1388
01:04:48,759 --> 01:04:54,400
and this is the thing which quite a few

1389
01:04:52,239 --> 01:05:01,568
of you got confused about last time

1390
01:04:54,400 --> 01:05:06,599
right if our batch size is 64 and our we

1391
01:05:01,568 --> 01:05:11,798
have 24 25 million numbers in our list

1392
01:05:06,599 --> 01:05:14,680
we are not creating items of length 64

1393
01:05:11,798 --> 01:05:17,380
we're not doing that we're creating 64

1394
01:05:14,679 --> 01:05:20,858
items in total so each of them is of

1395
01:05:17,380 --> 01:05:23,140
size T divided by 64 which is three

1396
01:05:20,858 --> 01:05:27,788
hundred and ninety thousand okay so

1397
01:05:23,139 --> 01:05:32,409
that's what that's what we do here when

1398
01:05:27,789 --> 01:05:36,069
we reshape it so that this axis here is

1399
01:05:32,409 --> 01:05:37,960
of length 64 and then this minus one is

1400
01:05:36,068 --> 01:05:40,150
everything else so that's three hundred

1401
01:05:37,960 --> 01:05:42,849
and whatever a thousand three hundred

1402
01:05:40,150 --> 01:05:46,568
ninety thousand blob okay and then we

1403
01:05:42,849 --> 01:05:49,480
transpose it so that means that we now

1404
01:05:46,568 --> 01:05:52,298
have 64 columns three hundred ninety

1405
01:05:49,480 --> 01:05:57,099
thousand rows and then what we do each

1406
01:05:52,298 --> 01:05:59,288
time we do an iterate is we grab one

1407
01:05:57,099 --> 01:06:00,730
batch of some sequence length we'll look

1408
01:05:59,289 --> 01:06:03,999
at that in a moment but basically it's

1409
01:06:00,730 --> 01:06:10,449
approximately equal to VP TT which we

1410
01:06:03,998 --> 01:06:16,838
set to 70 stands for back prop through

1411
01:06:10,449 --> 01:06:23,230
time and we just grab that many rows

1412
01:06:16,838 --> 01:06:25,599
okay so from i to i plus 70 rows and

1413
01:06:23,230 --> 01:06:28,090
then we try to predict that plus one

1414
01:06:25,599 --> 01:06:31,230
remember so we're trying to predict when

1415
01:06:28,090 --> 01:06:33,760
past where a wrap - so we've got 64

1416
01:06:31,230 --> 01:06:36,039
columns and each of those is one

1417
01:06:33,760 --> 01:06:36,790
sixty-fourth of our 25 million or

1418
01:06:36,039 --> 01:06:40,719
whatever it was

1419
01:06:36,789 --> 01:06:44,110
tokens you know hundreds of thousands

1420
01:06:40,719 --> 01:06:46,449
long and we just grab you know 17 at a

1421
01:06:44,110 --> 01:06:47,860
time so each of those columns each time

1422
01:06:46,449 --> 01:06:51,489
and Gravatt is going to kind of hook up

1423
01:06:47,860 --> 01:06:53,110
to the previous column okay and so

1424
01:06:51,489 --> 01:06:55,149
that's why we get this consistency this

1425
01:06:53,110 --> 01:07:00,250
language model is it's stateful which is

1426
01:06:55,150 --> 01:07:02,950
really important pretty much all the

1427
01:07:00,250 --> 01:07:07,409
cool stuff in the language model is is

1428
01:07:02,949 --> 01:07:10,419
stolen from Steven Mara T's AWD LS TM

1429
01:07:07,409 --> 01:07:14,379
including this little trick here which

1430
01:07:10,420 --> 01:07:16,539
is if we always grab 70 at a time and

1431
01:07:14,380 --> 01:07:18,400
then we go back at me to a new epoch

1432
01:07:16,539 --> 01:07:21,159
we're going to grab exactly the same

1433
01:07:18,400 --> 01:07:24,730
batches every time there's no randomness

1434
01:07:21,159 --> 01:07:26,349
now normally we shuffle out data every

1435
01:07:24,730 --> 01:07:28,059
time we do an epoch or every time we

1436
01:07:26,349 --> 01:07:29,769
grab some data we grab it at random you

1437
01:07:28,059 --> 01:07:33,400
can't do that with a language model

1438
01:07:29,769 --> 01:07:35,289
because this set has to join up to the

1439
01:07:33,400 --> 01:07:38,260
previous set because it's fun to trying

1440
01:07:35,289 --> 01:07:40,989
to learn the sentence right and if you

1441
01:07:38,260 --> 01:07:43,450
suddenly jump somewhere else then that

1442
01:07:40,989 --> 01:07:48,069
doesn't make any sense as a sentence so

1443
01:07:43,449 --> 01:07:50,739
Stevens idea is to say okay well since

1444
01:07:48,070 --> 01:07:53,440
we can't shuffle the order let's step

1445
01:07:50,739 --> 01:07:56,439
randomly change the size the sequence

1446
01:07:53,440 --> 01:08:00,670
length okay and so basically he says all

1447
01:07:56,440 --> 01:08:04,570
right 95% of the time we'll use vacate

1448
01:08:00,670 --> 01:08:07,809
80 70 but 5% of the time we'll use half

1449
01:08:04,570 --> 01:08:09,640
that right and then he says you know

1450
01:08:07,809 --> 01:08:11,949
what I'm not even going to make that the

1451
01:08:09,639 --> 01:08:14,949
sequence length I'm going to create a

1452
01:08:11,949 --> 01:08:17,199
normally distributed random number with

1453
01:08:14,949 --> 01:08:19,420
that average and a standard deviation of

1454
01:08:17,199 --> 01:08:23,289
5 and I'll make that the sequence length

1455
01:08:19,420 --> 01:08:25,359
right so the sequence length is seventy

1456
01:08:23,289 --> 01:08:27,630
ish and that means every time we go

1457
01:08:25,359 --> 01:08:30,460
through we're getting slightly different

1458
01:08:27,630 --> 01:08:34,480
batches so we've got that little bit of

1459
01:08:30,460 --> 01:08:35,970
extra randomness I asked him Steven

1460
01:08:34,479 --> 01:08:38,238
marady where he came up with this idea

1461
01:08:35,970 --> 01:08:41,180
did he think of it

1462
01:08:38,238 --> 01:08:43,608
and he was like I think I thought of it

1463
01:08:41,180 --> 01:08:45,890
but it seemed so obvious and I bet I

1464
01:08:43,609 --> 01:08:47,180
didn't think of it which is like true of

1465
01:08:45,890 --> 01:08:48,770
like every time I come up with an idea

1466
01:08:47,180 --> 01:08:50,450
or deke why I think it always seems so

1467
01:08:48,770 --> 01:08:54,890
obvious that um somebody else is thought

1468
01:08:50,449 --> 01:08:57,889
of it but I think he thought of it so

1469
01:08:54,890 --> 01:09:00,440
yeah so this is like a nice thing to

1470
01:08:57,890 --> 01:09:02,569
look at if you're trying to do something

1471
01:09:00,439 --> 01:09:05,389
a bit unusual with the data motor it's

1472
01:09:02,569 --> 01:09:08,180
like okay here's a simple kind of role

1473
01:09:05,390 --> 01:09:09,829
model you can use as to creating a data

1474
01:09:08,180 --> 01:09:16,100
loader from scratch something that spits

1475
01:09:09,829 --> 01:09:18,079
out batches of do so so our language

1476
01:09:16,100 --> 01:09:20,569
model loader just took in all of the

1477
01:09:18,079 --> 01:09:23,019
documents concatenated together along

1478
01:09:20,569 --> 01:09:26,630
with a batch size and the VPT

1479
01:09:23,020 --> 01:09:28,549
now generally speaking we want to create

1480
01:09:26,630 --> 01:09:30,739
a learner and the way we normally do

1481
01:09:28,548 --> 01:09:32,750
that is by getting a model data object

1482
01:09:30,738 --> 01:09:34,909
and they're calling some kind of method

1483
01:09:32,750 --> 01:09:36,798
which have various names but sometimes

1484
01:09:34,909 --> 01:09:38,988
you don't often we call that method get

1485
01:09:36,798 --> 01:09:41,000
model and so the idea is that the model

1486
01:09:38,988 --> 01:09:44,209
data object has enough information to

1487
01:09:41,000 --> 01:09:46,759
know what kind of model to give you so

1488
01:09:44,210 --> 01:09:51,699
we have to create that model data object

1489
01:09:46,759 --> 01:09:56,149
which means we need that that class and

1490
01:09:51,699 --> 01:09:57,829
so that's very easy to do right so here

1491
01:09:56,149 --> 01:09:59,988
are all of the pieces we're going to

1492
01:09:57,829 --> 01:10:05,238
create a custom learner a custom model

1493
01:09:59,988 --> 01:10:06,829
data class and a custom class so a model

1494
01:10:05,238 --> 01:10:08,750
data class again this one doesn't

1495
01:10:06,829 --> 01:10:11,738
inherit from anything so you really see

1496
01:10:08,750 --> 01:10:14,449
it is there's almost nothing to do right

1497
01:10:11,738 --> 01:10:16,849
you need to tell it most importantly

1498
01:10:14,449 --> 01:10:17,389
what's your training set give it a data

1499
01:10:16,850 --> 01:10:19,730
loader

1500
01:10:17,390 --> 01:10:22,310
what's the validation set give it a data

1501
01:10:19,729 --> 01:10:25,189
loader and optionally give it a test set

1502
01:10:22,310 --> 01:10:29,600
data loader plus anything else that

1503
01:10:25,189 --> 01:10:32,529
needs to know right so it might need to

1504
01:10:29,600 --> 01:10:35,780
know the VPT

1505
01:10:32,529 --> 01:10:38,719
it needs to know the number of tokens

1506
01:10:35,779 --> 01:10:43,460
that's the vocab size it needs to know

1507
01:10:38,719 --> 01:10:45,409
what is the padding index and so that it

1508
01:10:43,460 --> 01:10:47,329
can save temporary files and models

1509
01:10:45,409 --> 01:10:49,609
model data as always need to know the

1510
01:10:47,329 --> 01:10:51,859
path ok and so we just grab all that

1511
01:10:49,609 --> 01:10:52,969
stuff and we dump it right

1512
01:10:51,859 --> 01:10:54,469
and that's it that's the entire

1513
01:10:52,970 --> 01:10:57,170
initializer there's no logic there at

1514
01:10:54,470 --> 01:11:01,449
all okay so then all of the work happens

1515
01:10:57,170 --> 01:11:03,560
inside get model right and so get model

1516
01:11:01,449 --> 01:11:06,889
calls something we'll look at later

1517
01:11:03,560 --> 01:11:12,110
which just grabs a normal pie torch and

1518
01:11:06,890 --> 01:11:15,140
end module architecture okay and Chuck's

1519
01:11:12,109 --> 01:11:17,989
it on the GPU note with PI torch

1520
01:11:15,140 --> 01:11:21,140
normally we would save cuda with faster

1521
01:11:17,989 --> 01:11:23,529
i it's better to say to GPU and the

1522
01:11:21,140 --> 01:11:26,690
reason is that if you don't have a GPU

1523
01:11:23,529 --> 01:11:29,659
it'll leave it on the cpu and it also

1524
01:11:26,689 --> 01:11:31,639
provides a global variable you can set

1525
01:11:29,659 --> 01:11:35,599
to choose whether it goes on the GPU or

1526
01:11:31,640 --> 01:11:37,640
not so it's a it's a better approach so

1527
01:11:35,600 --> 01:11:40,910
we wrapped the model in a language model

1528
01:11:37,640 --> 01:11:44,750
and the language model is this basically

1529
01:11:40,909 --> 01:11:47,960
a language model is a subclass of basic

1530
01:11:44,750 --> 01:11:51,590
model it basically almost does nothing

1531
01:11:47,960 --> 01:11:52,970
except it defines layer groups and so

1532
01:11:51,590 --> 01:11:54,739
remember how when we do like

1533
01:11:52,970 --> 01:11:57,140
discriminative learning rates where

1534
01:11:54,739 --> 01:11:59,779
different layers have different learning

1535
01:11:57,140 --> 01:12:03,680
rates or like we freeze different

1536
01:11:59,779 --> 01:12:05,659
amounts we don't provide a different

1537
01:12:03,680 --> 01:12:07,730
learning rate for every layer because

1538
01:12:05,659 --> 01:12:08,899
there can be like a thousand layers we

1539
01:12:07,729 --> 01:12:11,599
provide a different learning rate for

1540
01:12:08,899 --> 01:12:13,849
every layer group right so when you

1541
01:12:11,600 --> 01:12:17,840
create a custom model you just have to

1542
01:12:13,850 --> 01:12:21,920
override this one thing which returns a

1543
01:12:17,840 --> 01:12:24,520
list of all of your layer groups and so

1544
01:12:21,920 --> 01:12:28,100
in this case my last layer group

1545
01:12:24,520 --> 01:12:30,950
contains the last part of the model and

1546
01:12:28,100 --> 01:12:33,470
one bit of drop out and the rest of it

1547
01:12:30,949 --> 01:12:36,079
this star here means Paul is apart so

1548
01:12:33,470 --> 01:12:41,079
this is basically going to be one layer

1549
01:12:36,079 --> 01:12:45,529
per RNN layer okay so that's all that is

1550
01:12:41,079 --> 01:12:48,170
and then finally turn that into a

1551
01:12:45,529 --> 01:12:50,029
learner and so alone oh you just pass in

1552
01:12:48,170 --> 01:12:52,460
the model and it turns it into a real

1553
01:12:50,029 --> 01:12:54,920
owner in this case we have overridden

1554
01:12:52,460 --> 01:12:58,699
learner and the only thing we've done is

1555
01:12:54,920 --> 01:13:02,810
to say I want the default loss function

1556
01:12:58,699 --> 01:13:04,788
to be across entropy okay so this you

1557
01:13:02,810 --> 01:13:07,340
know entire set of

1558
01:13:04,788 --> 01:13:10,550
model customer model data custom learner

1559
01:13:07,340 --> 01:13:13,969
or fits on a single screen and they

1560
01:13:10,550 --> 01:13:16,010
always basically look like this right so

1561
01:13:13,969 --> 01:13:19,279
that's a kind of little dig inside this

1562
01:13:16,010 --> 01:13:20,780
pretty boring part of the code this so

1563
01:13:19,279 --> 01:13:23,238
the interesting part of this code base

1564
01:13:20,779 --> 01:13:25,009
is getting out now because that language

1565
01:13:23,238 --> 01:13:31,629
model is actually the thing that gives

1566
01:13:25,010 --> 01:13:35,510
us our AWD lsdm and it actually contains

1567
01:13:31,630 --> 01:13:37,368
the big idea that the big incredibly

1568
01:13:35,510 --> 01:13:39,920
simple idea that everybody else here

1569
01:13:37,368 --> 01:13:41,988
thinks is really obvious that everybody

1570
01:13:39,920 --> 01:13:46,190
in the NLP community I spoke to thought

1571
01:13:41,988 --> 01:13:48,288
was insane which is basically every

1572
01:13:46,189 --> 01:13:49,428
model can be thought of pretty much

1573
01:13:48,288 --> 01:13:52,518
every model can be thought of as a

1574
01:13:49,429 --> 01:13:54,980
backbone plus a head and if you pre

1575
01:13:52,519 --> 01:13:58,429
train the backbone and stick on a random

1576
01:13:54,979 --> 01:14:02,988
head you can do fine tuning and that's a

1577
01:13:58,429 --> 01:14:04,519
good idea all right and so here's these

1578
01:14:02,988 --> 01:14:06,529
two bits of the code literally right

1579
01:14:04,519 --> 01:14:09,260
next to each other this is kind of all

1580
01:14:06,529 --> 01:14:14,238
there is inside this bit of faster I've

1581
01:14:09,260 --> 01:14:17,269
dot LM RNN here's gate language model

1582
01:14:14,238 --> 01:14:20,479
here's gate classifier that language

1583
01:14:17,269 --> 01:14:23,000
model creates an R and n encoder and

1584
01:14:20,479 --> 01:14:25,359
then creates a sequential model that

1585
01:14:23,000 --> 01:14:28,250
sticks on top of that a linear decoder

1586
01:14:25,359 --> 01:14:30,259
classifier creates an R Ln encoder and

1587
01:14:28,250 --> 01:14:31,908
then a sequential model that sticks on

1588
01:14:30,260 --> 01:14:34,250
top of that appalling linear classifier

1589
01:14:31,908 --> 01:14:36,138
well see these what these differences

1590
01:14:34,250 --> 01:14:38,689
are in a moment but you get the basic

1591
01:14:36,139 --> 01:14:40,069
idea right they're basically doing

1592
01:14:38,689 --> 01:14:47,618
pretty much the same thing they've got

1593
01:14:40,069 --> 01:14:49,639
this it's head linear layer on top so

1594
01:14:47,618 --> 01:14:51,229
that's worth digging in a little bit

1595
01:14:49,639 --> 01:14:53,868
deeper and seeing what's going on here

1596
01:14:51,229 --> 01:14:56,209
yes Rachel there was a question earlier

1597
01:14:53,868 --> 01:15:00,710
about whether that any of this

1598
01:14:56,210 --> 01:15:02,179
translates to other languages yeah this

1599
01:15:00,710 --> 01:15:06,828
whole thing works in any language you

1600
01:15:02,179 --> 01:15:09,828
like obviously would you have to retrain

1601
01:15:06,828 --> 01:15:13,819
your language model on a corpus from

1602
01:15:09,828 --> 01:15:16,038
that language yeah yeah so the wiki text

1603
01:15:13,819 --> 01:15:18,429
103 pre-trained language model knows

1604
01:15:16,038 --> 01:15:22,819
English right

1605
01:15:18,430 --> 01:15:25,039
you could use it maybe as a pre train

1606
01:15:22,819 --> 01:15:27,619
start for like a French short German

1607
01:15:25,039 --> 01:15:30,369
model start by retraining the embedding

1608
01:15:27,619 --> 01:15:34,699
layer from scratch might be helpful

1609
01:15:30,369 --> 01:15:36,229
Chinese maybe not so much but like given

1610
01:15:34,699 --> 01:15:39,429
that a language model can be trained

1611
01:15:36,229 --> 01:15:41,989
from any and labeled documents at all

1612
01:15:39,430 --> 01:15:44,510
you'll never have to do that right

1613
01:15:41,989 --> 01:15:46,729
because every every almost every

1614
01:15:44,510 --> 01:15:49,630
language in the world has you know

1615
01:15:46,729 --> 01:15:52,609
plenty of documents you can grab

1616
01:15:49,630 --> 01:15:56,119
newspapers web pages you know

1617
01:15:52,609 --> 01:15:59,000
parliamentary records whatever you know

1618
01:15:56,119 --> 01:16:01,819
as long as you've got a few thousand

1619
01:15:59,000 --> 01:16:03,199
documents showing somewhat normal usage

1620
01:16:01,819 --> 01:16:05,539
of that language you can create a

1621
01:16:03,199 --> 01:16:08,179
language model and so I know some of our

1622
01:16:05,539 --> 01:16:10,250
students you know one of our students is

1623
01:16:08,180 --> 01:16:14,060
a bill collector in a week very

1624
01:16:10,250 --> 01:16:17,119
embarrassing tried this approach for

1625
01:16:14,060 --> 01:16:19,580
Thai and he said like the first model he

1626
01:16:17,119 --> 01:16:23,449
built easily beat the previous Dennis

1627
01:16:19,579 --> 01:16:25,010
the anti classifier like it's yeah like

1628
01:16:23,449 --> 01:16:28,429
for those of you that are international

1629
01:16:25,010 --> 01:16:31,789
fellows this is an easy way for you to

1630
01:16:28,430 --> 01:16:33,770
kind of to whip out a paper in which you

1631
01:16:31,789 --> 01:16:35,600
you know either create the first ever

1632
01:16:33,770 --> 01:16:37,370
classifier in your language or beat

1633
01:16:35,600 --> 01:16:39,410
everybody else's classifier in your

1634
01:16:37,369 --> 01:16:41,359
language and then you can tell them that

1635
01:16:39,409 --> 01:16:43,039
you've been a student of deep moaning

1636
01:16:41,359 --> 01:16:49,699
for six months and piss off all the

1637
01:16:43,039 --> 01:16:51,560
academics in your country okay so here's

1638
01:16:49,699 --> 01:16:54,949
our edit encoder

1639
01:16:51,560 --> 01:16:57,440
it's just a standard inin module most of

1640
01:16:54,949 --> 01:17:01,489
the text in it is actually just

1641
01:16:57,439 --> 01:17:03,229
documentation as you can see it it looks

1642
01:17:01,489 --> 01:17:05,359
like there's more going on in it then

1643
01:17:03,229 --> 01:17:07,879
there actually is but really all there

1644
01:17:05,359 --> 01:17:10,759
is is we create an embedding layer we

1645
01:17:07,880 --> 01:17:17,270
create an LST M for each layer that's

1646
01:17:10,760 --> 01:17:19,730
been asked for that's it everything else

1647
01:17:17,270 --> 01:17:21,470
in it is dropped out right basically all

1648
01:17:19,729 --> 01:17:24,679
of the interesting stuff just about in

1649
01:17:21,470 --> 01:17:29,480
the AWA DL STM paper is all of the

1650
01:17:24,680 --> 01:17:30,980
places you can put drop out and then the

1651
01:17:29,479 --> 01:17:31,609
forward is basically the same thing

1652
01:17:30,979 --> 01:17:36,799
right

1653
01:17:31,609 --> 01:17:42,349
it's call the embedding layer add some

1654
01:17:36,800 --> 01:17:45,079
dropout go through each layer call that

1655
01:17:42,350 --> 01:17:49,820
are a ten liya append it to our list of

1656
01:17:45,079 --> 01:17:56,539
our ports add dropout that's about it

1657
01:17:49,819 --> 01:18:03,409
okay so it's it's it's really pretty

1658
01:17:56,539 --> 01:18:05,060
straightforward and the the paper you

1659
01:18:03,409 --> 01:18:07,550
want to be reading as I've mentioned is

1660
01:18:05,060 --> 01:18:10,190
the AWD lsdm paper which is this one

1661
01:18:07,550 --> 01:18:13,489
here regularizing and optimizing lsdm

1662
01:18:10,189 --> 01:18:16,279
language models and it's it's

1663
01:18:13,489 --> 01:18:20,599
well-written and pretty accessible right

1664
01:18:16,279 --> 01:18:23,539
and entirely implemented inside fast AI

1665
01:18:20,600 --> 01:18:26,840
as well right so you can see all of the

1666
01:18:23,539 --> 01:18:29,239
code for that paper and like a lot of

1667
01:18:26,840 --> 01:18:31,159
the code actually is I'm shamelessly

1668
01:18:29,239 --> 01:18:35,800
plagiarized with Stephens permission

1669
01:18:31,159 --> 01:18:38,029
from his excellent github repo WBLS TM

1670
01:18:35,800 --> 01:18:41,480
and the process of which I picked some

1671
01:18:38,029 --> 01:18:47,449
of his bugs as well I even told him

1672
01:18:41,479 --> 01:18:48,889
about them so yeah so I talked you know

1673
01:18:47,449 --> 01:18:50,720
I'm talking and increasingly about

1674
01:18:48,890 --> 01:18:54,200
please read the papers so here's the

1675
01:18:50,720 --> 01:18:56,570
paper please read this paper and it

1676
01:18:54,199 --> 01:19:01,489
refers to other papers so for things

1677
01:18:56,569 --> 01:19:04,609
like why is it that the encoder wait and

1678
01:19:01,489 --> 01:19:10,599
the decoder wait are the same right well

1679
01:19:04,609 --> 01:19:18,619
it's because there's this thing called

1680
01:19:10,600 --> 01:19:20,060
tie weights this is inside this is

1681
01:19:18,619 --> 01:19:21,559
inside that gate language model there's

1682
01:19:20,060 --> 01:19:26,930
a thing called tie weights the defaults

1683
01:19:21,560 --> 01:19:31,280
to true and if it's true then the we

1684
01:19:26,930 --> 01:19:33,380
actually tie we literally use the same

1685
01:19:31,279 --> 01:19:36,050
weight matrix for the encoder and the

1686
01:19:33,380 --> 01:19:38,539
decoder so like they're literally

1687
01:19:36,050 --> 01:19:41,449
pointing at the same block of memory if

1688
01:19:38,539 --> 01:19:43,640
you like and so why is that what's the

1689
01:19:41,449 --> 01:19:45,019
result of it that's one of the citations

1690
01:19:43,640 --> 01:19:46,700
and Stephens paper

1691
01:19:45,020 --> 01:19:49,340
is also a well-written paper you can go

1692
01:19:46,699 --> 01:19:51,699
and look up and learn about wait time so

1693
01:19:49,340 --> 01:19:54,319
there's a lot of cool stuff in there

1694
01:19:51,699 --> 01:19:56,539
okay so we have basically a standard

1695
01:19:54,319 --> 01:19:58,069
iron in the only reason where it's not

1696
01:19:56,539 --> 01:20:00,380
standard is it's just got lots more

1697
01:19:58,069 --> 01:20:04,099
types of dropout in it and then a

1698
01:20:00,380 --> 01:20:09,079
sequential model on top of that we stick

1699
01:20:04,100 --> 01:20:11,870
a linear decoder which is literally half

1700
01:20:09,079 --> 01:20:15,559
the screen of code it's got a single

1701
01:20:11,869 --> 01:20:18,550
linear layer we initialize the weights

1702
01:20:15,560 --> 01:20:22,789
to some range we add some dropout and

1703
01:20:18,550 --> 01:20:25,279
that's it so it's a linear layer okay so

1704
01:20:22,789 --> 01:20:27,380
we've got an iron N on top of that

1705
01:20:25,279 --> 01:20:29,840
mystic linear layer with dropout and we

1706
01:20:27,380 --> 01:20:39,909
finished okay so that's the language

1707
01:20:29,840 --> 01:20:44,090
model so um what dropout you choose

1708
01:20:39,909 --> 01:20:48,019
matters a lot and through a lot of

1709
01:20:44,090 --> 01:20:50,119
experimentation I found a bunch of

1710
01:20:48,020 --> 01:20:51,980
dropouts you can see here we've got like

1711
01:20:50,119 --> 01:20:54,529
each of these corresponds to a

1712
01:20:51,979 --> 01:20:56,239
particular accurate a bunch of dropouts

1713
01:20:54,529 --> 01:21:00,889
that tend to work pretty well for

1714
01:20:56,239 --> 01:21:04,969
language models but if you have less

1715
01:21:00,890 --> 01:21:07,940
data for your language model you'll need

1716
01:21:04,970 --> 01:21:10,340
more dropout if you have more data you

1717
01:21:07,939 --> 01:21:11,629
can benefit from less dropout you don't

1718
01:21:10,340 --> 01:21:14,329
want to regularize more than you have to

1719
01:21:11,630 --> 01:21:15,949
make sense right rather than having to

1720
01:21:14,329 --> 01:21:19,640
tune every one of these five things

1721
01:21:15,949 --> 01:21:22,220
right my claim is they're already pretty

1722
01:21:19,640 --> 01:21:24,440
good ratios to each other so just tune

1723
01:21:22,220 --> 01:21:26,000
this number I just multiply it all by

1724
01:21:24,439 --> 01:21:29,839
something okay

1725
01:21:26,000 --> 01:21:32,500
so there's really just one number you

1726
01:21:29,840 --> 01:21:34,789
have to choose so if you're overfitting

1727
01:21:32,500 --> 01:21:36,710
then you'll need to increase this number

1728
01:21:34,789 --> 01:21:38,149
if you're underfitting you'll need to

1729
01:21:36,710 --> 01:21:40,730
decrease this number because other than

1730
01:21:38,149 --> 01:21:50,960
that these ratio is actually seem pretty

1731
01:21:40,729 --> 01:21:52,699
good so one important idea which may

1732
01:21:50,960 --> 01:21:56,689
seem pretty minor but again it's

1733
01:21:52,699 --> 01:21:58,729
incredibly controversial is that we

1734
01:21:56,689 --> 01:22:00,529
should measure accuracy

1735
01:21:58,729 --> 01:22:03,289
when we look at a language model so

1736
01:22:00,529 --> 01:22:05,539
normally language models we look at this

1737
01:22:03,289 --> 01:22:08,899
this lost value which is just cross

1738
01:22:05,539 --> 01:22:11,060
entropy loss but specifically where you

1739
01:22:08,899 --> 01:22:13,689
nearly always take e to the power of

1740
01:22:11,060 --> 01:22:14,840
that which the NLP community calls

1741
01:22:13,689 --> 01:22:18,889
perplexity

1742
01:22:14,840 --> 01:22:23,840
okay so perplexity is just a ^ cross

1743
01:22:18,890 --> 01:22:26,690
entropy there's a lot of problems with

1744
01:22:23,840 --> 01:22:29,869
comparing things based on cross entropy

1745
01:22:26,689 --> 01:22:31,579
loss I'm not sure I've got time to go

1746
01:22:29,869 --> 01:22:35,720
into it in detail now but the basic

1747
01:22:31,579 --> 01:22:37,519
problem is that it's kind of like that

1748
01:22:35,720 --> 01:22:40,250
thing we learned about focal loss cross

1749
01:22:37,520 --> 01:22:42,140
entropy loss if you're right yet wants

1750
01:22:40,250 --> 01:22:44,750
you to be really confident that you're

1751
01:22:42,140 --> 01:22:47,510
right you know so it really penalized as

1752
01:22:44,750 --> 01:22:49,850
a model that doesn't kind of say like

1753
01:22:47,510 --> 01:22:51,530
I'm so sure this is wrong it's wrong

1754
01:22:49,850 --> 01:22:53,329
whereas accuracy doesn't care at all

1755
01:22:51,529 --> 01:22:54,559
about how confident you are this cop

1756
01:22:53,329 --> 01:22:56,479
cares about whether you're right and

1757
01:22:54,560 --> 01:22:59,210
this is much more often than the thing

1758
01:22:56,479 --> 01:23:01,489
which you care about in real life so

1759
01:22:59,210 --> 01:23:05,119
this accuracy is what how many how often

1760
01:23:01,489 --> 01:23:06,909
do we guess the next word correctly and

1761
01:23:05,119 --> 01:23:10,069
I just find that a much more stable

1762
01:23:06,909 --> 01:23:14,809
number to keep track of so so that's a

1763
01:23:10,069 --> 01:23:17,389
simple little thing that I do okay so so

1764
01:23:14,810 --> 01:23:31,250
we train for a while

1765
01:23:17,390 --> 01:23:35,230
and we get down to a 3.9 frost entropy

1766
01:23:31,250 --> 01:23:35,229
loss and if you go e ^ that

1767
01:23:40,420 --> 01:23:45,079
and to kind of give you a sense of like

1768
01:23:42,829 --> 01:23:49,819
what's happened with language models if

1769
01:23:45,079 --> 01:23:52,488
you look at academic papers from about

1770
01:23:49,819 --> 01:23:54,769
18 months ago you'll see them talking

1771
01:23:52,488 --> 01:23:58,549
about perplexities stayed at the our

1772
01:23:54,770 --> 01:24:01,880
complexities of like over a hundred okay

1773
01:23:58,550 --> 01:24:03,829
like the the the rate at which our

1774
01:24:01,880 --> 01:24:06,020
ability to kind of understand language

1775
01:24:03,829 --> 01:24:09,079
and I think like measuring language

1776
01:24:06,020 --> 01:24:12,110
model accuracy or perplexity is not a

1777
01:24:09,079 --> 01:24:13,460
terrible proxy for understanding

1778
01:24:12,109 --> 01:24:14,630
language if I can guess what you're

1779
01:24:13,460 --> 01:24:16,760
going to say next

1780
01:24:14,630 --> 01:24:18,590
it's you know I pretty much need to

1781
01:24:16,760 --> 01:24:19,670
understand language pretty well and also

1782
01:24:18,590 --> 01:24:21,409
the kind of things you might talk about

1783
01:24:19,670 --> 01:24:25,779
pretty well

1784
01:24:21,409 --> 01:24:27,619
so this numbers just come down so much

1785
01:24:25,779 --> 01:24:30,469
it's been amazing

1786
01:24:27,619 --> 01:24:31,909
NLP in the last 12 to 18 months

1787
01:24:30,469 --> 01:24:35,779
and it's going to come down a lot more

1788
01:24:31,909 --> 01:24:38,119
it really feels like 2011-2012 computer

1789
01:24:35,779 --> 01:24:40,069
vision you know we're just starting to

1790
01:24:38,119 --> 01:24:42,859
understand transfer learning and fine

1791
01:24:40,069 --> 01:24:44,929
tuning and these basic models are

1792
01:24:42,859 --> 01:24:47,059
getting so much so much better so

1793
01:24:44,929 --> 01:24:51,730
everything you thought about like what

1794
01:24:47,060 --> 01:24:54,590
NLP can and can't do it's very rapidly

1795
01:24:51,729 --> 01:24:56,689
going out of that like there's - lots of

1796
01:24:54,590 --> 01:24:58,850
stuff NLP is not good at to be clear

1797
01:24:56,689 --> 01:25:00,319
right just like in 2012 there was what's

1798
01:24:58,850 --> 01:25:03,140
the stuff computer vision wasn't good at

1799
01:25:00,319 --> 01:25:06,049
but it's changing incredibly rapidly and

1800
01:25:03,140 --> 01:25:08,530
now is a very very good time to be

1801
01:25:06,050 --> 01:25:11,630
getting very very good at NLP or

1802
01:25:08,529 --> 01:25:12,859
starting startups based on NLP because

1803
01:25:11,630 --> 01:25:16,130
there's a whole bunch of stuff which

1804
01:25:12,859 --> 01:25:19,849
computers would absolutely at two

1805
01:25:16,130 --> 01:25:22,489
years ago and now like not quite as good

1806
01:25:19,850 --> 01:25:25,190
of people and then next year they'll be

1807
01:25:22,488 --> 01:25:28,218
much better than people yeah two

1808
01:25:25,189 --> 01:25:32,379
questions one what is your ratio of

1809
01:25:28,219 --> 01:25:32,380
paper reading versus coding in a week

1810
01:25:33,429 --> 01:25:39,020
gosh what do you think Rachel you say me

1811
01:25:36,439 --> 01:25:40,250
I mean it's a lot more coding right it's

1812
01:25:39,020 --> 01:25:42,230
a lot more coding I feel like it also

1813
01:25:40,250 --> 01:25:44,750
really varies from week to week like I

1814
01:25:42,229 --> 01:25:48,589
feel like they're like with that

1815
01:25:44,750 --> 01:25:50,119
bounding box stuff you know that like

1816
01:25:48,590 --> 01:25:53,270
with that bounding box stuff there was

1817
01:25:50,119 --> 01:25:55,130
all these papers and no man through

1818
01:25:53,270 --> 01:25:56,630
and so I didn't even know which one to

1819
01:25:55,130 --> 01:25:59,270
read first and then I'd read the

1820
01:25:56,630 --> 01:26:01,369
citations and didn't understand any of

1821
01:25:59,270 --> 01:26:02,990
them and so there was a few weeks of

1822
01:26:01,369 --> 01:26:06,340
just kind of reading papers before I

1823
01:26:02,989 --> 01:26:09,079
even know what to start coding

1824
01:26:06,340 --> 01:26:10,789
that's unusual though like most of the

1825
01:26:09,079 --> 01:26:12,739
time I'm yeah I don't know

1826
01:26:10,789 --> 01:26:14,750
anytime I start reading a paper I'm

1827
01:26:12,739 --> 01:26:16,809
always convinced that I'm not smart

1828
01:26:14,750 --> 01:26:20,300
enough to understand it always

1829
01:26:16,810 --> 01:26:24,710
regardless of the paper and somehow

1830
01:26:20,300 --> 01:26:25,789
eventually I do but yeah I try to spend

1831
01:26:24,710 --> 01:26:29,359
as much time as I can

1832
01:26:25,789 --> 01:26:31,340
coding and then the second question is

1833
01:26:29,359 --> 01:26:33,739
your dropout rate the same through the

1834
01:26:31,340 --> 01:26:36,110
training or do you adjust it and the

1835
01:26:33,739 --> 01:26:39,079
weights accordingly I just say one more

1836
01:26:36,109 --> 01:26:42,170
thing about the last bit which is very

1837
01:26:39,079 --> 01:26:48,409
often like the vast majority nearly

1838
01:26:42,170 --> 01:26:49,819
always I after I've read a paper even

1839
01:26:48,409 --> 01:26:52,130
after I've read the bit that says this

1840
01:26:49,819 --> 01:26:53,599
is the problem I'm trying to solve I'll

1841
01:26:52,130 --> 01:26:55,190
kind of stop there and try to implement

1842
01:26:53,600 --> 01:26:56,990
something but I think might solve that

1843
01:26:55,189 --> 01:26:58,519
problem and then I'll go back and read

1844
01:26:56,989 --> 01:26:59,719
the paper and I read little bits about

1845
01:26:58,520 --> 01:27:01,880
like all these are how I solve these

1846
01:26:59,720 --> 01:27:02,960
problem bits and I'll be like oh that's

1847
01:27:01,880 --> 01:27:04,779
a good idea and then I'll try to

1848
01:27:02,960 --> 01:27:07,939
implement those and so like that's why

1849
01:27:04,779 --> 01:27:11,029
for example I didn't actually implement

1850
01:27:07,939 --> 01:27:13,819
SSD you know like my custom here is not

1851
01:27:11,029 --> 01:27:15,529
the same it's because I kind of read the

1852
01:27:13,819 --> 01:27:18,079
gist of it and then I tried to create

1853
01:27:15,529 --> 01:27:19,429
something best as I cord and then go

1854
01:27:18,079 --> 01:27:22,039
back to the papers and try to see why

1855
01:27:19,430 --> 01:27:25,070
and so so by the time I got to the focal

1856
01:27:22,039 --> 01:27:27,829
most paper Rachael will tell you I was

1857
01:27:25,069 --> 01:27:29,989
like driving myself crazy with like how

1858
01:27:27,829 --> 01:27:31,850
come I can't find small objects how come

1859
01:27:29,989 --> 01:27:33,920
it's always predicting background you

1860
01:27:31,850 --> 01:27:37,490
know I read the focal loss paper and I

1861
01:27:33,920 --> 01:27:41,480
was like that's why you know so like

1862
01:27:37,489 --> 01:27:42,829
it's so much better when you deeply

1863
01:27:41,479 --> 01:27:44,959
understand the problem they're trying to

1864
01:27:42,829 --> 01:27:46,850
solve and I do find the vast majority of

1865
01:27:44,960 --> 01:27:48,260
the time by the time I read that bit of

1866
01:27:46,850 --> 01:27:51,110
the paper which is like solving a

1867
01:27:48,260 --> 01:27:53,390
problem I'm then like yeah but these

1868
01:27:51,109 --> 01:27:55,519
three ideas I came up with they didn't

1869
01:27:53,390 --> 01:27:57,350
try you know and he suddenly realized

1870
01:27:55,520 --> 01:28:00,200
that you've got new ideas or else if you

1871
01:27:57,350 --> 01:28:03,380
just implement the paper you know

1872
01:28:00,199 --> 01:28:05,300
mindlessly it's you know you tend not to

1873
01:28:03,380 --> 01:28:06,630
have these insights about better ways to

1874
01:28:05,300 --> 01:28:10,900
do it

1875
01:28:06,630 --> 01:28:13,060
yeah um varying drop out is really

1876
01:28:10,899 --> 01:28:17,229
interesting and there are some recent

1877
01:28:13,060 --> 01:28:20,440
papers actually that suggest gradually

1878
01:28:17,229 --> 01:28:21,969
changing drop out and it was either a

1879
01:28:20,439 --> 01:28:24,639
good idea to gradually make it smaller

1880
01:28:21,970 --> 01:28:29,170
or gradually make it bigger I'm not sure

1881
01:28:24,640 --> 01:28:30,910
which let's try maybe one of us can try

1882
01:28:29,170 --> 01:28:33,699
and find it during the week I haven't

1883
01:28:30,909 --> 01:28:35,710
seen it widely used I tried it a little

1884
01:28:33,699 --> 01:28:40,809
bit with the most recent paper I wrote

1885
01:28:35,710 --> 01:28:44,640
and it I had some good results I think I

1886
01:28:40,810 --> 01:28:47,980
was graduating like you get smaller yeah

1887
01:28:44,640 --> 01:28:49,270
and then the next question is am I

1888
01:28:47,979 --> 01:28:51,339
correct in thinking that this language

1889
01:28:49,270 --> 01:28:53,050
model is built on word embeddings would

1890
01:28:51,340 --> 01:28:58,449
it be valuable to try this with phrase

1891
01:28:53,050 --> 01:29:00,220
or sentence embeddings I ask that I ask

1892
01:28:58,449 --> 01:29:03,010
this because I saw from Google the other

1893
01:29:00,220 --> 01:29:04,510
day universal sentence encoder yeah no

1894
01:29:03,010 --> 01:29:05,800
this is like this is much better than

1895
01:29:04,510 --> 01:29:07,659
that you like to shoot I mean like it

1896
01:29:05,800 --> 01:29:10,360
this is this is not just an embedding of

1897
01:29:07,659 --> 01:29:13,059
a sentence this is an entire model right

1898
01:29:10,359 --> 01:29:18,179
so an embedding by definition is like a

1899
01:29:13,060 --> 01:29:20,740
fixed thing oh I think they're asking

1900
01:29:18,180 --> 01:29:22,390
they're saying that this language well i

1901
01:29:20,739 --> 01:29:25,000
the first question is is this language

1902
01:29:22,390 --> 01:29:28,180
model built on word embedded yes but but

1903
01:29:25,000 --> 01:29:32,260
it's not saying it's a sentence or a

1904
01:29:28,180 --> 01:29:35,400
phrase embedding is always a model that

1905
01:29:32,260 --> 01:29:37,630
creates that right and we've got a model

1906
01:29:35,399 --> 01:29:39,849
that's like trying to understand

1907
01:29:37,630 --> 01:29:42,640
language it's not just as phrase it's

1908
01:29:39,850 --> 01:29:44,350
not just a sentence you know it's a it's

1909
01:29:42,640 --> 01:29:45,730
a document in the end and it's not just

1910
01:29:44,350 --> 01:29:48,160
an embedding that's we're training

1911
01:29:45,729 --> 01:29:51,189
through the whole thing so like this has

1912
01:29:48,159 --> 01:29:53,769
been a huge problem with NLP for years

1913
01:29:51,189 --> 01:29:56,409
now is this attachment they have to

1914
01:29:53,770 --> 01:29:58,690
embeddings and so even the the paper

1915
01:29:56,409 --> 01:30:01,149
that the community's been most excited

1916
01:29:58,689 --> 01:30:05,289
about recently from the from AI to the

1917
01:30:01,149 --> 01:30:07,659
Allen Institute called Elmo yo mo and

1918
01:30:05,289 --> 01:30:08,920
they found much better results across

1919
01:30:07,659 --> 01:30:11,949
lots of models but again it was an

1920
01:30:08,920 --> 01:30:14,920
embedding they took a fixed model and

1921
01:30:11,949 --> 01:30:18,010
created a fixed set of numbers which

1922
01:30:14,920 --> 01:30:19,600
they then fed into a model but in in

1923
01:30:18,010 --> 01:30:22,949
computer vision we've known for

1924
01:30:19,600 --> 01:30:25,120
is that that approach of having a fixed

1925
01:30:22,949 --> 01:30:27,250
fixed set of features

1926
01:30:25,119 --> 01:30:29,199
they're called hyper columns in in

1927
01:30:27,250 --> 01:30:30,869
computer vision people stopped using

1928
01:30:29,199 --> 01:30:34,779
them like three or four years ago

1929
01:30:30,869 --> 01:30:38,590
because fine-tuning the entire model

1930
01:30:34,779 --> 01:30:39,880
works much better right so for those of

1931
01:30:38,590 --> 01:30:42,489
you that have spent quite a lot of time

1932
01:30:39,880 --> 01:30:43,890
with NLP and not much time with computer

1933
01:30:42,489 --> 01:30:47,019
vision you're going to have to start

1934
01:30:43,890 --> 01:30:49,869
relearning right all that stuff you have

1935
01:30:47,020 --> 01:30:52,360
been told about this idea that there are

1936
01:30:49,869 --> 01:30:55,300
these things called embeddings and that

1937
01:30:52,359 --> 01:30:58,299
you learn them ahead of time and then

1938
01:30:55,300 --> 01:31:01,449
you apply these things whether it be

1939
01:30:58,300 --> 01:31:04,690
word level or phrase level or whatever

1940
01:31:01,449 --> 01:31:06,939
level don't do that all right you want

1941
01:31:04,689 --> 01:31:09,909
to actually create a pre trained model

1942
01:31:06,939 --> 01:31:12,669
and fine-tune it and to it then you'll

1943
01:31:09,909 --> 01:31:20,439
see some you'll see some specific

1944
01:31:12,670 --> 01:31:23,230
results all right so as you answer the

1945
01:31:20,439 --> 01:31:25,419
existing lines for using accuracy

1946
01:31:23,229 --> 01:31:27,309
instead of perplexity as a metric for

1947
01:31:25,420 --> 01:31:29,380
the model could we work that into the

1948
01:31:27,310 --> 01:31:31,480
loss function rather than just use it as

1949
01:31:29,380 --> 01:31:33,550
a metric no you never want to do that

1950
01:31:31,479 --> 01:31:37,959
whether it be computer vision or NLP or

1951
01:31:33,550 --> 01:31:40,029
whatever it's too bumpy right so first

1952
01:31:37,960 --> 01:31:41,590
have to be spying is a loss function and

1953
01:31:40,029 --> 01:31:43,449
other thing instead of I use it in

1954
01:31:41,590 --> 01:31:45,460
addition to you know I think it's good

1955
01:31:43,449 --> 01:31:47,920
to look at the accuracy and to look at

1956
01:31:45,460 --> 01:31:50,380
the cross entropy but for your loss

1957
01:31:47,920 --> 01:31:54,329
function you need something nice and

1958
01:31:50,380 --> 01:31:54,329
smooth accuracy doesn't work very well

1959
01:31:54,449 --> 01:31:58,269
you'll see there's two different

1960
01:31:56,109 --> 01:32:01,059
versions of save is save and save

1961
01:31:58,270 --> 01:32:09,670
encoder save saves the whole model as

1962
01:32:01,060 --> 01:32:11,560
per usual save encoder saves just that

1963
01:32:09,670 --> 01:32:13,659
bit right in other words in the

1964
01:32:11,560 --> 01:32:16,630
sequential model it saves just that bit

1965
01:32:13,659 --> 01:32:19,029
and not that bit in other words you know

1966
01:32:16,630 --> 01:32:21,220
this bit which is the bit that actually

1967
01:32:19,029 --> 01:32:23,949
makes it into a language model we don't

1968
01:32:21,220 --> 01:32:27,460
care about the classifier we just care

1969
01:32:23,949 --> 01:32:30,059
about that bit okay so that's why we

1970
01:32:27,460 --> 01:32:32,739
save two different models here

1971
01:32:30,060 --> 01:32:34,330
okay so let's now create the classifier

1972
01:32:32,739 --> 01:32:35,619
okay and I'm going to go through this

1973
01:32:34,329 --> 01:32:37,750
bit pretty quickly because it's the same

1974
01:32:35,619 --> 01:32:39,010
but like when you go back during the

1975
01:32:37,750 --> 01:32:41,140
week and look at the code convince

1976
01:32:39,010 --> 01:32:43,659
yourself it's the same right we do get

1977
01:32:41,140 --> 01:32:48,700
all P tiresias P again Chuck sighs again

1978
01:32:43,659 --> 01:32:52,569
get all again save those tokens again we

1979
01:32:48,699 --> 01:32:53,679
don't create a new I to s vocabulary we

1980
01:32:52,569 --> 01:32:55,649
obviously want to use the same

1981
01:32:53,680 --> 01:32:58,570
vocabulary we had in the language model

1982
01:32:55,649 --> 01:33:03,359
okay too because we're about to reload

1983
01:32:58,569 --> 01:33:06,759
the same encoder okay same default dict

1984
01:33:03,359 --> 01:33:09,429
same way of creating our Americanized

1985
01:33:06,760 --> 01:33:12,940
list which as per before we can save

1986
01:33:09,430 --> 01:33:14,530
okay so that's all the same later on we

1987
01:33:12,939 --> 01:33:19,509
can reload those rather than having to

1988
01:33:14,529 --> 01:33:22,300
rebuild them so all of our hacker

1989
01:33:19,510 --> 01:33:24,220
parameters are the same we're not all of

1990
01:33:22,300 --> 01:33:25,630
them sorry the construction of the modal

1991
01:33:24,220 --> 01:33:30,180
hyper parameters are the same we can

1992
01:33:25,630 --> 01:33:33,100
change the drop hat optimize a function

1993
01:33:30,180 --> 01:33:37,380
pick a batch size that is as big as you

1994
01:33:33,100 --> 01:33:41,500
can that doesn't run out of memory and

1995
01:33:37,380 --> 01:33:48,760
so this bits a bit interesting there's

1996
01:33:41,500 --> 01:33:52,840
some fun stuff going on here the basic

1997
01:33:48,760 --> 01:33:54,400
idea here is that for the classifier we

1998
01:33:52,840 --> 01:33:56,140
do really want to look at one you know

1999
01:33:54,399 --> 01:33:58,059
our document right we need to say is

2000
01:33:56,140 --> 01:34:00,910
this document positive or negative and

2001
01:33:58,060 --> 01:34:02,920
so we do want to shuffle the documents

2002
01:34:00,909 --> 01:34:08,139
right that's because we we like to

2003
01:34:02,920 --> 01:34:10,329
shuffle things but those documents are

2004
01:34:08,140 --> 01:34:12,160
different lengths and so if we stick

2005
01:34:10,329 --> 01:34:14,019
them all into one batch and this is a

2006
01:34:12,159 --> 01:34:15,099
handy thing that last AI does for you

2007
01:34:14,020 --> 01:34:16,690
you can stick things of different

2008
01:34:15,100 --> 01:34:18,340
lengths into a batch and it will

2009
01:34:16,689 --> 01:34:21,219
automatically pet them so you don't have

2010
01:34:18,340 --> 01:34:23,409
to worry about that okay but if they're

2011
01:34:21,220 --> 01:34:24,760
wildly different lengths then you're

2012
01:34:23,409 --> 01:34:26,500
going to be wasting a lot of computation

2013
01:34:24,760 --> 01:34:28,539
times then what you one thing there

2014
01:34:26,500 --> 01:34:30,670
that's 2,000 words long and everything

2015
01:34:28,539 --> 01:34:33,489
else 250 words long and that means you

2016
01:34:30,670 --> 01:34:35,039
end up with a 2005 tensor all right

2017
01:34:33,489 --> 01:34:38,500
that's pretty annoying

2018
01:34:35,039 --> 01:34:41,050
so James Bradbury who's actually one of

2019
01:34:38,500 --> 01:34:42,970
Stephen Rarity's colleagues and the guy

2020
01:34:41,050 --> 01:34:47,350
who came up with torch text

2021
01:34:42,970 --> 01:34:55,180
came up with a neat idea which was let's

2022
01:34:47,350 --> 01:34:58,390
sort the data set by length ish right so

2023
01:34:55,180 --> 01:35:03,220
kind of make it so the first things in

2024
01:34:58,390 --> 01:35:06,070
the list on the whole shorter and the

2025
01:35:03,220 --> 01:35:10,930
things at the end but a little bit

2026
01:35:06,069 --> 01:35:15,369
random as well okay and so I'll show you

2027
01:35:10,930 --> 01:35:20,350
how I implemented that so the first

2028
01:35:15,369 --> 01:35:23,680
thing we need is a data set right and so

2029
01:35:20,350 --> 01:35:26,650
we have a data set passing in the the

2030
01:35:23,680 --> 01:35:28,690
documents and their labels and so here's

2031
01:35:26,649 --> 01:35:32,139
a text data set and it inherits from

2032
01:35:28,689 --> 01:35:35,889
data data set here is data set from

2033
01:35:32,140 --> 01:35:38,260
torch from PI torch and actually data

2034
01:35:35,890 --> 01:35:39,760
set doesn't do anything at all it says

2035
01:35:38,260 --> 01:35:41,409
you need to get item and if you don't

2036
01:35:39,760 --> 01:35:42,880
have one you're gonna get an error

2037
01:35:41,409 --> 01:35:44,829
you need a length if you don't have one

2038
01:35:42,880 --> 01:35:49,449
you're gonna get an error okay so this

2039
01:35:44,829 --> 01:35:52,420
is an abstract class so we're going to

2040
01:35:49,449 --> 01:35:55,269
pass in our X we're going to pass in our

2041
01:35:52,420 --> 01:36:00,069
Y and get item is going to grab the X

2042
01:35:55,270 --> 01:36:01,350
and grab the Y and return them it

2043
01:36:00,069 --> 01:36:03,729
couldn't be much simpler right

2044
01:36:01,350 --> 01:36:05,320
optionally they could reverse it

2045
01:36:03,729 --> 01:36:07,059
optionally it could stick an end or

2046
01:36:05,319 --> 01:36:08,349
stream at the end optionally it started

2047
01:36:07,060 --> 01:36:10,420
beginning we're not doing any of those

2048
01:36:08,350 --> 01:36:12,130
things so literally all we're doing is

2049
01:36:10,420 --> 01:36:14,050
we're putting in an X putting in a way

2050
01:36:12,130 --> 01:36:15,840
and that grab an item we're returning

2051
01:36:14,050 --> 01:36:21,279
the X and the y as a couple

2052
01:36:15,840 --> 01:36:23,319
okay and the length is X yes so that

2053
01:36:21,279 --> 01:36:25,719
that's that's all the data sets right

2054
01:36:23,319 --> 01:36:29,199
something with a length that you can in

2055
01:36:25,720 --> 01:36:32,650
days so to turn it into a data loader

2056
01:36:29,199 --> 01:36:35,079
you simply pass the data set to the data

2057
01:36:32,649 --> 01:36:37,420
loader constructor and it's now going to

2058
01:36:35,079 --> 01:36:40,180
go ahead and give you a batch of that at

2059
01:36:37,420 --> 01:36:41,739
a time normally you can say shuffle

2060
01:36:40,180 --> 01:36:43,539
equals true or shuffle equals false

2061
01:36:41,739 --> 01:36:47,050
it'll decide whether to randomize it for

2062
01:36:43,539 --> 01:36:49,869
you in this case though we're actually

2063
01:36:47,050 --> 01:36:51,880
going to pass in a sample up perimeter

2064
01:36:49,869 --> 01:36:56,010
and the sampler is a class we're going

2065
01:36:51,880 --> 01:36:59,430
to define that tells the data loader

2066
01:36:56,010 --> 01:37:01,440
shuffle okay so for the validation set

2067
01:36:59,430 --> 01:37:03,360
we're going to define something that

2068
01:37:01,439 --> 01:37:06,210
actually just sorts right it just

2069
01:37:03,359 --> 01:37:08,009
deterministically sorts it so the that

2070
01:37:06,210 --> 01:37:10,020
all the shortest documents will be at

2071
01:37:08,010 --> 01:37:11,430
the start or the longest documents will

2072
01:37:10,020 --> 01:37:15,390
be at the end and that's going to

2073
01:37:11,430 --> 01:37:17,010
minimize the amount of padding okay for

2074
01:37:15,390 --> 01:37:19,310
the training sampler we're going to

2075
01:37:17,010 --> 01:37:27,539
create this thing I called a sort H

2076
01:37:19,310 --> 01:37:30,000
sampler which also sorts ish right so

2077
01:37:27,539 --> 01:37:31,649
this is where like I really like hi

2078
01:37:30,000 --> 01:37:33,869
torch is that they came up with this

2079
01:37:31,649 --> 01:37:37,799
idea for an API for their data loader

2080
01:37:33,869 --> 01:37:39,180
where we can like hook in new classes to

2081
01:37:37,800 --> 01:37:43,529
make it behave in different ways right

2082
01:37:39,180 --> 01:37:45,320
so here's a sort sampler but is simply

2083
01:37:43,529 --> 01:37:47,609
something which again it has a length

2084
01:37:45,319 --> 01:37:49,739
which is the length of the data source

2085
01:37:47,609 --> 01:37:52,619
and it has an iterator

2086
01:37:49,739 --> 01:37:59,130
which is simply an iterator which goes

2087
01:37:52,619 --> 01:38:03,140
through the data source sorted by length

2088
01:37:59,130 --> 01:38:07,010
well the key and I pass in as the key a

2089
01:38:03,140 --> 01:38:13,980
lambda function which returns the links

2090
01:38:07,010 --> 01:38:15,510
okay and so for the sort ish sampler I

2091
01:38:13,979 --> 01:38:17,489
won't go through the details but it

2092
01:38:15,510 --> 01:38:21,630
basically does the same thing with a

2093
01:38:17,489 --> 01:38:23,969
little bit of randomness okay so it's a

2094
01:38:21,630 --> 01:38:26,190
really beautiful you know just another

2095
01:38:23,970 --> 01:38:29,490
of these beautiful little design things

2096
01:38:26,189 --> 01:38:32,099
in pi torch that I discovered I could

2097
01:38:29,489 --> 01:38:35,099
take James Bradbury's ideas which he had

2098
01:38:32,100 --> 01:38:37,680
like written a whole new set of classes

2099
01:38:35,100 --> 01:38:42,510
around and I could actually just use the

2100
01:38:37,680 --> 01:38:45,690
inbuilt hooks inside Python you will

2101
01:38:42,510 --> 01:38:47,010
notice data loader it's not actually PI

2102
01:38:45,689 --> 01:38:48,659
tortoise data loader it's actually

2103
01:38:47,010 --> 01:38:50,909
faster ice data loader but it's

2104
01:38:48,659 --> 01:38:54,000
basically almost entirely plagiarized

2105
01:38:50,909 --> 01:38:56,099
from PI torch but customized in some

2106
01:38:54,000 --> 01:38:57,930
ways to make it faster mainly by using

2107
01:38:56,100 --> 01:39:00,539
multi-threading instead of multi

2108
01:38:57,930 --> 01:39:04,320
processing that's Rachel does the

2109
01:39:00,539 --> 01:39:06,359
pre-trained LS TM depth it'd be bt te to

2110
01:39:04,319 --> 01:39:08,670
match with the new one we're training no

2111
01:39:06,359 --> 01:39:11,519
no that the be PTT doesn't need to match

2112
01:39:08,670 --> 01:39:13,920
that's just like how many things do we

2113
01:39:11,520 --> 01:39:17,940
look at at a time it's about nothing to

2114
01:39:13,920 --> 01:39:19,380
do with the architecture okay so well

2115
01:39:17,939 --> 01:39:20,939
now we can call that function we just

2116
01:39:19,380 --> 01:39:23,039
wore before get our inane classifier

2117
01:39:20,939 --> 01:39:26,609
it's going to create exactly the same

2118
01:39:23,039 --> 01:39:28,800
encoder more or less okay and we're

2119
01:39:26,609 --> 01:39:33,079
going to pass in the same architectural

2120
01:39:28,800 --> 01:39:36,900
details as before but this time we can

2121
01:39:33,079 --> 01:39:38,670
the head that we add on you've got a few

2122
01:39:36,899 --> 01:39:40,399
more things you can do one is you can

2123
01:39:38,670 --> 01:39:44,279
you can add more than one hidden layer

2124
01:39:40,399 --> 01:39:48,389
so this layers here says this is what

2125
01:39:44,279 --> 01:39:51,239
the input to my classifier section my

2126
01:39:48,390 --> 01:39:53,940
head is going to be this is the output

2127
01:39:51,239 --> 01:39:55,710
of the first layer this is the output of

2128
01:39:53,939 --> 01:39:57,629
the second layer and you can add as many

2129
01:39:55,710 --> 01:39:59,699
as you like so you can basically create

2130
01:39:57,630 --> 01:40:02,670
a little multi-layer neural net

2131
01:39:59,699 --> 01:40:05,130
classifier at the end and so did oh

2132
01:40:02,670 --> 01:40:08,609
these are the dropouts to go after each

2133
01:40:05,130 --> 01:40:11,609
of these layers okay and then here are

2134
01:40:08,609 --> 01:40:13,799
all of the AWD lsdm dropouts which we're

2135
01:40:11,609 --> 01:40:17,670
going to basically plagiarize that idea

2136
01:40:13,800 --> 01:40:20,750
for our classifier we're going to use

2137
01:40:17,670 --> 01:40:23,699
the errand in learn oh just like before

2138
01:40:20,750 --> 01:40:26,279
we're going to use discriminative

2139
01:40:23,699 --> 01:40:30,239
learning rates different layers actually

2140
01:40:26,279 --> 01:40:31,979
this is a separate here you can try

2141
01:40:30,239 --> 01:40:33,389
using wait okay or not I've been

2142
01:40:31,979 --> 01:40:36,929
fiddling around a bit with that to see

2143
01:40:33,390 --> 01:40:39,450
what happens and so we start out just

2144
01:40:36,930 --> 01:40:41,990
training the last layer and we get

2145
01:40:39,449 --> 01:40:44,970
ninety two point nine percent accuracy

2146
01:40:41,989 --> 01:40:46,409
then we freeze one more layer unfreeze

2147
01:40:44,970 --> 01:40:48,900
one more layer get ninety three point

2148
01:40:46,409 --> 01:40:54,590
three accuracy and then we fine-tune the

2149
01:40:48,899 --> 01:40:54,589
whole thing and after three epochs

2150
01:40:58,909 --> 01:41:06,510
okay so this was so here is the famous

2151
01:41:03,750 --> 01:41:09,270
James pepperi we're talking about this

2152
01:41:06,510 --> 01:41:13,050
was kind of the main attempt before our

2153
01:41:09,270 --> 01:41:14,220
paper came along at using a pre train

2154
01:41:13,050 --> 01:41:15,779
model

2155
01:41:14,220 --> 01:41:22,039
and what they did is they used a pre

2156
01:41:15,779 --> 01:41:25,529
trained translation model but they

2157
01:41:22,039 --> 01:41:28,579
didn't fine tune the whole thing they

2158
01:41:25,529 --> 01:41:34,949
just took the the activations of the

2159
01:41:28,579 --> 01:41:49,890
translation model and when they tried

2160
01:41:34,949 --> 01:41:52,199
IMDB they got they got 91.8% which we

2161
01:41:49,890 --> 01:41:55,640
beat easily after only fine-tuning one

2162
01:41:52,199 --> 01:41:55,639
layer okay

2163
01:41:56,399 --> 01:42:01,799
they weren't state-of-the-art they're

2164
01:41:57,659 --> 01:42:04,889
the state of the art is 94.1 which we

2165
01:42:01,800 --> 01:42:08,279
beat after fine-tuning the whole thing

2166
01:42:04,890 --> 01:42:12,410
for three epochs and so by the end we're

2167
01:42:08,279 --> 01:42:15,059
at ninety four point eight which is

2168
01:42:12,409 --> 01:42:16,559
obviously a huge difference because like

2169
01:42:15,060 --> 01:42:19,650
in terms of error rate that's gone down

2170
01:42:16,560 --> 01:42:22,500
from from five point nine and then I'll

2171
01:42:19,649 --> 01:42:25,729
tell you a simple little trick is go

2172
01:42:22,500 --> 01:42:28,710
back to the start of this notebook and

2173
01:42:25,729 --> 01:42:32,339
reverse the order of all of the

2174
01:42:28,710 --> 01:42:35,220
documents and then rerun the whole thing

2175
01:42:32,340 --> 01:42:41,670
right and when you get to the bit that

2176
01:42:35,220 --> 01:42:43,980
says WT 103 replace this FWD before word

2177
01:42:41,670 --> 01:42:46,500
with PWD for backward

2178
01:42:43,979 --> 01:42:49,829
that's a backward english-language model

2179
01:42:46,500 --> 01:42:52,470
that learns to read English backwards so

2180
01:42:49,829 --> 01:42:54,988
if you redo this whole thing put all the

2181
01:42:52,470 --> 01:42:56,909
documents in Reverse and change this to

2182
01:42:54,988 --> 01:42:59,519
backward you now have a second

2183
01:42:56,909 --> 01:43:02,460
classifier which classifiers things by

2184
01:42:59,520 --> 01:43:05,670
positive or negative sentiment based on

2185
01:43:02,460 --> 01:43:09,239
the reverse document if you then take

2186
01:43:05,670 --> 01:43:10,020
the two predictions and take the average

2187
01:43:09,238 --> 01:43:11,559
of them

2188
01:43:10,020 --> 01:43:13,570
you basically have like a

2189
01:43:11,560 --> 01:43:15,280
directional model if you've trained each

2190
01:43:13,569 --> 01:43:17,369
bit separately that gets you to

2191
01:43:15,279 --> 01:43:19,989
ninety-five point four percent accuracy

2192
01:43:17,369 --> 01:43:22,269
okay so we can replace eclis load up

2193
01:43:19,989 --> 01:43:25,300
from five point nine to four point six

2194
01:43:22,270 --> 01:43:28,990
so this kind of like twenty percent

2195
01:43:25,300 --> 01:43:30,850
change in the state-of-the-art is it's

2196
01:43:28,989 --> 01:43:32,550
like it's almost unheard of you know

2197
01:43:30,850 --> 01:43:35,890
it's like you have to go back to like

2198
01:43:32,550 --> 01:43:37,960
Jeffrey Hinton's imagenet computer

2199
01:43:35,890 --> 01:43:39,460
vision thing where they drop like thirty

2200
01:43:37,960 --> 01:43:41,319
percent off the state-of-the-art like it

2201
01:43:39,460 --> 01:43:45,430
doesn't happen very often and so you can

2202
01:43:41,319 --> 01:43:50,170
see this idea of like just use transfer

2203
01:43:45,430 --> 01:43:53,079
learning it's ridiculously powerful that

2204
01:43:50,170 --> 01:43:57,130
every new feel thinks their new field is

2205
01:43:53,079 --> 01:44:03,069
too special and you can't do it right so

2206
01:43:57,130 --> 01:44:05,079
it's a big opportunity for all of us so

2207
01:44:03,069 --> 01:44:07,479
we turn this into a paper and when I say

2208
01:44:05,079 --> 01:44:09,760
we I did it with this guy Sebastian

2209
01:44:07,479 --> 01:44:13,419
Trudeau now you might remember his name

2210
01:44:09,760 --> 01:44:15,369
because in lesson five I told you that I

2211
01:44:13,420 --> 01:44:17,829
actually had shared lesson four with

2212
01:44:15,369 --> 01:44:19,960
Sebastian because I think he's a an

2213
01:44:17,829 --> 01:44:22,029
awesome researcher who I thought might

2214
01:44:19,960 --> 01:44:26,890
like it I didn't know him personally at

2215
01:44:22,029 --> 01:44:29,399
all and much to my surprise he actually

2216
01:44:26,890 --> 01:44:31,990
watched the damn video I was like what

2217
01:44:29,399 --> 01:44:33,969
you know what an LP research is gonna

2218
01:44:31,989 --> 01:44:35,460
watch some beginners video but he

2219
01:44:33,970 --> 01:44:37,900
watched the whole video he was like

2220
01:44:35,460 --> 01:44:39,550
that's actually quite fantastic that's

2221
01:44:37,899 --> 01:44:43,750
like fun thank you very much that's

2222
01:44:39,550 --> 01:44:46,869
awesome coming from you and he said hey

2223
01:44:43,750 --> 01:44:49,510
we should turn this into a paper and I

2224
01:44:46,869 --> 01:44:50,859
said I don't write papers I don't care

2225
01:44:49,510 --> 01:44:55,180
about papers are not interested in

2226
01:44:50,859 --> 01:44:58,119
papers that sounds really boring and he

2227
01:44:55,180 --> 01:45:03,190
said okay how about I write the paper

2228
01:44:58,119 --> 01:45:04,510
for you and I said you can't really

2229
01:45:03,189 --> 01:45:06,219
write a paper about this yet because

2230
01:45:04,510 --> 01:45:07,360
you'd have to do like studies to compare

2231
01:45:06,220 --> 01:45:08,740
it to other things they're called

2232
01:45:07,359 --> 01:45:10,960
oblation studies to see which if it's

2233
01:45:08,739 --> 01:45:12,250
actually work like there's no rigor here

2234
01:45:10,960 --> 01:45:14,439
I just put in everything that came in my

2235
01:45:12,250 --> 01:45:17,020
head and checked it all together and it

2236
01:45:14,439 --> 01:45:18,519
happened to work it's like okay what if

2237
01:45:17,020 --> 01:45:20,470
I write all the paper and all the

2238
01:45:18,520 --> 01:45:23,020
oblation studies then can we read the

2239
01:45:20,470 --> 01:45:25,449
paper and I said well

2240
01:45:23,020 --> 01:45:29,170
it's like a whole library that like I

2241
01:45:25,449 --> 01:45:31,149
haven't documented and like you know I'm

2242
01:45:29,170 --> 01:45:33,579
not going to yet and like you don't know

2243
01:45:31,149 --> 01:45:35,139
how it all works he said okay if I wrote

2244
01:45:33,579 --> 01:45:36,909
the paper and do the ablation studies

2245
01:45:35,140 --> 01:45:38,650
and figure out from scratch how the code

2246
01:45:36,909 --> 01:45:44,079
works without bothering you then can we

2247
01:45:38,649 --> 01:45:48,129
write the paper I was like yeah if you

2248
01:45:44,079 --> 01:45:50,500
did a whole thing the paper I was like

2249
01:45:48,130 --> 01:45:52,000
okay and so then two days later he comes

2250
01:45:50,500 --> 01:45:56,289
back he says okay I've done a draft with

2251
01:45:52,000 --> 01:46:00,970
a paper so I share this story to say

2252
01:45:56,289 --> 01:46:03,850
like if you're some you know student in

2253
01:46:00,970 --> 01:46:07,090
Ireland and you want to like do good

2254
01:46:03,850 --> 01:46:12,760
work don't did anybody stop you right I

2255
01:46:07,090 --> 01:46:15,310
did not encourage him at least right but

2256
01:46:12,760 --> 01:46:16,659
in the end it's like look I want to do

2257
01:46:15,310 --> 01:46:20,320
this work I think it's going to be good

2258
01:46:16,659 --> 01:46:22,090
and I'll figure it out and you know he

2259
01:46:20,319 --> 01:46:23,679
wrote a fantastic paper and he did the

2260
01:46:22,090 --> 01:46:26,860
ablation studies and he figured out how

2261
01:46:23,680 --> 01:46:29,890
fast I works and now we're planning to

2262
01:46:26,859 --> 01:46:32,049
write another paper together and so like

2263
01:46:29,890 --> 01:46:33,520
there's some you've got to be a bit

2264
01:46:32,050 --> 01:46:36,970
careful right cuz sometimes I get

2265
01:46:33,520 --> 01:46:39,430
messages from random people saying like

2266
01:46:36,970 --> 01:46:42,430
I've got lots of good ideas can we have

2267
01:46:39,430 --> 01:46:44,590
coffee I don't want to I don't you know

2268
01:46:42,430 --> 01:46:46,090
I can have coffee in my office any time

2269
01:46:44,590 --> 01:46:48,789
thank you

2270
01:46:46,090 --> 01:46:50,440
but it's very different to say like hey

2271
01:46:48,789 --> 01:46:52,269
I took your ideas and I wrote a paper

2272
01:46:50,439 --> 01:46:53,529
and I did a bunch of experiments and I

2273
01:46:52,270 --> 01:46:57,940
figured out how your code works they

2274
01:46:53,529 --> 01:47:00,009
added documentation to it you know sure

2275
01:46:57,939 --> 01:47:03,339
we could all submit this to a conference

2276
01:47:00,010 --> 01:47:06,850
you sort of mean like there's nothing to

2277
01:47:03,340 --> 01:47:08,710
stop you doing amazing work and if you

2278
01:47:06,850 --> 01:47:11,950
do amazing work that like helps somebody

2279
01:47:08,710 --> 01:47:14,829
else like in this case okay I'm happy

2280
01:47:11,949 --> 01:47:17,380
that we have a paper I don't care about

2281
01:47:14,829 --> 01:47:19,239
papers but I think it's cool that you

2282
01:47:17,380 --> 01:47:21,690
know these ideas now have this rigorous

2283
01:47:19,239 --> 01:47:26,260
study like let me show you what he did

2284
01:47:21,689 --> 01:47:27,939
so he took all my code right so I'd

2285
01:47:26,260 --> 01:47:29,860
already done all the faster I got text

2286
01:47:27,939 --> 01:47:34,449
and stuff like that and as you've seen

2287
01:47:29,859 --> 01:47:36,429
it lets us work with large corpuses so

2288
01:47:34,449 --> 01:47:38,319
you know Sebastian is fantastic

2289
01:47:36,430 --> 01:47:40,180
we well-read and he said here's a paper

2290
01:47:38,319 --> 01:47:41,949
that young lakorn some guys just came

2291
01:47:40,180 --> 01:47:44,590
out with where they tried lots of

2292
01:47:41,949 --> 01:47:46,720
different classification datasets so I'm

2293
01:47:44,590 --> 01:47:48,670
gonna try running your code on all these

2294
01:47:46,720 --> 01:47:50,980
data sets and so these are the data sets

2295
01:47:48,670 --> 01:47:53,079
right and so some of them had you know

2296
01:47:50,979 --> 01:47:54,579
many many hundreds of thousands of

2297
01:47:53,079 --> 01:47:57,550
documents and they were far bigger than

2298
01:47:54,579 --> 01:48:03,609
and I think I had tried but I thought it

2299
01:47:57,550 --> 01:48:06,220
should work okay and so you know he had

2300
01:48:03,609 --> 01:48:08,229
a few good little ideas as we went along

2301
01:48:06,220 --> 01:48:13,140
and so you should like totally make sure

2302
01:48:08,229 --> 01:48:16,179
you you read the paper write this paper

2303
01:48:13,140 --> 01:48:18,160
and he said well this thing that you

2304
01:48:16,180 --> 01:48:20,800
called in the lessons differential

2305
01:48:18,159 --> 01:48:22,659
learning rates differential kind of

2306
01:48:20,800 --> 01:48:25,150
means something else like maybe we

2307
01:48:22,659 --> 01:48:26,409
should rename it so it's now called

2308
01:48:25,149 --> 01:48:29,199
discriminative learning rate so this

2309
01:48:26,409 --> 01:48:30,220
idea that we had from part one where we

2310
01:48:29,199 --> 01:48:33,069
use different learning rates for

2311
01:48:30,220 --> 01:48:35,170
different layers after doing some

2312
01:48:33,069 --> 01:48:37,420
literature research it does seem like

2313
01:48:35,170 --> 01:48:40,180
that hasn't been done before so it's now

2314
01:48:37,420 --> 01:48:42,039
officially I think discriminative

2315
01:48:40,180 --> 01:48:43,510
learning rates and so all these ideas

2316
01:48:42,039 --> 01:48:45,970
like this is something we learnt in

2317
01:48:43,510 --> 01:48:49,630
Lesson one but it now has an equation

2318
01:48:45,970 --> 01:48:50,800
with Greek and everything okay so when

2319
01:48:49,630 --> 01:48:52,569
you see an equation with Greek and

2320
01:48:50,800 --> 01:48:54,070
everything that doesn't necessarily mean

2321
01:48:52,569 --> 01:48:56,340
it's more complex than anything we did

2322
01:48:54,069 --> 01:48:59,679
in Lesson one because this one isn't

2323
01:48:56,340 --> 01:49:02,440
again that idea of like I'm freezing a

2324
01:48:59,680 --> 01:49:04,810
layer at a time also it seems to never

2325
01:49:02,439 --> 01:49:07,119
do number four so it's now a thing and

2326
01:49:04,810 --> 01:49:13,750
it's got the very clever name gradual

2327
01:49:07,119 --> 01:49:17,409
I'm freezing so then one promised what

2328
01:49:13,750 --> 01:49:20,199
we're going to look at this slanted

2329
01:49:17,409 --> 01:49:22,180
triangular learning rates so this

2330
01:49:20,199 --> 01:49:25,569
actually was not my idea

2331
01:49:22,180 --> 01:49:27,730
Leslie Smith my favorite researchers who

2332
01:49:25,569 --> 01:49:31,210
you all now know about emailed me a

2333
01:49:27,729 --> 01:49:33,489
while ago and said I'm so over so

2334
01:49:31,210 --> 01:49:34,630
learning rates I don't do that anymore I

2335
01:49:33,489 --> 01:49:37,029
now do it's like a different version

2336
01:49:34,630 --> 01:49:38,350
where I have one cycle which goes up

2337
01:49:37,029 --> 01:49:41,590
quickly at the start and then slowly

2338
01:49:38,350 --> 01:49:43,180
down afterwards and he said I often find

2339
01:49:41,590 --> 01:49:44,860
it works better I've tried going back

2340
01:49:43,180 --> 01:49:47,699
over all of my old data sets and it

2341
01:49:44,859 --> 01:49:49,289
works better for all of them everyone

2342
01:49:47,698 --> 01:49:51,779
so this is what the learning rate looks

2343
01:49:49,289 --> 01:49:54,779
like right you can use it in fast AI

2344
01:49:51,779 --> 01:49:58,769
just by adding use CLR equals to your

2345
01:49:54,779 --> 01:50:01,019
fit this first number is the ratio

2346
01:49:58,770 --> 01:50:03,020
between the highest learning rate and

2347
01:50:01,020 --> 01:50:07,320
the lowest learning rate so here this is

2348
01:50:03,020 --> 01:50:10,770
1/32 of that the second number is the

2349
01:50:07,319 --> 01:50:13,920
ratio between the first peak and the

2350
01:50:10,770 --> 01:50:17,820
last peak and so the basic idea is if

2351
01:50:13,920 --> 01:50:19,800
you're doing a cycle length 10 that you

2352
01:50:17,819 --> 01:50:22,889
want the first cycle sorry the first

2353
01:50:19,800 --> 01:50:25,139
epoch to be the upward bit and the other

2354
01:50:22,889 --> 01:50:27,600
nine epochs to be the downward bit then

2355
01:50:25,139 --> 01:50:29,310
you would use 10 and I find that works

2356
01:50:27,600 --> 01:50:31,949
pretty well that was also Leslie

2357
01:50:29,310 --> 01:50:35,250
suggestion is make about a tenth of it

2358
01:50:31,948 --> 01:50:37,769
the upward bit and about my intense the

2359
01:50:35,250 --> 01:50:40,130
down would be since he told me about it

2360
01:50:37,770 --> 01:50:43,199
actually it was just maybe two days ago

2361
01:50:40,130 --> 01:50:45,000
he wrote this amazing paper a

2362
01:50:43,198 --> 01:50:48,178
disciplined approach to neural network

2363
01:50:45,000 --> 01:50:49,920
hyper parameters in which he describes

2364
01:50:48,179 --> 01:50:53,069
something very slightly different to

2365
01:50:49,920 --> 01:50:56,250
this again but the same basic idea this

2366
01:50:53,069 --> 01:50:59,519
is a must-read paper you know it's got

2367
01:50:56,250 --> 01:51:04,139
all the kinds of ideas that fast AI

2368
01:50:59,520 --> 01:51:06,199
talks about a lot in great depth and

2369
01:51:04,139 --> 01:51:10,230
nobody else is talking about this stuff

2370
01:51:06,198 --> 01:51:11,969
it's it's kind of a slog unfortunately

2371
01:51:10,229 --> 01:51:13,289
Leslie had to go away on a trip before

2372
01:51:11,969 --> 01:51:16,260
he really had time to edit it properly

2373
01:51:13,289 --> 01:51:17,789
so it's a little bit slow reading but

2374
01:51:16,260 --> 01:51:21,360
it's don't let that stop you it's

2375
01:51:17,789 --> 01:51:23,130
amazing so this triangle this is the

2376
01:51:21,359 --> 01:51:25,109
equation from my paper was the best year

2377
01:51:23,130 --> 01:51:27,600
and Sebastian was like Jeremy can you

2378
01:51:25,109 --> 01:51:30,299
send me the math equation behind that

2379
01:51:27,600 --> 01:51:32,520
poetry rotation if I just wrote the code

2380
01:51:30,300 --> 01:51:37,770
I could not get it into math so he

2381
01:51:32,520 --> 01:51:44,340
figured out the math for it so you might

2382
01:51:37,770 --> 01:51:48,830
have noticed the first layer of our

2383
01:51:44,340 --> 01:51:53,429
classifier was equal to embedding size x

2384
01:51:48,829 --> 01:51:54,899
3y times 3 times 3 because and again

2385
01:51:53,429 --> 01:51:59,340
this seems to be something which people

2386
01:51:54,899 --> 01:52:00,598
haven't done before so new idea concat

2387
01:51:59,340 --> 01:52:03,360
pooling

2388
01:52:00,599 --> 01:52:05,969
which is that we take the average pool

2389
01:52:03,359 --> 01:52:08,819
the average pooling over the sequence of

2390
01:52:05,969 --> 01:52:11,429
the activations the max pooling of the

2391
01:52:08,819 --> 01:52:13,380
sequence over the activations and the

2392
01:52:11,429 --> 01:52:15,779
final set of activations and just

2393
01:52:13,380 --> 01:52:18,090
concatenate them all together again this

2394
01:52:15,779 --> 01:52:20,939
is something which we talked about in

2395
01:52:18,090 --> 01:52:23,489
part one but doesn't seem to be in the

2396
01:52:20,939 --> 01:52:24,899
literature before so it's now called

2397
01:52:23,488 --> 01:52:27,209
Combe kept pulling and again it's now

2398
01:52:24,899 --> 01:52:30,649
got an equation and everything but this

2399
01:52:27,210 --> 01:52:33,349
is the entirety of the implementation

2400
01:52:30,649 --> 01:52:35,549
pull with average pull with max

2401
01:52:33,349 --> 01:52:39,599
concatenate those two along with the

2402
01:52:35,550 --> 01:52:41,429
final sequence so you know you can go

2403
01:52:39,599 --> 01:52:47,159
through this paper and see how the

2404
01:52:41,429 --> 01:52:49,440
faster I code implements each piece so

2405
01:52:47,158 --> 01:52:52,170
then to me one of the kind of

2406
01:52:49,439 --> 01:52:55,439
interesting pieces is the difference

2407
01:52:52,170 --> 01:52:58,109
between RNN and coda which you've

2408
01:52:55,439 --> 01:52:59,819
already seen and multi batch are in an

2409
01:52:58,109 --> 01:53:02,488
encoder so what's the difference there

2410
01:52:59,819 --> 01:53:04,319
okay so the key difference is that the

2411
01:53:02,488 --> 01:53:08,118
the normal RN an encoder for the

2412
01:53:04,319 --> 01:53:14,518
language model we could just do V PTT

2413
01:53:08,118 --> 01:53:18,299
chunk at a time right no problem and

2414
01:53:14,519 --> 01:53:20,460
predict the next book but for the

2415
01:53:18,300 --> 01:53:22,409
classifier we need to do the whole

2416
01:53:20,460 --> 01:53:24,300
document we need to do the whole movie

2417
01:53:22,408 --> 01:53:26,429
review before we decide if it's positive

2418
01:53:24,300 --> 01:53:28,769
or negative and the whole movie review

2419
01:53:26,429 --> 01:53:34,019
can easily be 2,000 words long and I

2420
01:53:28,769 --> 01:53:37,739
can't fit 2,000 words worth of you know

2421
01:53:34,019 --> 01:53:40,349
gradients in my GPU memory for every

2422
01:53:37,738 --> 01:53:42,868
single one of my activations well sorry

2423
01:53:40,349 --> 01:53:43,670
for every one of my weights so what do I

2424
01:53:42,868 --> 01:53:47,308
do

2425
01:53:43,670 --> 01:53:50,269
and so the idea was very simple which is

2426
01:53:47,309 --> 01:53:55,889
I go through my whole sequence length

2427
01:53:50,269 --> 01:53:59,340
one batch of BPT be PTT at a time right

2428
01:53:55,889 --> 01:54:01,710
and I call super dot forward so in other

2429
01:53:59,340 --> 01:54:03,210
words the eridan encoder all right so

2430
01:54:01,710 --> 01:54:07,319
just couldn't call the usual iron in

2431
01:54:03,210 --> 01:54:10,760
encoder to grab its outputs and then

2432
01:54:07,319 --> 01:54:13,808
I've got this maximum sequence length

2433
01:54:10,760 --> 01:54:18,070
parameter where it says okay

2434
01:54:13,809 --> 01:54:21,219
if you've as long as you're doing no

2435
01:54:18,069 --> 01:54:24,130
more than that sequence length then

2436
01:54:21,219 --> 01:54:28,380
start appending it to my list of outputs

2437
01:54:24,130 --> 01:54:34,690
okay so in other words the thing that it

2438
01:54:28,380 --> 01:54:37,809
sends back to this pooling is is only as

2439
01:54:34,689 --> 01:54:41,348
much there's only as many activations as

2440
01:54:37,809 --> 01:54:43,270
we've asked it to keep right and so that

2441
01:54:41,349 --> 01:54:44,770
way you can basically just figure out

2442
01:54:43,270 --> 01:54:48,809
how much

2443
01:54:44,770 --> 01:54:52,389
what's max SEC do you can your

2444
01:54:48,809 --> 01:54:55,929
particular GPU handle that so it's still

2445
01:54:52,389 --> 01:54:58,750
using the whole document but let's say

2446
01:54:55,929 --> 01:55:00,578
max SEC is a thousand thousand words and

2447
01:54:58,750 --> 01:55:02,800
your longest document length is two

2448
01:55:00,578 --> 01:55:05,139
thousand words right that it's still

2449
01:55:02,800 --> 01:55:08,320
going through the I am creating state

2450
01:55:05,139 --> 01:55:11,319
for those first thousand words right but

2451
01:55:08,319 --> 01:55:14,618
it's not actually going to store the

2452
01:55:11,319 --> 01:55:15,880
activations for the backdrop the first

2453
01:55:14,618 --> 01:55:18,939
thousand is only going to keep the last

2454
01:55:15,880 --> 01:55:23,650
thousand right so that means that it

2455
01:55:18,939 --> 01:55:26,019
can't back propagate the loss back to

2456
01:55:23,649 --> 01:55:28,269
any decisions that any state that was

2457
01:55:26,020 --> 01:55:31,480
created in the first thousand words you

2458
01:55:28,270 --> 01:55:35,199
know basically that's that's now gone so

2459
01:55:31,479 --> 01:55:36,908
it's a really simple piece of code you

2460
01:55:35,198 --> 01:55:39,759
know and honestly when I wrote it it was

2461
01:55:36,908 --> 01:55:41,679
like I didn't spend much time thinking

2462
01:55:39,760 --> 01:55:44,590
about it it seems so obviously the only

2463
01:55:41,679 --> 01:55:46,750
way that this could possibly work but

2464
01:55:44,590 --> 01:55:49,119
again it's it seems to be a new thing so

2465
01:55:46,750 --> 01:55:51,729
we now have back prop through time for

2466
01:55:49,118 --> 01:55:54,339
text classification yes I think so you

2467
01:55:51,729 --> 01:55:58,779
can see there's lots of little pieces in

2468
01:55:54,340 --> 01:56:02,279
this paper so what was the result right

2469
01:55:58,779 --> 01:56:05,859
and so the result was on every single

2470
01:56:02,279 --> 01:56:09,819
data set we tried we got a better result

2471
01:56:05,859 --> 01:56:16,089
than any previous academic for text

2472
01:56:09,819 --> 01:56:20,799
classification so IMDB trekked 6aj News

2473
01:56:16,090 --> 01:56:23,679
dbpedia Yelp all different types and

2474
01:56:20,800 --> 01:56:25,750
honestly IMDB was the only one I spent

2475
01:56:23,679 --> 01:56:26,779
any time trying to optimize the model so

2476
01:56:25,750 --> 01:56:28,399
like most of them

2477
01:56:26,779 --> 01:56:30,050
we just did it like whatever came out

2478
01:56:28,399 --> 01:56:32,000
first so if we actually spent time with

2479
01:56:30,050 --> 01:56:34,130
it I think this would be a lot better

2480
01:56:32,000 --> 01:56:37,520
that and the things that these are

2481
01:56:34,130 --> 01:56:39,800
comparing to most of them are like

2482
01:56:37,520 --> 01:56:42,140
you'll see like they're different on

2483
01:56:39,800 --> 01:56:43,940
each table because like they're

2484
01:56:42,140 --> 01:56:45,829
optimized you know these like customized

2485
01:56:43,939 --> 01:56:48,769
algorithms on the whole so this is

2486
01:56:45,829 --> 01:56:50,809
saying like one simple fine-tuning

2487
01:56:48,770 --> 01:56:57,350
algorithm can beat these really

2488
01:56:50,810 --> 01:56:59,600
customized algorithms and so here's the

2489
01:56:57,350 --> 01:57:01,730
like one of the really cool things that

2490
01:56:59,600 --> 01:57:03,530
Sebastian did was his ablation studies

2491
01:57:01,729 --> 01:57:04,599
all right which is I was really keen

2492
01:57:03,529 --> 01:57:09,079
that if you're going to publish a paper

2493
01:57:04,600 --> 01:57:12,950
we had to say why does it work right so

2494
01:57:09,079 --> 01:57:15,559
Sebastian went through and tried you

2495
01:57:12,949 --> 01:57:17,869
know removing all of those different

2496
01:57:15,560 --> 01:57:22,520
contributions I mentioned right so what

2497
01:57:17,869 --> 01:57:23,779
if we don't use gradual freezing what if

2498
01:57:22,520 --> 01:57:26,000
we don't use discriminative learning

2499
01:57:23,779 --> 01:57:29,739
rates what if instead of discriminating

2500
01:57:26,000 --> 01:57:36,369
rates we use cosign annealing what if we

2501
01:57:29,739 --> 01:57:40,579
don't do any pre training with Wikipedia

2502
01:57:36,369 --> 01:57:41,899
what if we don't do any fine tuning and

2503
01:57:40,579 --> 01:57:45,559
then the really interesting one to me

2504
01:57:41,899 --> 01:57:47,929
was what's the validation error rate on

2505
01:57:45,560 --> 01:57:50,930
IMDB if we only use a hundred training

2506
01:57:47,930 --> 01:57:52,930
examples this is 200 business 500 and

2507
01:57:50,930 --> 01:57:59,810
you can see you know very interestingly

2508
01:57:52,930 --> 01:58:01,880
the the full version of this approach is

2509
01:57:59,810 --> 01:58:04,370
nearly as accurate on just a hundred

2510
01:58:01,880 --> 01:58:08,569
training examples like it's still very

2511
01:58:04,369 --> 01:58:10,250
accurate versus for 20,000 training

2512
01:58:08,569 --> 01:58:13,309
examples we also if your training from

2513
01:58:10,250 --> 01:58:15,739
scratch on 100 it's like almost random

2514
01:58:13,310 --> 01:58:17,510
all right so kind of like it's what I

2515
01:58:15,739 --> 01:58:20,539
expected you know I've kind of said to

2516
01:58:17,510 --> 01:58:22,250
Sebastian I really think that this this

2517
01:58:20,539 --> 01:58:24,590
is most beneficial when you don't have

2518
01:58:22,250 --> 01:58:26,270
much data and this is like where fast a

2519
01:58:24,590 --> 01:58:28,159
is most interested in contributing right

2520
01:58:26,270 --> 01:58:30,380
there's like small data regimes small

2521
01:58:28,159 --> 01:58:33,769
compute regimes and so forth and so he

2522
01:58:30,380 --> 01:58:38,359
did these studies to check so I want to

2523
01:58:33,770 --> 01:58:41,860
show you a couple of tricks as to how

2524
01:58:38,359 --> 01:58:41,859
you can run these kinds

2525
01:58:42,760 --> 01:58:48,039
the first trick is is something which I

2526
01:58:46,329 --> 01:58:50,590
know you're all gonna find really handy

2527
01:58:48,039 --> 01:58:51,670
I know you're hoping annoyed when you're

2528
01:58:50,590 --> 01:58:53,199
running something in a Jeep and a

2529
01:58:51,670 --> 01:58:56,319
notebook and you lose your internet

2530
01:58:53,199 --> 01:58:58,029
connection for long enough that it

2531
01:58:56,319 --> 01:58:59,829
decides you've gone away and then your

2532
01:58:58,029 --> 01:59:03,130
session disappears and you have to start

2533
01:58:59,829 --> 01:59:08,019
it again from Spanish ok so what do you

2534
01:59:03,130 --> 01:59:11,230
do there's a very simple cool thing

2535
01:59:08,020 --> 01:59:13,930
called VNC where basically you can

2536
01:59:11,229 --> 01:59:17,229
install on your AWS instance or paper

2537
01:59:13,930 --> 01:59:21,659
space or whatever X Windows a

2538
01:59:17,229 --> 01:59:28,569
lightweight window manager a VNC server

2539
01:59:21,659 --> 01:59:31,000
firefox a terminal and some fonts chuck

2540
01:59:28,569 --> 01:59:35,769
these lines at the end of your VNC x

2541
01:59:31,000 --> 01:59:40,569
startup configuration file and then run

2542
01:59:35,770 --> 01:59:46,570
this command it's now running a server

2543
01:59:40,569 --> 01:59:48,789
where if you now run for it's now

2544
01:59:46,569 --> 01:59:52,119
running a server where you can then run

2545
01:59:48,789 --> 01:59:55,810
the type VNC viewer or any VNC viewer on

2546
01:59:52,119 --> 01:59:58,510
your computer and you point it at your

2547
01:59:55,810 --> 02:00:05,650
server right but specifically what you

2548
01:59:58,510 --> 02:00:09,130
do as you go you use SSH port forwarding

2549
02:00:05,649 --> 02:00:12,460
to port forward port five nine one three

2550
02:00:09,130 --> 02:00:18,970
to localhost five nine one three right

2551
02:00:12,460 --> 02:00:21,819
and so then you connect to port five

2552
02:00:18,970 --> 02:00:23,800
nine one three on on localhost

2553
02:00:21,819 --> 02:00:25,719
it will send it off to port five nine

2554
02:00:23,800 --> 02:00:28,000
one three on your server which is the

2555
02:00:25,720 --> 02:00:31,060
VNC port because you said : thirteen

2556
02:00:28,000 --> 02:00:33,550
here and it will display an X Windows

2557
02:00:31,060 --> 02:00:36,130
desktop and then you can click on the

2558
02:00:33,550 --> 02:00:39,070
Linux start like button and click on

2559
02:00:36,130 --> 02:00:41,680
Firefox and you now have Firefox and so

2560
02:00:39,069 --> 02:00:44,079
you can now run and you'll see here in

2561
02:00:41,680 --> 02:00:46,350
Firefox it says localhost because this

2562
02:00:44,079 --> 02:00:49,869
Firefox is running on my AWS server

2563
02:00:46,350 --> 02:00:51,520
right and so you now run Firefox you

2564
02:00:49,869 --> 02:00:53,670
start your thing running and then you

2565
02:00:51,520 --> 02:00:56,219
close your VNC viewer

2566
02:00:53,670 --> 02:00:58,710
remembering that Firefox is like

2567
02:00:56,219 --> 02:01:00,960
displaying on this virtual VNC display

2568
02:00:58,710 --> 02:01:03,179
not in the real display and so then

2569
02:01:00,960 --> 02:01:05,489
later on that day you log back into VNC

2570
02:01:03,179 --> 02:01:08,460
viewer and it flops up again so it's

2571
02:01:05,488 --> 02:01:10,379
like you're persistent desktop and it's

2572
02:01:08,460 --> 02:01:13,679
shockingly fast it works really well

2573
02:01:10,380 --> 02:01:15,869
okay so there's trick number one and

2574
02:01:13,679 --> 02:01:18,690
there's lots of different VNC servers

2575
02:01:15,869 --> 02:01:22,289
and clients and whatever but this one

2576
02:01:18,689 --> 02:01:24,689
works fine for me so there you go so you

2577
02:01:22,289 --> 02:01:30,869
can see here I connect to localhost five

2578
02:01:24,689 --> 02:01:33,389
nine one three trick number two is to

2579
02:01:30,869 --> 02:01:35,670
create Python scripts but this is what

2580
02:01:33,390 --> 02:01:37,260
we ended up doing so I ended up creating

2581
02:01:35,670 --> 02:01:38,819
like a little Python script for

2582
02:01:37,260 --> 02:01:41,190
Sebastian to kind of say this is the

2583
02:01:38,819 --> 02:01:42,630
basic steps you need to do and now you

2584
02:01:41,189 --> 02:01:43,979
need to create like different versions

2585
02:01:42,630 --> 02:01:45,510
certain thing else and I suggested to

2586
02:01:43,979 --> 02:01:47,939
him that he tried using this thing

2587
02:01:45,510 --> 02:01:51,570
called Google fire what Google fire does

2588
02:01:47,939 --> 02:01:53,639
is you create a function with shitloads

2589
02:01:51,569 --> 02:01:55,559
of parameters right and so these are all

2590
02:01:53,640 --> 02:01:56,940
the things that Sebastian wanted to try

2591
02:01:55,560 --> 02:01:58,710
doing different drop out amounts

2592
02:01:56,939 --> 02:02:02,339
different learning rates do I use pre

2593
02:01:58,710 --> 02:02:03,929
training or not do I use CLR or not do I

2594
02:02:02,340 --> 02:02:06,390
use discriminative learning rate or not

2595
02:02:03,929 --> 02:02:08,850
do I go backwards or not blah blah blah

2596
02:02:06,390 --> 02:02:10,500
right so you create a function and then

2597
02:02:08,850 --> 02:02:11,760
you add something saying if 9 equals

2598
02:02:10,500 --> 02:02:13,829
main fire dot fire

2599
02:02:11,760 --> 02:02:15,300
and the function name you do nothing

2600
02:02:13,829 --> 02:02:17,640
else at all you don't have to add any

2601
02:02:15,300 --> 02:02:20,300
metadata any docstrings anything at all

2602
02:02:17,640 --> 02:02:23,900
and you then call that script and

2603
02:02:20,300 --> 02:02:28,260
automatically you now have a command

2604
02:02:23,899 --> 02:02:31,500
that's it right so that's a super

2605
02:02:28,260 --> 02:02:34,619
fantastic easy way to run lots of

2606
02:02:31,500 --> 02:02:36,600
different variations in a terminal and

2607
02:02:34,619 --> 02:02:38,279
this is like this is ends up being

2608
02:02:36,600 --> 02:02:41,190
easier if you want to do lots of

2609
02:02:38,279 --> 02:02:42,329
variations than using a notebook because

2610
02:02:41,189 --> 02:02:44,428
you can just like have a bash script

2611
02:02:42,329 --> 02:02:53,369
that tries all of them and spits them

2612
02:02:44,429 --> 02:02:55,560
all out you're fine inside the dl2

2613
02:02:53,369 --> 02:02:58,890
course directory there's now something

2614
02:02:55,560 --> 02:03:01,380
called IMDB scripts and I've put there

2615
02:02:58,890 --> 02:03:04,320
all of the scripts that Sebastian and I

2616
02:03:01,380 --> 02:03:07,199
used so you'll see because we needed to

2617
02:03:04,319 --> 02:03:10,979
like tokenize every

2618
02:03:07,198 --> 02:03:13,139
or data set we had to turn every dataset

2619
02:03:10,979 --> 02:03:15,119
numerical eyes every data set we had to

2620
02:03:13,140 --> 02:03:16,650
train a language model on every data set

2621
02:03:15,119 --> 02:03:18,090
we had to train a classifier every data

2622
02:03:16,649 --> 02:03:19,469
set we have to do all of those things in

2623
02:03:18,090 --> 02:03:21,270
a variety of different ways to compare

2624
02:03:19,469 --> 02:03:24,119
them we have a script for all those

2625
02:03:21,270 --> 02:03:32,670
things so you can check out and see all

2626
02:03:24,119 --> 02:03:35,159
of the scripts that we used when you're

2627
02:03:32,670 --> 02:03:36,869
doing a lot of scripts and stuff they've

2628
02:03:35,159 --> 02:03:39,840
got different code all over the place

2629
02:03:36,869 --> 02:03:41,399
eventually it might get frustrating that

2630
02:03:39,840 --> 02:03:42,000
you want to you know you don't want a

2631
02:03:41,399 --> 02:03:44,069
symlink

2632
02:03:42,000 --> 02:03:46,109
you're faster yo library again and again

2633
02:03:44,069 --> 02:03:48,630
but you probably don't want to pip

2634
02:03:46,109 --> 02:03:50,969
install it because that version tends to

2635
02:03:48,630 --> 02:03:52,880
be a little bit old we move so fast you

2636
02:03:50,969 --> 02:03:57,750
want to use the current version in get

2637
02:03:52,880 --> 02:04:02,130
if you say pip install - a dot from the

2638
02:03:57,750 --> 02:04:04,409
fast AI repos base it does something

2639
02:04:02,130 --> 02:04:08,069
quite neat which is basically creates a

2640
02:04:04,409 --> 02:04:09,988
symlink - the faster a library to get

2641
02:04:08,069 --> 02:04:15,389
you know in your get installation right

2642
02:04:09,988 --> 02:04:16,979
here inside your site packages directory

2643
02:04:15,390 --> 02:04:21,119
your site packages directory is like

2644
02:04:16,979 --> 02:04:24,389
your main you know Python library and so

2645
02:04:21,119 --> 02:04:28,140
if you do this you can then access fast

2646
02:04:24,390 --> 02:04:30,119
AI from anywhere but every time you do

2647
02:04:28,140 --> 02:04:34,079
get pull you've got the most recent

2648
02:04:30,119 --> 02:04:36,269
version one downside of this is that it

2649
02:04:34,079 --> 02:04:38,789
installs any updated versions of

2650
02:04:36,270 --> 02:04:42,420
packages from Kipp which can kind of

2651
02:04:38,789 --> 02:04:45,448
confuse Condor a little bit so another

2652
02:04:42,420 --> 02:04:48,719
alternative here is just to Simla --nk

2653
02:04:45,448 --> 02:04:51,659
the FASTA a library to your site

2654
02:04:48,719 --> 02:04:54,510
packages library like that works just as

2655
02:04:51,659 --> 02:04:58,198
well and then you can use faster i'll

2656
02:04:54,510 --> 02:05:00,449
again from anywhere and it's quite handy

2657
02:04:58,198 --> 02:05:02,069
when you want to kind of run scripts

2658
02:05:00,448 --> 02:05:07,948
that use past AI from different

2659
02:05:02,069 --> 02:05:13,889
directories under system okay so one

2660
02:05:07,948 --> 02:05:16,789
more thing before we which is something

2661
02:05:13,890 --> 02:05:16,789
you can try if you like

2662
02:05:17,000 --> 02:05:25,880
you don't have to tokenize words instead

2663
02:05:23,390 --> 02:05:29,420
of tokenizing words you can tokenize

2664
02:05:25,880 --> 02:05:31,909
what are called sub word units and so

2665
02:05:29,420 --> 02:05:35,300
for example unsupervised

2666
02:05:31,909 --> 02:05:40,430
for example unsupervised quickly

2667
02:05:35,300 --> 02:05:44,630
tokenized his on supervised tokenizer

2668
02:05:40,430 --> 02:05:46,820
could be tokenized as token either right

2669
02:05:44,630 --> 02:05:49,279
and then you could do the same thing the

2670
02:05:46,819 --> 02:05:51,349
language model that works on sub units a

2671
02:05:49,279 --> 02:05:59,809
classifier that works on sub word units

2672
02:05:51,350 --> 02:06:02,750
etc so how well does that work I started

2673
02:05:59,810 --> 02:06:05,450
playing with it and with not too much

2674
02:06:02,750 --> 02:06:07,760
playing I was getting classification

2675
02:06:05,449 --> 02:06:10,519
results that were nearly as good as

2676
02:06:07,760 --> 02:06:16,000
using word level tokenization not quite

2677
02:06:10,520 --> 02:06:19,160
as good but nearly as good I suspect

2678
02:06:16,000 --> 02:06:20,899
with more careful thinking and playing

2679
02:06:19,159 --> 02:06:24,529
around maybe I could have got as good or

2680
02:06:20,899 --> 02:06:31,849
better but even if I couldn't if you

2681
02:06:24,529 --> 02:06:34,849
create a a sub word unit wikitext model

2682
02:06:31,850 --> 02:06:38,120
then IMDB model language model and then

2683
02:06:34,850 --> 02:06:40,250
classifier forwards and backwards force

2684
02:06:38,119 --> 02:06:42,170
upward units and then ensemble it with

2685
02:06:40,250 --> 02:06:45,829
the forwards and backwards word level

2686
02:06:42,170 --> 02:06:47,840
ones you should be able to beat us right

2687
02:06:45,829 --> 02:06:52,579
so here's an approach you may be able to

2688
02:06:47,840 --> 02:06:54,529
beat our state if ya was on Google has

2689
02:06:52,579 --> 02:06:57,289
as Sebastian told me about this

2690
02:06:54,529 --> 02:06:59,569
particular project is great Google has a

2691
02:06:57,289 --> 02:07:02,539
project called sentence peace which

2692
02:06:59,569 --> 02:07:07,279
actually uses a neural net to figure out

2693
02:07:02,539 --> 02:07:09,680
the optimal splitting up of words and so

2694
02:07:07,279 --> 02:07:13,189
you end up with a vocabulary of sub word

2695
02:07:09,680 --> 02:07:15,619
units in my planning around I found that

2696
02:07:13,189 --> 02:07:19,000
creating a vocabulary of about 30,000

2697
02:07:15,619 --> 02:07:21,710
sub word units seems to be about optimal

2698
02:07:19,000 --> 02:07:22,670
so if you're interested there's

2699
02:07:21,710 --> 02:07:26,119
something you can try

2700
02:07:22,670 --> 02:07:28,909
it's a bit of a pain to install it's C++

2701
02:07:26,119 --> 02:07:30,880
it doesn't have great error messages but

2702
02:07:28,909 --> 02:07:33,819
it will work

2703
02:07:30,880 --> 02:07:36,900
- like before it and if anybody tries to

2704
02:07:33,819 --> 02:07:40,179
so I'm happy to drop them get it working

2705
02:07:36,899 --> 02:07:44,408
there's been little if any experiments

2706
02:07:40,179 --> 02:07:46,779
with ensemble abroad and word level

2707
02:07:44,408 --> 02:07:49,719
stuff classification and I do think it

2708
02:07:46,779 --> 02:07:50,738
should be the best approach alright

2709
02:07:49,719 --> 02:07:52,179
thanks everybody

2710
02:07:50,738 --> 02:07:53,179
have a great week and see you next

2711
02:07:52,179 --> 02:07:56,949
Monday

2712
02:07:53,180 --> 02:07:56,949
[Applause]

