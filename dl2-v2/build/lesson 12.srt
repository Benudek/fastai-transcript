1
00:00:00,500 --> 00:00:08,150
so we're going to be talking about Ganz

2
00:00:03,899 --> 00:00:16,469
today who has heard of Ganz

3
00:00:08,150 --> 00:00:18,439
yeah most of you very hot technology but

4
00:00:16,469 --> 00:00:20,969
definitely deserving to be in the

5
00:00:18,439 --> 00:00:24,480
cutting edge deep learning part of the

6
00:00:20,969 --> 00:00:27,809
course because they're not quite proven

7
00:00:24,480 --> 00:00:29,699
to be necessarily useful for anything

8
00:00:27,809 --> 00:00:31,500
but they're nearly there they're

9
00:00:29,699 --> 00:00:34,340
definitely going to get there and we're

10
00:00:31,500 --> 00:00:37,109
going to focus on the things where

11
00:00:34,340 --> 00:00:38,940
they're definitely definitely going to

12
00:00:37,109 --> 00:00:40,409
be useful in practice and there's a

13
00:00:38,939 --> 00:00:41,849
number of areas where they may turn out

14
00:00:40,409 --> 00:00:43,889
to be useful in practice but we don't

15
00:00:41,850 --> 00:00:45,390
know yet so I think the area that we're

16
00:00:43,890 --> 00:00:47,909
going to be that they're definitely

17
00:00:45,390 --> 00:00:49,439
going to be useful in practice is the

18
00:00:47,909 --> 00:00:52,019
kind of thing you see on the left here

19
00:00:49,439 --> 00:00:55,500
which is for example turning drawings

20
00:00:52,020 --> 00:00:58,579
into rendered pictures this comes from a

21
00:00:55,500 --> 00:01:02,460
paper that's just came out two days ago

22
00:00:58,579 --> 00:01:06,150
so there's a very active research going

23
00:01:02,460 --> 00:01:08,969
on right now before we get there though

24
00:01:06,150 --> 00:01:12,780
let's talk about some interesting stuff

25
00:01:08,969 --> 00:01:14,700
from the last class this was an

26
00:01:12,780 --> 00:01:17,310
interesting thing that one of our

27
00:01:14,700 --> 00:01:21,478
diversity fellows Christine Payne did

28
00:01:17,310 --> 00:01:23,670
Christine has a master's in medicine

29
00:01:21,478 --> 00:01:25,530
from Stanford and so she OC had an

30
00:01:23,670 --> 00:01:28,590
interest in thinking what would it look

31
00:01:25,530 --> 00:01:32,129
like if we built a language model of

32
00:01:28,590 --> 00:01:34,409
medicine and one of the things that we

33
00:01:32,129 --> 00:01:36,030
briefly touched on back in less than

34
00:01:34,409 --> 00:01:37,939
four but didn't really talk much about

35
00:01:36,030 --> 00:01:41,820
last time is this idea you can actually

36
00:01:37,938 --> 00:01:43,319
seed a generative language model which

37
00:01:41,819 --> 00:01:45,809
basically means you've trained a

38
00:01:43,319 --> 00:01:48,029
language model on some corpus and then

39
00:01:45,810 --> 00:01:50,310
you're going to generate some text from

40
00:01:48,030 --> 00:01:52,920
that language model and so you can start

41
00:01:50,310 --> 00:01:55,170
off by feeding it a few words you know

42
00:01:52,920 --> 00:01:57,540
to basically say here's the first few

43
00:01:55,170 --> 00:01:59,368
words to create the hidden state in the

44
00:01:57,540 --> 00:02:02,700
language model and then generate from

45
00:01:59,368 --> 00:02:04,739
there please and so when Christine so

46
00:02:02,700 --> 00:02:07,020
Christine did something clever which was

47
00:02:04,739 --> 00:02:10,139
to kind of pick up was to seed it with a

48
00:02:07,019 --> 00:02:12,000
question and then repeat the question so

49
00:02:10,139 --> 00:02:13,539
it's 3 times Christine three times and

50
00:02:12,000 --> 00:02:17,468
then where'd it generate

51
00:02:13,539 --> 00:02:19,658
from there and so she fed a language

52
00:02:17,468 --> 00:02:21,430
model lots of different medical texts

53
00:02:19,658 --> 00:02:23,889
and then fed at this question what is

54
00:02:21,430 --> 00:02:26,829
the prevalence of malaria and the model

55
00:02:23,889 --> 00:02:29,108
said in the US about 10 percent of the

56
00:02:26,829 --> 00:02:30,730
population has the virus but only about

57
00:02:29,109 --> 00:02:33,340
one percent is infected with a virus

58
00:02:30,729 --> 00:02:34,628
about 50 to 80 million infected she said

59
00:02:33,340 --> 00:02:37,750
what's the treatment for ectopic

60
00:02:34,628 --> 00:02:39,280
pregnancy and it said it's a safe and

61
00:02:37,750 --> 00:02:40,509
safe treatment for women with the

62
00:02:39,280 --> 00:02:42,128
history or symptoms may have a

63
00:02:40,509 --> 00:02:44,438
significant impact and clinical response

64
00:02:42,128 --> 00:02:46,929
most important factor is development and

65
00:02:44,438 --> 00:02:49,500
management of ectopic privacy etc and so

66
00:02:46,930 --> 00:02:53,379
what I find interesting about this is

67
00:02:49,500 --> 00:02:56,590
you know it's it's pretty close to it

68
00:02:53,378 --> 00:02:58,449
being a to me as somebody who doesn't

69
00:02:56,590 --> 00:02:59,620
have a masters in medicine from Stanford

70
00:02:58,449 --> 00:03:04,149
are pretty kind of close to being a

71
00:02:59,620 --> 00:03:05,769
believable answer to the question but it

72
00:03:04,150 --> 00:03:07,480
really has no bearing on reality

73
00:03:05,769 --> 00:03:11,289
whatsoever and I kind of think it's an

74
00:03:07,479 --> 00:03:16,328
interesting kind of ethical and user

75
00:03:11,289 --> 00:03:18,489
experience quandary so actually I'm

76
00:03:16,329 --> 00:03:20,889
involved also in a company called dr. a

77
00:03:18,489 --> 00:03:23,229
that's trying to basically doing a

78
00:03:20,889 --> 00:03:25,419
number of things but in the end provide

79
00:03:23,229 --> 00:03:27,040
an app for doctors and patients which

80
00:03:25,419 --> 00:03:29,709
can help kind of create a conversation

81
00:03:27,039 --> 00:03:31,989
or user interface around helping them

82
00:03:29,709 --> 00:03:34,959
with their medical issues and I've been

83
00:03:31,989 --> 00:03:37,060
kind of continually saying to the

84
00:03:34,959 --> 00:03:40,569
software engineers on that team please

85
00:03:37,060 --> 00:03:42,400
don't try to create a generative model

86
00:03:40,568 --> 00:03:43,750
using like an LST em or something

87
00:03:42,400 --> 00:03:47,500
because they're going to be really good

88
00:03:43,750 --> 00:03:50,620
at creating bad advice that sounds

89
00:03:47,500 --> 00:03:53,709
impressive you know kind of like you

90
00:03:50,620 --> 00:03:57,370
know political pundits or tenured

91
00:03:53,709 --> 00:04:02,250
professors you know people who can say

92
00:03:57,370 --> 00:04:04,329
with great Authority so I think

93
00:04:02,250 --> 00:04:05,789
yeah so I thought it was really I

94
00:04:04,329 --> 00:04:09,879
thought it was a really interesting

95
00:04:05,789 --> 00:04:12,698
experiment and great to see you know

96
00:04:09,878 --> 00:04:14,219
what what about diversity fellows doing

97
00:04:12,699 --> 00:04:18,639
I mean this is why we have this program

98
00:04:14,219 --> 00:04:20,649
I suppose I should just say masters in

99
00:04:18,639 --> 00:04:23,650
medicine actually a Juilliard trained

100
00:04:20,649 --> 00:04:25,569
classical musician or on actually also a

101
00:04:23,649 --> 00:04:26,479
Princeton valedictorian in physics so

102
00:04:25,569 --> 00:04:28,430
also a

103
00:04:26,480 --> 00:04:31,520
woman's a computing expert yeah okay so

104
00:04:28,430 --> 00:04:32,990
she does a bit of everything so yeah

105
00:04:31,519 --> 00:04:36,439
really impressive group of people and

106
00:04:32,990 --> 00:04:38,780
great to see such exciting kind of ideas

107
00:04:36,439 --> 00:04:42,160
coming out and if you're wondering you

108
00:04:38,779 --> 00:04:44,539
know I've done some interesting

109
00:04:42,160 --> 00:04:45,950
experiments should I let people know

110
00:04:44,540 --> 00:04:49,250
about it

111
00:04:45,949 --> 00:04:51,819
well I Kristin mentioned this in the

112
00:04:49,250 --> 00:04:56,089
forum I went on to mention it on Twitter

113
00:04:51,819 --> 00:04:57,379
too which I got this response you are

114
00:04:56,089 --> 00:04:59,959
you looking for a job you may be

115
00:04:57,379 --> 00:05:03,259
wondering whose AVM aerotrain is well he

116
00:04:59,959 --> 00:05:05,509
is the founder of a hot new medical AI

117
00:05:03,259 --> 00:05:08,449
startup he was previously the head of

118
00:05:05,509 --> 00:05:10,339
engineering at Quora before that he was

119
00:05:08,449 --> 00:05:11,990
the guy net flips around the data

120
00:05:10,339 --> 00:05:14,989
science team and built their recommender

121
00:05:11,990 --> 00:05:17,930
systems and so this is what happens if

122
00:05:14,990 --> 00:05:21,530
you do something cool let people know

123
00:05:17,930 --> 00:05:24,129
about it and get get noticed by awesome

124
00:05:21,529 --> 00:05:24,129
people like stevia

125
00:05:25,600 --> 00:05:35,080
so let's talk about sci-fi 10 and the

126
00:05:32,660 --> 00:05:39,290
reason I'm going to talk about so far 10

127
00:05:35,079 --> 00:05:43,219
is that we're going to be looking at

128
00:05:39,290 --> 00:05:45,740
some more you know bare-bones PI torch

129
00:05:43,220 --> 00:05:47,980
stuff today to build these generative

130
00:05:45,740 --> 00:05:50,629
adversarial models there's there's no

131
00:05:47,980 --> 00:05:53,270
really fast AI support to speak up at

132
00:05:50,629 --> 00:05:54,860
all Dan's at the moment I'm sure there

133
00:05:53,269 --> 00:05:55,819
will be soon enough apparently there

134
00:05:54,860 --> 00:05:57,949
isn't so we're going to be building a

135
00:05:55,819 --> 00:06:00,579
lot of models from scratch now it's been

136
00:05:57,949 --> 00:06:02,779
a while since we've done much you know

137
00:06:00,579 --> 00:06:06,409
serious model building a little bit of

138
00:06:02,779 --> 00:06:08,029
model building I guess for our bounding

139
00:06:06,410 --> 00:06:10,490
box staff but really all the interesting

140
00:06:08,029 --> 00:06:13,429
stuff there was the loss function so we

141
00:06:10,490 --> 00:06:15,139
looked at sci-fi 10 in the plat 1 of the

142
00:06:13,430 --> 00:06:18,230
course and we built something which was

143
00:06:15,139 --> 00:06:20,589
getting about 85% accuracy and I

144
00:06:18,230 --> 00:06:22,610
remember a couple of hours to Train

145
00:06:20,589 --> 00:06:25,159
interestingly there's a competition

146
00:06:22,610 --> 00:06:27,199
going on now to see who can actually

147
00:06:25,160 --> 00:06:29,210
train sci-fi turn the fastest going

148
00:06:27,199 --> 00:06:31,849
through this earth Stepford Dawn bench

149
00:06:29,209 --> 00:06:36,409
and currently so the goal is to get it

150
00:06:31,850 --> 00:06:37,700
to train to 94% accuracy so everything

151
00:06:36,410 --> 00:06:38,689
to see if we can build an architecture

152
00:06:37,699 --> 00:06:40,218
then can get

153
00:06:38,689 --> 00:06:43,338
four percent accuracy because that's a

154
00:06:40,218 --> 00:06:44,748
lot better than our previous attempt and

155
00:06:43,338 --> 00:06:45,918
so hopefully in doing so we'll learn

156
00:06:44,749 --> 00:06:48,588
something about creating good

157
00:06:45,918 --> 00:06:52,878
architectures that will be then useful

158
00:06:48,588 --> 00:06:56,868
for looking at these gams today but I

159
00:06:52,879 --> 00:06:58,809
think also it's useful because I've been

160
00:06:56,869 --> 00:07:01,629
kind of looking much more deeply into

161
00:06:58,809 --> 00:07:04,069
the last few years papers about

162
00:07:01,629 --> 00:07:06,079
different kinds of CN n architectures

163
00:07:04,069 --> 00:07:08,360
and realize that a lot of the insights

164
00:07:06,079 --> 00:07:10,189
in those papers are not being widely

165
00:07:08,360 --> 00:07:12,439
leveraged and clearly not widely

166
00:07:10,189 --> 00:07:13,999
understood so I want to show you what

167
00:07:12,439 --> 00:07:19,729
happens if we can leverage some of that

168
00:07:13,999 --> 00:07:23,569
understanding so I've got this note book

169
00:07:19,728 --> 00:07:24,949
called sci-fi 10 darknet that's because

170
00:07:23,569 --> 00:07:27,050
the the particular architecture we're

171
00:07:24,949 --> 00:07:29,360
going to look at is it's quite is really

172
00:07:27,050 --> 00:07:30,860
very close to the darknet architecture

173
00:07:29,360 --> 00:07:33,110
but you'll see in the process that the

174
00:07:30,860 --> 00:07:35,479
darknet architecture has in not the

175
00:07:33,110 --> 00:07:38,329
whole euro version three end-to-end

176
00:07:35,478 --> 00:07:39,829
thing but just the part of it that they

177
00:07:38,329 --> 00:07:42,468
pre trained on imagenet to do

178
00:07:39,829 --> 00:07:45,679
classification it's almost like the most

179
00:07:42,468 --> 00:07:48,978
generic simple architecture almost you

180
00:07:45,678 --> 00:07:52,428
could come up with and so it's a really

181
00:07:48,978 --> 00:07:53,899
great starting point for experiments so

182
00:07:52,428 --> 00:07:55,938
we're going to call it dark net but it's

183
00:07:53,899 --> 00:07:57,288
not quite that net and you can fiddle

184
00:07:55,939 --> 00:07:59,119
around with it to create things that

185
00:07:57,288 --> 00:08:03,528
definitely aren't dark now it's really

186
00:07:59,119 --> 00:08:08,089
just the basis of nearly any modern

187
00:08:03,528 --> 00:08:11,149
ResNet based architecture so so far 10

188
00:08:08,088 --> 00:08:14,538
remember is a fairly small data set the

189
00:08:11,149 --> 00:08:16,338
images are only 32 by 32 in size and I

190
00:08:14,538 --> 00:08:20,149
think it's a really great data set to

191
00:08:16,338 --> 00:08:21,709
work with because it's you can you can

192
00:08:20,149 --> 00:08:24,110
train it you know relatively quickly

193
00:08:21,709 --> 00:08:25,278
unlike image net it's a relatively small

194
00:08:24,110 --> 00:08:27,319
amount of data

195
00:08:25,278 --> 00:08:29,178
unlike image net now it's actually quite

196
00:08:27,319 --> 00:08:31,489
hard to recognize the images because 32

197
00:08:29,178 --> 00:08:33,288
by 32 is it's kind of too small to

198
00:08:31,488 --> 00:08:35,870
easily see what's going on so it's it's

199
00:08:33,288 --> 00:08:38,990
somewhat challenging so I think it's a

200
00:08:35,870 --> 00:08:41,688
really underappreciated data set because

201
00:08:38,990 --> 00:08:44,000
it's old you know and you know who a

202
00:08:41,688 --> 00:08:46,429
deep mind or even an AI wants to work

203
00:08:44,000 --> 00:08:48,828
with a small old data set when they

204
00:08:46,429 --> 00:08:50,599
could use their entire server room to

205
00:08:48,828 --> 00:08:51,799
process something much bigger but you

206
00:08:50,600 --> 00:08:52,159
know to me I think this is a really

207
00:08:51,799 --> 00:08:59,659
great

208
00:08:52,159 --> 00:09:01,639
data set to focus on so so go ahead and

209
00:08:59,659 --> 00:09:04,189
and kind of import our usual stuff and

210
00:09:01,639 --> 00:09:08,810
we're going to try and build a network

211
00:09:04,190 --> 00:09:10,430
from scratch to train this with one

212
00:09:08,809 --> 00:09:13,159
thing that I think is a really good

213
00:09:10,429 --> 00:09:15,278
exercise for anybody who's not a hundred

214
00:09:13,159 --> 00:09:17,870
percent confident with their kind of

215
00:09:15,278 --> 00:09:21,860
broadcasting and pi torch and so forth

216
00:09:17,870 --> 00:09:24,948
basic skills is figure out how I came up

217
00:09:21,860 --> 00:09:27,769
with these numbers okay so these numbers

218
00:09:24,948 --> 00:09:29,328
are the averages for each channel and

219
00:09:27,769 --> 00:09:33,169
the standard deviations for each channel

220
00:09:29,328 --> 00:09:34,578
insofar cap so try and that's a bit of a

221
00:09:33,169 --> 00:09:36,078
homework just make sure you can recreate

222
00:09:34,578 --> 00:09:39,229
those numbers and see if you can do it

223
00:09:36,078 --> 00:09:42,169
and you know no more than a couple of

224
00:09:39,230 --> 00:09:44,959
lines of code you know no loops all

225
00:09:42,169 --> 00:09:49,549
right ideally I want to kind of do it in

226
00:09:44,958 --> 00:09:51,349
one go Ken alright because these are

227
00:09:49,549 --> 00:09:54,740
fairly small we can use a larger batch

228
00:09:51,350 --> 00:10:01,310
size unusual 256 and the size of these

229
00:09:54,740 --> 00:10:03,620
images is 32 transformations normally we

230
00:10:01,309 --> 00:10:06,078
kind of have this standard set of sidon

231
00:10:03,620 --> 00:10:08,179
transformations we used for photos of

232
00:10:06,078 --> 00:10:09,438
normal objects we're not going to use

233
00:10:08,179 --> 00:10:12,679
that here though because these images

234
00:10:09,438 --> 00:10:14,389
are so small that trying to rotate a 32

235
00:10:12,679 --> 00:10:17,088
by 32 image a bit is going to introduce

236
00:10:14,389 --> 00:10:19,938
a lot of you know blocky kind of

237
00:10:17,089 --> 00:10:21,889
distortions so the kind of standard

238
00:10:19,938 --> 00:10:26,649
transforms that people tend to use is a

239
00:10:21,889 --> 00:10:29,720
random horizontal flip and then we add

240
00:10:26,649 --> 00:10:33,500
size divided by 8 so 4 pixels of padding

241
00:10:29,720 --> 00:10:35,509
on each side and one thing which I find

242
00:10:33,500 --> 00:10:37,159
works really well is by default fast AI

243
00:10:35,509 --> 00:10:39,139
doesn't add black padding which

244
00:10:37,159 --> 00:10:41,208
basically every other library does we

245
00:10:39,139 --> 00:10:43,850
actually take the last 4 pixels of the

246
00:10:41,208 --> 00:10:46,399
existing photo and flip it and reflect

247
00:10:43,850 --> 00:10:48,409
it and we find that we get much better

248
00:10:46,399 --> 00:10:52,970
results by using this reflection padding

249
00:10:48,409 --> 00:10:55,969
by default so now that we've got a 36 by

250
00:10:52,970 --> 00:10:59,600
36 image this sort of transforms in

251
00:10:55,970 --> 00:11:01,550
training will randomly pick a 32 by 32

252
00:10:59,600 --> 00:11:04,759
crop so we got a little bit of variation

253
00:11:01,549 --> 00:11:05,750
but not he's all right so we can use a

254
00:11:04,759 --> 00:11:08,090
normal from past

255
00:11:05,750 --> 00:11:11,899
grab our data so we now need an

256
00:11:08,090 --> 00:11:14,180
architecture and what we're going to do

257
00:11:11,899 --> 00:11:19,879
is we've got to create an architecture

258
00:11:14,179 --> 00:11:23,389
which fits in one screen okay so this is

259
00:11:19,879 --> 00:11:26,269
from scratch as you can see the only you

260
00:11:23,389 --> 00:11:31,100
know I'm using the predefined come 2d

261
00:11:26,269 --> 00:11:34,819
veteran onto a daily key value modules

262
00:11:31,100 --> 00:11:36,500
but I'm not using any blocks or anything

263
00:11:34,820 --> 00:11:38,120
they're all being defined so the entire

264
00:11:36,500 --> 00:11:41,360
thing is here on one screen so if you're

265
00:11:38,120 --> 00:11:44,960
ever wondering can I understand a modern

266
00:11:41,360 --> 00:11:50,529
good quality architecture absolutely

267
00:11:44,960 --> 00:11:50,530
let's study let's study this one okay so

268
00:11:50,559 --> 00:11:54,889
my basic starting point with an

269
00:11:53,029 --> 00:11:57,339
architecture is to say okay it's it's

270
00:11:54,889 --> 00:11:59,059
it's a stacked bunch of layers and

271
00:11:57,340 --> 00:12:00,830
generally speaking there's going to be

272
00:11:59,059 --> 00:12:02,299
some kind of hierarchy of layers so at

273
00:12:00,830 --> 00:12:03,800
the very bottom level there's things

274
00:12:02,299 --> 00:12:06,589
like a convolutional layer and a batch

275
00:12:03,799 --> 00:12:09,769
nom layer but generally speaking any

276
00:12:06,590 --> 00:12:10,910
time you have a convolution you're

277
00:12:09,769 --> 00:12:12,110
probably going to have some standard

278
00:12:10,909 --> 00:12:15,289
sequence and normally it's going to be

279
00:12:12,110 --> 00:12:18,440
con fetch norm then a nonlinear

280
00:12:15,289 --> 00:12:22,159
activation like a value right so you

281
00:12:18,440 --> 00:12:24,350
know I try to start kind of right from

282
00:12:22,159 --> 00:12:26,539
the top by saying okay what are my basic

283
00:12:24,350 --> 00:12:28,399
units going to be and so by defining it

284
00:12:26,539 --> 00:12:33,139
here that way I don't have to worry

285
00:12:28,399 --> 00:12:34,519
about I don't have to worry about kind

286
00:12:33,139 --> 00:12:35,990
of trying to try to keep everything

287
00:12:34,519 --> 00:12:38,509
consistent it's going to make everything

288
00:12:35,990 --> 00:12:40,279
a lot simpler so here's my con flavor

289
00:12:38,509 --> 00:12:44,299
and so anytime I say come flare

290
00:12:40,279 --> 00:12:45,220
I mean cons batch norm rally now I'm not

291
00:12:44,299 --> 00:12:51,289
quite saying

292
00:12:45,220 --> 00:12:53,450
value I'm saying leaky value and that's

293
00:12:51,289 --> 00:12:55,099
I think we've briefly mentioned it

294
00:12:53,450 --> 00:13:03,850
before but the basic idea is that

295
00:12:55,100 --> 00:13:03,850
normally Lu looks like that right

296
00:13:05,830 --> 00:13:18,950
hopefully you all know that now a leaky

297
00:13:11,269 --> 00:13:21,379
rail you looks like that right so this

298
00:13:18,950 --> 00:13:23,720
part as before has a gradient of 1 and

299
00:13:21,379 --> 00:13:26,000
this part has a gradient of it can vary

300
00:13:23,720 --> 00:13:28,790
but something around point one or point

301
00:13:26,000 --> 00:13:32,509
zero one is common now and the idea

302
00:13:28,789 --> 00:13:34,789
behind it is that when you're in this

303
00:13:32,509 --> 00:13:37,069
negative zone here you don't end up with

304
00:13:34,789 --> 00:13:40,819
a zero gradient which makes it very hard

305
00:13:37,070 --> 00:13:44,209
to update it in practice people have

306
00:13:40,820 --> 00:13:45,800
found leaky value more useful on smaller

307
00:13:44,208 --> 00:13:48,109
datasets and less useful and big

308
00:13:45,799 --> 00:13:49,759
datasets but it's interesting that for

309
00:13:48,110 --> 00:13:51,350
the Yola version 3 paper they did use

310
00:13:49,759 --> 00:13:55,039
early key value and got great

311
00:13:51,350 --> 00:13:56,870
performance from it so it really makes

312
00:13:55,039 --> 00:13:59,028
things worse and it often makes things

313
00:13:56,870 --> 00:14:00,528
better so it's probably not bad if you

314
00:13:59,028 --> 00:14:05,179
need to create your own architecture to

315
00:14:00,528 --> 00:14:08,208
make that your default go to is to use

316
00:14:05,179 --> 00:14:11,719
leaky value ok you'll notice I don't

317
00:14:08,208 --> 00:14:13,819
define a pipe launch module here I just

318
00:14:11,720 --> 00:14:15,709
go ahead and go sequential this is

319
00:14:13,820 --> 00:14:18,290
something that if you read other

320
00:14:15,708 --> 00:14:20,149
people's height watch code it's really

321
00:14:18,289 --> 00:14:22,039
underutilized people tend to write

322
00:14:20,149 --> 00:14:24,769
everything is applied watch module with

323
00:14:22,039 --> 00:14:26,360
an inert and a forward but if you're if

324
00:14:24,769 --> 00:14:28,220
the thing you want is just a sequence of

325
00:14:26,360 --> 00:14:30,200
things one after the other

326
00:14:28,220 --> 00:14:32,089
it's much more concise and easy to

327
00:14:30,200 --> 00:14:33,709
understand to just make it a sequential

328
00:14:32,089 --> 00:14:35,779
right so I just got a simple plain

329
00:14:33,708 --> 00:14:43,549
function it just returns a sequential

330
00:14:35,778 --> 00:14:44,958
model alright so I mentioned that

331
00:14:43,549 --> 00:14:48,319
there's generally kind of a number of

332
00:14:44,958 --> 00:14:50,899
hierarchies of kind of units in most

333
00:14:48,320 --> 00:14:54,470
modern networks and I think we know now

334
00:14:50,899 --> 00:14:58,009
that the the kind of next level in this

335
00:14:54,470 --> 00:15:00,589
unit hierarchy for rez nets and kind of

336
00:14:58,009 --> 00:15:02,838
this this is a type of resin it is the

337
00:15:00,589 --> 00:15:12,620
is the the res block or the residual

338
00:15:02,839 --> 00:15:16,459
block I quoted here as Leia and back

339
00:15:12,620 --> 00:15:18,350
when we lasted sci-fi 10 I over simplify

340
00:15:16,458 --> 00:15:21,739
this I cheated a little bit

341
00:15:18,350 --> 00:15:25,699
we had X coming in and we put that

342
00:15:21,740 --> 00:15:31,610
through a conf and then we added it back

343
00:15:25,698 --> 00:15:34,188
up to X to go out okay so we ended up so

344
00:15:31,610 --> 00:15:42,379
but in general you know we've got your

345
00:15:34,188 --> 00:15:45,198
output is equal to your input plus some

346
00:15:42,379 --> 00:15:47,028
function of your input right and the

347
00:15:45,198 --> 00:15:52,068
thing we did last year was we meant we

348
00:15:47,028 --> 00:15:56,318
made F was a 2d conf okay

349
00:15:52,068 --> 00:16:05,328
but actually the in the real res block

350
00:15:56,318 --> 00:16:13,969
is actually two of them okay so it's

351
00:16:05,328 --> 00:16:16,549
actually Khan of Khan's of X okay and

352
00:16:13,970 --> 00:16:18,170
when I say conf I'm using this as a

353
00:16:16,549 --> 00:16:23,359
shortcut for outcome flare in other

354
00:16:18,169 --> 00:16:25,549
words in other words calm Vectren on

355
00:16:23,360 --> 00:16:28,938
real you okay so you can see here I've

356
00:16:25,549 --> 00:16:31,219
created two coms and here it is I take

357
00:16:28,938 --> 00:16:33,139
my ex put it through the first com put

358
00:16:31,220 --> 00:16:35,720
it through the second con and add it

359
00:16:33,139 --> 00:16:49,100
back up to my input again to get my

360
00:16:35,720 --> 00:16:51,199
basic Reds block okay so one kind of

361
00:16:49,100 --> 00:16:55,569
interesting approach or one interesting

362
00:16:51,198 --> 00:16:58,519
insight here is kind of what are the

363
00:16:55,568 --> 00:17:02,088
number of channels in these convolutions

364
00:16:58,519 --> 00:17:04,939
right so we've got coming in some ni

365
00:17:02,089 --> 00:17:08,110
some number of input channels number of

366
00:17:04,939 --> 00:17:10,939
inputs well number of input filters okay

367
00:17:08,109 --> 00:17:12,529
the way that the darknet folks set

368
00:17:10,939 --> 00:17:14,259
things up is they said okay we're going

369
00:17:12,529 --> 00:17:17,418
to make every one of these rez layers

370
00:17:14,259 --> 00:17:19,788
spit out the same number of channels

371
00:17:17,419 --> 00:17:21,169
that came in and I kind of liked that

372
00:17:19,788 --> 00:17:23,449
that's why I used it here because it

373
00:17:21,169 --> 00:17:24,890
makes life simpler right and so what

374
00:17:23,449 --> 00:17:27,649
they did is they said okay let's have

375
00:17:24,890 --> 00:17:30,919
the first cons have the number of

376
00:17:27,648 --> 00:17:31,339
channels and then the second con double

377
00:17:30,919 --> 00:17:34,370
it again

378
00:17:31,339 --> 00:17:37,099
so ni goes to ni / - and then ni / -

379
00:17:34,369 --> 00:17:39,199
goes to ni right so you've kind of got

380
00:17:37,099 --> 00:17:43,039
this like funneling thing where if

381
00:17:39,200 --> 00:17:45,500
you've got like 64 channels coming in

382
00:17:43,039 --> 00:17:48,710
you kind of get squished down with a

383
00:17:45,500 --> 00:17:51,190
first come down to 32 channels and then

384
00:17:48,710 --> 00:17:53,269
taken back up again to 64 channels

385
00:17:51,190 --> 00:17:57,980
coming out

386
00:17:53,269 --> 00:18:00,980
yes retro why is in place equals true in

387
00:17:57,980 --> 00:18:03,380
the leaky rally oh thanks for asking a

388
00:18:00,980 --> 00:18:08,559
lot of people forget this I don't know

389
00:18:03,380 --> 00:18:11,720
about it but this is a really important

390
00:18:08,559 --> 00:18:13,940
memory technique if you think about it

391
00:18:11,720 --> 00:18:16,069
this cone flower it's like the lowest

392
00:18:13,940 --> 00:18:17,600
level thing so pretty much everything in

393
00:18:16,069 --> 00:18:22,089
our resin it once it's all put together

394
00:18:17,599 --> 00:18:22,089
is going to be complex complex complex

395
00:18:22,539 --> 00:18:28,670
if you don't have in place equals true

396
00:18:26,259 --> 00:18:33,019
it's going to create a whole separate

397
00:18:28,670 --> 00:18:35,900
piece of memory for the output of the

398
00:18:33,019 --> 00:18:37,549
value so like it's gonna allocate a

399
00:18:35,900 --> 00:18:41,690
whole bunch of memory that's that's

400
00:18:37,549 --> 00:18:43,190
totally unnecessary and actually since I

401
00:18:41,690 --> 00:18:45,830
wrote this I come up came up with an

402
00:18:43,190 --> 00:18:47,210
another idea the other day which I'll

403
00:18:45,829 --> 00:18:49,970
now implement which is you could do the

404
00:18:47,210 --> 00:18:56,630
same thing for the res layer rather than

405
00:18:49,970 --> 00:18:59,900
going let's just reorder this to say X

406
00:18:56,630 --> 00:19:02,090
plus that you can actually do the same

407
00:18:59,900 --> 00:19:05,150
thing here yeah hopefully some of you

408
00:19:02,089 --> 00:19:07,369
might remember that in PI torch pretty

409
00:19:05,150 --> 00:19:10,220
much every function has an underscore

410
00:19:07,369 --> 00:19:15,889
suffix version which says do that in

411
00:19:10,220 --> 00:19:21,130
place so plus there's also a add and so

412
00:19:15,890 --> 00:19:23,900
that's add in place and so that's now

413
00:19:21,130 --> 00:19:25,970
suddenly reduced my memory there as well

414
00:19:23,900 --> 00:19:27,800
so these are these are really handy

415
00:19:25,970 --> 00:19:29,509
little tricks and I actually forgot the

416
00:19:27,799 --> 00:19:31,190
in place equals true at first for this

417
00:19:29,509 --> 00:19:32,990
in my literally he was having to

418
00:19:31,190 --> 00:19:33,980
decrease my batch size to much lower

419
00:19:32,990 --> 00:19:35,660
amounts and I mean you should be

420
00:19:33,980 --> 00:19:38,620
possible and it was driving me crazy and

421
00:19:35,660 --> 00:19:41,360
then I realized that that was missing

422
00:19:38,619 --> 00:19:43,789
you can also do that with drop out by

423
00:19:41,359 --> 00:19:44,919
the way if you have dropped out so drop

424
00:19:43,789 --> 00:19:47,420
out and

425
00:19:44,920 --> 00:19:50,150
all the activation functions you can do

426
00:19:47,420 --> 00:19:52,220
in place and then generally any

427
00:19:50,150 --> 00:19:53,470
arithmetic operation you can do in place

428
00:19:52,220 --> 00:19:57,110
as well

429
00:19:53,470 --> 00:20:02,420
why is bias usually like in ResNet set

430
00:19:57,109 --> 00:20:05,559
to false in the conflate yeah so if

431
00:20:02,420 --> 00:20:07,610
you're watching the video pause now and

432
00:20:05,559 --> 00:20:08,929
see if you can figure this out right

433
00:20:07,609 --> 00:20:11,209
because this is a really interesting

434
00:20:08,930 --> 00:20:14,720
question is like why don't we need bias

435
00:20:11,210 --> 00:20:18,670
okay so I wait for you to pause okay

436
00:20:14,720 --> 00:20:18,670
welcome back so if you figured it out

437
00:20:18,819 --> 00:20:22,789
here's the thing right

438
00:20:20,599 --> 00:20:26,569
immediately after the Kahn is a batch

439
00:20:22,789 --> 00:20:29,420
not and remember batch norm has to learn

440
00:20:26,569 --> 00:20:32,089
herbal parameters for each activation

441
00:20:29,420 --> 00:20:35,480
the the kind of the thing you multiply

442
00:20:32,089 --> 00:20:38,299
by and the thing you add so since we're

443
00:20:35,480 --> 00:20:40,250
if we if we had bias here to add and

444
00:20:38,299 --> 00:20:41,809
then we add another thing here we're

445
00:20:40,250 --> 00:20:43,339
adding to you things which is totally

446
00:20:41,809 --> 00:20:45,169
pointless like that's two weights where

447
00:20:43,339 --> 00:20:48,769
one would do right so if you have a

448
00:20:45,170 --> 00:20:51,350
batch norm after a cons then you can you

449
00:20:48,769 --> 00:20:53,210
can either say in the batch norm don't

450
00:20:51,349 --> 00:20:55,459
don't include the add bit there please

451
00:20:53,210 --> 00:21:00,500
or easier is just to say don't include

452
00:20:55,460 --> 00:21:02,840
the bias in it there's no particular

453
00:21:00,500 --> 00:21:04,849
harm but again it's it's it's gonna take

454
00:21:02,839 --> 00:21:07,959
more memory because that's more

455
00:21:04,849 --> 00:21:12,919
gradients that it has to keep track of

456
00:21:07,960 --> 00:21:16,250
so best to avoid also another thing

457
00:21:12,920 --> 00:21:19,340
little trick is most people's comm

458
00:21:16,250 --> 00:21:20,480
players have padding as a parameter but

459
00:21:19,339 --> 00:21:22,669
generally speaking you should be able to

460
00:21:20,480 --> 00:21:24,799
calculate the padding basically enough

461
00:21:22,670 --> 00:21:27,890
right and I see people like trying to

462
00:21:24,799 --> 00:21:29,779
like implement you know special same

463
00:21:27,890 --> 00:21:31,730
padding modules and all kinds of stuff

464
00:21:29,779 --> 00:21:34,730
like that but like if you've got a

465
00:21:31,730 --> 00:21:38,390
stride one and you've got or pretty much

466
00:21:34,730 --> 00:21:42,559
any strategy and you've got padding of

467
00:21:38,390 --> 00:21:44,830
oh sorry and kernel size of three right

468
00:21:42,559 --> 00:21:48,349
then obviously that's going to overlap

469
00:21:44,829 --> 00:21:50,990
by kind of one unit on each side so we

470
00:21:48,349 --> 00:21:54,019
want padding of one or else if its

471
00:21:50,990 --> 00:21:55,730
stride one then we don't need any

472
00:21:54,019 --> 00:21:59,079
padding so in general padding of kernel

473
00:21:55,730 --> 00:22:01,759
size integer divided by two

474
00:21:59,079 --> 00:22:03,259
that's what you need there's some tweaks

475
00:22:01,759 --> 00:22:05,929
sometimes but in this case this works

476
00:22:03,259 --> 00:22:09,200
perfectly well so again trying to

477
00:22:05,929 --> 00:22:11,090
simplify my code by having the computer

478
00:22:09,200 --> 00:22:15,440
calculates stuff for me rather than me

479
00:22:11,089 --> 00:22:17,240
having to do it myself another thing

480
00:22:15,440 --> 00:22:20,210
here with the two common players so we

481
00:22:17,240 --> 00:22:22,038
we kind of have this idea of a

482
00:22:20,210 --> 00:22:23,390
bottleneck this idea of reducing the

483
00:22:22,038 --> 00:22:26,390
channels and then increasing them again

484
00:22:23,390 --> 00:22:29,179
is also what kernel size we use so

485
00:22:26,390 --> 00:22:30,799
here's a one by one conf right and so

486
00:22:29,179 --> 00:22:32,538
this is again something you might want

487
00:22:30,798 --> 00:22:36,139
to pause the video now and think about

488
00:22:32,538 --> 00:22:42,950
what's the one by one con really what

489
00:22:36,140 --> 00:22:47,950
actually happens in a one by one con so

490
00:22:42,950 --> 00:22:52,808
if we've got you know a little 4x4 grid

491
00:22:47,950 --> 00:22:56,058
here right and of course there's a

492
00:22:52,808 --> 00:22:58,970
filters or channels access as well maybe

493
00:22:56,058 --> 00:23:01,940
that's like 32 okay and we're going to

494
00:22:58,970 --> 00:23:04,850
do a one by one cons so what's the

495
00:23:01,940 --> 00:23:14,840
kernel for a one by one Kong's gonna

496
00:23:04,849 --> 00:23:16,339
look like it's gonna be 1 by 32 right so

497
00:23:14,839 --> 00:23:19,970
remember when we talked about the the

498
00:23:16,339 --> 00:23:21,678
kernel size we never mention that last

499
00:23:19,970 --> 00:23:23,450
piece but let's say it's 1 by 1 by 32

500
00:23:21,679 --> 00:23:25,610
because that's part of the filters in

501
00:23:23,450 --> 00:23:28,788
and filters out so in other words then

502
00:23:25,609 --> 00:23:31,879
what happens is this this one thing gets

503
00:23:28,788 --> 00:23:34,460
placed first of all here on the first

504
00:23:31,880 --> 00:23:39,260
cell and we basically get a dot product

505
00:23:34,460 --> 00:23:41,720
of that 32 deep bit with this 32 bit

506
00:23:39,259 --> 00:23:45,859
deep bit and that's going to give us our

507
00:23:41,720 --> 00:23:48,169
first oops and that's going to give us

508
00:23:45,859 --> 00:23:50,089
our first output alright and then we're

509
00:23:48,169 --> 00:23:51,440
going to take that 32 bit bit and put it

510
00:23:50,089 --> 00:23:54,168
with the second one to get the second

511
00:23:51,440 --> 00:23:58,220
output right so it's basically going to

512
00:23:54,169 --> 00:24:02,210
be a bunch of little dot pallets ok for

513
00:23:58,220 --> 00:24:06,579
each point in the grid so it's what it

514
00:24:02,210 --> 00:24:06,579
basically is then is a

515
00:24:09,430 --> 00:24:14,440
it's basically something which is

516
00:24:11,049 --> 00:24:18,819
allowing us to to to kind of change the

517
00:24:14,440 --> 00:24:23,529
dimensionality in whatever way we want

518
00:24:18,819 --> 00:24:27,399
in the in the channel dimension and so

519
00:24:23,529 --> 00:24:29,829
that would be that would be one of our

520
00:24:27,400 --> 00:24:33,430
filters right and so in this case we're

521
00:24:29,829 --> 00:24:35,980
creating an AI divided by two of these

522
00:24:33,430 --> 00:24:37,570
right so we're going to have ni divided

523
00:24:35,980 --> 00:24:39,970
by two of these dot products always

524
00:24:37,569 --> 00:24:42,609
different basically different weighted

525
00:24:39,970 --> 00:24:46,450
averages of the input channels okay so

526
00:24:42,609 --> 00:24:50,699
it basically lets us you know with very

527
00:24:46,450 --> 00:24:54,779
little computation at this additional

528
00:24:50,700 --> 00:24:57,160
step of calculations and nonlinearities

529
00:24:54,779 --> 00:24:58,869
so that's what that's a cool trick you

530
00:24:57,160 --> 00:25:01,240
know this idea of taking advantage of

531
00:24:58,869 --> 00:25:03,699
these one by one cons creating this

532
00:25:01,240 --> 00:25:05,559
bottleneck and then pulling it out again

533
00:25:03,700 --> 00:25:08,039
with three by three comms so that's

534
00:25:05,559 --> 00:25:10,779
actually going to take advantage of the

535
00:25:08,039 --> 00:25:13,539
you know the 2d nature of the input

536
00:25:10,779 --> 00:25:18,220
properly real so one by one comm doesn't

537
00:25:13,539 --> 00:25:21,629
take advantage of that at all so these

538
00:25:18,220 --> 00:25:24,759
two lines of code there's not much in it

539
00:25:21,630 --> 00:25:27,060
but it's a really great test of your

540
00:25:24,759 --> 00:25:30,609
understanding and kind of your intuition

541
00:25:27,059 --> 00:25:33,639
about what's going on why is it that a

542
00:25:30,609 --> 00:25:35,649
one by one cond going from ni to NI over

543
00:25:33,640 --> 00:25:37,720
two channels followed by a three by

544
00:25:35,650 --> 00:25:40,630
three conferring from ni over to de and

545
00:25:37,720 --> 00:25:42,910
i and i channels like why does it work

546
00:25:40,630 --> 00:25:45,850
why do the tensor ranks line up why do

547
00:25:42,910 --> 00:25:48,210
the dimensions or line up nicely why is

548
00:25:45,849 --> 00:25:50,199
it a good idea what's it really doing

549
00:25:48,210 --> 00:25:52,059
like it's a really good thing to fiddle

550
00:25:50,200 --> 00:25:54,880
a fiddle around with maybe create some

551
00:25:52,059 --> 00:25:56,889
small ones in jupiter notebook you know

552
00:25:54,880 --> 00:25:58,750
run them yourself see what inputs now

553
00:25:56,890 --> 00:26:02,460
let's come in and out you know really

554
00:25:58,750 --> 00:26:05,410
get a feel for that once you've done so

555
00:26:02,460 --> 00:26:09,910
you can then play around with different

556
00:26:05,410 --> 00:26:16,050
things right and there's actually one of

557
00:26:09,910 --> 00:26:16,050
the really unappreciated papers

558
00:26:16,690 --> 00:26:25,059
is this one wide residual networks okay

559
00:26:21,460 --> 00:26:27,400
and it's really quite a simple paper but

560
00:26:25,059 --> 00:26:31,379
what they do is they basically fiddle

561
00:26:27,400 --> 00:26:34,330
around with with these two lines of code

562
00:26:31,380 --> 00:26:37,570
right and what they do is they say well

563
00:26:34,329 --> 00:26:40,869
what if this wasn't 2/2 but what if it

564
00:26:37,569 --> 00:26:42,639
was times two like that'd be totally

565
00:26:40,869 --> 00:26:49,139
allowable all right that's going to line

566
00:26:42,640 --> 00:26:49,140
up nicely or what if we had another

567
00:26:50,759 --> 00:26:55,839
comfrey

568
00:26:51,910 --> 00:27:00,550
after this and so this was actually ni

569
00:26:55,839 --> 00:27:02,679
over to - ni / - and then this is an AO

570
00:27:00,549 --> 00:27:05,470
to again that's going to work right

571
00:27:02,680 --> 00:27:07,930
kernel say is one three one going to

572
00:27:05,470 --> 00:27:09,309
half the number of kernels leave it at

573
00:27:07,930 --> 00:27:11,680
half and then double it again at the end

574
00:27:09,309 --> 00:27:16,980
and so they come up with this kind of

575
00:27:11,680 --> 00:27:19,600
simple notation for basically defining

576
00:27:16,980 --> 00:27:22,630
what this can look like and then they

577
00:27:19,599 --> 00:27:27,490
show lots of experiments and basically

578
00:27:22,630 --> 00:27:29,680
what they show is that this approach of

579
00:27:27,490 --> 00:27:31,539
a bottlenecking

580
00:27:29,680 --> 00:27:33,400
of decreasing the number of channels

581
00:27:31,539 --> 00:27:36,849
which is like almost universal and

582
00:27:33,400 --> 00:27:38,140
resinates is probably not a good idea in

583
00:27:36,849 --> 00:27:40,509
fact from the experiments definitely not

584
00:27:38,140 --> 00:27:42,610
a good idea because what happens is it

585
00:27:40,509 --> 00:27:44,470
lets you create really deep networks

586
00:27:42,609 --> 00:27:46,859
right and the guys who created resinates

587
00:27:44,470 --> 00:27:49,029
got particularly famous recruiting a

588
00:27:46,859 --> 00:27:50,949
1001 bio network

589
00:27:49,029 --> 00:27:52,660
ok but the thing about a thousand and

590
00:27:50,950 --> 00:27:55,269
one layers is you can't calculate layer

591
00:27:52,660 --> 00:27:56,740
two until you're finished layer 1 from

592
00:27:55,269 --> 00:27:58,269
jakarta calculate layer 3 until you

593
00:27:56,740 --> 00:28:02,470
finish calculating where - so it's

594
00:27:58,269 --> 00:28:04,720
sequential GPUs don't like sequential so

595
00:28:02,470 --> 00:28:07,990
what they showed is that if you have

596
00:28:04,720 --> 00:28:10,720
less layers fat with more active with

597
00:28:07,990 --> 00:28:12,190
more calculations per layer and so one

598
00:28:10,720 --> 00:28:16,450
easy way to do that would be to remove

599
00:28:12,190 --> 00:28:17,529
the / - no other changes all right like

600
00:28:16,450 --> 00:28:19,660
try this at home

601
00:28:17,529 --> 00:28:21,910
try running sci-fi and see what happens

602
00:28:19,660 --> 00:28:25,210
right or make even more apply it back to

603
00:28:21,910 --> 00:28:27,940
you or fiddle around and that basically

604
00:28:25,210 --> 00:28:29,890
lets your GPU do more work and it's very

605
00:28:27,940 --> 00:28:30,909
interesting because the vast majority of

606
00:28:29,890 --> 00:28:32,619
papers that talk

607
00:28:30,909 --> 00:28:35,559
about performance of different

608
00:28:32,618 --> 00:28:39,970
architectures never actually time how

609
00:28:35,558 --> 00:28:42,788
long it takes to run a batch throat like

610
00:28:39,970 --> 00:28:45,069
they literally say this one requires X

611
00:28:42,788 --> 00:28:47,319
number of floating-point operations per

612
00:28:45,069 --> 00:28:49,210
batch but then they never actually

613
00:28:47,319 --> 00:28:51,158
bother to run the damn thing like a

614
00:28:49,210 --> 00:28:53,528
proper experimentalist and find out

615
00:28:51,159 --> 00:28:55,809
whether it's faster or slower and so a

616
00:28:53,528 --> 00:28:58,720
lot of the architectures that are really

617
00:28:55,808 --> 00:29:00,970
famous now turn out to be slow as

618
00:28:58,720 --> 00:29:03,899
molasses and take crap loads of memory

619
00:29:00,970 --> 00:29:06,009
and just totally useless because the

620
00:29:03,898 --> 00:29:07,839
that the research has never actually

621
00:29:06,009 --> 00:29:09,639
bothered to see whether they're fast and

622
00:29:07,839 --> 00:29:12,278
to actually see whether they fit in RAM

623
00:29:09,638 --> 00:29:15,128
with normal size batch batch sizes so

624
00:29:12,278 --> 00:29:18,220
the wide ResNet paper is unusual in that

625
00:29:15,128 --> 00:29:21,009
it actually times how long it takes as

626
00:29:18,220 --> 00:29:22,629
does the yellow version 3 paper which

627
00:29:21,009 --> 00:29:23,558
which made the same insight I'm not sure

628
00:29:22,628 --> 00:29:25,538
they might have missed the wide

629
00:29:23,558 --> 00:29:27,339
resonance paper because that the yellow

630
00:29:25,538 --> 00:29:30,519
version 3 paper came to a lot of the

631
00:29:27,339 --> 00:29:32,108
same conclusions but I'm not even sure

632
00:29:30,519 --> 00:29:33,729
they cited the wide resinous paper so

633
00:29:32,108 --> 00:29:37,538
they might not be aware that all that

634
00:29:33,729 --> 00:29:39,489
works being done but they're both both

635
00:29:37,538 --> 00:29:41,470
great to see people actually timing

636
00:29:39,489 --> 00:29:41,950
things and noticing what actually makes

637
00:29:41,470 --> 00:29:46,749
sense

638
00:29:41,950 --> 00:29:48,460
yes which cell you looked really hot in

639
00:29:46,749 --> 00:29:49,839
the paper which came out but I noticed

640
00:29:48,460 --> 00:29:55,108
that you don't use it what's your

641
00:29:49,839 --> 00:29:57,970
opinion on cell you so sell you is

642
00:29:55,108 --> 00:29:59,618
something largely for fully connected

643
00:29:57,970 --> 00:30:02,108
layers which allows you to get rid of

644
00:29:59,618 --> 00:30:03,368
batch norm and the basic idea is that if

645
00:30:02,108 --> 00:30:06,728
you use this different activation

646
00:30:03,368 --> 00:30:09,098
function it's it's kind of self

647
00:30:06,729 --> 00:30:12,249
normalizing that's what yes and so it

648
00:30:09,098 --> 00:30:14,229
stands for so self normalizing means

649
00:30:12,249 --> 00:30:16,028
that all always remain at a unit

650
00:30:14,229 --> 00:30:18,659
standard deviation and zero mean and

651
00:30:16,028 --> 00:30:21,729
therefore you don't need that pattern on

652
00:30:18,659 --> 00:30:23,109
it hasn't really gone anywhere and the

653
00:30:21,729 --> 00:30:25,720
reason it hasn't really gone anywhere is

654
00:30:23,108 --> 00:30:27,608
because it's incredibly finicky you have

655
00:30:25,720 --> 00:30:29,979
to use a very specific initialization

656
00:30:27,608 --> 00:30:34,079
otherwise it doesn't start with exactly

657
00:30:29,979 --> 00:30:36,098
the right standard deviation of mean

658
00:30:34,079 --> 00:30:38,470
very hard to use it with things like

659
00:30:36,098 --> 00:30:40,088
embeddings if you do then you have to

660
00:30:38,470 --> 00:30:41,470
use a particular kind of embedding

661
00:30:40,088 --> 00:30:44,750
initialization which doesn't necessarily

662
00:30:41,470 --> 00:30:48,548
actually make sense for embeddings

663
00:30:44,750 --> 00:30:50,659
so you know and you do all this work

664
00:30:48,548 --> 00:30:51,829
very hard to get it right

665
00:30:50,659 --> 00:30:53,480
and if you do finally get it right

666
00:30:51,829 --> 00:30:55,099
what's the point well you've managed to

667
00:30:53,480 --> 00:30:58,370
get rid of some batch norm layers which

668
00:30:55,099 --> 00:30:59,839
weren't really hurting you anyway and

669
00:30:58,369 --> 00:31:01,639
it's interesting because that paper that

670
00:30:59,839 --> 00:31:03,829
sell me a paper I think one of the

671
00:31:01,640 --> 00:31:04,759
reasons people noticed it or in my

672
00:31:03,829 --> 00:31:06,619
experience the main reason people

673
00:31:04,759 --> 00:31:09,589
noticed it was because it was created by

674
00:31:06,619 --> 00:31:11,959
the inventor of LST m/s and also it had

675
00:31:09,589 --> 00:31:14,599
a huge mathematical appendix and people

676
00:31:11,960 --> 00:31:17,630
were like lots of maths from a famous

677
00:31:14,599 --> 00:31:22,129
guy this must be great you know but in

678
00:31:17,630 --> 00:31:23,809
practice I don't see anybody using it to

679
00:31:22,130 --> 00:31:25,159
get any state-of-the-art results or win

680
00:31:23,808 --> 00:31:31,369
any competitions or anything like that

681
00:31:25,159 --> 00:31:32,870
okay so this is like some of the tiniest

682
00:31:31,369 --> 00:31:34,489
bits of code we've seen but there's so

683
00:31:32,869 --> 00:31:37,819
much here and it's fascinating to play

684
00:31:34,490 --> 00:31:40,099
with so now we've brought this block

685
00:31:37,819 --> 00:31:41,720
which is built on this block and then

686
00:31:40,099 --> 00:31:45,379
we're going to create another block on

687
00:31:41,720 --> 00:31:47,929
top of that block okay so we're going to

688
00:31:45,380 --> 00:31:49,070
call this a group layer and it's going

689
00:31:47,929 --> 00:31:52,788
to create a bucket it's going to contain

690
00:31:49,069 --> 00:31:55,788
a bunch of rez layers and so a group

691
00:31:52,788 --> 00:32:00,230
layer it's going to have some number of

692
00:31:55,788 --> 00:32:02,599
channels or filters coming in okay and

693
00:32:00,230 --> 00:32:05,900
what we're going to do is we're going to

694
00:32:02,599 --> 00:32:09,849
double the number of channels coming in

695
00:32:05,900 --> 00:32:13,159
by just using a standard comm flare

696
00:32:09,849 --> 00:32:16,459
optionally we'll have the grid size by

697
00:32:13,159 --> 00:32:20,179
using a stride of two okay and then

698
00:32:16,460 --> 00:32:22,130
we're going to do a whole bunch of rez

699
00:32:20,179 --> 00:32:24,140
blocks a whole bunch of rez layers we

700
00:32:22,130 --> 00:32:26,240
can pick how many that could be two or

701
00:32:24,140 --> 00:32:28,520
three or eight back because remember

702
00:32:26,240 --> 00:32:31,339
these rez layers don't change the grid

703
00:32:28,519 --> 00:32:33,200
size and they don't change the number of

704
00:32:31,339 --> 00:32:35,869
channels so you can add as many as you

705
00:32:33,200 --> 00:32:37,909
like anywhere you like without causing

706
00:32:35,869 --> 00:32:40,908
any problems and this is going to use

707
00:32:37,909 --> 00:32:42,770
more computation and more RAM but

708
00:32:40,909 --> 00:32:45,409
there's no reason other than that you

709
00:32:42,769 --> 00:32:48,548
can't add as many as you like so a group

710
00:32:45,409 --> 00:32:51,140
layer therefore it's going to end up

711
00:32:48,548 --> 00:32:52,940
doubling the number of channels because

712
00:32:51,140 --> 00:32:55,270
if this initial convolution which

713
00:32:52,940 --> 00:32:57,590
doubles the number of channels

714
00:32:55,269 --> 00:32:58,190
its initial convolution doubles the

715
00:32:57,589 --> 00:32:59,839
number of channels

716
00:32:58,190 --> 00:33:03,200
and depending on what we pass in a

717
00:32:59,839 --> 00:33:05,839
stride it may also have the grid size if

718
00:33:03,200 --> 00:33:09,798
we put straight equals to and then we

719
00:33:05,839 --> 00:33:12,829
can do a whole bunch of res block

720
00:33:09,798 --> 00:33:17,418
computations as many as we like all

721
00:33:12,829 --> 00:33:19,869
right so then to define our darknet or

722
00:33:17,419 --> 00:33:23,240
whatever we want to call this thing

723
00:33:19,869 --> 00:33:25,729
we're just going to pass in something

724
00:33:23,240 --> 00:33:29,058
that looks like this

725
00:33:25,730 --> 00:33:32,509
and what this says is create five group

726
00:33:29,058 --> 00:33:34,849
layers the first one will contain one of

727
00:33:32,509 --> 00:33:39,500
these extra rez layers the second will

728
00:33:34,849 --> 00:33:44,469
contain two then 4 then 6 then 3 and I

729
00:33:39,500 --> 00:33:44,470
want you to start with 32 filters

730
00:33:46,058 --> 00:33:56,720
alright so the first one of these res

731
00:33:52,278 --> 00:33:59,329
res layers will contain 32 filters and

732
00:33:56,720 --> 00:34:01,819
they'll just be one extra rez layer the

733
00:33:59,329 --> 00:34:03,558
second one it's going to double the

734
00:34:01,819 --> 00:34:05,418
number of filters because that's what we

735
00:34:03,558 --> 00:34:06,769
do each time we have a new group layer

736
00:34:05,419 --> 00:34:10,519
we double the number so the second one

737
00:34:06,769 --> 00:34:14,179
will have 64 and then 128 then 256 and

738
00:34:10,519 --> 00:34:16,489
then 512 and then that'll be it all

739
00:34:14,179 --> 00:34:18,398
right so that's going to be like nearly

740
00:34:16,489 --> 00:34:21,019
all of the network is going to be those

741
00:34:18,398 --> 00:34:23,989
bunches of layers and remember every one

742
00:34:21,019 --> 00:34:27,199
of those group layers also has one

743
00:34:23,989 --> 00:34:29,568
convolution of this data okay

744
00:34:27,199 --> 00:34:32,239
and so then all we have is before that

745
00:34:29,568 --> 00:34:34,568
all happens we're going to have one

746
00:34:32,239 --> 00:34:36,918
convolutional layer at the very start

747
00:34:34,568 --> 00:34:39,219
and at the very end we're going to do

748
00:34:36,918 --> 00:34:42,648
our standard adaptive average pooling

749
00:34:39,219 --> 00:34:44,388
flatten and a linear layer to create the

750
00:34:42,648 --> 00:34:46,628
number of classes out at the edge

751
00:34:44,389 --> 00:34:49,429
alright so one convolution at the end

752
00:34:46,628 --> 00:34:52,128
that we're pulling and one linear layer

753
00:34:49,429 --> 00:34:55,369
at the other end and then in the middle

754
00:34:52,128 --> 00:34:58,250
these group layers each one consisting

755
00:34:55,369 --> 00:35:02,858
of a convolution or layer followed by n

756
00:34:58,250 --> 00:35:04,789
number of resumes and that's that's it

757
00:35:02,858 --> 00:35:08,869
again I think we've mentioned this a few

758
00:35:04,789 --> 00:35:11,429
times but I'm yet to see any code out

759
00:35:08,869 --> 00:35:14,280
there any any exam

760
00:35:11,429 --> 00:35:16,558
Falls anything anywhere that uses

761
00:35:14,280 --> 00:35:20,339
adaptive average cooling everyone I've

762
00:35:16,559 --> 00:35:22,950
seen rats it like this and then spits a

763
00:35:20,338 --> 00:35:25,349
particular number here right which means

764
00:35:22,949 --> 00:35:26,848
that it's now tied to a particular image

765
00:35:25,349 --> 00:35:27,180
size which definitely isn't what you

766
00:35:26,849 --> 00:35:30,088
want

767
00:35:27,179 --> 00:35:32,429
so most people even the top researchers

768
00:35:30,088 --> 00:35:35,219
I speak to most of them are still under

769
00:35:32,429 --> 00:35:37,759
the impression that a specific

770
00:35:35,219 --> 00:35:40,618
architecture is tied to a specific size

771
00:35:37,760 --> 00:35:42,300
and that's a huge problem when people

772
00:35:40,619 --> 00:35:45,599
think that because it really limits

773
00:35:42,300 --> 00:35:47,579
their ability to like use smaller sizes

774
00:35:45,599 --> 00:35:48,930
to kind of kick-start their modeling or

775
00:35:47,579 --> 00:35:53,880
to use smaller sizes for doing

776
00:35:48,929 --> 00:35:55,500
experiments and stuff like that again

777
00:35:53,880 --> 00:35:57,210
you'll notice I'm using sequential here

778
00:35:55,500 --> 00:35:59,280
rather than a nice way to create

779
00:35:57,210 --> 00:36:00,720
architectures is to start out by

780
00:35:59,280 --> 00:36:02,519
creating a list in this case this is a

781
00:36:00,719 --> 00:36:05,039
list with just one comp layer in and

782
00:36:02,519 --> 00:36:08,550
then my function here make group layer

783
00:36:05,039 --> 00:36:10,829
it just returns another list right so

784
00:36:08,550 --> 00:36:12,599
then I can just go plus equals patter

785
00:36:10,829 --> 00:36:15,059
pending that list to the previous list

786
00:36:12,599 --> 00:36:16,619
and then I could go plus equals to a pen

787
00:36:15,059 --> 00:36:19,230
this bunch of things to that list and

788
00:36:16,619 --> 00:36:21,000
then finally sequential of all those

789
00:36:19,230 --> 00:36:23,548
layers right so there's a very nice

790
00:36:21,000 --> 00:36:26,460
thing so now my forward is just self dot

791
00:36:23,548 --> 00:36:29,608
layers okay so here's a kind of you know

792
00:36:26,460 --> 00:36:32,068
this is a nice kind of picture of how to

793
00:36:29,608 --> 00:36:34,588
make your architectures as simple as

794
00:36:32,068 --> 00:36:37,469
possible okay so you can now go ahead

795
00:36:34,588 --> 00:36:38,759
and create this and as I say you can

796
00:36:37,469 --> 00:36:40,739
fiddle around you know you could even

797
00:36:38,760 --> 00:36:42,809
parameterize this too to make it a

798
00:36:40,739 --> 00:36:44,669
number that you kind of pass in here's

799
00:36:42,809 --> 00:36:47,519
to pass in different numbers so it's not

800
00:36:44,670 --> 00:36:49,170
too maybe it's times two instead you

801
00:36:47,519 --> 00:36:50,699
could pass in things that change the

802
00:36:49,170 --> 00:36:52,470
kernel size or change the number of

803
00:36:50,699 --> 00:36:54,328
convolutional layers you know fiddle

804
00:36:52,469 --> 00:36:56,459
around with it and maybe you can create

805
00:36:54,329 --> 00:36:58,798
something I've actually got a version of

806
00:36:56,460 --> 00:37:01,858
this which I'm about to run for you

807
00:36:58,798 --> 00:37:04,318
which kind of implements all of the

808
00:37:01,858 --> 00:37:06,630
different parameters that's in that wide

809
00:37:04,318 --> 00:37:09,659
ResNet paper so I could fiddle around

810
00:37:06,630 --> 00:37:12,210
let's see what worked well so once we've

811
00:37:09,659 --> 00:37:14,279
got that we can use confluent from

812
00:37:12,210 --> 00:37:17,460
bottle data to take our pipe watch model

813
00:37:14,280 --> 00:37:19,230
module and that a model data object and

814
00:37:17,460 --> 00:37:22,139
turn them into a learner give it a

815
00:37:19,230 --> 00:37:24,530
criterion that's a metric so if we like

816
00:37:22,139 --> 00:37:28,039
and then we can call fit in a way

817
00:37:24,530 --> 00:37:30,110
go could you please explain adaptive

818
00:37:28,039 --> 00:37:36,409
average pooling how does setting to one

819
00:37:30,110 --> 00:37:38,030
work sure before I do I just want to

820
00:37:36,409 --> 00:37:41,719
like since we've only got a certain

821
00:37:38,030 --> 00:37:48,700
amount of time in this class I wanted to

822
00:37:41,719 --> 00:37:51,799
see I do want to see how we go you know

823
00:37:48,699 --> 00:37:54,559
with this simple network against these

824
00:37:51,800 --> 00:37:57,130
state-of-the-art results so to make life

825
00:37:54,559 --> 00:38:00,889
a little easier so we can reconstruct

826
00:37:57,130 --> 00:38:04,309
later so I've got the command ready to

827
00:38:00,889 --> 00:38:05,779
go so we've basically take taken all

828
00:38:04,309 --> 00:38:08,210
that stuff and put it into a simple

829
00:38:05,780 --> 00:38:10,550
little Python script and I've modified

830
00:38:08,210 --> 00:38:12,619
some of those parameters I mentioned to

831
00:38:10,550 --> 00:38:14,330
create something I've caught of wrn 22

832
00:38:12,619 --> 00:38:16,819
Network which doesn't officially exist

833
00:38:14,329 --> 00:38:18,289
but it's got a bunch of changes to the

834
00:38:16,820 --> 00:38:21,890
parameters we talked about based on my

835
00:38:18,289 --> 00:38:25,219
experiments we're going to use the new

836
00:38:21,889 --> 00:38:27,139
leslie smith one cycle thing so there's

837
00:38:25,219 --> 00:38:28,819
quite a bunch of cool stuff here so the

838
00:38:27,139 --> 00:38:31,250
one cycle implementation was done by our

839
00:38:28,820 --> 00:38:33,670
students yoga i think i don't know how

840
00:38:31,250 --> 00:38:36,710
to pronounce his name exactly so where

841
00:38:33,670 --> 00:38:38,889
this the trains life our experiments

842
00:38:36,710 --> 00:38:41,720
were largely done by brett currents and

843
00:38:38,889 --> 00:38:44,750
stuff like getting the half position

844
00:38:41,719 --> 00:38:47,899
floating-point implementation integrated

845
00:38:44,750 --> 00:38:50,900
into fast AI was done by Andrew Shaw

846
00:38:47,900 --> 00:38:52,160
so it's been a cool kind of bunch of

847
00:38:50,900 --> 00:38:55,369
different student projects coming

848
00:38:52,159 --> 00:38:57,859
together to allow us to run this so this

849
00:38:55,369 --> 00:39:03,710
is going to run actually on a AWS Amazon

850
00:38:57,860 --> 00:39:06,769
AWS p3 which has eight GPUs the p3 has

851
00:39:03,710 --> 00:39:09,170
these newer Volta architecture GPUs

852
00:39:06,769 --> 00:39:12,289
which actually have special support for

853
00:39:09,170 --> 00:39:15,110
half precision floating point first AI

854
00:39:12,289 --> 00:39:17,570
is the first library I know of to

855
00:39:15,110 --> 00:39:18,200
actually integrate the volatile

856
00:39:17,570 --> 00:39:19,910
optimized

857
00:39:18,199 --> 00:39:22,519
half position floating point into the

858
00:39:19,909 --> 00:39:26,629
library so we can just go learn half now

859
00:39:22,519 --> 00:39:28,849
and get that support automatically and

860
00:39:26,630 --> 00:39:30,800
is also the first one to integrate one

861
00:39:28,849 --> 00:39:35,210
cycle so these are the parameters for

862
00:39:30,800 --> 00:39:36,950
the one cycle so we can go ahead and get

863
00:39:35,210 --> 00:39:38,079
this running so what this actually does

864
00:39:36,949 --> 00:39:42,460
is it's

865
00:39:38,079 --> 00:39:43,989
using PI torches multi-gpu support since

866
00:39:42,460 --> 00:39:46,240
there are eight GPUs it's actually going

867
00:39:43,989 --> 00:39:48,789
to fire off eight separate Python

868
00:39:46,239 --> 00:39:51,549
processors and each one's going to train

869
00:39:48,789 --> 00:39:54,759
on a little bit and then at the end it's

870
00:39:51,550 --> 00:39:56,920
going to pass the gradient updates back

871
00:39:54,760 --> 00:39:59,080
to kind of the master process that's

872
00:39:56,920 --> 00:40:03,550
going to integrate them all together so

873
00:39:59,079 --> 00:40:06,519
you'll see here they are right lots of

874
00:40:03,550 --> 00:40:10,060
progress bars will pop up together and

875
00:40:06,519 --> 00:40:13,409
you can see it's training you know three

876
00:40:10,059 --> 00:40:20,289
or four seconds when you do it this way

877
00:40:13,409 --> 00:40:22,389
where else when I had where else when I

878
00:40:20,289 --> 00:40:25,000
was training earlier I was getting let's

879
00:40:22,389 --> 00:40:26,920
see 30 epochs in I was getting about 30

880
00:40:25,000 --> 00:40:29,619
seconds per clock so doing it this way

881
00:40:26,920 --> 00:40:33,240
we can kind of train things like 10

882
00:40:29,619 --> 00:40:36,190
times faster or so which is pretty cool

883
00:40:33,239 --> 00:40:37,929
okay so we'll leave that running so you

884
00:40:36,190 --> 00:40:39,700
are asking about adaptive average

885
00:40:37,929 --> 00:40:47,049
pooling and I think specifically is

886
00:40:39,699 --> 00:40:53,109
what's the number one doing so normally

887
00:40:47,050 --> 00:40:56,320
when we're doing average pooling let's

888
00:40:53,110 --> 00:41:05,860
say we've got four by four let's say we

889
00:40:56,320 --> 00:41:11,200
did average pooling - comma - okay then

890
00:41:05,860 --> 00:41:16,570
that creates a 2x2 area and takes the

891
00:41:11,199 --> 00:41:22,210
average of those four right and then we

892
00:41:16,570 --> 00:41:24,370
can pass in the stride all right so if

893
00:41:22,210 --> 00:41:27,340
we said stride one then the next one is

894
00:41:24,369 --> 00:41:29,799
we look at this block of 2x2 and take

895
00:41:27,340 --> 00:41:31,840
that average and so forth right so

896
00:41:29,800 --> 00:41:35,710
that's like what a normal to go to

897
00:41:31,840 --> 00:41:37,570
average pooling would be and so that

898
00:41:35,710 --> 00:41:42,340
would in that case if we didn't have any

899
00:41:37,570 --> 00:41:46,140
parting that would spit out a 3x3 okay

900
00:41:42,340 --> 00:41:48,579
because it's true here to here to here

901
00:41:46,139 --> 00:41:51,730
okay and if we added padding we can make

902
00:41:48,579 --> 00:41:54,099
it three by three as well that's right

903
00:41:51,730 --> 00:41:56,170
so if we wanted to spit out something we

904
00:41:54,099 --> 00:42:00,659
didn't want 3x3 what if we wanted one by

905
00:41:56,170 --> 00:42:05,260
one right then we could say average pull

906
00:42:00,659 --> 00:42:10,629
four comma four right and so that's

907
00:42:05,260 --> 00:42:14,200
gonna do 4 comma 4 and average the whole

908
00:42:10,630 --> 00:42:18,390
lot right and that would spit out one by

909
00:42:14,199 --> 00:42:22,689
one but that's just one way to do it

910
00:42:18,389 --> 00:42:25,000
rather than saying the size of the the

911
00:42:22,690 --> 00:42:27,369
kind of appalling filter

912
00:42:25,000 --> 00:42:29,650
why don't we instead say well I don't

913
00:42:27,369 --> 00:42:33,130
care what the size of the input grid is

914
00:42:29,650 --> 00:42:38,320
I always want one by one right so that's

915
00:42:33,130 --> 00:42:41,559
where then you say adaptive average Hall

916
00:42:38,320 --> 00:42:43,330
and now you don't say what's the size of

917
00:42:41,559 --> 00:42:44,769
the palling filter you instead say

918
00:42:43,329 --> 00:42:47,049
what's the size of the app what I want

919
00:42:44,769 --> 00:42:49,300
and so I want something that serve one

920
00:42:47,050 --> 00:42:52,360
by one and if you already put a single

921
00:42:49,300 --> 00:42:55,769
int it assumes you mean one by one so in

922
00:42:52,360 --> 00:42:59,440
this case adaptive average pooling one

923
00:42:55,769 --> 00:43:01,570
with a four by four grid coming in is

924
00:42:59,440 --> 00:43:04,570
the same as average pooling for Hummer

925
00:43:01,570 --> 00:43:06,910
four if it was a 7x7 grid coming in it

926
00:43:04,570 --> 00:43:08,710
would be the same as 7 comma 7 right so

927
00:43:06,909 --> 00:43:10,239
it's the same operation it's just

928
00:43:08,710 --> 00:43:12,130
expressing it in a way that says

929
00:43:10,239 --> 00:43:19,769
regardless of the input I want something

930
00:43:12,130 --> 00:43:23,190
of that sized output please okay

931
00:43:19,769 --> 00:43:29,920
how's our little thing going along oh

932
00:43:23,190 --> 00:43:33,579
okay well we got 294 and it took three

933
00:43:29,920 --> 00:43:35,170
minutes and 11 seconds and the previous

934
00:43:33,579 --> 00:43:38,799
state of the art was one hour and seven

935
00:43:35,170 --> 00:43:40,180
minutes so was it worth fiddling around

936
00:43:38,800 --> 00:43:41,620
with those parameters and learning a

937
00:43:40,179 --> 00:43:43,179
little bit about how these architectures

938
00:43:41,619 --> 00:43:45,489
actually work and not just using what

939
00:43:43,179 --> 00:43:49,119
came out of the box well holy we

940
00:43:45,489 --> 00:43:50,649
just used a publicly available instance

941
00:43:49,119 --> 00:43:54,609
we use the spot instance so it's cost

942
00:43:50,650 --> 00:43:58,090
you that so that cost us like $8 per

943
00:43:54,610 --> 00:44:02,640
hour for three minutes cost us a few

944
00:43:58,090 --> 00:44:05,140
cents to train this from scratch

945
00:44:02,639 --> 00:44:08,440
20 times faster than anybody's ever done

946
00:44:05,139 --> 00:44:10,480
it before so that's like the most crazy

947
00:44:08,440 --> 00:44:12,849
state at the at result I think we see

948
00:44:10,480 --> 00:44:16,210
we've seen many but this one just blew

949
00:44:12,849 --> 00:44:19,140
it out of the water right and so you

950
00:44:16,210 --> 00:44:21,099
know this is kind of partly thanks to

951
00:44:19,139 --> 00:44:23,650
just fiddling around with those

952
00:44:21,099 --> 00:44:26,740
parameters of the architecture mainly

953
00:44:23,650 --> 00:44:29,050
frankly about using Leslie Smith's one

954
00:44:26,739 --> 00:44:32,078
cycle thing and swampers implementation

955
00:44:29,050 --> 00:44:35,500
of that and remember not only so does to

956
00:44:32,079 --> 00:44:41,339
remind you of what that's doing it's

957
00:44:35,500 --> 00:44:45,070
basically saying this is batches right

958
00:44:41,338 --> 00:44:50,349
and this is mining right okay

959
00:44:45,070 --> 00:44:52,780
it creates an upward path that's equally

960
00:44:50,349 --> 00:44:55,630
long as the downward path right so it's

961
00:44:52,780 --> 00:44:59,290
a true clr triangular cycle called

962
00:44:55,630 --> 00:45:03,910
learning rate as per usual you can pick

963
00:44:59,289 --> 00:45:07,358
the ratio between those two numbers

964
00:45:03,909 --> 00:45:10,088
right so X divided by Y in this case is

965
00:45:07,358 --> 00:45:16,328
is a lumber that you get to pick in this

966
00:45:10,088 --> 00:45:20,199
case we picked 50 okay so we started out

967
00:45:16,329 --> 00:45:22,180
with a much smaller one here and then

968
00:45:20,199 --> 00:45:24,039
it's got this cool idea which is you get

969
00:45:22,179 --> 00:45:25,960
to say what percentage of your epochs

970
00:45:24,039 --> 00:45:28,929
then is spent going from the bottom of

971
00:45:25,960 --> 00:45:32,170
this down all the way down pretty much

972
00:45:28,929 --> 00:45:34,269
to zero and that's what this second

973
00:45:32,170 --> 00:45:38,409
number here is so 15 percent of the

974
00:45:34,269 --> 00:45:42,789
batches are spent going from the bottom

975
00:45:38,409 --> 00:45:44,259
of our triangle even further so

976
00:45:42,789 --> 00:45:48,039
importantly though with that's not the

977
00:45:44,260 --> 00:45:53,130
only thing one cycle does we also have

978
00:45:48,039 --> 00:46:00,599
momentum right and momentum goes from

979
00:45:53,130 --> 00:46:00,599
point nine five to 0.85 like this

980
00:46:05,239 --> 00:46:08,599
in other words when the loading rates

981
00:46:07,849 --> 00:46:10,309
really low

982
00:46:08,599 --> 00:46:11,749
we use a lot of momentum and when the

983
00:46:10,309 --> 00:46:12,980
learning rates really high we use very

984
00:46:11,748 --> 00:46:15,858
little momentum which makes a lot of

985
00:46:12,980 --> 00:46:18,349
sense but until Leslie Smith showed this

986
00:46:15,858 --> 00:46:21,469
in that paper I've never seen anybody do

987
00:46:18,349 --> 00:46:25,460
it before so it's a really really cool

988
00:46:21,469 --> 00:46:29,199
trick so you can now use that by using

989
00:46:25,460 --> 00:46:31,130
the UCLA beta parameter in in first AI

990
00:46:29,199 --> 00:46:33,429
and you should be able to basically

991
00:46:31,130 --> 00:46:36,380
replicate this state-of-the-art result

992
00:46:33,429 --> 00:46:37,788
you can use it on your own computer or

993
00:46:36,380 --> 00:46:39,548
your paper space obviously the only

994
00:46:37,789 --> 00:46:42,380
thing you won't get is the multi-gpu

995
00:46:39,548 --> 00:46:45,679
piece but that makes it a bit easier to

996
00:46:42,380 --> 00:46:50,630
Train anyway so in a single GPU you

997
00:46:45,679 --> 00:46:54,288
should be able to beat this this this on

998
00:46:50,630 --> 00:46:57,440
a single GPU yeah make group layer

999
00:46:54,289 --> 00:46:59,960
contains stride equals two so this means

1000
00:46:57,440 --> 00:47:01,759
stride is one for layer one and two for

1001
00:46:59,960 --> 00:47:04,548
everything else what's the logic behind

1002
00:47:01,759 --> 00:47:10,159
it usually the strides I have seen our

1003
00:47:04,548 --> 00:47:12,440
odd no strides are either one or two I

1004
00:47:10,159 --> 00:47:15,588
think you're thinking of kernel sizes so

1005
00:47:12,440 --> 00:47:17,989
strata calls to means that I jump to

1006
00:47:15,588 --> 00:47:20,179
across and so stride of two means that

1007
00:47:17,989 --> 00:47:21,980
you have your grid size so I think you

1008
00:47:20,179 --> 00:47:26,778
might've cos got confused between stride

1009
00:47:21,980 --> 00:47:29,298
and kernel size there and so if we have

1010
00:47:26,778 --> 00:47:32,210
a stride of one the grid size doesn't

1011
00:47:29,298 --> 00:47:35,358
change if you have a stride of two then

1012
00:47:32,210 --> 00:47:39,798
it does and so in this case because this

1013
00:47:35,358 --> 00:47:41,630
is for say fire 10 32 by 32 is small and

1014
00:47:39,798 --> 00:47:43,670
we don't get to have the grid size very

1015
00:47:41,630 --> 00:47:46,160
often right because pretty quickly we're

1016
00:47:43,670 --> 00:47:51,950
going to run out of cells and so that's

1017
00:47:46,159 --> 00:47:55,179
why the first layer has a stride of one

1018
00:47:51,949 --> 00:47:59,419
so we don't decrease the grid size

1019
00:47:55,179 --> 00:48:01,518
straightaway as you play and it's kind

1020
00:47:59,420 --> 00:48:04,159
of a nice way of doing it because that's

1021
00:48:01,518 --> 00:48:08,088
why we kind of have a low number here so

1022
00:48:04,159 --> 00:48:10,159
we can we can start out with you know

1023
00:48:08,088 --> 00:48:12,199
not too much computation on the big grid

1024
00:48:10,159 --> 00:48:14,210
and then we can gradually doing more and

1025
00:48:12,199 --> 00:48:16,009
more computation as the grids get

1026
00:48:14,210 --> 00:48:17,000
smaller and smaller but because the

1027
00:48:16,009 --> 00:48:24,550
smaller grid

1028
00:48:17,000 --> 00:48:24,550
the computation will take less time okay

1029
00:48:26,380 --> 00:48:31,910
so I think so that we can do all of our

1030
00:48:29,809 --> 00:48:47,469
gaining in one go let's take a slightly

1031
00:48:31,909 --> 00:48:47,469
early break and come back at 7:30 okay

1032
00:48:49,449 --> 00:48:55,608
so we're going to talk about generative

1033
00:48:51,889 --> 00:48:58,489
adversarial networks also known as ganz

1034
00:48:55,608 --> 00:49:02,840
and specifically we're going to focus on

1035
00:48:58,489 --> 00:49:05,659
the Vasa Stein gang caper which included

1036
00:49:02,840 --> 00:49:07,160
some guy called seumas chintala who went

1037
00:49:05,659 --> 00:49:11,719
on to create some piece of software

1038
00:49:07,159 --> 00:49:14,269
called hide watch the Vasa Stein Gann

1039
00:49:11,719 --> 00:49:15,618
was heavily influenced by the Sun is

1040
00:49:14,269 --> 00:49:17,630
going to call this w again that's the

1041
00:49:15,619 --> 00:49:19,369
time the DC gain or deep convolution

1042
00:49:17,630 --> 00:49:21,039
deep convolutional generative

1043
00:49:19,369 --> 00:49:28,519
adversarial networks paper which also

1044
00:49:21,039 --> 00:49:33,858
seemeth was involved with so it's a

1045
00:49:28,519 --> 00:49:40,070
really interesting paper to read a lot

1046
00:49:33,858 --> 00:49:43,400
of it looks like this the good news is

1047
00:49:40,070 --> 00:49:46,609
you can skip those bits because there's

1048
00:49:43,400 --> 00:49:51,170
also a bit that looks like this which

1049
00:49:46,608 --> 00:49:54,019
says do these things all right now I

1050
00:49:51,170 --> 00:49:56,539
will say though but like a lot of papers

1051
00:49:54,019 --> 00:49:58,730
have a theoretical section which seems

1052
00:49:56,539 --> 00:50:03,500
to be they're entirely to get past the

1053
00:49:58,730 --> 00:50:05,659
reviewers need for theory that's not

1054
00:50:03,500 --> 00:50:07,909
true with a W again paper the theory bit

1055
00:50:05,659 --> 00:50:09,799
is actually really interesting like you

1056
00:50:07,909 --> 00:50:13,549
don't need to know what to use it but if

1057
00:50:09,800 --> 00:50:15,320
you want to learn about like some some

1058
00:50:13,550 --> 00:50:18,650
cool ideas and see the thinking behind

1059
00:50:15,320 --> 00:50:21,369
why this particular algorithm it's

1060
00:50:18,650 --> 00:50:23,630
absolutely fascinating and almost nobody

1061
00:50:21,369 --> 00:50:26,088
before this paper came out I didn't know

1062
00:50:23,630 --> 00:50:28,849
literally I'd Lou knew nobody who had

1063
00:50:26,088 --> 00:50:30,199
studied the math that it's based on so

1064
00:50:28,849 --> 00:50:32,420
like everybody had

1065
00:50:30,199 --> 00:50:34,730
the method is based on ins and so the

1066
00:50:32,420 --> 00:50:36,380
paper does a pretty good job of laying

1067
00:50:34,730 --> 00:50:39,050
out all the pieces you have to do a

1068
00:50:36,380 --> 00:50:41,960
bunch of reading yourself so if you're

1069
00:50:39,050 --> 00:50:45,289
interested like in digging into the

1070
00:50:41,960 --> 00:50:47,510
deeper math behind some paper to see

1071
00:50:45,289 --> 00:50:50,088
what it's like to study it I would pick

1072
00:50:47,510 --> 00:50:51,859
this one because at the end of that

1073
00:50:50,088 --> 00:50:57,500
theory section you'll come away saying

1074
00:50:51,858 --> 00:51:00,980
like okay I can see now why they made

1075
00:50:57,500 --> 00:51:03,019
this algorithm the way it is okay and

1076
00:51:00,980 --> 00:51:04,130
then having come up with that idea like

1077
00:51:03,019 --> 00:51:05,900
the other thing is often these

1078
00:51:04,130 --> 00:51:07,309
theoretical sections are very clearly

1079
00:51:05,900 --> 00:51:09,108
added after they come up with the

1080
00:51:07,309 --> 00:51:10,730
algorithm they'll come up the algorithm

1081
00:51:09,108 --> 00:51:14,029
based on intuition and experiments and

1082
00:51:10,730 --> 00:51:16,070
then later on post hoc justify it or

1083
00:51:14,030 --> 00:51:17,680
else this one you can clearly see it's

1084
00:51:16,070 --> 00:51:20,240
like okay let's actually think about

1085
00:51:17,679 --> 00:51:22,069
what's going on in Ganz and think about

1086
00:51:20,239 --> 00:51:26,118
what they need to do and then come up

1087
00:51:22,070 --> 00:51:30,710
with the algorithm so the basic idea of

1088
00:51:26,119 --> 00:51:31,940
a began is it's a generative model okay

1089
00:51:30,710 --> 00:51:37,039
so it's something that is going to

1090
00:51:31,940 --> 00:51:41,929
create sentences or create images kind

1091
00:51:37,039 --> 00:51:45,889
of generate stuff right and it's going

1092
00:51:41,929 --> 00:51:47,618
to try and create stuff which is very

1093
00:51:45,889 --> 00:51:50,210
hard to tell the difference between

1094
00:51:47,619 --> 00:51:52,338
generated stuff and real stuff

1095
00:51:50,210 --> 00:51:55,670
that's what generative model could be

1096
00:51:52,338 --> 00:51:59,000
used to face what a video you know a

1097
00:51:55,670 --> 00:52:02,180
very well-known controversial thing of

1098
00:51:59,000 --> 00:52:04,400
deep fakes and fake pornography and

1099
00:52:02,179 --> 00:52:09,679
stuff happening at the moment could be

1100
00:52:04,400 --> 00:52:13,369
used to fake somebody's voice it could

1101
00:52:09,679 --> 00:52:14,750
be used to fake the answer to a medical

1102
00:52:13,369 --> 00:52:15,980
question but in that case it's not

1103
00:52:14,750 --> 00:52:18,318
really a fake right it could be a

1104
00:52:15,980 --> 00:52:20,420
generative answer to a medical question

1105
00:52:18,318 --> 00:52:22,789
that's actually a good answer right so

1106
00:52:20,420 --> 00:52:26,599
you like generating language you could

1107
00:52:22,789 --> 00:52:31,460
generate a caption to a to an image for

1108
00:52:26,599 --> 00:52:36,130
example so generative models have lots

1109
00:52:31,460 --> 00:52:38,659
of apple interesting adaptations but

1110
00:52:36,130 --> 00:52:40,700
generally speaking they're they need to

1111
00:52:38,659 --> 00:52:43,519
be good enough that for example if

1112
00:52:40,699 --> 00:52:45,769
you're using it too you know

1113
00:52:43,519 --> 00:52:47,960
automatically create a new scene for

1114
00:52:45,769 --> 00:52:50,750
Carrie Fisher in the next Star Wars

1115
00:52:47,960 --> 00:52:53,088
movies and she's not around to play that

1116
00:52:50,750 --> 00:52:54,980
part anymore you want to you know try

1117
00:52:53,088 --> 00:52:57,588
and generate an image of her that looks

1118
00:52:54,980 --> 00:52:59,539
the same then it has to fool the Star

1119
00:52:57,588 --> 00:53:01,250
Wars audience into thinking like okay

1120
00:52:59,539 --> 00:53:03,019
that that doesn't look like some weird

1121
00:53:01,250 --> 00:53:06,170
Carrie Fisher that looks like the real

1122
00:53:03,019 --> 00:53:08,329
Carrie Fisher or if you're trying to

1123
00:53:06,170 --> 00:53:11,000
generate an answer to a medical question

1124
00:53:08,329 --> 00:53:13,490
you want to generate English that reads

1125
00:53:11,000 --> 00:53:18,079
you know nicely and clearly and it

1126
00:53:13,489 --> 00:53:20,899
sounds authority of meaningful so the

1127
00:53:18,079 --> 00:53:23,240
idea of a generative adversarial Network

1128
00:53:20,900 --> 00:53:26,358
is we're going to create not just a

1129
00:53:23,239 --> 00:53:29,509
generative model to create say the the

1130
00:53:26,358 --> 00:53:32,480
the the generated image that a second

1131
00:53:29,510 --> 00:53:34,190
model that's going to try to pick which

1132
00:53:32,480 --> 00:53:35,780
ones are real and which ones are

1133
00:53:34,190 --> 00:53:38,269
generated and we're going to call them

1134
00:53:35,780 --> 00:53:39,319
fake okay so which ones are real and

1135
00:53:38,269 --> 00:53:42,039
which ones are fake so we've got a

1136
00:53:39,318 --> 00:53:45,679
generator that's going to create our

1137
00:53:42,039 --> 00:53:48,048
fake content and a discriminator that's

1138
00:53:45,679 --> 00:53:49,608
going to try to get good at recognizing

1139
00:53:48,048 --> 00:53:51,409
which ones are real and which ones are

1140
00:53:49,608 --> 00:53:53,210
fake so they're going to be two models

1141
00:53:51,409 --> 00:53:54,920
and then they're going to be adversarial

1142
00:53:53,210 --> 00:53:57,199
meaning the generator is going to treat

1143
00:53:54,920 --> 00:53:59,659
a tree trying to keep getting better at

1144
00:53:57,199 --> 00:54:01,669
falling the discriminator into thinking

1145
00:53:59,659 --> 00:54:02,960
that fake is real and the discriminator

1146
00:54:01,670 --> 00:54:05,088
is going to try to keep getting better

1147
00:54:02,960 --> 00:54:07,338
at discriminating between the real and

1148
00:54:05,088 --> 00:54:10,759
the fake and they're going to go head to

1149
00:54:07,338 --> 00:54:15,650
head like that right and it's basically

1150
00:54:10,760 --> 00:54:17,480
as easy as I just described it really is

1151
00:54:15,650 --> 00:54:19,220
but it's going to build two models in

1152
00:54:17,480 --> 00:54:22,280
play torch we're going to create a

1153
00:54:19,219 --> 00:54:24,019
training loop that first of all says the

1154
00:54:22,280 --> 00:54:25,490
loss function for the discriminator is

1155
00:54:24,019 --> 00:54:27,769
can you tell the density real and fake

1156
00:54:25,489 --> 00:54:29,209
and then update the weights of that and

1157
00:54:27,769 --> 00:54:31,130
then we're going to create a loss

1158
00:54:29,210 --> 00:54:32,510
function for the generator which is just

1159
00:54:31,130 --> 00:54:35,180
going to say can you generate something

1160
00:54:32,510 --> 00:54:37,040
which pulls the discriminator and update

1161
00:54:35,179 --> 00:54:37,969
the weights true from that loss and

1162
00:54:37,039 --> 00:54:41,869
we're going to look through that a few

1163
00:54:37,969 --> 00:54:45,409
times and see what happens and so let's

1164
00:54:41,869 --> 00:54:47,720
come back to the pseudo code here if the

1165
00:54:45,409 --> 00:54:50,500
algorithm and let's read the real code

1166
00:54:47,719 --> 00:54:50,500
first

1167
00:54:51,619 --> 00:54:57,269
so there's lots of different things you

1168
00:54:53,820 --> 00:55:00,090
can do with Gans and we're going to do

1169
00:54:57,269 --> 00:55:02,789
something that's kind of boring but easy

1170
00:55:00,090 --> 00:55:03,780
to understand and and it's kind of cool

1171
00:55:02,789 --> 00:55:05,429
that it's even possible

1172
00:55:03,780 --> 00:55:08,820
which is we're just going to generate

1173
00:55:05,429 --> 00:55:10,739
some pictures from now we're just going

1174
00:55:08,820 --> 00:55:12,150
to get it to draw some pictures okay and

1175
00:55:10,739 --> 00:55:15,479
specifically we're going to get it to

1176
00:55:12,150 --> 00:55:17,760
draw pictures of bedrooms right you'll

1177
00:55:15,480 --> 00:55:19,679
find if you hopefully get a chance to

1178
00:55:17,760 --> 00:55:20,790
play around of this during the week with

1179
00:55:19,679 --> 00:55:23,309
your own data sets

1180
00:55:20,789 --> 00:55:27,000
if you pick a dataset that's very varied

1181
00:55:23,309 --> 00:55:29,730
like imagenet and then get again to try

1182
00:55:27,000 --> 00:55:32,519
and create image net pictures it tends

1183
00:55:29,730 --> 00:55:33,840
not to do so well because it's it's not

1184
00:55:32,519 --> 00:55:36,150
really clear enough what do you want a

1185
00:55:33,840 --> 00:55:38,550
picture of alright so it's better to

1186
00:55:36,150 --> 00:55:40,500
give it for example the there's a data

1187
00:55:38,550 --> 00:55:42,930
set called celeb a which has pictures of

1188
00:55:40,500 --> 00:55:45,869
celebrities faces that works great with

1189
00:55:42,929 --> 00:55:48,179
dance you create really clear celebrity

1190
00:55:45,869 --> 00:55:49,920
faces that don't actually exist the

1191
00:55:48,179 --> 00:55:51,929
bedroom data set also a good one right

1192
00:55:49,920 --> 00:55:53,670
lots of things pictures of the same kind

1193
00:55:51,929 --> 00:55:56,639
of thing okay so that's just a

1194
00:55:53,670 --> 00:55:59,599
suggestion so there's something called

1195
00:55:56,639 --> 00:56:04,769
the L son same classification data set

1196
00:55:59,599 --> 00:56:08,429
you can download it using these steps

1197
00:56:04,769 --> 00:56:11,280
I've also it's pretty huge and so I've

1198
00:56:08,429 --> 00:56:13,769
actually created a kegel data set of a

1199
00:56:11,280 --> 00:56:15,780
20% sample right so unless you're really

1200
00:56:13,769 --> 00:56:20,000
excited about generating bedroom images

1201
00:56:15,780 --> 00:56:22,680
you might prefer to grab the 20% sample

1202
00:56:20,000 --> 00:56:25,650
so then we do the normal steps of

1203
00:56:22,679 --> 00:56:27,929
creating some different paths and in

1204
00:56:25,650 --> 00:56:30,269
this case you know I as we do before I

1205
00:56:27,929 --> 00:56:33,149
find it much easier to kind of grow the

1206
00:56:30,269 --> 00:56:37,079
CSV route when it comes to handling our

1207
00:56:33,150 --> 00:56:39,030
our data so I just generate a CSV with

1208
00:56:37,079 --> 00:56:42,150
the list of files that we want and a

1209
00:56:39,030 --> 00:56:45,210
fake label of 0 because we don't really

1210
00:56:42,150 --> 00:56:47,849
have labels for these at all and I

1211
00:56:45,210 --> 00:56:50,519
actually create two part two CSV files

1212
00:56:47,849 --> 00:56:52,699
one that contains everything in that

1213
00:56:50,519 --> 00:56:56,340
bedroom data set and one that just

1214
00:56:52,699 --> 00:56:57,989
contains a random 10% that's just nice

1215
00:56:56,340 --> 00:57:00,390
to do that because then I can most of

1216
00:56:57,989 --> 00:57:02,909
the time use the sample when I'm

1217
00:57:00,389 --> 00:57:04,828
experimenting because like because

1218
00:57:02,909 --> 00:57:07,318
there's well over a million files

1219
00:57:04,829 --> 00:57:09,950
just reading in the list takes a while

1220
00:57:07,318 --> 00:57:09,949
okay

1221
00:57:10,518 --> 00:57:17,058
so this will look pretty familiar so

1222
00:57:13,949 --> 00:57:19,828
here's a con block this is before I

1223
00:57:17,059 --> 00:57:22,200
realized that sequential models are much

1224
00:57:19,829 --> 00:57:23,849
better so if you compare this to my

1225
00:57:22,199 --> 00:57:25,169
previous con lock with a sequential

1226
00:57:23,849 --> 00:57:29,369
model there's just a lot more lines of

1227
00:57:25,170 --> 00:57:34,130
code here but you know it does the same

1228
00:57:29,369 --> 00:57:38,459
thing of doing con rally better on okay

1229
00:57:34,130 --> 00:57:39,869
and we calculate our padding and here's

1230
00:57:38,458 --> 00:57:41,728
a bias pulse so this is the same as

1231
00:57:39,869 --> 00:57:49,140
before basically but with a little bit

1232
00:57:41,728 --> 00:57:50,098
more code alright so the first thing

1233
00:57:49,139 --> 00:57:52,920
we're going to do is we're going to

1234
00:57:50,099 --> 00:57:56,119
build a discriminator so a discriminator

1235
00:57:52,920 --> 00:57:59,759
is going to receive as input an image

1236
00:57:56,119 --> 00:58:05,130
okay and it's going to spit out a number

1237
00:57:59,759 --> 00:58:08,369
and the number is meant to be lower if

1238
00:58:05,130 --> 00:58:10,769
it thinks this image is real okay now of

1239
00:58:08,369 --> 00:58:12,749
course the what does it do for a lower

1240
00:58:10,768 --> 00:58:14,008
number thing doesn't appear in the

1241
00:58:12,748 --> 00:58:16,198
architecture that'll be in the loss

1242
00:58:14,009 --> 00:58:18,179
function so what we have to do is create

1243
00:58:16,199 --> 00:58:24,959
something that takes an image and spits

1244
00:58:18,179 --> 00:58:31,469
out a number okay so a lot of this code

1245
00:58:24,958 --> 00:58:34,318
is borrowed from the original authors of

1246
00:58:31,469 --> 00:58:37,019
this paper of the paper so some of the

1247
00:58:34,318 --> 00:58:40,429
naming scheme and stuff is different to

1248
00:58:37,018 --> 00:58:40,429
what we're used to so sorry about that

1249
00:58:42,619 --> 00:58:45,989
but hopefully I've tried to make it look

1250
00:58:44,759 --> 00:58:47,248
at least somewhat familiar I probably

1251
00:58:45,989 --> 00:58:49,409
should have renamed things a little bit

1252
00:58:47,248 --> 00:58:51,919
but it looks very similar to actually

1253
00:58:49,409 --> 00:58:54,298
what we had before we start out with a

1254
00:58:51,920 --> 00:58:58,409
convolution so in become block is

1255
00:58:54,298 --> 00:59:02,190
conquering a veteran on okay and then we

1256
00:58:58,409 --> 00:59:04,469
have a bunch of extra comm flares this

1257
00:59:02,190 --> 00:59:06,119
is not gonna use a residual right so it

1258
00:59:04,469 --> 00:59:07,229
looks very similar to before a bunch of

1259
00:59:06,119 --> 00:59:10,699
extra layers but these are going to

1260
00:59:07,228 --> 00:59:15,389
become players rather than res layers

1261
00:59:10,699 --> 00:59:19,420
and then at the end we need to append

1262
00:59:15,389 --> 00:59:22,299
enough stride to

1263
00:59:19,420 --> 00:59:25,300
enough strive to calm players that we

1264
00:59:22,300 --> 00:59:28,869
decrease the size the grid size down to

1265
00:59:25,300 --> 00:59:31,059
be no bigger than 4x4 right so it's

1266
00:59:28,869 --> 00:59:32,980
going to keep using straight to divide

1267
00:59:31,059 --> 00:59:35,260
the size by twos tried to divide by

1268
00:59:32,980 --> 00:59:37,630
those by two and till our grid size is

1269
00:59:35,260 --> 00:59:40,089
no bigger than four and so this is quite

1270
00:59:37,630 --> 00:59:42,309
a nice way of like creating as many

1271
00:59:40,088 --> 00:59:44,349
layers as you need in a network to

1272
00:59:42,309 --> 00:59:47,230
handle arbitrary sized images and turn

1273
00:59:44,349 --> 00:59:50,710
them into a fixed loan grid size yes

1274
00:59:47,230 --> 00:59:53,500
Rachel does again need a lot more data

1275
00:59:50,710 --> 00:59:58,088
than say dogs versus cats or NLP or is

1276
00:59:53,500 --> 01:00:01,019
it comparable you know honestly I'm kind

1277
00:59:58,088 --> 01:00:06,279
of embarrassed to say I am NOT an expert

1278
01:00:01,019 --> 01:00:08,800
practitioner in games so you know the

1279
01:00:06,280 --> 01:00:11,800
stuff i teach in part one is stuff I'm

1280
01:00:08,800 --> 01:00:13,180
happy to say I know the best way to you

1281
01:00:11,800 --> 01:00:14,530
know pretty close the best way to do

1282
01:00:13,179 --> 01:00:16,029
these things and so I can show you

1283
01:00:14,530 --> 01:00:18,280
stayed at the out results like I just

1284
01:00:16,030 --> 01:00:21,670
did with safe at ten with the help of

1285
01:00:18,280 --> 01:00:25,230
some of my students of course I'm not

1286
01:00:21,670 --> 01:00:25,230
there at all with Ganz

1287
01:00:25,469 --> 01:00:30,730
so I'm not quite sure how much you need

1288
01:00:28,000 --> 01:00:34,030
like in general Pitt seems unique quite

1289
01:00:30,730 --> 01:00:35,588
a lot but remember the only reason we

1290
01:00:34,030 --> 01:00:37,269
didn't need too much and dogs and cats

1291
01:00:35,588 --> 01:00:39,880
is because we had a pre trained model

1292
01:00:37,269 --> 01:00:43,809
and could we leverage pre-trained and

1293
01:00:39,880 --> 01:00:46,030
models and fine-tune them probably I

1294
01:00:43,809 --> 01:00:48,250
don't think anybody's done it as far as

1295
01:00:46,030 --> 01:00:49,900
I know that could be really interesting

1296
01:00:48,250 --> 01:00:51,969
thing for people that are kind of think

1297
01:00:49,900 --> 01:00:53,829
about an experiment with maybe people

1298
01:00:51,969 --> 01:00:55,618
have done it and there's some literature

1299
01:00:53,829 --> 01:00:58,269
there I haven't come across so I'm

1300
01:00:55,619 --> 01:01:00,338
somewhat familiar with the main pieces

1301
01:00:58,269 --> 01:01:03,579
of literature and Gans but I don't know

1302
01:01:00,338 --> 01:01:05,679
all of it so maybe I've missed something

1303
01:01:03,579 --> 01:01:07,000
about transfer learning in games but

1304
01:01:05,679 --> 01:01:10,000
that would be the trick to not needing

1305
01:01:07,000 --> 01:01:12,519
too much data and so it's the huge

1306
01:01:10,000 --> 01:01:14,469
speed-up a combination of one cycle

1307
01:01:12,519 --> 01:01:16,838
learning rate and momentum annealing

1308
01:01:14,469 --> 01:01:20,048
plus the eight GPU parallel training in

1309
01:01:16,838 --> 01:01:21,880
the half precision is that only possible

1310
01:01:20,048 --> 01:01:25,449
to do the half precision calculation

1311
01:01:21,880 --> 01:01:27,099
with consumer GPU another question why

1312
01:01:25,449 --> 01:01:29,679
is the calculation eight times faster

1313
01:01:27,099 --> 01:01:31,920
from single to half precision while from

1314
01:01:29,679 --> 01:01:35,498
double the single is only two times fast

1315
01:01:31,920 --> 01:01:37,740
okay so the SyFy ten result it's not

1316
01:01:35,498 --> 01:01:40,268
eight times faster from single to half

1317
01:01:37,739 --> 01:01:43,508
it's about two or three times as fast

1318
01:01:40,268 --> 01:01:46,629
from single to half the Nvidia claims

1319
01:01:43,509 --> 01:01:50,469
about the about the flops performance of

1320
01:01:46,630 --> 01:01:52,809
the tensor cause are academically

1321
01:01:50,469 --> 01:01:56,230
correct but in practice meaningless

1322
01:01:52,809 --> 01:01:58,150
because it really depends on what calls

1323
01:01:56,230 --> 01:02:01,619
you need for what pieces so about two or

1324
01:01:58,150 --> 01:02:01,619
three X improvement for half

1325
01:02:01,710 --> 01:02:09,670
so yeah the half precision helps a bit

1326
01:02:05,009 --> 01:02:12,579
the the extra GPUs helps a bit the one

1327
01:02:09,670 --> 01:02:14,798
cycle helps an enormous amount then

1328
01:02:12,579 --> 01:02:16,329
another key piece was the playing around

1329
01:02:14,798 --> 01:02:19,059
with the parameters that I told you

1330
01:02:16,329 --> 01:02:22,298
about so kind of ten reading the wide

1331
01:02:19,059 --> 01:02:23,920
ResNet paper carefully identifying the

1332
01:02:22,298 --> 01:02:27,119
kinds of things that they found there

1333
01:02:23,920 --> 01:02:29,139
and then writing a version of the

1334
01:02:27,119 --> 01:02:30,910
architecture you just saw that made it

1335
01:02:29,139 --> 01:02:32,920
really easy for me to fiddle around with

1336
01:02:30,909 --> 01:02:35,920
prep one action for me for Brett pans to

1337
01:02:32,920 --> 01:02:37,599
fiddle around with parameters staying up

1338
01:02:35,920 --> 01:02:41,019
all night trying every possible

1339
01:02:37,599 --> 01:02:42,880
combination of different kernel sizes

1340
01:02:41,018 --> 01:02:45,068
and numbers of kernels and numbers of

1341
01:02:42,880 --> 01:02:48,880
layer groups and size of layer groups

1342
01:02:45,068 --> 01:02:51,278
and the amount of remember we did a

1343
01:02:48,880 --> 01:02:52,960
bottleneck but actually we tended to

1344
01:02:51,278 --> 01:02:55,088
focus not on bottlenecks bonds instead

1345
01:02:52,960 --> 01:02:56,920
on widening so we actually like things

1346
01:02:55,088 --> 01:02:58,480
that increase the size and then decrease

1347
01:02:56,920 --> 01:03:01,450
it because it takes better advantage of

1348
01:02:58,480 --> 01:03:03,880
the GPU so though all those things

1349
01:03:01,449 --> 01:03:07,629
combined together I'd say the one cycle

1350
01:03:03,880 --> 01:03:09,548
was perhaps the most critical but but

1351
01:03:07,630 --> 01:03:10,989
every one of those resulted in a big

1352
01:03:09,548 --> 01:03:13,929
speed-up that's why we were able to get

1353
01:03:10,989 --> 01:03:20,829
this 30x improvement over the

1354
01:03:13,929 --> 01:03:22,690
state-of-the-art so factor and we got

1355
01:03:20,829 --> 01:03:25,869
some ideas for other things like after

1356
01:03:22,690 --> 01:03:27,909
this Dorn bench finishes and you know

1357
01:03:25,869 --> 01:03:29,200
maybe we'll try and go even further see

1358
01:03:27,909 --> 01:03:37,449
if we can beat one minute one day

1359
01:03:29,199 --> 01:03:38,980
that'll be fun okay so so here's okay so

1360
01:03:37,449 --> 01:03:40,268
here's our discriminator right I mean

1361
01:03:38,980 --> 01:03:43,088
that the important thing to remember

1362
01:03:40,268 --> 01:03:44,089
about an architecture is it doesn't do

1363
01:03:43,088 --> 01:03:46,429
anything

1364
01:03:44,090 --> 01:03:49,100
rather than have some input tensor size

1365
01:03:46,429 --> 01:03:51,079
and rank and some output tensor size and

1366
01:03:49,099 --> 01:03:54,079
range so this is going to spit out you

1367
01:03:51,079 --> 01:03:56,000
see the last calm here has one channel

1368
01:03:54,079 --> 01:03:57,529
right this is a bit different to what

1369
01:03:56,000 --> 01:04:00,530
we're used to right because normally our

1370
01:03:57,530 --> 01:04:02,060
last thing is a linear block right but

1371
01:04:00,530 --> 01:04:04,820
our last thing here is a common block

1372
01:04:02,059 --> 01:04:08,449
right and it's only got one channel but

1373
01:04:04,820 --> 01:04:11,210
it's got a grid size of something around

1374
01:04:08,449 --> 01:04:12,799
4 by 4 it's no more than 4 by 4 so we're

1375
01:04:11,210 --> 01:04:18,050
gonna spit out a let's say it's provoke

1376
01:04:12,800 --> 01:04:23,660
or a 4 by 4 by 1 cancer so what we then

1377
01:04:18,050 --> 01:04:26,960
do is we then take the mean of that fat

1378
01:04:23,659 --> 01:04:28,879
so it goes from 4 by 4 by 1 to the

1379
01:04:26,960 --> 01:04:31,400
scalar right so this is kind of like the

1380
01:04:28,880 --> 01:04:32,570
ultimate adaptive average pulling right

1381
01:04:31,400 --> 01:04:34,670
because we've got something with just

1382
01:04:32,570 --> 01:04:36,860
one channel we take the mean so this is

1383
01:04:34,670 --> 01:04:38,960
a bit yeah a bit different normally we

1384
01:04:36,860 --> 01:04:41,390
first do average pooling and then we put

1385
01:04:38,960 --> 01:04:43,159
it through a fully connected layer to

1386
01:04:41,389 --> 01:04:44,809
get our one thing out in this case

1387
01:04:43,159 --> 01:04:48,129
though but getting one channel out and

1388
01:04:44,809 --> 01:04:51,079
then taking the mean of with that I

1389
01:04:48,130 --> 01:04:52,700
haven't fiddled around with like why did

1390
01:04:51,079 --> 01:04:53,989
we do it that way what would instead

1391
01:04:52,699 --> 01:04:56,719
happen if we did the usual average

1392
01:04:53,989 --> 01:04:58,819
Pauling followed by a fully connected

1393
01:04:56,719 --> 01:05:03,139
layer would it work better would it not

1394
01:04:58,820 --> 01:05:04,880
I I don't know I rather suspect it would

1395
01:05:03,139 --> 01:05:08,239
work better if we did it like the normal

1396
01:05:04,880 --> 01:05:10,099
way but I haven't tried it and I don't

1397
01:05:08,239 --> 01:05:11,809
really have a good enough intuition to

1398
01:05:10,099 --> 01:05:13,880
know whether I'm missing something

1399
01:05:11,809 --> 01:05:15,199
finished the experimenter truck if

1400
01:05:13,880 --> 01:05:16,670
somebody wants to stick an adaptive

1401
01:05:15,199 --> 01:05:18,259
average Pauling layer here and a fully

1402
01:05:16,670 --> 01:05:21,470
connected layer afterwards with a single

1403
01:05:18,260 --> 01:05:24,110
output it should keep working you know

1404
01:05:21,469 --> 01:05:27,379
we should do something the loss will go

1405
01:05:24,110 --> 01:05:28,910
down to see whether it works okay so

1406
01:05:27,380 --> 01:05:31,160
that's the discriminator right so it's

1407
01:05:28,909 --> 01:05:32,719
gonna be a training loop let's put let's

1408
01:05:31,159 --> 01:05:35,000
assume we've already got a generator

1409
01:05:32,719 --> 01:05:37,699
somebody says okay Jeremy here's a

1410
01:05:35,000 --> 01:05:39,110
generator it generates bedrooms ok I

1411
01:05:37,699 --> 01:05:40,189
want you to build a model that can

1412
01:05:39,110 --> 01:05:40,820
figure out which ones are real and which

1413
01:05:40,190 --> 01:05:42,349
ones aren't

1414
01:05:40,820 --> 01:05:44,930
so I'm going to take the data set and

1415
01:05:42,349 --> 01:05:47,179
I'm going to basically label a bunch of

1416
01:05:44,929 --> 01:05:49,190
images which are fake bedrooms from the

1417
01:05:47,179 --> 01:05:51,980
generator and a bunch of images of real

1418
01:05:49,190 --> 01:05:54,380
bedrooms from my else I'm data set to

1419
01:05:51,980 --> 01:05:56,420
stick a 1 or a 0 and h1 and then I'll

1420
01:05:54,380 --> 01:05:57,890
try to get there discriminator to tell

1421
01:05:56,420 --> 01:06:04,010
the difference ok so

1422
01:05:57,889 --> 01:06:05,750
that's gonna be simple enough but I

1423
01:06:04,010 --> 01:06:09,950
haven't been given a generator I need to

1424
01:06:05,750 --> 01:06:11,599
build one so a generator and we haven't

1425
01:06:09,949 --> 01:06:12,619
talked about the last function yet okay

1426
01:06:11,599 --> 01:06:14,360
we're just going to assume that there's

1427
01:06:12,619 --> 01:06:17,929
some loss function that does this thing

1428
01:06:14,360 --> 01:06:19,970
okay so a generator is also an

1429
01:06:17,929 --> 01:06:21,739
architecture which doesn't do anything

1430
01:06:19,969 --> 01:06:25,099
by itself until we have a loss function

1431
01:06:21,739 --> 01:06:28,069
and data but what are the ranks and

1432
01:06:25,099 --> 01:06:31,309
sizes of the tenses well the input to

1433
01:06:28,070 --> 01:06:34,789
the generator is going to be a vector of

1434
01:06:31,309 --> 01:06:36,829
random numbers okay and in the paper

1435
01:06:34,789 --> 01:06:38,630
they call that the prior but it's going

1436
01:06:36,829 --> 01:06:43,730
to be a vector of random numbers how big

1437
01:06:38,630 --> 01:06:45,530
I don't know some big 64 128 right and

1438
01:06:43,730 --> 01:06:47,809
the idea is that a different bunch of

1439
01:06:45,530 --> 01:06:50,990
random numbers will generate a different

1440
01:06:47,809 --> 01:06:55,940
bedroom okay so that's the idea so our

1441
01:06:50,989 --> 01:07:01,849
our gann generator sorry has to take as

1442
01:06:55,940 --> 01:07:04,429
input a vector and it's going to take

1443
01:07:01,849 --> 01:07:05,719
that vector so here's our import all

1444
01:07:04,429 --> 01:07:09,440
right and it's going to stick it through

1445
01:07:05,719 --> 01:07:10,609
in this case so sequential model and the

1446
01:07:09,440 --> 01:07:15,920
sequential models going to take that

1447
01:07:10,610 --> 01:07:22,730
vector it's going to turn it into a 2 by

1448
01:07:15,920 --> 01:07:25,159
2 sorry a turn it into a well I rank for

1449
01:07:22,730 --> 01:07:28,510
tensor or if we take off the batch bit

1450
01:07:25,159 --> 01:07:32,089
around three tensor height by width by

1451
01:07:28,510 --> 01:07:40,520
three okay so you can see at the end

1452
01:07:32,090 --> 01:07:41,720
here our final step here NC number of

1453
01:07:40,519 --> 01:07:43,639
channels so I think that's going to have

1454
01:07:41,719 --> 01:07:49,099
to end up being 3 equals going to create

1455
01:07:43,639 --> 01:07:53,329
a 3 channel image of some size yes

1456
01:07:49,099 --> 01:07:56,599
Rachel in coms block forward is there a

1457
01:07:53,329 --> 01:07:58,819
reason my batch dorm comes after Lu ie

1458
01:07:56,599 --> 01:08:01,969
self dot batch norm dot self dot right

1459
01:07:58,820 --> 01:08:04,990
no that's not it's just what they had in

1460
01:08:01,969 --> 01:08:10,239
the code I borrowed from I think that

1461
01:08:04,989 --> 01:08:10,239
the order is reversed yeah so

1462
01:08:10,840 --> 01:08:17,150
again unless my intuition about Dan's is

1463
01:08:15,110 --> 01:08:18,739
all wrong and say some some reason need

1464
01:08:17,149 --> 01:08:23,210
to be different to what I'm used to I

1465
01:08:18,739 --> 01:08:24,559
would normally expect to yeah I would

1466
01:08:23,210 --> 01:08:27,560
actually no sorry I would normally

1467
01:08:24,560 --> 01:08:30,020
expect to go rally your then batch norm

1468
01:08:27,560 --> 01:08:34,160
that this is actually the order that

1469
01:08:30,020 --> 01:08:36,140
makes more sense to me but I think the

1470
01:08:34,159 --> 01:08:40,670
order I had in the darknet was what they

1471
01:08:36,140 --> 01:08:42,710
used in the darknet paper so I don't

1472
01:08:40,670 --> 01:08:47,300
know everybody seems to have a different

1473
01:08:42,710 --> 01:08:49,550
order of these things and in fact most

1474
01:08:47,300 --> 01:08:54,610
people for sci-fi 10 have a different

1475
01:08:49,550 --> 01:08:54,610
order again which is they actually go

1476
01:08:55,300 --> 01:09:00,470
bien

1477
01:08:56,689 --> 01:09:02,269
then rally then conf which is kind of a

1478
01:09:00,470 --> 01:09:04,850
quirky way of thinking about it but it

1479
01:09:02,270 --> 01:09:06,380
turns out that for often for residual

1480
01:09:04,850 --> 01:09:09,170
blocks that works better

1481
01:09:06,380 --> 01:09:11,359
that's called a pre activation ResNet so

1482
01:09:09,170 --> 01:09:14,270
if you google for pre activation ResNet

1483
01:09:11,359 --> 01:09:17,000
you can see that so yeah there's there's

1484
01:09:14,270 --> 01:09:18,560
a few not so much papers but more blog

1485
01:09:17,000 --> 01:09:19,909
posts out there where people will have

1486
01:09:18,560 --> 01:09:23,300
experimented with different orders of

1487
01:09:19,909 --> 01:09:25,939
those things and yeah it seems to depend

1488
01:09:23,300 --> 01:09:28,279
a lot on what specific data said it is

1489
01:09:25,939 --> 01:09:30,199
and what you're doing with although in

1490
01:09:28,279 --> 01:09:32,539
general the difference in performance is

1491
01:09:30,199 --> 01:09:36,679
small enough you won't care unless it's

1492
01:09:32,539 --> 01:09:40,159
through a competition okay

1493
01:09:36,680 --> 01:09:42,800
so the generator needs to start with a

1494
01:09:40,159 --> 01:09:46,039
vector and end up with a rank three

1495
01:09:42,800 --> 01:09:49,010
tensor we don't really know how to do

1496
01:09:46,039 --> 01:09:51,289
that yet so how do we do that how do we

1497
01:09:49,010 --> 01:09:55,310
start with a vector and turn it into a

1498
01:09:51,289 --> 01:09:59,359
rank three tensor we need to use

1499
01:09:55,310 --> 01:10:00,560
something called a deconvolution and a

1500
01:09:59,359 --> 01:10:03,699
deconvolution

1501
01:10:00,560 --> 01:10:08,420
is or as they call it an fie torch

1502
01:10:03,699 --> 01:10:10,670
transposed convolution same name

1503
01:10:08,420 --> 01:10:14,090
that's right same same thing different

1504
01:10:10,670 --> 01:10:17,899
name and so a deconvolution is something

1505
01:10:14,090 --> 01:10:23,239
which rather than decreasing the grid

1506
01:10:17,899 --> 01:10:24,769
size it increases the grid size so as

1507
01:10:23,239 --> 01:10:26,359
with all things

1508
01:10:24,770 --> 01:10:30,230
it's easiest to see in an Excel

1509
01:10:26,359 --> 01:10:32,238
spreadsheet so here's a convolution

1510
01:10:30,229 --> 01:10:35,000
right we start let's say with a four by

1511
01:10:32,238 --> 01:10:38,839
four grid cell okay with a single

1512
01:10:35,000 --> 01:10:41,300
channel a single photo and let's put it

1513
01:10:38,840 --> 01:10:44,270
through a three by three kernel again

1514
01:10:41,300 --> 01:10:48,529
with a single output filter okay so

1515
01:10:44,270 --> 01:10:51,949
we've got a single channel in a single

1516
01:10:48,529 --> 01:10:54,889
filter kernel and so if we don't add any

1517
01:10:51,948 --> 01:10:57,289
padding we're going to end up with two

1518
01:10:54,890 --> 01:10:58,100
by two right because that three by three

1519
01:10:57,289 --> 01:11:01,039
you can go in

1520
01:10:58,100 --> 01:11:03,140
one two three four places right can go

1521
01:11:01,039 --> 01:11:05,439
in one of two places across some one of

1522
01:11:03,140 --> 01:11:09,170
two places down if there's no padding

1523
01:11:05,439 --> 01:11:10,669
okay so there's our there's our

1524
01:11:09,170 --> 01:11:12,829
convolution right remember the

1525
01:11:10,670 --> 01:11:15,289
convolution is just the sum of the

1526
01:11:12,829 --> 01:11:17,859
product of the kernel and the

1527
01:11:15,289 --> 01:11:22,219
appropriate grid cell so there's a

1528
01:11:17,859 --> 01:11:26,420
there's our standard 3x3 upon one

1529
01:11:22,219 --> 01:11:28,909
channel one filter so the idea now is I

1530
01:11:26,420 --> 01:11:34,699
want to go the opposite direction I want

1531
01:11:28,909 --> 01:11:39,050
to start with my 2x2 and I want to

1532
01:11:34,698 --> 01:11:41,269
create a 4x4 and specifically I want to

1533
01:11:39,050 --> 01:11:44,270
create the same 4x4 that I started with

1534
01:11:41,270 --> 01:11:47,600
and I want to do that by using a

1535
01:11:44,270 --> 01:11:50,810
convolution so how would I do that well

1536
01:11:47,600 --> 01:11:53,960
if I have a 3x3 convolution then if I

1537
01:11:50,810 --> 01:12:00,650
want to create a 4x4 output I'm going to

1538
01:11:53,960 --> 01:12:02,840
need to create this much padding that

1539
01:12:00,649 --> 01:12:07,189
goes with this much padding I'm going to

1540
01:12:02,840 --> 01:12:11,119
end up with one two three four by one

1541
01:12:07,189 --> 01:12:13,189
two three four you see why that is so

1542
01:12:11,119 --> 01:12:15,380
this filter can go at any one of four

1543
01:12:13,189 --> 01:12:20,000
places across from four places up and

1544
01:12:15,380 --> 01:12:22,279
down so let's say my convolutional

1545
01:12:20,000 --> 01:12:26,600
filter was just a bunch of zeros then I

1546
01:12:22,279 --> 01:12:29,420
can calculate my my error for each cell

1547
01:12:26,600 --> 01:12:32,660
just by taking this attraction and then

1548
01:12:29,420 --> 01:12:35,390
I can get the sum of absolute values

1549
01:12:32,659 --> 01:12:37,670
they are one loss later summing up the

1550
01:12:35,390 --> 01:12:38,090
absolute values of those errors all

1551
01:12:37,670 --> 01:12:43,329
right

1552
01:12:38,090 --> 01:12:46,850
so now I could use optimization so in

1553
01:12:43,329 --> 01:12:49,250
Excel that's called solver to do a

1554
01:12:46,850 --> 01:12:53,180
gradient descent okay so I'm going to

1555
01:12:49,250 --> 01:12:56,989
set that cell equal to a minimum try and

1556
01:12:53,180 --> 01:13:02,630
reduce my loss by changing my filter

1557
01:12:56,989 --> 01:13:04,489
okay and I'll go solve okay and you can

1558
01:13:02,630 --> 01:13:06,350
see it's come up with a filter such that

1559
01:13:04,489 --> 01:13:08,599
you know fifteen point seven compared to

1560
01:13:06,350 --> 01:13:11,480
sixteen seventeen that's right seventeen

1561
01:13:08,600 --> 01:13:13,910
point it so it's not perfect right and

1562
01:13:11,479 --> 01:13:16,759
in general you can't assume that a

1563
01:13:13,909 --> 01:13:18,649
deconvolution can exactly create the

1564
01:13:16,760 --> 01:13:22,489
same you know the exact thing that you

1565
01:13:18,649 --> 01:13:23,899
want because there's just not enough you

1566
01:13:22,489 --> 01:13:25,010
know there's only nine things here and

1567
01:13:23,899 --> 01:13:27,469
there's sixteen things you're trying to

1568
01:13:25,010 --> 01:13:30,880
create right but it's it's made a pretty

1569
01:13:27,470 --> 01:13:35,230
good attempt okay so this is what a

1570
01:13:30,880 --> 01:13:39,710
deconvolution looks like asteroid one

1571
01:13:35,229 --> 01:13:44,119
3x3 deep deconvolution on a 2x2 grid

1572
01:13:39,710 --> 01:13:45,380
cell input did you have a question how

1573
01:13:44,119 --> 01:13:47,479
difficult is it to create a

1574
01:13:45,380 --> 01:13:52,220
discriminator to identify fake news

1575
01:13:47,479 --> 01:13:53,359
versus real news well you don't need

1576
01:13:52,220 --> 01:13:56,060
anything special that's just a

1577
01:13:53,359 --> 01:13:59,049
classifier right so you would just use

1578
01:13:56,060 --> 01:14:01,760
the NLP classifier from previous

1579
01:13:59,050 --> 01:14:04,610
previous to previous class and listen

1580
01:14:01,760 --> 01:14:07,400
for right it's there's nothing like it

1581
01:14:04,609 --> 01:14:09,920
and in that case there's no generative

1582
01:14:07,399 --> 01:14:12,049
piece right so you just need the data

1583
01:14:09,920 --> 01:14:13,489
set that says these are the things that

1584
01:14:12,050 --> 01:14:14,960
we believe are fake news and these are

1585
01:14:13,489 --> 01:14:18,859
the things we consider to be real news

1586
01:14:14,960 --> 01:14:21,529
and it should actually work very well

1587
01:14:18,859 --> 01:14:23,539
you know like as to the best of our

1588
01:14:21,529 --> 01:14:25,849
knowledge if you if you try it you

1589
01:14:23,539 --> 01:14:27,829
should get a you know as good a result

1590
01:14:25,850 --> 01:14:29,420
as anybody else has got whether it's

1591
01:14:27,829 --> 01:14:34,600
good enough to be useful to practice I

1592
01:14:29,420 --> 01:14:34,600
don't know as I say that it's very hard

1593
01:14:35,500 --> 01:14:44,119
very hard there's not a good solution

1594
01:14:40,850 --> 01:14:45,770
that does that well but I don't think

1595
01:14:44,119 --> 01:14:47,510
anybody in our course has tried and

1596
01:14:45,770 --> 01:14:50,120
nobody else outside our cause knows of

1597
01:14:47,510 --> 01:14:51,270
this technique right so like there's

1598
01:14:50,119 --> 01:14:53,550
beaners we've

1599
01:14:51,270 --> 01:14:55,740
as we've learned we've just had a very

1600
01:14:53,550 --> 01:15:01,800
significant jump in NLP classification

1601
01:14:55,739 --> 01:15:04,019
capabilities and yeah I mean I think

1602
01:15:01,800 --> 01:15:05,730
it's obviously the best you could do I

1603
01:15:04,020 --> 01:15:09,300
think at this stage would be to generate

1604
01:15:05,729 --> 01:15:13,199
a kind of a triage that says these

1605
01:15:09,300 --> 01:15:14,850
things look pretty sketchy based on how

1606
01:15:13,199 --> 01:15:16,559
they're written and then you know some

1607
01:15:14,850 --> 01:15:19,680
human could go and in fact check them

1608
01:15:16,560 --> 01:15:21,930
you know I mean an NLP classifier and I

1609
01:15:19,680 --> 01:15:24,570
run in kind of fact-check things but it

1610
01:15:21,930 --> 01:15:28,710
could recognize like all these these are

1611
01:15:24,569 --> 01:15:31,229
written in that kind of you know that

1612
01:15:28,710 --> 01:15:33,149
kind of highly popularized style which

1613
01:15:31,229 --> 01:15:34,739
often fake news is written in and so

1614
01:15:33,149 --> 01:15:36,239
maybe these ones are worth paying

1615
01:15:34,739 --> 01:15:38,010
attention to

1616
01:15:36,239 --> 01:15:41,699
I think that would probably the best you

1617
01:15:38,010 --> 01:15:46,350
could hope for without drawing on some

1618
01:15:41,699 --> 01:15:48,840
kind of external data sources yeah but

1619
01:15:46,350 --> 01:15:50,490
it's important to remember you know the

1620
01:15:48,840 --> 01:15:51,810
discriminator is basically just a

1621
01:15:50,489 --> 01:15:53,579
classifier and you don't need any

1622
01:15:51,810 --> 01:15:54,690
special techniques beyond what we've

1623
01:15:53,579 --> 01:16:03,029
already learned to do an LP

1624
01:15:54,689 --> 01:16:06,449
classification ok so to to do that kind

1625
01:16:03,029 --> 01:16:08,819
of deconvolution in in pi torch just say

1626
01:16:06,449 --> 01:16:11,159
chrome transporters 2d and in the normal

1627
01:16:08,819 --> 01:16:13,859
way you say the number of input channels

1628
01:16:11,159 --> 01:16:16,199
the number of output channels the kernel

1629
01:16:13,859 --> 01:16:18,599
size the stride the padding the bias so

1630
01:16:16,199 --> 01:16:20,010
these parameters are all the same right

1631
01:16:18,600 --> 01:16:22,260
and the reason it's called a comm

1632
01:16:20,010 --> 01:16:24,930
transpose is because actually it turns

1633
01:16:22,260 --> 01:16:26,460
out that this is this is the same as the

1634
01:16:24,930 --> 01:16:29,730
calculation of the gradient of

1635
01:16:26,460 --> 01:16:33,960
convolution that's that's that's

1636
01:16:29,729 --> 01:16:36,179
basically why they call it that so this

1637
01:16:33,960 --> 01:16:38,279
is a really nice example back on the

1638
01:16:36,180 --> 01:16:41,130
older Theano website that comes from a

1639
01:16:38,279 --> 01:16:43,050
really nice paper which actually shows

1640
01:16:41,130 --> 01:16:45,569
you some visualizations so this is

1641
01:16:43,050 --> 01:16:47,010
actually the one we just saw of doing a

1642
01:16:45,569 --> 01:16:49,949
2x2 deconvolution

1643
01:16:47,010 --> 01:16:51,930
if there's a stride to then you don't

1644
01:16:49,949 --> 01:16:53,099
just have padding around the outside but

1645
01:16:51,930 --> 01:16:57,210
you actually have to put padding in the

1646
01:16:53,100 --> 01:16:58,770
middle as well okay they're not actually

1647
01:16:57,210 --> 01:17:02,279
quite implemented this way because this

1648
01:16:58,770 --> 01:17:03,780
is slow to do in practice you in a

1649
01:17:02,279 --> 01:17:05,059
different way but it all happens behind

1650
01:17:03,779 --> 01:17:08,809
the scenes we don't have to worry

1651
01:17:05,060 --> 01:17:10,280
better okay yeah this we've we've talked

1652
01:17:08,810 --> 01:17:13,160
about this convolution arithmetic

1653
01:17:10,279 --> 01:17:15,500
tutorial before and if you're still not

1654
01:17:13,159 --> 01:17:16,939
comfortable with convolutions and in

1655
01:17:15,500 --> 01:17:19,939
order to get comfortable with D

1656
01:17:16,939 --> 01:17:22,369
convolutions this is a great site to go

1657
01:17:19,939 --> 01:17:24,919
to if you want to see the paper just

1658
01:17:22,369 --> 01:17:27,559
Google for convolution arithmetic it'll

1659
01:17:24,920 --> 01:17:30,970
be the first thing that comes up let's

1660
01:17:27,560 --> 01:17:30,970
do it now so you know you've found it

1661
01:17:32,350 --> 01:17:38,329
here it is

1662
01:17:35,439 --> 01:17:41,719
and so that Theano tutorial actually

1663
01:17:38,329 --> 01:17:48,619
comes from this paper but the paper

1664
01:17:41,720 --> 01:17:50,840
doesn't have the animated gifs okay so

1665
01:17:48,619 --> 01:17:52,760
it's interesting there no decomp block

1666
01:17:50,840 --> 01:17:54,500
looks identical to a comp lock except

1667
01:17:52,760 --> 01:17:57,140
it's got the word transpose written here

1668
01:17:54,500 --> 01:17:59,750
okay we just go comb for Elliot better

1669
01:17:57,140 --> 01:18:01,579
on this before it's about input filters

1670
01:17:59,750 --> 01:18:06,050
output fit filters the only difference

1671
01:18:01,579 --> 01:18:11,119
is that stride 2 means that the grid

1672
01:18:06,050 --> 01:18:14,810
size will double rather than half both

1673
01:18:11,119 --> 01:18:16,760
nn comp transpose to D and n n dot up

1674
01:18:14,810 --> 01:18:19,220
sample seemed to do the same thing

1675
01:18:16,760 --> 01:18:21,770
ie expand grid size height and width

1676
01:18:19,220 --> 01:18:23,869
from the previous layer can we say that

1677
01:18:21,770 --> 01:18:26,000
Kampf transpose to D is always better

1678
01:18:23,869 --> 01:18:28,970
than up sample since up sample is merely

1679
01:18:26,000 --> 01:18:36,500
resizing and filling unknown unknowns by

1680
01:18:28,970 --> 01:18:42,680
zeros or interpolation no you can't so

1681
01:18:36,500 --> 01:18:45,529
there's a fantastic interactive paper on

1682
01:18:42,680 --> 01:18:48,050
distilled pub called deconvolution and

1683
01:18:45,529 --> 01:18:50,899
checkerboard artifacts which points out

1684
01:18:48,050 --> 01:18:53,420
that what we're doing right now is

1685
01:18:50,899 --> 01:18:57,500
extremely suboptimal but the good news

1686
01:18:53,420 --> 01:18:59,480
is everybody else does it if you have a

1687
01:18:57,500 --> 01:19:01,909
look here could you see these

1688
01:18:59,479 --> 01:19:04,759
checkerboard architect artifacts like

1689
01:19:01,909 --> 01:19:07,130
it's all like that blue light blue dark

1690
01:19:04,760 --> 01:19:09,560
blue light blue you know you kind of and

1691
01:19:07,130 --> 01:19:12,500
so these are all from from actual papers

1692
01:19:09,560 --> 01:19:14,360
right and basically they noticed every

1693
01:19:12,500 --> 01:19:16,460
one of these papers with generative

1694
01:19:14,359 --> 01:19:18,948
models has these checkerboard artifacts

1695
01:19:16,460 --> 01:19:21,769
and what they realized

1696
01:19:18,948 --> 01:19:26,538
is it's because when you have a stride

1697
01:19:21,769 --> 01:19:30,289
to convolution of size three kernel they

1698
01:19:26,538 --> 01:19:33,050
overlap right and so you basically get

1699
01:19:30,288 --> 01:19:35,929
like some pixels get twice as much kind

1700
01:19:33,050 --> 01:19:38,748
of active some grid cells I guess wise

1701
01:19:35,929 --> 01:19:42,288
as much activation and so even if you

1702
01:19:38,748 --> 01:19:44,840
start with random weights you end up

1703
01:19:42,288 --> 01:19:49,368
with a check of what artifact so you can

1704
01:19:44,840 --> 01:19:53,170
kind of see it here right and so the

1705
01:19:49,368 --> 01:19:58,759
deeper you get kind of the worse it gets

1706
01:19:53,170 --> 01:20:01,038
their advice is actually less director

1707
01:19:58,760 --> 01:20:03,889
than it ought to be I've found that for

1708
01:20:01,038 --> 01:20:07,248
most generative models up sampling is

1709
01:20:03,889 --> 01:20:11,150
better right so all if you do end up

1710
01:20:07,248 --> 01:20:13,569
sample then all it does is it's it's

1711
01:20:11,149 --> 01:20:15,469
basically doing pooling right basically

1712
01:20:13,569 --> 01:20:17,208
but it's kind of its kind of the

1713
01:20:15,469 --> 01:20:20,420
opposite of Pauline right it says let's

1714
01:20:17,208 --> 01:20:23,689
replace this one pixel or this one grid

1715
01:20:20,420 --> 01:20:25,069
cell with four two by two and there's a

1716
01:20:23,689 --> 01:20:27,319
number of ways to up sample one is just

1717
01:20:25,069 --> 01:20:29,599
to kind of copy it across to those four

1718
01:20:27,319 --> 01:20:31,998
and other is to use kind of lit by

1719
01:20:29,599 --> 01:20:33,319
linear or bicubic interpolation there

1720
01:20:31,998 --> 01:20:35,599
are various techniques to kind of try

1721
01:20:33,319 --> 01:20:37,639
and create a smooth off sample version

1722
01:20:35,599 --> 01:20:41,510
and you can pretty much choose any of

1723
01:20:37,639 --> 01:20:45,170
them in pi torch so if you do a two by

1724
01:20:41,510 --> 01:20:48,409
two up sample and then a regular stride

1725
01:20:45,170 --> 01:20:51,199
one three by three khans that's like

1726
01:20:48,408 --> 01:20:55,219
another way of doing the same kind of

1727
01:20:51,198 --> 01:20:57,848
thing as a commons plus well it's it's

1728
01:20:55,219 --> 01:21:00,708
doubling the grid size and doing some

1729
01:20:57,849 --> 01:21:04,038
convolutional arithmetic on it

1730
01:21:00,708 --> 01:21:06,529
and i found for generative models and it

1731
01:21:04,038 --> 01:21:10,578
pretty much always works better and in

1732
01:21:06,529 --> 01:21:12,198
that distorter pub publication they kind

1733
01:21:10,578 --> 01:21:13,908
of indicate that maybe that's a good

1734
01:21:12,198 --> 01:21:15,469
approach but they don't just come out

1735
01:21:13,908 --> 01:21:18,589
and say just do this whereas i would

1736
01:21:15,469 --> 01:21:21,739
just say just do this having said that

1737
01:21:18,590 --> 01:21:24,078
for gains i haven't had that much

1738
01:21:21,738 --> 01:21:25,819
success with it yet and I think it

1739
01:21:24,078 --> 01:21:27,529
probably requires some tweaking to get

1740
01:21:25,819 --> 01:21:31,009
it to work as I'm sure some people have

1741
01:21:27,529 --> 01:21:32,179
got it to work the the issue I think is

1742
01:21:31,010 --> 01:21:39,159
that in the earliest

1743
01:21:32,180 --> 01:21:42,860
ages it doesn't create enough noise

1744
01:21:39,159 --> 01:21:45,139
ahead I don't have it here

1745
01:21:42,859 --> 01:21:47,269
I had a version actually where I tried

1746
01:21:45,140 --> 01:21:49,789
to do it within an up sample and you

1747
01:21:47,270 --> 01:21:52,790
could kind of see that the noise didn't

1748
01:21:49,789 --> 01:21:54,859
look very noisy so anyway it's an

1749
01:21:52,789 --> 01:21:56,779
interesting question but um next week

1750
01:21:54,859 --> 01:21:58,909
when we look at style transfer and

1751
01:21:56,779 --> 01:22:01,759
super-resolution and stuff I think

1752
01:21:58,909 --> 01:22:05,750
you'll see and ended up sample really

1753
01:22:01,760 --> 01:22:07,880
comes into its own okay so the generator

1754
01:22:05,750 --> 01:22:10,430
we can now basically start with the

1755
01:22:07,880 --> 01:22:11,630
vector we can decide and say like okay

1756
01:22:10,430 --> 01:22:13,520
does not think of it as a vector but

1757
01:22:11,630 --> 01:22:15,109
actually it's a one by one grid cell and

1758
01:22:13,520 --> 01:22:17,330
then we can turn it into a four by four

1759
01:22:15,109 --> 01:22:19,429
and they by eight and so forth and so

1760
01:22:17,329 --> 01:22:22,579
that's why we have to make sure it's a

1761
01:22:19,430 --> 01:22:24,500
it's a suitable multiple so that we can

1762
01:22:22,579 --> 01:22:26,899
actually create something of the right

1763
01:22:24,500 --> 01:22:28,670
size and so you can see it's doing the

1764
01:22:26,899 --> 01:22:31,339
exact opposite as before right it's

1765
01:22:28,670 --> 01:22:39,500
making the cell size smaller and smaller

1766
01:22:31,340 --> 01:22:41,869
by two at a time as long as it can sorry

1767
01:22:39,500 --> 01:22:43,600
bigger and bigger so this cell size

1768
01:22:41,869 --> 01:22:46,609
bigger and bigger as long as it can

1769
01:22:43,600 --> 01:22:49,789
until it gets to half the size that we

1770
01:22:46,609 --> 01:22:52,849
want and then finally we add one more on

1771
01:22:49,789 --> 01:22:58,430
at the end sorry we add n more on at the

1772
01:22:52,850 --> 01:23:01,220
end of just with no stride and then we

1773
01:22:58,430 --> 01:23:06,110
add one more calm transpose to finally

1774
01:23:01,220 --> 01:23:09,280
get to the size that we wanted and we're

1775
01:23:06,109 --> 01:23:13,719
done finally we put that through a fan

1776
01:23:09,279 --> 01:23:17,179
and that's got a force us to be in the

1777
01:23:13,720 --> 01:23:20,090
zero to one range because of course we

1778
01:23:17,180 --> 01:23:25,190
don't want to spit out arbitrary size

1779
01:23:20,090 --> 01:23:26,989
pixel values okay so that's so we've got

1780
01:23:25,189 --> 01:23:29,750
to generate our architecture which spits

1781
01:23:26,989 --> 01:23:31,429
out an image of some given size with the

1782
01:23:29,750 --> 01:23:32,930
correct number with the correct whatever

1783
01:23:31,430 --> 01:23:37,700
a readout of all correct number of

1784
01:23:32,930 --> 01:23:40,990
channels and with values between zero

1785
01:23:37,699 --> 01:23:45,639
and one so at this point we can now

1786
01:23:40,989 --> 01:23:45,639
create our model data object

1787
01:23:45,649 --> 01:23:50,960
these things take a while to train so I

1788
01:23:48,289 --> 01:23:57,739
just made it 128 by 128 so this is just

1789
01:23:50,960 --> 01:23:59,359
a convenient way to make it a faster and

1790
01:23:57,739 --> 01:24:00,260
that's going to be the size of the input

1791
01:23:59,359 --> 01:24:02,859
but then we're going to use

1792
01:24:00,260 --> 01:24:05,350
transformations to turn it into 64 by 64

1793
01:24:02,859 --> 01:24:08,539
okay

1794
01:24:05,350 --> 01:24:10,370
there's been more recent advances which

1795
01:24:08,539 --> 01:24:12,710
have attempted to really increase this

1796
01:24:10,369 --> 01:24:14,149
up to kind of like high resolution sizes

1797
01:24:12,710 --> 01:24:17,060
but they still tend to require either

1798
01:24:14,149 --> 01:24:19,670
like a batch size of 1 or like lots and

1799
01:24:17,060 --> 01:24:21,200
lots of GPUs or whatever so we're kind

1800
01:24:19,670 --> 01:24:24,980
of trying to do things that we can do 1

1801
01:24:21,199 --> 01:24:27,189
conceal all consumer GPUs yeah so here's

1802
01:24:24,979 --> 01:24:32,689
an example of one of the 64 by 64

1803
01:24:27,189 --> 01:24:34,519
bedrooms okay so we're gonna do if

1804
01:24:32,689 --> 01:24:37,519
pretty much everything manually so let's

1805
01:24:34,520 --> 01:24:42,110
go ahead and create our two models our

1806
01:24:37,520 --> 01:24:44,480
generator and our discriminator and as

1807
01:24:42,109 --> 01:24:49,969
you can see they're DC can so in other

1808
01:24:44,479 --> 01:24:51,889
words they're the same modules that came

1809
01:24:49,970 --> 01:24:53,770
up were appeared in this paper so if

1810
01:24:51,890 --> 01:24:56,270
you're interested in reading the papers

1811
01:24:53,770 --> 01:24:59,000
you it's well worth going back and

1812
01:24:56,270 --> 01:25:00,710
looking at the DC gain paper to see what

1813
01:24:59,000 --> 01:25:02,060
these architectures are because it's

1814
01:25:00,710 --> 01:25:03,800
assumed that when you read the vasa

1815
01:25:02,060 --> 01:25:09,080
stein gain paper that you already know

1816
01:25:03,800 --> 01:25:12,909
that yes you don't we use a sigmoid if

1817
01:25:09,079 --> 01:25:12,909
we want values between 0 and 1

1818
01:25:13,220 --> 01:25:19,909
I always forget which ones which okay so

1819
01:25:15,890 --> 01:25:30,310
sick yes so sigmoid is 0 to 1 fan is 1

1820
01:25:19,909 --> 01:25:33,500
to minus 1 I think what will happen is

1821
01:25:30,310 --> 01:25:35,060
I'm gonna have to check that I beg you

1822
01:25:33,500 --> 01:25:36,920
remember thinking about this when I was

1823
01:25:35,060 --> 01:25:39,470
writing this notebook and realizing that

1824
01:25:36,920 --> 01:25:40,670
1 2 minus 1 made sense for some reason

1825
01:25:39,470 --> 01:25:43,130
but I can't remember what that reason

1826
01:25:40,670 --> 01:25:45,380
was now so let me get back here about

1827
01:25:43,130 --> 01:25:49,789
that during the week and remind me if I

1828
01:25:45,380 --> 01:25:51,319
forget it's good question thank you ok

1829
01:25:49,789 --> 01:25:53,180
so we've got our generator in a

1830
01:25:51,319 --> 01:25:58,069
discriminator so we need a function that

1831
01:25:53,180 --> 01:25:59,659
returns a prior vector so a bunch of

1832
01:25:58,069 --> 01:26:02,210
noise

1833
01:25:59,659 --> 01:26:06,260
so we do that by creating a bunch of

1834
01:26:02,210 --> 01:26:07,970
zeros NZ is the size of Zed so like very

1835
01:26:06,260 --> 01:26:10,100
often in our code if you see a

1836
01:26:07,970 --> 01:26:11,780
mysterious letter it's because that's

1837
01:26:10,100 --> 01:26:14,210
the letter they used in the paper that

1838
01:26:11,779 --> 01:26:18,050
says ed is the size about noise vector

1839
01:26:14,210 --> 01:26:20,119
okay so there's the size of our noise

1840
01:26:18,050 --> 01:26:23,119
vector and then we use a normal

1841
01:26:20,119 --> 01:26:25,819
distribution to generate random numbers

1842
01:26:23,119 --> 01:26:26,930
inside that and that needs to be a

1843
01:26:25,819 --> 01:26:29,210
variable because it's going to be

1844
01:26:26,930 --> 01:26:33,860
participating in the in the gradient

1845
01:26:29,210 --> 01:26:36,199
updates so here's an example of creating

1846
01:26:33,859 --> 01:26:42,049
some noise and so here are four

1847
01:26:36,199 --> 01:26:45,550
different pieces of noise okay so we

1848
01:26:42,050 --> 01:26:50,560
need an optimizer in order to update our

1849
01:26:45,550 --> 01:26:52,880
gradients in the vasa stein game paper

1850
01:26:50,560 --> 01:26:55,220
they told us to use rmsprop

1851
01:26:52,880 --> 01:26:57,260
so that's fine that's so when you see

1852
01:26:55,220 --> 01:27:00,020
this thing saying do an rmsprop update

1853
01:26:57,260 --> 01:27:04,280
in a paper that's nice we can just do an

1854
01:27:00,020 --> 01:27:09,110
rmsprop update with pipe watch okay and

1855
01:27:04,279 --> 01:27:11,210
they suggested a learning rate of five

1856
01:27:09,109 --> 01:27:12,649
Enid five I think I found one a nigga

1857
01:27:11,210 --> 01:27:15,649
four seem to work so I was made a little

1858
01:27:12,649 --> 01:27:18,109
bit bigger so now we need a training

1859
01:27:15,649 --> 01:27:22,099
loop and so this is the thing that's

1860
01:27:18,109 --> 01:27:24,489
going to implement this algorithm so a

1861
01:27:22,100 --> 01:27:26,960
training loop is going to go through

1862
01:27:24,489 --> 01:27:29,619
some number of epochs that we get to

1863
01:27:26,960 --> 01:27:32,779
pick so that's going to be a parameter

1864
01:27:29,619 --> 01:27:34,579
and so remember when you do everything

1865
01:27:32,779 --> 01:27:36,380
manually you've got to remember all the

1866
01:27:34,579 --> 01:27:39,229
manual steps to do so one is that you

1867
01:27:36,380 --> 01:27:41,840
have to set your modules into training

1868
01:27:39,229 --> 01:27:43,699
mode when you're training them and into

1869
01:27:41,840 --> 01:27:46,369
evaluation mode when you're evaluating

1870
01:27:43,699 --> 01:27:50,000
them because in training mode batch norm

1871
01:27:46,369 --> 01:27:52,010
updates happen and dropout happens in

1872
01:27:50,000 --> 01:27:53,720
evaluation mode those two things get

1873
01:27:52,010 --> 01:27:57,250
turned off okay so it's basically

1874
01:27:53,720 --> 01:28:00,140
difference so put it into training mode

1875
01:27:57,250 --> 01:28:02,359
we're going to grab a iterator from our

1876
01:28:00,140 --> 01:28:04,220
training data loader we're going to see

1877
01:28:02,359 --> 01:28:08,389
how many steps we have to go through and

1878
01:28:04,220 --> 01:28:10,579
then we'll use two qdm to give us a

1879
01:28:08,390 --> 01:28:13,670
progress bar and then we're going to go

1880
01:28:10,579 --> 01:28:20,899
through that many steps okay

1881
01:28:13,670 --> 01:28:26,899
so the first step of this algorithm is

1882
01:28:20,899 --> 01:28:34,219
to update the is to update the

1883
01:28:26,899 --> 01:28:37,429
discriminator so in this one I'm just

1884
01:28:34,219 --> 01:28:38,840
trying to remember yes they don't call

1885
01:28:37,429 --> 01:28:42,649
it a discriminator they call it a critic

1886
01:28:38,840 --> 01:28:46,250
right so W are the weights of the of the

1887
01:28:42,649 --> 01:28:48,408
critic so the first step is to train our

1888
01:28:46,250 --> 01:28:49,939
critic a little bit and then we're going

1889
01:28:48,408 --> 01:28:51,349
to train our generator a little bit and

1890
01:28:49,939 --> 01:28:53,809
then we go go back to the top of the

1891
01:28:51,350 --> 01:28:56,270
loop right so this inner so we got a

1892
01:28:53,810 --> 01:28:58,219
while loop on the outside okay so here's

1893
01:28:56,270 --> 01:29:00,350
our while loop on the outside and then

1894
01:28:58,219 --> 01:29:02,510
inside that there's another loop for the

1895
01:29:00,350 --> 01:29:05,119
critic and so here's a little loop

1896
01:29:02,510 --> 01:29:07,550
inside that for the critic okay we call

1897
01:29:05,118 --> 01:29:09,019
it a discriminator so what we're going

1898
01:29:07,550 --> 01:29:11,960
to do now is we're going to try we've

1899
01:29:09,020 --> 01:29:13,880
got we've got a generator and at the

1900
01:29:11,960 --> 01:29:16,310
moment it's random right so our

1901
01:29:13,880 --> 01:29:18,710
generator is going to generate stuff

1902
01:29:16,310 --> 01:29:20,719
that looks something like this right and

1903
01:29:18,710 --> 01:29:22,429
so we need to first of all teach our

1904
01:29:20,719 --> 01:29:25,368
discriminator to tell the difference

1905
01:29:22,429 --> 01:29:29,389
between that and a bedroom right which

1906
01:29:25,368 --> 01:29:31,519
shouldn't be too hard you would hope so

1907
01:29:29,389 --> 01:29:35,210
we just do it in basically the usual way

1908
01:29:31,520 --> 01:29:38,150
but there's a few little tweaks so first

1909
01:29:35,210 --> 01:29:40,939
of all we're going to grab a mini batch

1910
01:29:38,149 --> 01:29:44,299
of real bedroom photos so we can just

1911
01:29:40,939 --> 01:29:51,289
grab the next batch from our iterator

1912
01:29:44,300 --> 01:29:56,119
turn it into a variable okay then we're

1913
01:29:51,289 --> 01:30:01,579
going to calculate the loss for that

1914
01:29:56,118 --> 01:30:07,000
right so this is going to be how much

1915
01:30:01,579 --> 01:30:07,000
does the discriminator think this looks

1916
01:30:07,238 --> 01:30:12,769
this looks fake right through the real

1917
01:30:10,340 --> 01:30:15,350
ones book fake and then we're going to

1918
01:30:12,770 --> 01:30:18,380
create some fake images and to do that

1919
01:30:15,350 --> 01:30:20,210
will create some random noise and we'll

1920
01:30:18,380 --> 01:30:21,529
stick it through our generator which at

1921
01:30:20,210 --> 01:30:24,079
this stage is just a bunch of random

1922
01:30:21,529 --> 01:30:26,170
weights okay and that's going to create

1923
01:30:24,079 --> 01:30:28,090
a mini batch of fake images

1924
01:30:26,170 --> 01:30:31,060
okay and so then we'll put that through

1925
01:30:28,090 --> 01:30:35,020
the same discriminator module as before

1926
01:30:31,060 --> 01:30:38,710
okay to get the loss for that so how

1927
01:30:35,020 --> 01:30:40,060
fake to the fake ones look remember when

1928
01:30:38,710 --> 01:30:43,329
you do everything manually you have to

1929
01:30:40,060 --> 01:30:45,520
zero the gradients in in your loop and

1930
01:30:43,329 --> 01:30:47,050
if you've forgotten about that go back

1931
01:30:45,520 --> 01:30:49,989
to the part one lesson where we do

1932
01:30:47,050 --> 01:30:54,640
everything from scratch so now finally

1933
01:30:49,988 --> 01:30:58,448
the total discriminator loss is equal to

1934
01:30:54,640 --> 01:31:03,369
the real loss - the fake loss okay and

1935
01:30:58,448 --> 01:31:05,259
so you can see that here they don't talk

1936
01:31:03,369 --> 01:31:07,779
about the loss they actually just talk

1937
01:31:05,260 --> 01:31:10,570
about one of the gradient updates so

1938
01:31:07,779 --> 01:31:13,599
this here is a symbol for get the

1939
01:31:10,569 --> 01:31:16,809
gradients alright so inside here is the

1940
01:31:13,600 --> 01:31:18,850
loss right and like trying to like learn

1941
01:31:16,810 --> 01:31:19,510
to throw away in your head all of the

1942
01:31:18,850 --> 01:31:23,890
boring stuff

1943
01:31:19,510 --> 01:31:25,840
so when you see sum over m / M that

1944
01:31:23,890 --> 01:31:27,670
means take the average so just throw

1945
01:31:25,840 --> 01:31:29,529
that away and replace it with NP don't

1946
01:31:27,670 --> 01:31:31,630
mean in your head there's another NP

1947
01:31:29,529 --> 01:31:33,698
domain right so you want to get quick at

1948
01:31:31,630 --> 01:31:36,969
like being able to see these common

1949
01:31:33,698 --> 01:31:39,639
idioms so anytime you see one over m sum

1950
01:31:36,969 --> 01:31:41,350
over m you go okay and peed on me right

1951
01:31:39,640 --> 01:31:45,610
so we're taking the mean of and we're

1952
01:31:41,350 --> 01:31:48,280
taking the mean of so that's all fine X

1953
01:31:45,609 --> 01:31:51,250
I what's X I it looks like it's next to

1954
01:31:48,279 --> 01:31:53,198
the power of I but it's not right the

1955
01:31:51,250 --> 01:31:56,380
math notation is very overloaded they

1956
01:31:53,198 --> 01:32:02,198
showed us here what X I is and it's a

1957
01:31:56,380 --> 01:32:04,630
set of M samples from a batch of the

1958
01:32:02,198 --> 01:32:06,428
real data so in other words this is a

1959
01:32:04,630 --> 01:32:08,440
mini batch right so when that when you

1960
01:32:06,429 --> 01:32:11,560
see something same sample it means just

1961
01:32:08,439 --> 01:32:14,109
grab a row grab a row grab a row and you

1962
01:32:11,560 --> 01:32:16,870
can see here grab it m times and we'll

1963
01:32:14,109 --> 01:32:18,759
call the first row X parentheses 1 the

1964
01:32:16,869 --> 01:32:21,369
second row X parentheses 2 one of the

1965
01:32:18,760 --> 01:32:27,539
annoying things about math notation is

1966
01:32:21,369 --> 01:32:29,769
the the way that we index into arrays is

1967
01:32:27,539 --> 01:32:31,960
everybody uses different approaches

1968
01:32:29,770 --> 01:32:34,120
subscript superscript things in brackets

1969
01:32:31,960 --> 01:32:35,770
combinations commas square brackets

1970
01:32:34,119 --> 01:32:38,979
whatever right so you've just got to

1971
01:32:35,770 --> 01:32:40,010
look in the paper and be like ok at some

1972
01:32:38,979 --> 01:32:42,529
point they're going to say

1973
01:32:40,010 --> 01:32:45,560
take the I throw from this matrix or the

1974
01:32:42,529 --> 01:32:47,509
ithe image in this batch how are they

1975
01:32:45,560 --> 01:32:51,080
going to do it in this case let's say

1976
01:32:47,510 --> 01:32:52,730
superscript in parentheses okay so

1977
01:32:51,079 --> 01:32:54,909
that's all sample means and curly

1978
01:32:52,729 --> 01:32:58,179
brackets means it's just a set of them

1979
01:32:54,909 --> 01:33:02,359
this little squiggle followed by

1980
01:32:58,180 --> 01:33:04,970
something here means according to some

1981
01:33:02,359 --> 01:33:07,159
probability distribution and so in this

1982
01:33:04,970 --> 01:33:09,710
case like and very very often in papers

1983
01:33:07,159 --> 01:33:12,349
it simply means hey you've got a bunch

1984
01:33:09,710 --> 01:33:14,960
of data right grab a bit from it at

1985
01:33:12,350 --> 01:33:17,660
random okay so that's like that's the

1986
01:33:14,960 --> 01:33:20,779
the probability distribution of the data

1987
01:33:17,659 --> 01:33:24,019
you have is the data you have right

1988
01:33:20,779 --> 01:33:29,659
so this says grab em things at random

1989
01:33:24,020 --> 01:33:32,300
from your real data this says grab em

1990
01:33:29,659 --> 01:33:36,439
things at random from your prior samples

1991
01:33:32,300 --> 01:33:41,650
and so that means in other words call

1992
01:33:36,439 --> 01:33:46,969
create noise to create m random vectors

1993
01:33:41,649 --> 01:33:52,939
so now we've got em real images each one

1994
01:33:46,970 --> 01:33:57,289
gets put through our discriminator we've

1995
01:33:52,939 --> 01:34:01,239
got em bits of noise each one gets put

1996
01:33:57,289 --> 01:34:04,460
through our generator to create em

1997
01:34:01,239 --> 01:34:06,170
generated images each one of those gets

1998
01:34:04,460 --> 01:34:07,939
put through look FW that's the same

1999
01:34:06,170 --> 01:34:09,859
thing so these ones of those gets put

2000
01:34:07,939 --> 01:34:11,029
through our discriminator to try and

2001
01:34:09,859 --> 01:34:15,349
figure out whether they're fake or not

2002
01:34:11,029 --> 01:34:17,599
and so then it's this minus this and the

2003
01:34:15,350 --> 01:34:19,820
mean of that and then finally get the

2004
01:34:17,600 --> 01:34:22,280
gradient of that in order to figure out

2005
01:34:19,819 --> 01:34:27,909
how to use our own s prop to update our

2006
01:34:22,279 --> 01:34:32,569
weights using some learning rate okay so

2007
01:34:27,909 --> 01:34:34,279
in height or CH we don't have to worry

2008
01:34:32,569 --> 01:34:36,799
about getting the gradients we can just

2009
01:34:34,279 --> 01:34:38,439
specify their last bit okay and then

2010
01:34:36,800 --> 01:34:44,079
just say lost stop backward

2011
01:34:38,439 --> 01:34:44,079
discriminator optimizer diet step okay

2012
01:34:44,409 --> 01:34:51,680
now there's one key step right which is

2013
01:34:48,140 --> 01:34:53,960
that we have to keep all of our

2014
01:34:51,680 --> 01:34:56,810
activations

2015
01:34:53,960 --> 01:34:59,050
so all of our weights which are the

2016
01:34:56,810 --> 01:35:03,080
parameters in a page horch module in

2017
01:34:59,050 --> 01:35:07,760
this small range between 0.01 negative

2018
01:35:03,079 --> 01:35:09,738
0.01 and point out one why because the

2019
01:35:07,760 --> 01:35:14,840
mathematical assumptions that make this

2020
01:35:09,738 --> 01:35:18,109
algorithm work only only apply in like a

2021
01:35:14,840 --> 01:35:20,750
small ball all right so I'm not going to

2022
01:35:18,109 --> 01:35:22,250
tell I don't I think it's kind of

2023
01:35:20,750 --> 01:35:24,649
interesting to understand the math of

2024
01:35:22,250 --> 01:35:26,390
why that's the case but it's very

2025
01:35:24,649 --> 01:35:28,069
specific to this one paper and

2026
01:35:26,390 --> 01:35:30,170
understanding it won't help you

2027
01:35:28,069 --> 01:35:32,809
understand any other paper so only study

2028
01:35:30,170 --> 01:35:34,250
it you know if you're interested in you

2029
01:35:32,810 --> 01:35:36,380
know I think it's nicely explained I

2030
01:35:34,250 --> 01:35:39,529
think it's fun but it won't be

2031
01:35:36,380 --> 01:35:42,469
information that you're reuse elsewhere

2032
01:35:39,529 --> 01:35:44,329
unless you get super into dance I'll

2033
01:35:42,469 --> 01:35:47,420
also mention after the paper came out

2034
01:35:44,329 --> 01:35:49,789
and improved froster stein Gann came out

2035
01:35:47,420 --> 01:35:53,090
that said hey there are better ways to

2036
01:35:49,789 --> 01:35:54,529
ensure that your that your weight space

2037
01:35:53,090 --> 01:35:56,590
is in this tight ball which was

2038
01:35:54,529 --> 01:35:59,750
basically that kind of penalize

2039
01:35:56,590 --> 01:36:01,760
gradients that are too high so actually

2040
01:35:59,750 --> 01:36:05,210
nowadays they're at there is slightly

2041
01:36:01,760 --> 01:36:06,770
different ways to do this anyway that's

2042
01:36:05,210 --> 01:36:08,750
why this line of code there it's kind of

2043
01:36:06,770 --> 01:36:10,880
the key contribution this you know this

2044
01:36:08,750 --> 01:36:12,979
one line of code actually is the one

2045
01:36:10,880 --> 01:36:15,980
line of code you add to make it a versus

2046
01:36:12,979 --> 01:36:18,229
time again basically that the work was

2047
01:36:15,979 --> 01:36:19,309
all in knowing like that that's the

2048
01:36:18,229 --> 01:36:21,829
thing that you can do that makes

2049
01:36:19,310 --> 01:36:23,690
everything work better ok so at the end

2050
01:36:21,829 --> 01:36:24,890
of this we've got a discriminator that

2051
01:36:23,689 --> 01:36:27,729
can recognize an industry in real

2052
01:36:24,890 --> 01:36:31,579
bedrooms and our totally random crappy

2053
01:36:27,729 --> 01:36:33,709
generated images so let's now try and

2054
01:36:31,579 --> 01:36:36,710
create some better images so now set

2055
01:36:33,710 --> 01:36:39,949
trainable discriminator to false set

2056
01:36:36,710 --> 01:36:42,949
trainable the generator to true zero out

2057
01:36:39,949 --> 01:36:50,029
the gradients of the generator and now

2058
01:36:42,949 --> 01:36:53,599
our loss again is fw that's the did that

2059
01:36:50,029 --> 01:36:57,439
remember that's the discriminator of the

2060
01:36:53,600 --> 01:37:03,219
generator applied to some more random

2061
01:36:57,439 --> 01:37:06,710
noise okay so here's our random noise

2062
01:37:03,219 --> 01:37:09,119
here's our generator here's our

2063
01:37:06,710 --> 01:37:14,489
discriminator

2064
01:37:09,119 --> 01:37:15,539
I think I can remove that now because I

2065
01:37:14,488 --> 01:37:17,069
think I've put it inside the

2066
01:37:15,539 --> 01:37:20,039
discriminator but I won't change it now

2067
01:37:17,069 --> 01:37:23,630
because it's going to confuse me so it's

2068
01:37:20,039 --> 01:37:25,859
exactly the same as before where we did

2069
01:37:23,630 --> 01:37:28,529
generator on the noise and then pass a

2070
01:37:25,859 --> 01:37:30,210
discriminator but this time the thing

2071
01:37:28,529 --> 01:37:32,639
that's trainable is the generator not

2072
01:37:30,210 --> 01:37:36,449
the discriminator so in other words in

2073
01:37:32,640 --> 01:37:40,200
this pseudocode the thing they update is

2074
01:37:36,449 --> 01:37:44,029
theta which is the generators parameters

2075
01:37:40,199 --> 01:37:46,050
rather than W which is the

2076
01:37:44,029 --> 01:37:48,509
discriminators parameters and so

2077
01:37:46,050 --> 01:37:51,930
hopefully you'll see now that this this

2078
01:37:48,510 --> 01:37:55,020
W down here is telling you these are the

2079
01:37:51,930 --> 01:37:56,730
parameters of the discriminator this

2080
01:37:55,020 --> 01:37:59,700
theta down here is telling you these are

2081
01:37:56,729 --> 01:38:01,409
the actually better these this theta

2082
01:37:59,699 --> 01:38:05,189
here is telling you these are the

2083
01:38:01,409 --> 01:38:08,579
parameters of the generator okay

2084
01:38:05,189 --> 01:38:10,710
again it's not as universal mathematical

2085
01:38:08,579 --> 01:38:12,569
notation it's a thing they're doing in

2086
01:38:10,710 --> 01:38:16,859
this particular paper but it's kind of

2087
01:38:12,569 --> 01:38:18,359
nice when you see some some suffix like

2088
01:38:16,859 --> 01:38:22,199
that there's like try to think about

2089
01:38:18,359 --> 01:38:26,279
what it's telling you okay so it takes

2090
01:38:22,199 --> 01:38:28,069
noise generate some images try and

2091
01:38:26,279 --> 01:38:31,050
figure out if they're fake or real and

2092
01:38:28,069 --> 01:38:34,889
use that to get gradients with respect

2093
01:38:31,050 --> 01:38:37,110
to okay the generator as opposed to

2094
01:38:34,890 --> 01:38:39,180
earlier we got them with respect to the

2095
01:38:37,109 --> 01:38:41,819
discriminator and use that to update our

2096
01:38:39,180 --> 01:38:49,829
weights with our s prop with an alpha

2097
01:38:41,819 --> 01:38:53,069
learning rate okay you'll see that it's

2098
01:38:49,829 --> 01:38:57,210
kind of unfair that the discriminator is

2099
01:38:53,069 --> 01:39:00,179
getting trained and critic times which

2100
01:38:57,210 --> 01:39:05,039
they set to five for every time that we

2101
01:39:00,180 --> 01:39:06,329
train the generator once and the paper

2102
01:39:05,039 --> 01:39:09,689
talks a bit about this but the basic

2103
01:39:06,329 --> 01:39:12,059
idea is like there's no point making the

2104
01:39:09,689 --> 01:39:13,889
generator better if the discriminator

2105
01:39:12,060 --> 01:39:17,010
doesn't know how to discriminate yet

2106
01:39:13,890 --> 01:39:21,090
okay so that's why we've got this while

2107
01:39:17,010 --> 01:39:24,989
loop and here's that v right and

2108
01:39:21,090 --> 01:39:27,659
see something which was added I think in

2109
01:39:24,988 --> 01:39:32,098
the later paper or maybe a supplementary

2110
01:39:27,658 --> 01:39:36,439
material is the idea that from time to

2111
01:39:32,099 --> 01:39:39,329
time and a bunch of times at the start

2112
01:39:36,439 --> 01:39:41,219
you should do more steps at the

2113
01:39:39,329 --> 01:39:44,488
discriminator so kind of make sure that

2114
01:39:41,219 --> 01:39:48,060
the discriminator is pretty capable from

2115
01:39:44,488 --> 01:39:50,579
time to time okay so do a bunch of

2116
01:39:48,060 --> 01:39:51,840
epochs of training the discriminator a

2117
01:39:50,579 --> 01:39:54,029
bunch of times to get better at telling

2118
01:39:51,840 --> 01:39:56,639
the dentistry in real and fake and then

2119
01:39:54,029 --> 01:40:00,539
do one step of making the generator

2120
01:39:56,639 --> 01:40:04,260
being better at generating and that is

2121
01:40:00,539 --> 01:40:09,929
an epoch and so let's train that for one

2122
01:40:04,260 --> 01:40:14,360
epoch and then let's create some noise

2123
01:40:09,929 --> 01:40:14,359
so we can generate some examples

2124
01:40:14,929 --> 01:40:18,630
actually going to do that later let's

2125
01:40:16,859 --> 01:40:21,179
first of all decrease the learning rate

2126
01:40:18,630 --> 01:40:24,090
by 10 and do one more pass so we've now

2127
01:40:21,179 --> 01:40:28,760
done two epochs and now let's use our

2128
01:40:24,090 --> 01:40:32,310
noise to pass it to our generator okay

2129
01:40:28,760 --> 01:40:35,550
and then put it through our

2130
01:40:32,310 --> 01:40:39,710
denormalization to turn it back into

2131
01:40:35,550 --> 01:40:43,560
something we can see and then plot it

2132
01:40:39,710 --> 01:40:45,689
and we have some bedrooms okay there's

2133
01:40:43,560 --> 01:40:47,099
not real bedrooms and some of them don't

2134
01:40:45,689 --> 01:40:50,219
particularly like bedrooms but some of

2135
01:40:47,099 --> 01:40:53,789
them look a lot like bedrooms so that's

2136
01:40:50,219 --> 01:40:57,029
that's the idea okay that's again and I

2137
01:40:53,789 --> 01:41:00,210
think like the best way to think about

2138
01:40:57,029 --> 01:41:01,829
again is it's like an underlying

2139
01:41:00,210 --> 01:41:06,960
technology that you'll probably never

2140
01:41:01,829 --> 01:41:10,649
use like this but you'll use in lots of

2141
01:41:06,960 --> 01:41:14,250
interesting ways for example they're

2142
01:41:10,649 --> 01:41:16,399
going to use it to create now a cycle

2143
01:41:14,250 --> 01:41:16,399
game

2144
01:41:19,300 --> 01:41:28,340
and we're going to use the cycle Gann to

2145
01:41:21,979 --> 01:41:30,379
turn horses into zebras you could also

2146
01:41:28,340 --> 01:41:33,469
use it to turn mono prints into photos

2147
01:41:30,380 --> 01:41:37,880
or to turn photos of Yosemite in summer

2148
01:41:33,469 --> 01:41:40,908
into winter so it's gonna be pretty yes

2149
01:41:37,880 --> 01:41:43,460
Rachel two questions one is there any

2150
01:41:40,908 --> 01:41:45,828
reason for using rmsprop specifically as

2151
01:41:43,460 --> 01:41:49,849
the optimized optimizer as opposed to

2152
01:41:45,828 --> 01:41:51,500
Adam I don't remember it being

2153
01:41:49,849 --> 01:41:52,940
explicitly discussed in the paper I

2154
01:41:51,500 --> 01:41:55,250
don't know if it's just experimental or

2155
01:41:52,939 --> 01:41:56,509
the theoretical reason yeah have a look

2156
01:41:55,250 --> 01:41:58,578
in the paper and see what it says I

2157
01:41:56,510 --> 01:42:00,619
don't recall and which could be a

2158
01:41:58,578 --> 01:42:02,599
reasonable way of detecting overfitting

2159
01:42:00,618 --> 01:42:04,399
while training or of evaluating the

2160
01:42:02,599 --> 01:42:06,590
performance of one of these Gann models

2161
01:42:04,399 --> 01:42:08,779
once we are done training in other words

2162
01:42:06,590 --> 01:42:15,500
how does the notion of train validation

2163
01:42:08,779 --> 01:42:19,670
test sets translate to Ganz that's an

2164
01:42:15,500 --> 01:42:22,099
awesome question and there's a lot of

2165
01:42:19,670 --> 01:42:24,980
people who make jokes about how Cannes

2166
01:42:22,099 --> 01:42:27,260
is the one field where you don't need a

2167
01:42:24,979 --> 01:42:30,500
test set and people take advantage of

2168
01:42:27,260 --> 01:42:33,829
that by making stuff up and saying it

2169
01:42:30,500 --> 01:42:36,710
looks great there are some pretty famous

2170
01:42:33,828 --> 01:42:38,779
problems with with ganz one of the

2171
01:42:36,710 --> 01:42:41,750
famous problems with ganz is called mode

2172
01:42:38,779 --> 01:42:44,269
collapse and mode collapse happens where

2173
01:42:41,750 --> 01:42:45,859
you look at your bedrooms and it turns

2174
01:42:44,270 --> 01:42:48,489
out that there's basically only three

2175
01:42:45,859 --> 01:42:51,078
kinds of bedrooms that every possible

2176
01:42:48,488 --> 01:42:52,729
noise vector mapped or you look at your

2177
01:42:51,078 --> 01:42:54,170
gallery and it turns at all that turns

2178
01:42:52,729 --> 01:42:56,049
out they're all just the same thing

2179
01:42:54,170 --> 01:42:58,760
or there's just three different things

2180
01:42:56,050 --> 01:43:00,739
mode collapse is easy to see if you

2181
01:42:58,760 --> 01:43:03,559
collapse down to a small number of modes

2182
01:43:00,738 --> 01:43:06,109
like you know three or four but what if

2183
01:43:03,559 --> 01:43:06,590
you have a mode collapse down to 10,000

2184
01:43:06,109 --> 01:43:08,839
modes

2185
01:43:06,590 --> 01:43:10,880
so there's only 10,000 possible bedrooms

2186
01:43:08,840 --> 01:43:13,039
that all of your noise vectors collapse

2187
01:43:10,880 --> 01:43:14,599
to that's not like you wouldn't be able

2188
01:43:13,039 --> 01:43:16,039
to see it here right because it's pretty

2189
01:43:14,599 --> 01:43:20,900
unlikely you would have two identical

2190
01:43:16,039 --> 01:43:22,760
bedrooms out of 10,000 or what if every

2191
01:43:20,899 --> 01:43:25,460
one of these bedrooms is basically a

2192
01:43:22,760 --> 01:43:29,719
direct copy of what if the basically had

2193
01:43:25,460 --> 01:43:32,420
never memorized some input you know

2194
01:43:29,719 --> 01:43:36,739
could that be happening and the truth is

2195
01:43:32,420 --> 01:43:39,288
most papers don't do a good job or

2196
01:43:36,738 --> 01:43:45,018
sometimes any job of checking those

2197
01:43:39,288 --> 01:43:47,719
things so the question of like how do we

2198
01:43:45,019 --> 01:43:49,719
evaluate ganz and even like the point of

2199
01:43:47,719 --> 01:43:53,239
like hey maybe we should actually

2200
01:43:49,719 --> 01:43:56,929
evaluate games properly is something

2201
01:43:53,238 --> 01:44:01,009
that is not widely enough understood

2202
01:43:56,929 --> 01:44:04,328
even now and some people are trying to

2203
01:44:01,010 --> 01:44:07,610
you know really push so Ian Goodfellow

2204
01:44:04,328 --> 01:44:09,920
who a lot of you will know because he

2205
01:44:07,609 --> 01:44:11,929
came and spoke here at a lot of the book

2206
01:44:09,920 --> 01:44:15,199
club meetings last year and of course

2207
01:44:11,929 --> 01:44:17,989
was the first author on the most famous

2208
01:44:15,198 --> 01:44:22,009
deep learning book he is the inventor of

2209
01:44:17,988 --> 01:44:25,129
games and he's been sending a continuous

2210
01:44:22,010 --> 01:44:29,559
stream of tweets reminding people about

2211
01:44:25,130 --> 01:44:32,779
the importance of testing dance properly

2212
01:44:29,559 --> 01:44:36,529
so yeah if you see a paper that claims

2213
01:44:32,779 --> 01:44:37,969
exceptional gaen results then this is

2214
01:44:36,529 --> 01:44:40,130
definitely something to look at you know

2215
01:44:37,969 --> 01:44:46,670
is have they talked about mode collapse

2216
01:44:40,130 --> 01:44:48,529
have they talked about memorization okay

2217
01:44:46,670 --> 01:44:50,149
so um this is going to be really

2218
01:44:48,529 --> 01:44:51,800
straight forwards because it's just a

2219
01:44:50,149 --> 01:44:54,518
neural net right so all we're going to

2220
01:44:51,800 --> 01:44:59,029
do is we're going to create an input

2221
01:44:54,519 --> 01:45:01,489
containing lots of zebra photos and with

2222
01:44:59,029 --> 01:45:06,139
each one we'll pair it with an

2223
01:45:01,488 --> 01:45:08,058
equivalent horse photo and we'll just

2224
01:45:06,139 --> 01:45:09,349
train a neural net that goes from one to

2225
01:45:08,059 --> 01:45:11,840
the other or you could do the same thing

2226
01:45:09,349 --> 01:45:14,210
for every Monet painting create a

2227
01:45:11,840 --> 01:45:16,639
dataset containing the photo of the

2228
01:45:14,210 --> 01:45:19,189
place oh wait that's not possible

2229
01:45:16,639 --> 01:45:21,409
because the places that Monet painted

2230
01:45:19,189 --> 01:45:23,868
aren't there anymore and there aren't

2231
01:45:21,408 --> 01:45:26,920
exact zebra versions of horses and oh

2232
01:45:23,868 --> 01:45:29,509
wait how the hell is this kind of work

2233
01:45:26,920 --> 01:45:31,969
this seems to break everything we know

2234
01:45:29,510 --> 01:45:34,639
about what neural nets can do and how

2235
01:45:31,969 --> 01:45:36,260
they do them alright make sure you know

2236
01:45:34,639 --> 01:45:38,409
ask me a question just spoil a whole

2237
01:45:36,260 --> 01:45:40,889
train of thought come on very good

2238
01:45:38,408 --> 01:45:43,988
can Ganz be used for data augmentation

2239
01:45:40,889 --> 01:45:47,949
yeah absolutely you can use a gain for

2240
01:45:43,988 --> 01:45:50,829
data augmentation should you I don't

2241
01:45:47,948 --> 01:45:52,839
know like there are some papers that try

2242
01:45:50,829 --> 01:45:55,289
to do semi-supervised learning with ganz

2243
01:45:52,840 --> 01:45:57,460
I haven't found any that are like

2244
01:45:55,289 --> 01:46:00,460
particularly compelling showing

2245
01:45:57,460 --> 01:46:01,929
state-of-the-art results on really

2246
01:46:00,460 --> 01:46:07,569
interesting data sets that have been

2247
01:46:01,929 --> 01:46:09,399
widely studied I'm a little skeptical

2248
01:46:07,569 --> 01:46:11,170
and the reason I'm a little skeptical is

2249
01:46:09,399 --> 01:46:16,569
because in my experience if you train a

2250
01:46:11,170 --> 01:46:18,429
model with synthetic data the neural net

2251
01:46:16,569 --> 01:46:20,769
will become fantastically good at

2252
01:46:18,429 --> 01:46:23,469
recognizing the specific problems of

2253
01:46:20,770 --> 01:46:27,070
your synthetic data and that'll be ended

2254
01:46:23,469 --> 01:46:28,090
up what it's learning from and there are

2255
01:46:27,069 --> 01:46:30,130
lots of other ways of doing

2256
01:46:28,090 --> 01:46:33,850
semi-supervised models which do work

2257
01:46:30,130 --> 01:46:36,969
well there are some places that can work

2258
01:46:33,850 --> 01:46:38,370
for example you might remember Octavio

2259
01:46:36,969 --> 01:46:41,198
gawd created that fantastic

2260
01:46:38,369 --> 01:46:43,510
visualization in part one of like the

2261
01:46:41,198 --> 01:46:46,509
zooming ComNet where it kind of showed

2262
01:46:43,510 --> 01:46:49,810
or let her go through em list he at

2263
01:46:46,510 --> 01:46:52,570
least at that time had the was the

2264
01:46:49,810 --> 01:46:56,560
number one autonomous remote-controlled

2265
01:46:52,569 --> 01:46:59,279
car okay I mean in in autonomous remote

2266
01:46:56,560 --> 01:47:04,210
control car competitions and he trained

2267
01:46:59,279 --> 01:47:06,550
his model using synthetically augmented

2268
01:47:04,210 --> 01:47:09,189
data where he basically took real videos

2269
01:47:06,550 --> 01:47:13,000
of a car driving around the circuit and

2270
01:47:09,189 --> 01:47:15,369
added like fake people and fake other

2271
01:47:13,000 --> 01:47:19,569
cars and stuff like that and I think

2272
01:47:15,369 --> 01:47:22,179
that worked well because well a because

2273
01:47:19,569 --> 01:47:24,179
he's kind of a genius and B because I

2274
01:47:22,179 --> 01:47:26,590
think he had a kind of a world to find

2275
01:47:24,179 --> 01:47:33,940
kind of little subset that he had to

2276
01:47:26,590 --> 01:47:35,860
work in but yeah in general it's really

2277
01:47:33,939 --> 01:47:38,198
hard it's really really hard to use

2278
01:47:35,859 --> 01:47:40,869
synthetic data I've tried using

2279
01:47:38,198 --> 01:47:42,488
synthetic data and models for decades

2280
01:47:40,869 --> 01:47:44,319
now obviously not ganz cuz they're

2281
01:47:42,488 --> 01:47:50,459
pretty new but in general it's very hard

2282
01:47:44,319 --> 01:47:50,460
to do very interesting research question

2283
01:47:50,859 --> 01:48:00,549
all right so somehow these folks at

2284
01:47:56,319 --> 01:48:04,389
Berkeley created a model that can turn a

2285
01:48:00,550 --> 01:48:07,090
horse into a zebra despite not having

2286
01:48:04,389 --> 01:48:08,859
any photos unless they went out there

2287
01:48:07,090 --> 01:48:10,840
and painted horses and took

2288
01:48:08,859 --> 01:48:13,948
before-and-after shots but I believe

2289
01:48:10,840 --> 01:48:17,079
they didn't right

2290
01:48:13,948 --> 01:48:21,388
so how the hell did they do this it's

2291
01:48:17,079 --> 01:48:21,389
it's kind of it's kind of genius

2292
01:48:22,840 --> 01:48:30,579
I will say the person I know who's doing

2293
01:48:27,158 --> 01:48:32,849
the most interesting practice of cycle

2294
01:48:30,579 --> 01:48:34,000
Gann right now is one of our students

2295
01:48:32,850 --> 01:48:38,289
Elena

2296
01:48:34,000 --> 01:48:41,380
sarin she's the only artist I know of

2297
01:48:38,289 --> 01:48:44,469
who was a psychic an artist here's an

2298
01:48:41,380 --> 01:48:47,380
example I love she created this little

2299
01:48:44,469 --> 01:48:49,480
doodle in the top of left and then

2300
01:48:47,380 --> 01:48:51,190
trained a psycho Gann to turn it into

2301
01:48:49,479 --> 01:48:56,589
this beautiful painting in the bottom

2302
01:48:51,189 --> 01:48:59,319
row here's some more of her amazing

2303
01:48:56,590 --> 01:49:01,510
works and I think it's really

2304
01:48:59,319 --> 01:49:04,420
interesting like I I mentioned at the

2305
01:49:01,510 --> 01:49:07,449
start of this class that like Dan's are

2306
01:49:04,420 --> 01:49:10,779
in the category of like stuff that's not

2307
01:49:07,448 --> 01:49:12,908
there yet but it's nearly there and in

2308
01:49:10,779 --> 01:49:14,800
this case like there's at least one

2309
01:49:12,908 --> 01:49:16,809
person in the world now who's creating

2310
01:49:14,800 --> 01:49:19,300
beautiful and extraordinary artworks

2311
01:49:16,810 --> 01:49:20,860
using gas and there's lots of pairs

2312
01:49:19,300 --> 01:49:23,380
specifically cycle games and there's

2313
01:49:20,859 --> 01:49:25,118
actually like at least maybe a dozen

2314
01:49:23,380 --> 01:49:27,489
people I know of who are just doing

2315
01:49:25,118 --> 01:49:30,429
interesting creative work with neural

2316
01:49:27,488 --> 01:49:32,519
nets more generally and the field of

2317
01:49:30,429 --> 01:49:35,408
creative area is going to expand

2318
01:49:32,520 --> 01:49:36,670
dramatically and I think it's

2319
01:49:35,408 --> 01:49:38,408
interesting with Elena right I mean I

2320
01:49:36,670 --> 01:49:39,730
don't know her personally but from what

2321
01:49:38,408 --> 01:49:42,460
I understand of her background she's a

2322
01:49:39,729 --> 01:49:44,500
you know she's a software developer you

2323
01:49:42,460 --> 01:49:46,569
know it's her full-time job and an

2324
01:49:44,500 --> 01:49:49,658
artist as her hobby and she's kind of

2325
01:49:46,569 --> 01:49:51,939
started combining these two by saying

2326
01:49:49,658 --> 01:49:54,039
gosh I wonder what this particular tool

2327
01:49:51,939 --> 01:49:55,750
could bring to my art and so if you

2328
01:49:54,039 --> 01:49:59,738
follow her Twitter account we'll make

2329
01:49:55,750 --> 01:50:02,649
sure we add it on the wiki somebody can

2330
01:49:59,738 --> 01:50:04,359
find it as for Lena sarin sarin

2331
01:50:02,649 --> 01:50:07,170
she basically posts a new

2332
01:50:04,359 --> 01:50:13,839
almost every day and they're always

2333
01:50:07,170 --> 01:50:15,699
pretty amazing so here's the basic trick

2334
01:50:13,840 --> 01:50:23,590
okay and this is from the cycle Gann

2335
01:50:15,699 --> 01:50:25,389
paper we're going to have to kind of two

2336
01:50:23,590 --> 01:50:27,340
images assuming we're doing this with

2337
01:50:25,390 --> 01:50:29,470
images right but the key thing is

2338
01:50:27,340 --> 01:50:32,529
they're not paired images so we're not

2339
01:50:29,470 --> 01:50:35,199
we don't have a data set of horses and

2340
01:50:32,529 --> 01:50:39,090
the equivalent zebras we've got bunch of

2341
01:50:35,199 --> 01:50:42,729
horses bunch of zebras grab one horse

2342
01:50:39,090 --> 01:50:46,110
grab one zebra okay we've now got an X

2343
01:50:42,729 --> 01:50:50,619
so X let's say X is horse and Y is zebra

2344
01:50:46,109 --> 01:50:52,089
we're going to train a generator and

2345
01:50:50,619 --> 01:50:55,180
what they call here a mapping function

2346
01:50:52,090 --> 01:50:57,579
that turns horse into zebra we'll call

2347
01:50:55,180 --> 01:50:59,710
that mapping function G and we'll create

2348
01:50:57,579 --> 01:51:02,079
one mapping function generator that

2349
01:50:59,710 --> 01:51:06,329
turns a zebra into a horse and we'll

2350
01:51:02,079 --> 01:51:09,189
call that F well create a discriminator

2351
01:51:06,329 --> 01:51:10,539
just like we did before which is going

2352
01:51:09,189 --> 01:51:14,139
to get as good as possible at

2353
01:51:10,539 --> 01:51:16,449
recognizing real from fake horses so

2354
01:51:14,140 --> 01:51:17,770
that'll be DX and then another

2355
01:51:16,449 --> 01:51:20,319
discriminator which is going to be as

2356
01:51:17,770 --> 01:51:24,310
good as possible and recognizing real

2357
01:51:20,319 --> 01:51:27,789
from fake zebras I'll call that dy okay

2358
01:51:24,310 --> 01:51:29,680
so that's kind of our starting point but

2359
01:51:27,789 --> 01:51:32,439
then the key thing to making this worse

2360
01:51:29,680 --> 01:51:33,820
work okay so we're kind of generating a

2361
01:51:32,439 --> 01:51:35,079
loss function here right here's one bit

2362
01:51:33,819 --> 01:51:36,849
of the loss function here the second bit

2363
01:51:35,079 --> 01:51:38,439
of the loss function we're going to

2364
01:51:36,850 --> 01:51:40,930
create something called cycle

2365
01:51:38,439 --> 01:51:44,710
consistency loss which says after you

2366
01:51:40,930 --> 01:51:49,570
turn your horse into a zebra with your G

2367
01:51:44,710 --> 01:51:53,380
generator and check whether or not I can

2368
01:51:49,569 --> 01:51:54,460
recognize that it's real so I keep

2369
01:51:53,380 --> 01:51:56,440
forgetting which one's false and which

2370
01:51:54,460 --> 01:51:58,840
one zebra I apologize I forget my X's

2371
01:51:56,439 --> 01:52:00,729
and Y's backwards though my horse into a

2372
01:51:58,840 --> 01:52:04,300
zebra and then going to try and turn

2373
01:52:00,729 --> 01:52:07,449
that zebra back into the same horse that

2374
01:52:04,300 --> 01:52:09,279
I started with okay and so then I'm

2375
01:52:07,449 --> 01:52:15,449
going to have another function that's

2376
01:52:09,279 --> 01:52:17,739
going to check whether my this horse

2377
01:52:15,449 --> 01:52:18,539
which are generated knowing nothing

2378
01:52:17,739 --> 01:52:21,789
about

2379
01:52:18,539 --> 01:52:23,769
generated entirely from this zebra is

2380
01:52:21,789 --> 01:52:27,269
similar to the original horse or not

2381
01:52:23,770 --> 01:52:30,280
right so the idea would be if your

2382
01:52:27,270 --> 01:52:32,800
generated zebra doesn't look anything

2383
01:52:30,279 --> 01:52:35,289
like your original horse you've got no

2384
01:52:32,800 --> 01:52:38,350
chance of turning it back into the

2385
01:52:35,289 --> 01:52:41,109
original horse so a loss which compares

2386
01:52:38,350 --> 01:52:44,980
X hat to X is going to be really bad

2387
01:52:41,109 --> 01:52:46,929
unless you can go into Y and back out

2388
01:52:44,979 --> 01:52:48,549
again and you're probably only going to

2389
01:52:46,930 --> 01:52:51,250
be able to do that if you're able to

2390
01:52:48,550 --> 01:52:53,230
create a zebra that looks like the

2391
01:52:51,250 --> 01:52:55,630
original horse so that you know what the

2392
01:52:53,229 --> 01:52:58,229
original horse looked like and vice

2393
01:52:55,630 --> 01:53:02,230
versa take the original take your zebra

2394
01:52:58,229 --> 01:53:04,449
turn it into a fake horse and check that

2395
01:53:02,229 --> 01:53:07,419
you can recognize that and then try and

2396
01:53:04,449 --> 01:53:09,189
turn it back into the original zebra and

2397
01:53:07,420 --> 01:53:12,190
check that it looks like the original so

2398
01:53:09,189 --> 01:53:15,909
notice here this F right is our zebra

2399
01:53:12,189 --> 01:53:19,000
two horse this G is our horse two zebra

2400
01:53:15,909 --> 01:53:21,550
right so so the G and the F are kind of

2401
01:53:19,000 --> 01:53:24,760
doing two things they're both turning

2402
01:53:21,550 --> 01:53:27,520
the original horse into the zebra and

2403
01:53:24,760 --> 01:53:29,619
then turning the zebra back into the

2404
01:53:27,520 --> 01:53:31,630
original horse okay so notice that

2405
01:53:29,619 --> 01:53:33,609
there's only two generators right there

2406
01:53:31,630 --> 01:53:36,100
isn't a separate generator for the

2407
01:53:33,609 --> 01:53:37,809
reverse mapping you have to use the same

2408
01:53:36,100 --> 01:53:38,829
generator that was used for the original

2409
01:53:37,810 --> 01:53:41,560
mapping okay

2410
01:53:38,829 --> 01:53:44,140
so this is the cycle consistence you

2411
01:53:41,560 --> 01:53:47,130
lost and I just think this is like this

2412
01:53:44,140 --> 01:53:49,600
is genius you know like the idea that

2413
01:53:47,130 --> 01:53:50,489
this is a thing that could be even be

2414
01:53:49,600 --> 01:53:52,660
possible

2415
01:53:50,489 --> 01:53:54,729
honestly when this came out it just

2416
01:53:52,659 --> 01:53:57,010
never occurred to me as a thing that I

2417
01:53:54,729 --> 01:53:59,529
could even try and solve it seems so

2418
01:53:57,010 --> 01:54:01,000
obviously impossible and then the idea

2419
01:53:59,529 --> 01:54:08,079
that you can solve it like this I just

2420
01:54:01,000 --> 01:54:10,270
think it's it's so damn smart so it's

2421
01:54:08,079 --> 01:54:14,500
good to look at the equations in this

2422
01:54:10,270 --> 01:54:16,240
paper because they're just good example

2423
01:54:14,500 --> 01:54:17,590
like they're written pretty simply I you

2424
01:54:16,239 --> 01:54:19,329
know there's it's not like some of the

2425
01:54:17,590 --> 01:54:22,720
stuff in the fastest time game paper

2426
01:54:19,329 --> 01:54:24,579
which is just like lots of theoretical

2427
01:54:22,720 --> 01:54:26,170
proofs and whatever else in this case

2428
01:54:24,579 --> 01:54:27,880
they're you know they're just equations

2429
01:54:26,170 --> 01:54:29,859
that just lay out what's going on and

2430
01:54:27,880 --> 01:54:31,239
you really want to get to a point where

2431
01:54:29,859 --> 01:54:31,719
you where you can read them and

2432
01:54:31,239 --> 01:54:33,399
understand

2433
01:54:31,719 --> 01:54:39,460
so like let's kind of start talking

2434
01:54:33,399 --> 01:54:46,018
through them so we've got a horse and a

2435
01:54:39,460 --> 01:54:48,819
zebra okay so for some mapping function

2436
01:54:46,019 --> 01:54:51,729
G okay which is our horse - zebra

2437
01:54:48,819 --> 01:54:54,009
mapping function then there's AG and

2438
01:54:51,729 --> 01:54:55,539
loss right which is the bit we're

2439
01:54:54,010 --> 01:55:00,610
already familiar with it says I've got a

2440
01:54:55,538 --> 01:55:04,300
horse a zebra a fake zebra recognizer

2441
01:55:00,609 --> 01:55:06,339
and a horse - zebra generator okay and

2442
01:55:04,300 --> 01:55:11,849
the loss is except it's what we saw

2443
01:55:06,340 --> 01:55:17,760
before it's our ability to draw one

2444
01:55:11,849 --> 01:55:21,460
zebra out of our zebras okay and

2445
01:55:17,760 --> 01:55:29,079
recognize whether it's real or fake okay

2446
01:55:21,460 --> 01:55:31,989
and then generate a take a horse and

2447
01:55:29,078 --> 01:55:34,149
turn it into a zebra and recognize

2448
01:55:31,988 --> 01:55:37,839
whether that's real or fake okay and

2449
01:55:34,149 --> 01:55:39,788
then you're you then do one minus the

2450
01:55:37,840 --> 01:55:42,489
other and in this case they've got a log

2451
01:55:39,788 --> 01:55:44,738
in there the logs not terribly important

2452
01:55:42,488 --> 01:55:46,658
so this is this is the thing we just saw

2453
01:55:44,738 --> 01:55:49,478
that's that's why we did for sustained

2454
01:55:46,658 --> 01:55:52,839
gain first is this is just a standard

2455
01:55:49,479 --> 01:55:55,360
gann loss in math form did you have a

2456
01:55:52,840 --> 01:55:57,219
question right wrong all of this sounds

2457
01:55:55,359 --> 01:55:58,719
awfully like translating in one language

2458
01:55:57,219 --> 01:56:01,479
to another then back to the original

2459
01:55:58,719 --> 01:56:07,989
have gans or any equivalent been tried

2460
01:56:01,479 --> 01:56:09,829
in translation not that I'm not that I

2461
01:56:07,988 --> 01:56:13,019
know of

2462
01:56:09,829 --> 01:56:13,019
[Music]

2463
01:56:14,010 --> 01:56:19,289
yeah yeah because there's the Unversed

2464
01:56:16,929 --> 01:56:24,069
is this this unsupervised machine

2465
01:56:19,288 --> 01:56:27,099
translation which does kind of do

2466
01:56:24,069 --> 01:56:28,899
something like this but I don't I

2467
01:56:27,099 --> 01:56:30,849
haven't looked at it closely enough to

2468
01:56:28,899 --> 01:56:35,408
know if it's nearly identical or if it's

2469
01:56:30,849 --> 01:56:37,840
just vaguely similar yeah did so to kind

2470
01:56:35,408 --> 01:56:40,089
of back up to what I do know normally

2471
01:56:37,840 --> 01:56:42,279
with translation you require this kind

2472
01:56:40,090 --> 01:56:43,809
of paired input you require parallel

2473
01:56:42,279 --> 01:56:46,719
text so you know this is the French

2474
01:56:43,809 --> 01:56:49,869
translation of this English

2475
01:56:46,719 --> 01:56:52,269
I dunno there's been a couple of recent

2476
01:56:49,868 --> 01:56:55,589
papers that show the ability to create

2477
01:56:52,269 --> 01:56:58,119
good quality translation models without

2478
01:56:55,590 --> 01:57:00,760
paired data

2479
01:56:58,118 --> 01:57:01,868
I haven't implemented them and I don't

2480
01:57:00,760 --> 01:57:03,878
understand anything I haven't

2481
01:57:01,868 --> 01:57:06,969
implemented but yeah they may well be

2482
01:57:03,878 --> 01:57:10,229
doing the same basic idea we'll look at

2483
01:57:06,969 --> 01:57:16,989
it during the week and get back to you

2484
01:57:10,229 --> 01:57:19,360
okay all right so we're going to again

2485
01:57:16,988 --> 01:57:22,089
loss the next piece is the cycle

2486
01:57:19,359 --> 01:57:25,089
consistency loss right and so the basic

2487
01:57:22,090 --> 01:57:28,748
idea here is that we start with our

2488
01:57:25,090 --> 01:57:31,208
horse use our zebra generator on that to

2489
01:57:28,748 --> 01:57:33,429
create a zebra use our horse generator

2490
01:57:31,208 --> 01:57:35,590
on that to create a horse and then

2491
01:57:33,429 --> 01:57:39,340
compare that to the original horse and

2492
01:57:35,590 --> 01:57:41,708
this double lines were the one we've

2493
01:57:39,340 --> 01:57:43,989
seen this before this is the l1 loss

2494
01:57:41,708 --> 01:57:46,538
okay so this is the sum of the absolute

2495
01:57:43,988 --> 01:57:49,808
value of differences where else if this

2496
01:57:46,538 --> 01:57:51,878
was a turn it would be the l2 loss or

2497
01:57:49,809 --> 01:57:54,340
the two norm which would be the sum of

2498
01:57:51,878 --> 01:57:58,658
squared differences the square root of

2499
01:57:54,340 --> 01:58:02,399
it actually and again we now know this

2500
01:57:58,658 --> 01:58:07,719
squiggle idea okay which is from our

2501
01:58:02,399 --> 01:58:09,550
horses grab a horse okay that's so this

2502
01:58:07,719 --> 01:58:11,229
is what we mean by sample from a

2503
01:58:09,550 --> 01:58:13,329
distribution there's all kinds of

2504
01:58:11,229 --> 01:58:14,979
distributions but most commonly in these

2505
01:58:13,328 --> 01:58:17,438
papers were using an empirical

2506
01:58:14,979 --> 01:58:19,958
distribution in other words we've got

2507
01:58:17,439 --> 01:58:22,959
some rows of data grab a row okay so

2508
01:58:19,958 --> 01:58:26,109
when you see this thing squiggle other

2509
01:58:22,958 --> 01:58:29,918
thing this thing here when it says P

2510
01:58:26,109 --> 01:58:32,228
data that means grab something from the

2511
01:58:29,918 --> 01:58:35,559
data and we're going to call that thing

2512
01:58:32,229 --> 01:58:38,260
X so from our horses pictures grab a

2513
01:58:35,559 --> 01:58:40,239
horse turn it into a zebra turn it back

2514
01:58:38,260 --> 01:58:43,418
into a horse compare it to the original

2515
01:58:40,238 --> 01:58:45,128
and some of the absolute values okay do

2516
01:58:43,418 --> 01:58:47,260
that for horse to zebra do it for zebra

2517
01:58:45,128 --> 01:58:50,319
to horse as well add the two together

2518
01:58:47,260 --> 01:58:52,498
and that is our cycle consistency you

2519
01:58:50,319 --> 01:58:52,498
lost

2520
01:58:54,689 --> 01:59:01,379
okay so now we get our loss function and

2521
01:58:58,649 --> 01:59:05,129
the whole loss function depends on our

2522
01:59:01,380 --> 01:59:07,770
horse generator a zebra generator our

2523
01:59:05,130 --> 01:59:09,929
horse recognizer our zebra recognizer

2524
01:59:07,770 --> 01:59:15,179
discriminator and we're going to add up

2525
01:59:09,929 --> 01:59:18,118
the gain loss for recognizing horses the

2526
01:59:15,179 --> 01:59:21,600
gain loss for recognizing zebras and the

2527
01:59:18,118 --> 01:59:24,059
cycle cycle consistency loss for our two

2528
01:59:21,600 --> 01:59:26,340
generators okay and then we've got a

2529
01:59:24,060 --> 01:59:27,900
lambda here which hopefully we're kind

2530
01:59:26,340 --> 01:59:29,369
of used to this idea now that is when

2531
01:59:27,899 --> 01:59:32,759
you've got two different kinds of loss

2532
01:59:29,368 --> 01:59:34,649
you chuck in a parameter there you can

2533
01:59:32,760 --> 01:59:36,539
multiply them by so they're of account

2534
01:59:34,649 --> 01:59:40,710
about the same scale okay and we did a

2535
01:59:36,539 --> 01:59:43,529
similar thing with our bounding box loss

2536
01:59:40,710 --> 01:59:48,539
compared to our classifier loss when we

2537
01:59:43,529 --> 01:59:51,448
did that localization stuff okay

2538
01:59:48,539 --> 01:59:54,448
so then we're going to try to for this

2539
01:59:51,448 --> 01:59:56,399
map for this loss function maximize the

2540
01:59:54,448 --> 02:00:00,469
capability of the discriminators are to

2541
01:59:56,399 --> 02:00:03,479
discriminate discriminating whilst

2542
02:00:00,469 --> 02:00:05,819
minimizing that for the generators so

2543
02:00:03,479 --> 02:00:08,309
the generators and the discriminators

2544
02:00:05,819 --> 02:00:08,759
are going to be facing off against each

2545
02:00:08,310 --> 02:00:12,389
other

2546
02:00:08,760 --> 02:00:14,520
so when you see this min max thing in

2547
02:00:12,389 --> 02:00:16,289
papers you'll see it a lot that means it

2548
02:00:14,520 --> 02:00:19,050
basically means this idea that in your

2549
02:00:16,289 --> 02:00:20,340
training loop one thing is trying to

2550
02:00:19,050 --> 02:00:22,050
make something better the other is

2551
02:00:20,340 --> 02:00:23,579
trying to make something worse and you

2552
02:00:22,050 --> 02:00:25,320
generally there's lots of ways to do it

2553
02:00:23,579 --> 02:00:27,689
but most commonly you'll alternate

2554
02:00:25,319 --> 02:00:30,079
between the two and you'll often see

2555
02:00:27,689 --> 02:00:33,839
this just referred to in math papers as

2556
02:00:30,079 --> 02:00:35,309
minnows okay so we see min max you think

2557
02:00:33,840 --> 02:00:42,810
you should immediately think okay

2558
02:00:35,310 --> 02:00:45,060
adversarial training so let's look at

2559
02:00:42,810 --> 02:00:46,710
the code and we're only going to

2560
02:00:45,060 --> 02:00:49,610
probably might be able to finish this

2561
02:00:46,710 --> 02:00:54,448
today but we're going to do something

2562
02:00:49,609 --> 02:00:56,819
almost unheard of which is I started

2563
02:00:54,448 --> 02:00:58,500
looking at somebody else's code and I

2564
02:00:56,819 --> 02:01:00,299
was not so disgusted that I threw the

2565
02:00:58,500 --> 02:01:03,899
whole thing away and did it myself I

2566
02:01:00,300 --> 02:01:04,949
actually said I quite like this I like

2567
02:01:03,899 --> 02:01:06,639
it enough I'm going to show it to my

2568
02:01:04,948 --> 02:01:13,719
students

2569
02:01:06,640 --> 02:01:15,400
so this is where the code comes from so

2570
02:01:13,720 --> 02:01:19,750
this is one of the people that created

2571
02:01:15,399 --> 02:01:23,129
the original code for cycle games and

2572
02:01:19,750 --> 02:01:29,920
they've created a high-torque version

2573
02:01:23,130 --> 02:01:32,800
and I had to clean it up a little bit

2574
02:01:29,920 --> 02:01:36,730
but it's actually pretty damn good it's

2575
02:01:32,800 --> 02:01:40,119
I think the first time I found code that

2576
02:01:36,729 --> 02:01:44,559
I didn't feel the need to rewrite from

2577
02:01:40,119 --> 02:01:47,319
scratch before I showed it to you and so

2578
02:01:44,560 --> 02:01:48,940
the cool thing about this is one of the

2579
02:01:47,319 --> 02:01:50,409
reasons I liked doing it this way of

2580
02:01:48,939 --> 02:01:54,309
like finally finding something that's

2581
02:01:50,409 --> 02:01:56,889
not awful is that you're now going to

2582
02:01:54,310 --> 02:01:58,780
get to see almost all the bits of fast

2583
02:01:56,890 --> 02:02:01,060
AI or like all the relevant bits of fast

2584
02:01:58,779 --> 02:02:03,130
AI written in a different way about

2585
02:02:01,060 --> 02:02:04,900
somebody else right and so you're going

2586
02:02:03,130 --> 02:02:08,279
to get to see like oh how they do data

2587
02:02:04,899 --> 02:02:12,809
sets and data loaders and models and

2588
02:02:08,279 --> 02:02:16,979
training loops and so forth okay so

2589
02:02:12,810 --> 02:02:22,480
you'll find there's a C game directory

2590
02:02:16,979 --> 02:02:24,189
which is basically nearly this with some

2591
02:02:22,479 --> 02:02:27,549
cleanups which I hope to submit as a PR

2592
02:02:24,189 --> 02:02:29,529
sometime - it was written in a way that

2593
02:02:27,550 --> 02:02:30,880
unfortunately made it a bit over

2594
02:02:29,529 --> 02:02:32,559
connected to how they were using it as a

2595
02:02:30,880 --> 02:02:34,989
script but I so cleaned it up a little

2596
02:02:32,560 --> 02:02:37,450
bit so I could use it as a module but

2597
02:02:34,989 --> 02:02:41,949
other than that it's pretty similar so C

2598
02:02:37,449 --> 02:02:45,250
can is basically their code copied from

2599
02:02:41,949 --> 02:02:52,359
their from their github repo with some

2600
02:02:45,250 --> 02:02:54,369
minor changes so the way the second mini

2601
02:02:52,359 --> 02:02:56,319
library has been set up is that the

2602
02:02:54,369 --> 02:02:59,439
configuration options they're assuming

2603
02:02:56,319 --> 02:03:01,420
are being passed into like a script so

2604
02:02:59,439 --> 02:03:04,809
they've got this train options parser

2605
02:03:01,420 --> 02:03:08,800
method and so you can see I'm basically

2606
02:03:04,810 --> 02:03:12,220
past passing in an array of like script

2607
02:03:08,800 --> 02:03:14,619
options okay where's my data how many

2608
02:03:12,220 --> 02:03:17,920
threads do I want to drop out how many

2609
02:03:14,619 --> 02:03:20,469
iterations what am I going to call this

2610
02:03:17,920 --> 02:03:27,730
model rich GPU

2611
02:03:20,469 --> 02:03:30,730
okay so that gives me a opt object which

2612
02:03:27,729 --> 02:03:32,500
you can then see well you know what it

2613
02:03:30,729 --> 02:03:34,448
contains you'll see that it contains

2614
02:03:32,500 --> 02:03:36,520
some things I didn't mention that's

2615
02:03:34,448 --> 02:03:40,689
because it's got defaults for everything

2616
02:03:36,520 --> 02:03:43,960
else that I didn't mention okay so we're

2617
02:03:40,689 --> 02:03:45,669
going to rather than using fast AI stuff

2618
02:03:43,960 --> 02:03:47,289
we're going to largely use CG and stuff

2619
02:03:45,670 --> 02:03:51,130
so the first thing we're going to need

2620
02:03:47,289 --> 02:03:52,930
is a downloader and so this is also a

2621
02:03:51,130 --> 02:03:55,600
great opportunity for you again to

2622
02:03:52,930 --> 02:03:59,350
practice your ability to navigate

2623
02:03:55,600 --> 02:04:01,840
through code with your editor or IDE of

2624
02:03:59,350 --> 02:04:04,270
choice so we're going to start with

2625
02:04:01,840 --> 02:04:09,310
create data loader so you should be able

2626
02:04:04,270 --> 02:04:11,739
to go find symbol or in vim tag to jump

2627
02:04:09,310 --> 02:04:14,530
straight to create data loader and we

2628
02:04:11,738 --> 02:04:17,229
can see that's creating a custom data

2629
02:04:14,529 --> 02:04:20,939
set loader and then we can see custom

2630
02:04:17,229 --> 02:04:24,069
data set loader is a base data loader

2631
02:04:20,939 --> 02:04:26,710
okay so that doesn't really do anything

2632
02:04:24,069 --> 02:04:29,679
it creates okay so basically we can see

2633
02:04:26,710 --> 02:04:32,560
that it's going to use a standard PI

2634
02:04:29,680 --> 02:04:33,789
torch data loader so that's good and so

2635
02:04:32,560 --> 02:04:36,489
we know if you're to use a standard

2636
02:04:33,789 --> 02:04:38,920
plate or data loader you have to pass it

2637
02:04:36,488 --> 02:04:40,899
a data set and we know that a data set

2638
02:04:38,920 --> 02:04:45,130
is something that contains a length and

2639
02:04:40,899 --> 02:04:46,779
a an indexer so presumably when we look

2640
02:04:45,130 --> 02:04:50,289
at create data set it's going to do that

2641
02:04:46,779 --> 02:04:51,969
here is create data set okay so this

2642
02:04:50,289 --> 02:04:54,939
library actually does more than just

2643
02:04:51,969 --> 02:04:56,380
cycle gain it handles both aligned and

2644
02:04:54,939 --> 02:04:58,509
unaligned image pairs

2645
02:04:56,380 --> 02:05:01,329
we know that our image pairs are

2646
02:04:58,510 --> 02:05:03,219
unaligned so we're going to an online

2647
02:05:01,329 --> 02:05:07,869
data set okay here it is

2648
02:05:03,219 --> 02:05:12,550
and as expected it has a get item and a

2649
02:05:07,869 --> 02:05:15,849
length good and so the main obviously

2650
02:05:12,550 --> 02:05:19,539
the main the length is just whatever of

2651
02:05:15,850 --> 02:05:22,840
our so a and B is our horses and zebras

2652
02:05:19,539 --> 02:05:25,119
got two sets so whichever one is longer

2653
02:05:22,840 --> 02:05:28,140
is the length of the data loader and so

2654
02:05:25,119 --> 02:05:32,829
getitem is just going to go ahead and

2655
02:05:28,140 --> 02:05:34,539
randomly grab something from each of our

2656
02:05:32,829 --> 02:05:37,180
two

2657
02:05:34,539 --> 02:05:40,119
horses and zebras open them up with

2658
02:05:37,180 --> 02:05:43,210
pillow or PIL run them through some

2659
02:05:40,119 --> 02:05:45,130
transformations and then we could either

2660
02:05:43,210 --> 02:05:47,470
be turning horses into zebras or zebras

2661
02:05:45,130 --> 02:05:49,329
into horses so there's some direction

2662
02:05:47,470 --> 02:05:52,840
and then I'll just go ahead and then

2663
02:05:49,329 --> 02:05:54,670
return our horse and our zebra and our

2664
02:05:52,840 --> 02:05:58,210
path to the boss and the path of zebra

2665
02:05:54,670 --> 02:06:00,100
so yeah hopefully you can kind of see

2666
02:05:58,210 --> 02:06:03,270
that this is looking pretty similar to

2667
02:06:00,100 --> 02:06:06,190
the kind of stuff that fast AI does

2668
02:06:03,270 --> 02:06:07,930
first day I obviously does quite a lot

2669
02:06:06,189 --> 02:06:09,969
more when it comes to transforms and

2670
02:06:07,930 --> 02:06:12,760
performance and stuff like this but you

2671
02:06:09,970 --> 02:06:15,159
know remember this is like research code

2672
02:06:12,760 --> 02:06:18,460
for this one thing like it's pretty cool

2673
02:06:15,159 --> 02:06:20,979
that they did all this work so we've got

2674
02:06:18,460 --> 02:06:25,149
a data loader so we can go and load our

2675
02:06:20,979 --> 02:06:27,069
data into it and so that'll tell us how

2676
02:06:25,149 --> 02:06:30,179
many mini batches are in it that's the

2677
02:06:27,069 --> 02:06:32,859
length of the data loader in Piper watch

2678
02:06:30,180 --> 02:06:39,070
next step we've got a data loader is to

2679
02:06:32,859 --> 02:06:45,309
create a model so you can go to go tag

2680
02:06:39,069 --> 02:06:47,439
for create model there it is okay same

2681
02:06:45,310 --> 02:06:51,130
idea we've got different kinds of models

2682
02:06:47,439 --> 02:06:53,289
so we're gonna be doing a cycle gain so

2683
02:06:51,130 --> 02:06:55,029
here's our cycle gain model okay so

2684
02:06:53,289 --> 02:06:56,829
there's quite a lot of stuff in a cycle

2685
02:06:55,029 --> 02:06:59,829
gain model so let's go through and find

2686
02:06:56,829 --> 02:07:04,809
out what's going to be used but

2687
02:06:59,829 --> 02:07:08,050
basically at this stage we've just

2688
02:07:04,810 --> 02:07:10,870
called initializer and so when we

2689
02:07:08,050 --> 02:07:12,900
initialize it you can see it's going to

2690
02:07:10,869 --> 02:07:16,390
go through and it's going to define two

2691
02:07:12,899 --> 02:07:18,009
generators we're just not surprising a

2692
02:07:16,390 --> 02:07:27,850
generator for our horses and a generator

2693
02:07:18,010 --> 02:07:30,690
for a zebras here okay there's some way

2694
02:07:27,850 --> 02:07:33,480
for it to generate a pool of fake data

2695
02:07:30,689 --> 02:07:37,529
and then here we're going to grab our

2696
02:07:33,479 --> 02:07:42,479
our Dan loss and as we talked about our

2697
02:07:37,529 --> 02:07:44,529
cycle consistency loss is an l1 loss

2698
02:07:42,479 --> 02:07:47,048
that's interesting they're going to use

2699
02:07:44,529 --> 02:07:49,628
Adam so

2700
02:07:47,048 --> 02:07:52,838
sleepy recycle cans can Adam works

2701
02:07:49,628 --> 02:07:54,519
pretty well and so then we're going to

2702
02:07:52,838 --> 02:07:57,609
have an optimizer for our horse

2703
02:07:54,519 --> 02:08:01,029
discriminator and optimizer for our

2704
02:07:57,609 --> 02:08:06,669
zebra discriminator and an optimizer for

2705
02:08:01,029 --> 02:08:08,679
our generator okay the optimizer for the

2706
02:08:06,668 --> 02:08:11,108
generator is going to contain the

2707
02:08:08,679 --> 02:08:14,128
parameters both for the horse generator

2708
02:08:11,109 --> 02:08:17,529
and the zebra generator all in one place

2709
02:08:14,128 --> 02:08:19,689
so okay so the initializer is going to

2710
02:08:17,529 --> 02:08:21,039
set up all of the different networks and

2711
02:08:19,689 --> 02:08:26,079
wasp functions we need and they're all

2712
02:08:21,038 --> 02:08:29,009
going to be stored inside this model and

2713
02:08:26,078 --> 02:08:31,658
so then it prints out and shows us

2714
02:08:29,010 --> 02:08:33,338
exactly the piped watch bottles we have

2715
02:08:31,658 --> 02:08:36,278
and so it's interesting to see that

2716
02:08:33,338 --> 02:08:38,708
they're using resonance and so you can

2717
02:08:36,279 --> 02:08:44,709
see the resinates look pretty familiar

2718
02:08:38,708 --> 02:08:47,889
we've got con vet norm rally you can

2719
02:08:44,708 --> 02:08:50,648
fetch norm so instance norm is just the

2720
02:08:47,889 --> 02:08:53,109
same as batch norm basically but it

2721
02:08:50,649 --> 02:08:54,699
applies it to one image at a time and

2722
02:08:53,109 --> 02:08:59,949
the difference isn't particularly

2723
02:08:54,698 --> 02:09:03,458
important okay and you can see they're

2724
02:08:59,948 --> 02:09:06,568
doing on reflection padding just like we

2725
02:09:03,458 --> 02:09:08,588
are so you can kind of see like when you

2726
02:09:06,569 --> 02:09:10,149
when you try to build everything from

2727
02:09:08,588 --> 02:09:13,179
scratch like this it is a lot of work

2728
02:09:10,149 --> 02:09:16,689
and you know you can kind of forget the

2729
02:09:13,179 --> 02:09:19,748
little you know the nice little things

2730
02:09:16,689 --> 02:09:22,179
that fast AI does automatically for you

2731
02:09:19,748 --> 02:09:24,278
you kind of have to do all of them by

2732
02:09:22,179 --> 02:09:27,010
hand and only you end up with a subset

2733
02:09:24,279 --> 02:09:29,949
of them so you know over time hopefully

2734
02:09:27,010 --> 02:09:31,719
soon we'll get all of this Gans stuff

2735
02:09:29,948 --> 02:09:35,288
into fast AI and it'll be nice and easy

2736
02:09:31,719 --> 02:09:37,479
okay so we've got our model and remember

2737
02:09:35,288 --> 02:09:39,429
the model contains the loss functions

2738
02:09:37,479 --> 02:09:41,708
it contains the generators it contains

2739
02:09:39,429 --> 02:09:44,378
the discriminators all in one convenient

2740
02:09:41,708 --> 02:09:46,208
place so I've gone ahead and kind of

2741
02:09:44,378 --> 02:09:49,628
copied and pasted and slightly

2742
02:09:46,208 --> 02:09:51,429
refactored the training loop from their

2743
02:09:49,628 --> 02:09:54,429
from their code so that we can run it

2744
02:09:51,429 --> 02:09:56,918
inside the notebook so this one a lot

2745
02:09:54,429 --> 02:10:00,899
pretty familiar right loop to go through

2746
02:09:56,918 --> 02:10:05,130
each epoch and a loop to go through

2747
02:10:00,899 --> 02:10:08,729
the dealer before we did this we set up

2748
02:10:05,130 --> 02:10:10,350
a tower this is actually not a play

2749
02:10:08,729 --> 02:10:12,239
torch data set I think this is what they

2750
02:10:10,350 --> 02:10:15,810
used slightly confusingly to talk about

2751
02:10:12,239 --> 02:10:17,429
their you know their combined what we

2752
02:10:15,810 --> 02:10:21,090
would call a model data object I guess

2753
02:10:17,430 --> 02:10:23,690
all the data that they need whip through

2754
02:10:21,090 --> 02:10:26,369
that with TQ DM to get a progress bar

2755
02:10:23,689 --> 02:10:30,569
and so now we can go through and see

2756
02:10:26,369 --> 02:10:38,760
what happens in the model right so set

2757
02:10:30,569 --> 02:10:40,380
input so set input so so it's kind of a

2758
02:10:38,760 --> 02:10:43,440
different approach to what we do in fast

2759
02:10:40,380 --> 02:10:45,090
AI this is kind of neat you know it's

2760
02:10:43,439 --> 02:10:47,759
quite specific to cycle games but

2761
02:10:45,090 --> 02:10:50,369
basically internally inside this model

2762
02:10:47,760 --> 02:10:53,789
is this idea that we're going to go into

2763
02:10:50,369 --> 02:10:56,460
our data and grab you know we're knowing

2764
02:10:53,789 --> 02:10:58,380
when we're either going horse to zebra

2765
02:10:56,460 --> 02:11:00,480
or zebra to horse depending on which way

2766
02:10:58,380 --> 02:11:04,170
we go we are the you know a is over the

2767
02:11:00,479 --> 02:11:07,399
horse or the zebra and vice versa and if

2768
02:11:04,170 --> 02:11:11,190
necessary put it on the appropriate GPU

2769
02:11:07,399 --> 02:11:15,659
and then grab the appropriate paths okay

2770
02:11:11,189 --> 02:11:20,189
so the model now has a mini batch of

2771
02:11:15,659 --> 02:11:32,180
horses and a mini batch of zebras and so

2772
02:11:20,189 --> 02:11:34,229
now we optimize the parameters okay so

2773
02:11:32,180 --> 02:11:36,869
it's kind of nice to see it like this

2774
02:11:34,229 --> 02:11:43,409
you can see each step right so first of

2775
02:11:36,869 --> 02:11:45,300
all try to optimize the generators then

2776
02:11:43,409 --> 02:11:48,059
try to optimize the horse discriminator

2777
02:11:45,300 --> 02:11:51,360
then try to optimize the zebra

2778
02:11:48,060 --> 02:11:55,710
discriminator zero grad is part of pipe

2779
02:11:51,359 --> 02:11:58,170
torch step is part of pi torch so the

2780
02:11:55,710 --> 02:11:59,880
interesting bit is the actual thing that

2781
02:11:58,170 --> 02:12:07,369
captain twitch does the backpropagation

2782
02:11:59,880 --> 02:12:10,470
on the generator so here it is and let's

2783
02:12:07,369 --> 02:12:12,239
jump to the key pieces there's all the

2784
02:12:10,470 --> 02:12:13,720
bits or the formula that we basically

2785
02:12:12,239 --> 02:12:20,460
just saw yeah

2786
02:12:13,720 --> 02:12:20,460
the paper so let's take a horse and

2787
02:12:22,020 --> 02:12:28,000
generate a zebra so we know what a fake

2788
02:12:25,600 --> 02:12:29,470
zebra and let's now use the

2789
02:12:28,000 --> 02:12:31,689
discriminator to see if we can tell

2790
02:12:29,470 --> 02:12:35,350
whether it's fake or not okay so it

2791
02:12:31,689 --> 02:12:41,079
spread fake and then let's pop that into

2792
02:12:35,350 --> 02:12:47,260
our loss function which we set up

2793
02:12:41,079 --> 02:12:49,090
earlier to see if we can basically to

2794
02:12:47,260 --> 02:12:52,539
get a loss function based on that

2795
02:12:49,090 --> 02:12:56,110
prediction then let's do the same thing

2796
02:12:52,539 --> 02:12:58,300
to do the gain loss so take a go in the

2797
02:12:56,109 --> 02:13:01,119
opposite direction and then we need to

2798
02:12:58,300 --> 02:13:03,880
use the opposite discriminator and then

2799
02:13:01,119 --> 02:13:07,720
put that through the loss function again

2800
02:13:03,880 --> 02:13:11,470
and then let's do the cycle consistence

2801
02:13:07,720 --> 02:13:15,039
you loss okay so again we take our fake

2802
02:13:11,470 --> 02:13:19,030
which we created up here okay and try

2803
02:13:15,039 --> 02:13:22,899
and turn it back again into the original

2804
02:13:19,029 --> 02:13:24,579
and then let's use that last function of

2805
02:13:22,899 --> 02:13:26,979
cycle consistency loss function we

2806
02:13:24,579 --> 02:13:29,800
created earlier to compare it to the

2807
02:13:26,979 --> 02:13:34,599
real original and here's that lambda

2808
02:13:29,800 --> 02:13:36,489
right so there's some weight that we

2809
02:13:34,600 --> 02:13:38,410
used and that would set up actually we

2810
02:13:36,489 --> 02:13:40,779
just use the default that I suggested in

2811
02:13:38,409 --> 02:13:43,000
there options and then do the same for

2812
02:13:40,779 --> 02:13:49,569
the opposite direction and then add them

2813
02:13:43,000 --> 02:13:51,640
all together do the backward step and

2814
02:13:49,569 --> 02:13:58,000
that's it so we can then do the same

2815
02:13:51,640 --> 02:13:59,410
thing for the first discriminator and

2816
02:13:58,000 --> 02:14:08,130
since basically all the works being done

2817
02:13:59,409 --> 02:14:10,960
now there's much less to do here okay so

2818
02:14:08,130 --> 02:14:12,539
there that is so I won't step all

2819
02:14:10,960 --> 02:14:16,770
through it but it's basically the same

2820
02:14:12,539 --> 02:14:16,769
same basic stuff that we've already seen

2821
02:14:17,060 --> 02:14:23,750
so optimized parameters basically is

2822
02:14:21,560 --> 02:14:26,510
calculating the losses and doing the

2823
02:14:23,750 --> 02:14:30,890
optimizer step from time to time save

2824
02:14:26,510 --> 02:14:33,980
and print out some results and then from

2825
02:14:30,890 --> 02:14:35,630
time to time update the learning rate so

2826
02:14:33,979 --> 02:14:40,250
they've got some learning rate annealing

2827
02:14:35,630 --> 02:14:50,000
built in here as well isn't very

2828
02:14:40,250 --> 02:14:52,760
exciting but okay so they've basically

2829
02:14:50,000 --> 02:14:54,949
got some kind of like fast AI they've

2830
02:14:52,760 --> 02:14:56,300
got this idea of schedulers which you

2831
02:14:54,949 --> 02:14:59,809
can then use to update your learning

2832
02:14:56,300 --> 02:15:01,610
rates so I think kind of like you know

2833
02:14:59,810 --> 02:15:06,050
for those of you are interested in

2834
02:15:01,609 --> 02:15:08,569
better understanding deep learning api's

2835
02:15:06,050 --> 02:15:11,329
or interested in contributing more to

2836
02:15:08,569 --> 02:15:13,880
fast AI or interested in like creating

2837
02:15:11,329 --> 02:15:15,199
you know your own version of some of

2838
02:15:13,880 --> 02:15:18,859
this stuff in some different back-end

2839
02:15:15,199 --> 02:15:21,229
it's cool to like look at a second kind

2840
02:15:18,859 --> 02:15:23,329
of API that covers some subset that some

2841
02:15:21,229 --> 02:15:24,739
of the similar things to get a sense for

2842
02:15:23,329 --> 02:15:26,329
how are they solving some of these

2843
02:15:24,739 --> 02:15:30,949
problems and what are the similarities

2844
02:15:26,329 --> 02:15:33,859
and what are the differences so we train

2845
02:15:30,949 --> 02:15:39,409
that for a little while and then we can

2846
02:15:33,859 --> 02:15:43,489
just grab a few examples and here we

2847
02:15:39,409 --> 02:15:46,039
have them so here are horses here they

2848
02:15:43,489 --> 02:15:49,399
are a zebras and here they are back as

2849
02:15:46,039 --> 02:15:50,689
horses again here's a hot zebra into a

2850
02:15:49,399 --> 02:15:52,489
horse back of a zebra it's kind of

2851
02:15:50,689 --> 02:15:55,809
thrown away its head for some reason but

2852
02:15:52,489 --> 02:15:58,039
not so much a quick get it back again

2853
02:15:55,810 --> 02:15:59,539
this is a really interesting one like

2854
02:15:58,039 --> 02:16:01,010
this is obviously not what zebras look

2855
02:15:59,539 --> 02:16:03,619
like but if it's going to be a zebra

2856
02:16:01,010 --> 02:16:05,000
version of that horse it's also

2857
02:16:03,619 --> 02:16:06,590
interesting to see it's foliar

2858
02:16:05,000 --> 02:16:08,569
situations I guess it doesn't very often

2859
02:16:06,590 --> 02:16:13,310
see basically just an eyeball have no

2860
02:16:08,569 --> 02:16:15,529
idea how to do that one so some of them

2861
02:16:13,310 --> 02:16:17,990
don't work very well this one's done a

2862
02:16:15,529 --> 02:16:19,340
pretty good job this one's interesting

2863
02:16:17,989 --> 02:16:20,899
it's done a good job with that one in

2864
02:16:19,340 --> 02:16:24,890
that one but for some reason the one in

2865
02:16:20,899 --> 02:16:26,299
the middle didn't get go yeah this one's

2866
02:16:24,890 --> 02:16:29,969
a really weird shape but it's done a

2867
02:16:26,300 --> 02:16:35,139
reasonable job of it this one looks good

2868
02:16:29,969 --> 02:16:39,489
this one's pretty sloppy again for just

2869
02:16:35,138 --> 02:16:41,138
a head up there so you know I didn't it

2870
02:16:39,489 --> 02:16:43,629
took me quite a bit for me like 24 hours

2871
02:16:41,138 --> 02:16:46,179
to train it even that far so it's kind

2872
02:16:43,629 --> 02:16:48,489
of slow and I know Hellena is constantly

2873
02:16:46,179 --> 02:16:49,450
complaining on Twitter about how long

2874
02:16:48,489 --> 02:16:50,530
these things take

2875
02:16:49,450 --> 02:16:55,300
I don't know how she's so productive

2876
02:16:50,530 --> 02:16:58,500
with them so yeah I will mention one

2877
02:16:55,299 --> 02:17:02,409
more thing that just came out yesterday

2878
02:16:58,500 --> 02:17:05,439
which is there's now a multi-modal image

2879
02:17:02,409 --> 02:17:07,959
to image translation of unpaired and so

2880
02:17:05,439 --> 02:17:13,599
you can basically now create different

2881
02:17:07,959 --> 02:17:16,750
cats for instance from this dock so this

2882
02:17:13,599 --> 02:17:18,790
is basically not just creating one

2883
02:17:16,750 --> 02:17:20,319
example of the output that you want but

2884
02:17:18,790 --> 02:17:24,700
creating multiple ones so here's a house

2885
02:17:20,319 --> 02:17:27,429
cat - a big cat and here's a big cap -

2886
02:17:24,700 --> 02:17:28,510
house cat this is the paper yeah so this

2887
02:17:27,429 --> 02:17:31,659
came out like yesterday or the day

2888
02:17:28,510 --> 02:17:36,099
before I think I think it's pretty

2889
02:17:31,659 --> 02:17:37,750
amazing captain - dog so you can kind of

2890
02:17:36,099 --> 02:17:39,639
see how this technology is developing

2891
02:17:37,750 --> 02:17:41,769
and I think you know if you oh there's

2892
02:17:39,638 --> 02:17:44,978
so many opportunities to you know maybe

2893
02:17:41,769 --> 02:17:47,920
do this with music or speech or writing

2894
02:17:44,978 --> 02:17:50,949
or to create kind of tools for artists

2895
02:17:47,920 --> 02:17:52,840
or alright thanks everybody and see you

2896
02:17:50,950 --> 02:17:55,999
next week

2897
02:17:52,840 --> 02:17:55,998
[Applause]

