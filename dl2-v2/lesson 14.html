<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Lesson 14: Super resolution; Image segmentation with Unet</title>
    <meta charset="UTF-8">
  </head>
  <body>
  <p style="text-align: right"><a href="http://course.fast.ai/">fast.ai: Deep Learning Part 2 (v2) (2018)</a></p>
  <h1><a href="http://course.fast.ai/lessons/lesson14.html">Lesson 14: Super resolution; Image segmentation with Unet</a></h1>
  <h2>Outline</h2>
<p>In this final lesson, we do a deep dive into super resolution, an amazing technique that allows us to restore high resolution detail in our images, based on a convolutional neural network. In the process, we’ll look at a few modern techniques for faster and more reliable training of generative convnets.</p>

<p>We close with a look at image segmentation, in particular using the Unet architecture, a state of the art technique that has won many Kaggle competitions and is widely used in industry. Image segmentation models allow us to precisely classify every part of an image, right down to pixel level.</p>

<p>We hope you enjoyed your deep learning journey with us! Now that you’ve finished, be sure to drop by the forums to tell us how you’re using deep learning in your life or work, or what projects you’re considering working on now.</p>

  <h2>Video Timelines and Transcript</h2>

<h3>1. <a href="https://youtu.be/1-NYPQw5THU?t=1m25s">00:01:25</a></h3>

<ul style="list-style-type: square;">

<li><b> Time-Series and Structured Data</b></li>

<li><b>&amp; “Patient Mortality Risk Predictions in Pediatric Intensive Care, using RNN’s” (research)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Welcome to the last lesson, lesson: 14: we're going to be looking at image 
segmentation today, amongst other things, but before we do a bit of 
show-and-tell from last week, Elena Ali did something really interesting, 
which was she tried finding out what would happen if you did cycle Gain on 
just three or 400 images - and I really like these projects where people 
just go to Google Image Search. You know using the the API you're one of 
the libraries out there. Some of our students have created some very good 
libraries for interacting with Google images. Api download a bunch of stuff 
that they're interested in, in this case some photos and some stained glass 
windows and yeah with three or four hundred photos of that she trained a 
model. She trained actually a few different models. This is what I 
particularly liked and, as you can see, with quite a small number of 
images, you know she gets some very nice stained-glass effects. So I 
thought that was a interesting example of yeah, using pretty small amounts 
of data that was readily available, that she was able to download pretty 
quickly and there's more information about that on the forum if you're 
interested yeah. It's it's interesting to wonder about what kinds of things 
people will come up with with this kind of generative model, it's clearly a 
great artistic medium, it's clearly a great medium for forgeries and 
bakeries. I wonder what other kinds of things before will realize they can 
do with these kind of generative models.</p>

<p>I think audio is going to be the 
next big area and also very like interactive type stuff that Nvidia, I just 
released a paper showing a interactive kind of photo repair tour where you 
just like brush over an object, and it replaces it with. You know a deep 
learning generated replacement very nicely. Those kinds of interactive 
tools, I think, would be very interesting too. So before we talk about 
segmentation, we've got some stuff to finish up from last time, which is 
that we look at doing style transfer they actually directly. Optimizing 
pixels - and you know like with most of the things in part two: it's not so 
much that I'm wanting you to understand style transfer per se, but the kind 
of idea of optimizing your input directly and using activations as part of 
a loss function is really The key kind of takeaway here so it's 
interesting, then, to kind of see the photos effectively the follow-up 
paper you know not from the same people, but the paper that kind of came 
next in the in the sequence of these kind of vision, generative models with 
this One from Justin, Johnson and folks at Stanford, and it it actually 
does the same thing style transfer, but it does it in a different way. 
Rather than optimizing, the pixels we're going to go back to something much 
more familiar and optimize some weights, and so specifically we're going to 
train a model which learns to take a photo and translate it into a photo on 
this in the style of a particular artwork.</p>

<p>So each ComNet will learn to 
produce one kind of style. Now it turns out that getting to that point 
there's an intermediate point, which is, I actually think, kind of more 
more useful and Texas halfway there, which is something called super 
resolution, so we're actually going to start with super resolution because 
then we'll build on top of Super resolution to finish off the style, 
transfer, ComNet, based I'll transfer and so super resolution is where we 
take a low res image, we're going to take 72 by 72 and upscale up to a 
larger image 288 by 288. In our case, trying to create, you know a higher 
res image that looks as real as possible, and so this is a pretty 
challenging thing to do, because at 72 by 72, there's not that much 
information about a lot of the details and the cool thing is That we're 
going to do it in a way as we tend to do with vision models which is not 
tied to the input size. So you could totally then take this model that and 
apply it to a 288 by 288 image and get something that's four times bigger 
on each side so 16 times bigger than that, and and often it even kind of 
works better at that level, because you're really Introducing a lot of a 
lot of detail into the finer details and you could really print out a high 
resolution, print of something which earlier on was pretty big so later.</p>

<p>So 
this is the notebook or enhance, and it is a lot like that kind of CSI 
style enhancement where we're going to take something that appears like the 
information is just not there and we kind of invent it, but the confidence 
going to learn to invent it. In a way, that's consistent with the 
information that is there, so, hopefully you know it's kind of inventing 
the right information. One of the really nice things about this kind of 
problem is that we can create our own data set as big as we like without 
any labeling requirements, because we can easily create a low res image 
from a high-res image just by down sampling our images. So something I 
would love some of you to try doing the week would be to through other 
types of imaged image, translation where you can invent kind of late labels 
invent your dependent variable, for example, D skewing. You know so either 
recognize things that have been rotated by 90 degrees or better still, that 
have been rotated by five degrees and straighten them. Colorization, so 
turn make a bunch of images into black-and-white and learn to put the color 
back again. Noise reduction, you know, maybe do a really low quality, JPEG, 
save and learn to put it back to how it should have been, and so forth or 
yeah, maybe taking something. That's like in a 16 color palette and put it 
back to a higher color palette. I think these things are all interesting 
because they can like be used to take.</p>

<p>You know pictures that you may have 
taken back on crappy old digital cameras before there are high resolution, 
or you may have scanned in some old photos that kind of faded or whatever 
you know. I think it's really useful thing to go to do and also it's, but 
it's a good project, because it's like really similar to what we're doing 
here but different enough that you'll come come across some interesting 
challenges on the way, I'm sure so I'm going to use Some imagenet again 
again, you don't need to use all of image net at all. I just happened to 
have it lying around? You can download the one percent sample of image net 
from faster faster. They are. You can use any set of pictures. 
</p>

<h3>2. <a href="https://youtu.be/1-NYPQw5THU?t=7m30s">00:07:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Time-Series with Rossmann Store Sales (Kaggle)</b></li>

<li><b>&amp; 3rd place solution with “a very uncool NN ;-)”.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>You have lying around honestly and in this case, as I say, we don't really 
have labels per se, so I just got a give everything a label of zero just so 
we can use it with our existing infrastructure more easily. Now because I'm 
in this case pointing at a folder that contains all of image net, I 
certainly don't want to wait for all of image net to finish to run an 
epoch. So here I'm just most of the time I would set keep percent to like 
one or two percent, and then I just generate a bunch of random numbers and 
then I just grab those keeps those which are less than 0.02 and so that 
lets be quickly subsample. My rows, all right, so we're going to use vgg 16 
and vgg 16 is something that we haven't really looked at in this class. But 
it's a very simple, very simple model where we take and normal as you apply 
three-channel input and we basically run it through a number of 3x3 
convolutions and then from time to time we put it through a 2x2 Maps pool 
and then we do a few More 3x3 convolutions max Paul, so on so forth, and 
then this is kind of our backbone. I guess and then we we don't do an 
average pooling layer in a deputy of average pooling layer. After a few of 
these, we end up with this. You know 7x7 grid as usual. I think it's about 
7 by 7 by 512, something like that and so, rather than average polling.</p>

<p>We 
do something different, which is reflect the whole thing, so that spits out 
a very long vector of activations of size, 7 times 7 times 512 memory says 
correctly, and then that gets fed into two fully connected layers, each one 
of which has 409 6 activations and Then one more fully connected layer, 
which has, however, many classes. So if you think about it, the weight 
matrix here is huge. It's you know, 7 by 7 by 512 by 409 6, and it's 
because of that weight matrix, really that V gge went out of favor. Pretty 
quickly because it takes a lot of memory and takes a lot of computation and 
it's really slow and there's a lot of redundant stuff going on here, 
because really those 512 activations are not that specific to which of 
those 7x7 grid cells they're in right. But when you have this entire weight 
matrix here of every possible combination, it treats all of them, uniquely 
right and so that'll can also lead to generalization core ones, because 
there's just a lot of weights and so forth. My view is that there's you 
know that the approach that's used in every modern Network, which is here 
we do an adaptive average pulling care, ask that we be known as a global 
average calling for in fastai. We didn't really do a concat adaptive, 
concat pooling, which spits it straight down to a 512 long activation. I 
think that's throwing away too much geometry.</p>

<p>So to me, probably the 
correct answer is somewhere in between and will involve some kind of um 
factored convolution or some kind of tensor decomposition, which yeah, 
maybe some of us can think about in the coming months. So for now, anyway, 
we've gone from one extreme, which is the adaptive average pulling to the 
other extreme, which is this huge, flattened, collocation player. So a 
couple of things which are interesting about vgg that make it still useful 
today. The first one is that there's there's more interesting layers going 
on here with with most modern networks, including the resident family. We, 
the very first layer generally, is a seven by seven pawns or something 
similar, which means we and that's tried to right, which means we throw 
away half the grid size straight away, and so this little opportunity to 
use the the fine detail, because we never do Any computation with it - and 
so that's a bit of a problem for things like segmentation or super 
resolution models, because the fine detail matters right. We actually want 
to restore it, and then the second problem is that the adaptive average 
polling layer entirely throws away the geometry in the last few sections, 
which means that the rest of the moral doesn't really have as much 
interesting kind of learning. That geometry - is it otherwise might, and 
so, therefore, for things which dependent on position any kind of 
localization based approach, tor, anything that requires generative model 
is going to be less effective.</p>

<p>So one of the things I'm hoping you're 
hearing is I describe this. Is that probably none of the existing 
architectures are actually ideal? We can invent a new one. Then, actually, 
I just tried inventing a new one over the week, which was to take the the 
vgg head and attach it to a resonate that one and interestingly I found. I 
actually got a slightly better classifier than a normal ResNet, but it also 
was something with a little bit more useful information. You know it took, 
I don't know five or ten percent longer to Train, but nothing worth 
worrying about yeah. I think you know. Maybe we couldn't in Resident 
replace this, as we've talked about briefly before this very early 
convolution, with something more like an inceptions dam which has a bit 
more computation. I think there's definitely room for some nice little 
tweaks to these architectures so that we can build some models which are 
maybe more versatile. You know at the moment people tend to build 
architectures that just do one thing: they don't really think you know. 
What am i throwing away in terms of opportunity? Because that's that's how 
publishing works you know you published, like they've, got the state of the 
art and this one thing rather than you have created something that's good 
at lots of things so um.</p>

<p>So, for these reasons we're going to use vgg 
today, even though it's it's ancient and it's missing lots of great stuff. 
One thing we are going to do, though, is use a slightly more modern 
version, which is a version of vgg where batch norm has been added. After 
all, the convolutions, and so in fastai. Actually, when you ask for a vgg 
Network, you always get the best norm, one because that's basically always 
what you want. So this is actually very Gigi with batch mode and there's a 
16 in the 19. The 19 is way bigger and heavier and doesn't really is really 
any better. So we no one, really uses it. Okay, so we're going to go from 
72 by 72. Lr is low resolution input, size, low resolution, we're going to 
initially scale it up by x, 2. We're the batch size of 60 or to get a two 
times 72, so one by 44 by 144 output. So that's gon na be our stage stage. 
One we'll create our own data set for this, and the data set it's very 
worthwhile. Looking inside the faster I dot data set module and seeing 
what's there because, just about anything you'd want, we probably have 
something: that's almost what you want. So in this case I want a data set 
where my X's are images, and my y's also images. So there's already a files 
data set we can inherit from where the x's are images, and then i just 
inherit from that and i just copied and pasted the get x and turn that into 
get y. So i just opens an image.</p>

<p>So now I've got something where that X is 
an image and the y is an image and in both cases, what we're passing in is 
an array of flower names. I'm going to do some data augmentation, obviously 
with all of image net. We don't really need it, but this is mainly here, 
for you know anybody who's using smaller data sets to make the most of it. 
Random dihedral is referring to every possible 90-degree rotation and plus 
optional left/right flipping. So, though, the dihedral group of eight 
symmetries, normally, we don't use this transformation for image net 
pictures because, like you, don't normally flip blobs upside down, but in 
this case we're not trying to classify whether it's a dog or a cat. We're 
just trying to keep the general structure of it. So, actually, you know 
every possible flip is a reasonably sensible thing to do for this problem, 
so a creative validation set in the usual way, and you can see I'm kind of 
like using a few more slightly lower level functions. Generally speaking, I 
just copy and paste them out of the faster source code to you know find the 
bits I want so here's the bit, which takes an array of validation, set 
indexes and one or more arrays of variables and simply splits. So, in this 
case, the into a training and a validation set, and this into a training 
novella bit sorry, it yeah the training, validation set.</p>

<p>You give us our 
X's and our whites now, in this case the Train are the X and the y 
</p>

<h3>3. <a href="https://youtu.be/1-NYPQw5THU?t=18m">00:18:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Implementing the Rossman solution with Keras + TensorFlow + Pandas + Sklearn</b></li>

<li><b>Building Tables &amp; Exploratory Data Analysis (EDA)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>At the same import image and our output image of the same we're going to 
use transformations to make one of them lower resolution, so that's why 
these are the same. The same thing, okay, so the next thing that we need to 
do is to create our transformations as per usual and we're going to use 
this transform Y parameter like we did for bounding boxes, but rather than 
use, transform type dot, coordinate we're going to use, transform Type 
pixel, and so that tells our transformations framework, that your Y values 
are images with normal pixels in them, and so anything you do the X. You 
also need to do so. The way do the same thing: okay and you need to make 
sure any data augmentation transforms you use, have the same parameter as 
well. Okay, so you can see the possible transform types. You basically 
you've got classification which we're about to use the segmentation in the 
second half of today coordinates no transformation at all or pixel. 
Alright, so once we've got a dataset class and some X & amp Y training and 
validation sets, there's a handy. Little method called get datasets, which 
basically runs that constructor over all the different things that you have 
to return all the datasets. You need in exactly the right format to pass 
pass to a model data constructor as a constructor, in this case the image 
data constructor.</p>

<p>So we're kind of like going back under the covers of 
fastai, a little bit and building it up from scratch, and you know in the 
next few weeks this will all be wrapped up and refactored into something 
that you can do in a single step. In fastai, but the point of this is to 
learn, you know a bit about going under the covers, so something we've 
briefly seen before is that when we take images in, we transform them, not 
just the data augmentation that we also move the channels dimension up To 
the start, we subtract the mean divided by the standard deviation whatever. 
So, if we want to be able to display those pictures that have come out of 
our data sets or data loaders, we need to denormalize them, and so the 
model data objects. Data set has ad norm function. That knows how to do 
that. So I'm just going to give that a short name for convenience. So now 
I'm going to create a function that can show an image from a data set and 
if you pass in something saying this is a normalized image, then won'ting 
on it. Okay, so we can go ahead and have a look at that you'll see here. 
We've passed in size low-res as our size for the transforms and size 
high-res, as this is something new the size Y parameter. Okay, so the two 
bits are going to get different sizes, and so here you can see the two 
different resolutions of our X and our Y for a whole bunch of fish.</p>

<p>Okay, 
as you know, as per usual plot subplots, to create our two plots and then 
we can just use the different axes that came back to the stuff next to each 
other. So we can then have a look at a few different versions of the data 
transformation and there you can see them being clicked in all different 
directions. Okay, so let's create our model, so we're going to have an 
image coming in small image coming in and we want to have a big image 
coming out, and so we need to do some computation between those two to 
calculate what the big image would look like And so essentially, there's 
kind of two ways of doing that: computation. We could first of all do some 
up sampling and then do a few straight one kind of layers to do lots of 
computation or we could first do lots of straight one layers to do other 
computation and then at the end, do some up sampling. We've got to pick the 
second approach because we want to do lots of computation on something 
smaller, because it's much faster to do it. That way, and also like all 
that computation we get to kind of leverage during the up sampling process. 
So that's sampling. We know a couple of possible ways to do that we can use 
transposed or fractionally straited convolutions or we can use nearest 
neighbor up sampling, followed by a one by one conf and then in in the kind 
of do lots of computation section.</p>

<p>We could just have a whole bunch of 3x3 
coms right, but in this case particular it seems likely that ResNet blocks 
are going to be better because really the output and the input are very, 
very similar right. So we really want a kind of a flow through path that 
allows as little fussing around as possible, except kind of a minimal 
amount necessary to do our super resolution. And so, if we use ResNet 
blocks, then they have an identity path already right so, like you can 
imagine the most simple version where it does like a you know: bilinear 
sampling, kind of approach or something it could basically just go through 
identity box all the way Through and then in the up sampling blocks just 
learn to take the averages of the inputs and get something that's like not 
too terrible. So that's what we're going to do we're going to create 
something with five ResNet blocks and then for each 2x scale-up. We have to 
do we'll, have 1/2 sampling look so they're all going to consist of 
obviously, as per usual convolution layers, possibly with activation 
functions after many of them. So I kind of like to put my standard 
convolution block into a function, so I can refactor it more easily as per 
usual, I just won't worry about passing in padding and just calculate it 
directly as kernel size over two.</p>

<p>So one interesting thing about a little 
comic block here is that there's, no, that's not, which is pretty unusual 
for ResNet type models and the reason there's no batch norm it because I'm 
stealing ideas from this fantastic recent paper, which actually won a 
recent competition in super Resolution performance and to see how good this 
paper is: here's kind of a previous state of the art there's SR ResNet 
right and what they've done here is they've zoomed way in to a an up, 
sampled kind of natural or fence. This is the original and you can see in 
the kind of previous best approach, there's a whole lot of distortion and 
blurring going on right or else in their approach. It's it's nearly 
perfect. Alright. So like it was a really big step up this paper. They call 
their model, EDS are enhanced deep residual networks and they did two 
things differently to the kind of previous standard approaches. One was to 
take the ResNet blocks. This is a regular resident block and throw away the 
better. Not so why would they throw away the veteran or well the reason 
they would throw away? The batch norm is because batch norm changes stuff, 
and we want a nice straight through path that doesn't change stuff, okay, 
so the idea, basically here is like hey. If you don't want to fit all with 
the input more than you have to, then don't force it to have to calculate 
things like batching on parameters so throw away the and the second trick 
we'll see shortly.</p>

<p>Alright. So here's a con with no batch norm and so then 
we're going to create a residual block containing as per usual, two 
convolutions and, as you see in their approach they'd, even they don't even 
have a rail you after their second conf. Okay. So that's why I've only got 
activation on the first one. So a couple of interesting things here: one is 
that </p>

<h3>4. <a href="https://youtu.be/1-NYPQw5THU?t=27m15s">00:27:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Digress: categorical variable encodings and “Vtreat for R”</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>This idea of like having some kind of main ResNet path like conv, relia 
cons and then turning that into a rail you block by adding it back to the 
identity. It's something we do so often I kind of factored it out into a 
tiny little module called res sequential, which simply takes a bunch of 
layers that you want to put into your residual path. Turns that into a 
sequential model. Runs it and then adds it back to the input right. So, 
with this tat all module, we can now turn anything like conf activation 
cons into a resonate lock just by wrapping it in res sequential okay. But 
that's not quite all I'm doing because, like normally a res block, just has 
that and it's forward, but I've also got that what's risco Briscoe is the 
number zero point one? Why is it there? I'm not sure anybody quite notice, 
but the short answer is that the guy who invented batch norm also somewhat 
more recently, did a paper in which he showed for, I think, the first time 
the ability to Train imagenet in under an hour and the way he did It was 
fire up, lots and lots of machines and have them work in parallel to create 
really large batch sizes. Now, generally, when you increase the batch size 
by order n, you also increase the learning rate by order n to go with it. 
So generally, a very large batch size training means very high learning 
rate training as well, and he found that with these very large batch sizes 
of like 8,000 plus or even up to 32,000, that at the start of training, his 
activations would basically go straight to infinity And a lot of other 
people have found that we actually found that when we were competing in 
dawn bench both on the sofa and the imagenet competitions, that you know, 
we really struggled to make the most of even the eight GPUs that we were 
trying to take Advantage of because of these kind of challenges with these 
larger batch sizes and taking advantage of them so something that a 
Christian found.</p>

<p>This research was that if he in the ResNet blocks, if he 
multiplied them by some number smaller than one something like point one or 
point two, it really helped stabilize training start and that's kind of 
weird, because let mathematically it's kind of identical right, because, 
obviously, whatever I'm multiplying it by here. You know I could just scale 
the weights by the opposite amount here and have the same number. Okay. So, 
but it's kind of like we're not dealing with </p>

<h3>5. <a href="https://youtu.be/1-NYPQw5THU?t=30m15s">00:30:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Back to Rossmann solution</b></li>

<li><b>&amp; “Python for Data Analysis” (book)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Abstract math, you know we're dealing with, like you know, real 
optimization problems and different initializations and learning rates and 
whatever else, and so the problem of kind of whites disappearing off into 
infinity. I guess generally is really about that they're kind of the 
discrete and finite nature of computers in in practice, partly and so often 
yeah. These kind of little tricks can can make the difference. Alright. So 
in this case, we're just kind of toning things down based at least based on 
our initial initialization, and so there probably other ways to do this. 
For example, one approach from some folks at Nvidia called Lars le RS, 
which I briefly mentioned last week, is an approach which uses 
discriminative learning rates calculated in real time. Basically, looking 
at the ratio between the gradients and the activations to scale all 
learning rates by layer, and so they found that they didn't need this trick 
to scale it scale up the batch sizes, a lot, maybe a different 
initialization, which would be all that's necessary. The reason I mentioned 
this is not so much because I think a lot of you are likely to want to 
train on massive clusters of computers, but rather that I think a lot of 
you want to train models quickly, and that means using high learning rates 
and Ideally getting super convergence and I think these kinds of tricks are 
the tricks that we'll need to be able to get super convergence across more 
different, architectures and so forth, and you know other than Leslie 
Smith. No one else is really working on super convergence other than some 
fastai students nowadays.</p>

<p>So these kind of things about how do we train at 
very, very high learning rates? We're going to be have to be the ones who 
figure it out? Because, as far as I can tell, nobody else cares yet so so I 
think you know looking at the literature around, you know: training 
imagenet in one hour or more recently, there's now a train image net in 15 
minutes. These papers actually tell, I think, have some of the tricks to 
allow us to train things at home learning rates, and so here's one of them, 
and so, interestingly other than the train image net1 our paper, the only 
other place I've seen this mentioned was in this Pds, our paper and it's 
really cool because, like I know, people who win competitions, I just find 
them to be very pragmatic and well-read. You know lucky they actually have 
to get things to work, and so this paper describes an approach which 
actually worked better than anybody else's approach, and they did these 
pathetic things like throw away batch norm and use this little scaling 
factor which almost nobody seems to know About and stuff like that, ok, so 
that's where the point one comes from. So basically, our super-resolution 
ResNet is down and do a convolution to go from our three channels to 64 
channels, just to rich nut the space a little bit then also we've got 
actually a cannot 5h lots of these res blocks and we're just going to keep 
remember Every one of these res blocks is strike one, so the grid size 
doesn't change.</p>

<p>The number of filters doesn't change. It's just 64. All the 
way through well do one more convolution and then we'll do our app sampling 
by however much scale we asked for and then something I've added, which is 
a little idea, is just one batch norm here, because it kind of felt like it 
might be helpful. Just to scale the last layer and then finally a comb to 
go back to the three channels we want, so you can see that's basically 
here's lots and lots of computation and then a little bit of our sampling, 
just like we kind of described so the only Other piece here, then, is, and 
I also just dimension - you know, as you can see, as I'm tending to do now. 
This whole thing is done by creating just a list with layers and then at 
the end, turning that into a sequential model, and so my forward function 
is as simple as can be, so here's our app sampling and up sampling is a bit 
interesting because it is Not doing either of these two things, so let's 
talk a bit about up sampling here is a picture from the paper from not from 
the competition winning paper, but from this original paper and so they're 
saying hey, our approach is so much better, but look at their Approach, 
it's got goddamn artifacts in it. Alright, these just pop up everywhere, 
and so one of the reasons for this is that they use transposed convolutions 
and we all know don't use transposed convolutions. Okay, so here are 
transposed convolutions.</p>

<p>This is from this fantastic convolutional 
arithmetic paper that was shown also in the Theano Docs. If we're going 
from the blue is the original image, so 3x3 image up to a 5x5 image right 
or a 6x6. If we added a layer of padding, then all a transpose convolution 
does, is it uses a regular 3x3 cons, but it sticks white. You know zero 
pixels between every pair of pixels, alright, so that makes the input image 
bigger and when we run this convolution life over. It therefore gives us a 
larger output. Okay, but I mean that's obviously stupid because when we get 
here, for example, of the nine pixels coming in eight of them, a zero so 
like we're just wasting a whole lot of computation and then on the other 
hand, if we're slightly off over here, then </p>

<h3>6. <a href="https://youtu.be/1-NYPQw5THU?t=36m30s">00:36:30</a></h3>

<ul style="list-style-type: square;">

<li><b> What Jeremy does everytime he sees a ‘date’ in a structured ML model</b></li>

<li><b>&amp; other tips</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Four of our nine and on zero, but yet we only have one filter like one 
kernel to use, so it can't like change depending on how many zeros are 
coming in, so it has to kind of be suitable for both and it's just not 
possible right. So we end up with these artifacts, so one approach we've 
learnt to make it a bit better, is to not put white things here, but 
instead to copy this pixels value to each of these three locations. 
Alright, so that's a just a nearest neighbor up sampling. That's certainly 
a bit better all right, but it's still pretty crappy, because now still 
when we get to these nine here, four of them are exactly the same number 
all right and when we move across one then now we've got. You know a 
different situation entirely right and so to on where we are so in 
particular, if we're here, you know, there's going to be a lot less 
repetition. So again we have this problem where there's like wasted 
computation and too much structure in the data and it's going to lead to 
RFS again. So up sampling is better than transposed convolutions, it's you 
know better to copy them, rather than replace them with zeros. But it's 
still not quite good enough, so instead we're gon na do the pixel shuffle. 
So the pixel shuffle is an operation in this sub pixel convolutional neural 
network and it's a little bit mind-bending, but it's kind of fascinating, 
and so we start with our input.</p>

<p>We go through some convolutions to create 
some feature Maps for a while, until eventually we get to layer - and I we 
go to this layer - I minus one which has n I minus one feature Maps we're 
going to do another 3x3 cons and our goal here is To go from a 7x7 grid 
cell we're going to go a 3x3 up scaling, so we're going to go up to a 21 by 
21 grid cell. So how do we what's another way? We could do that to make it 
simpler. Let's just pick one face, just one filter, so we'll just take the 
topmost filter and just do a convolution over that just to see what happens 
and what we're going to do is we're going to use a convolution where the 
kernel size is. Is the number of filters is nine times bigger than we 
strictly speaking need? So if we needed 64 filters, we're actually going to 
do 64 times nine filters. Why is that right, and so uh here are: is the 
scale effect uh? So three right, so a squared 3 squared is 9. So here are 
the nine filters to cover one of these input layers, one of these input 
slices, but what we can do is we started with seven by seven and we turn it 
into seven by seven by nine right. Well, the output that we want is equal 
to 7 times 3 by 7 times 3, so in other words, there's an equal number of 
pixels here activations here, as there are our activations here, so we can 
literally reshuffle these seven by seven by nine activations to create This 
7 by 3, by 7 by 3 Matt. And so what we're going to do is we're going to 
take one little kind of tube here on the top left hand of each grid and 
we're going to put the purple one up in the top left.</p>

<p>And then the blue one 
one to the right and then the light blue one one to the right of that and 
then the slightly darker blue one and the middle of the far left, the green 
one in the middle and so forth. So each of these nine cells in the top left 
are going to end up in this little 3x3 section of our grid and then we're 
going to take. You know 2, comma 1 and take all of those 9 and move them to 
these 3 by 3. Part of the grid and so on and so forth, right and so we're 
going to end up having every one of these 7 by 7 by 9 activations inside 
this 7 by 3 by 7 by 3 image. So the first thing to realize is yes, of 
course, this works under some definition of works, because we have a learn: 
herbal convolution here and it's going to get some gradients which is going 
to do the best job it can of filling in the correct activation such That 
this output is the thing we want alright, so the first step is to realize: 
there's nothing particularly magical here. You know we can. We can create 
any architecture we like. We can move things around any. How we want to - 
and you know our wipes in the convolution - will do their best to do all we 
asked the real question is: is it good idea you know, is this an easier 
thing for it to do? You know and a more flexible thing for it to do, then 
the transposed convolution or the up sampling, followed by one by one 
month, and the short answer is yes, it is, and the reason it's better in 
short, is that the convolution here is happening in the Low resolution, 
seven by seven space, which is quite efficient, where else, if we first of 
all up sampled and then did our cons, then our con would be happening in 
the 21 by 21 space, which is a lot of computation right and furthermore, as 
we discuss there's A lot of replication in redundancy in the nearest 
neighbor sample version, so they actually show in this paper they actually, 
in fact, I think they have a follow-up technical note, where they kind of 
provide some more mathematical details as to exactly what work is being 
done and Show that the work really is more efficient, this way.</p>

<p>Okay. So 
that's what we're going to do all right, so we're going to have for our 
</p>

<h3>7. <a href="https://youtu.be/1-NYPQw5THU?t=43m">00:43:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Dealing with duration of special events (holidays, promotions) in Time-Series</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>App sampling, we have two steps. The first will be a three by three cons 
with R squared times more channels than we originally wanted, and then a 
pixel shuffle operation which moves everything in each grid cell into the 
little by our grids that are located throughout here. Okay, so here it is 
it's one line of code right and so here's the cons from number of in to 
number of filters out times four, because we're doing a scale to that 
sample all right. So two squared is four. So that's our convolution, and 
then here is our pixel. Shuffle it's built into high touch pixel shuffle is 
the thing that moves each thing into its right spot, so that will increase 
will up sample by a scale factor of two, and so we need to do that. Log 
base two scale time, so, if scale is for two times to go two times two, you 
go okay. So that's what this up sample here does great guess what that does 
not get rid of the checkerboard patterns. We still have checkerboard 
patterns, so I'm sure in great fury and frustration this same team from 
Twitter. I think this is back when they used to be at a startup called 
magic Pony that Twitter thought came back again with another paper saying: 
okay, this time we've got rid of the checkerboard okay. So so why do we 
still have? As you can see here, you still have a checkerboard right and so 
the reason we still have a checkerboard even after doing this is that when 
we randomly initialize this convolutional kernel at the start, it means 
that each of these nine pixels in this little 3x3 grid Over here are going 
to be totally randomly different, but then the next set of three pixels 
will be randomly different to each other, but will be very similar to their 
corresponding pixel in the previous 3x3 section.</p>

<p>So we're going to have 
repeating 3x3 things all the way across and so then, as we try to learn 
something better. It's starting from this like repeating 3x3 starting 
point, which is not what we want right. What we actually would want is for 
these three by three pixels to be the same. To start with, so to make these 
three by three pixels the same, we would need to make these nine channels 
the same here right for each filter and so the solution and his paper is 
very simple. It's that when we initialize this convolution that start when 
we randomly initialize it, we don't totally randomly initialize it. We 
randomly initialize one of the the r-squared sets of channels, and then we 
copy that to the other R squared, so they're all the same, and that way 
initially each of these three by threes will be the same, and so that is 
called I CNN, okay and That's what we're going to use in a moment so before 
we do. Let's take a quick look, so we've got this super resolution 
resinate, which just does lots of computation. You know with lots of ResNet 
blocks and then it does some up sampling and gets our final. Three channels 
out and then to make life faster, we're going to run this in parallel. One 
reason we want to run it in parallel is because Jurado told us that he has 
six GPUs, and this is what his computer looks like right now, and so I'm 
sure anybody who has more than one GPU has had this experience before.</p>

<p>So 
how do we get? How do we get these men working in together? All you need to 
do is to take your pytorch module and wrap it with N n data parallel, okay 
and once you've done that it copies it to each of your GPUs and will 
automatically run it in parallel. It scales pretty well done to two GPUs. 
Okay to three GPUs, better than nothing to four GPUs and beyond that 
performance does two go backwards: the by default. It will copy it to all 
of your GPUs. You can add an array of GPUs. Otherwise, if you want to avoid 
getting in trouble, for example, I have to share our box with you net and 
if I didn't put this here, then she would be yelling at me right now. Well, 
maybe you know, or according my plus, so this is how you avoid getting into 
trouble with you net. So one thing to be aware of here is that once you do 
this, it actually modifies your module. So if you now print out your 
module, let's say previously, it was just an endless sequential now, you'll 
find it's an N in dots as Crenshaw embedded inside a module called module 
right, and so, in other words, if you save something which you had n end 
updated Paralleled and then tried and load it back into something that you 
hadn't and end up beta paralleled, it'll say it doesn't match up, because 
one of them is embedded inside this module attribute and the other one 
isn't. It may also depend even on which GPU IDs, you have had a coffee too, 
so two possible solutions.</p>

<p>One is don't save the module M, but instead save 
the module, attribute m dot module because that's actually the the non data 
parallel bit or always put it on the same GPU IDs and then use data 
parallel and load and save that every time that's what I Was using this 
will be an easy thing for me to fix automatically in fastai and I'll. Do it 
pretty soon so it'll look for that module, attribution and deal with it 
automatically, but for now we have to do it manually. It's probably useful, 
to know. What's going on behind the scenes anyway? Alright, so we've got 
our module. You know I find it overrun like 50 or 60 percent faster on a 
1080 TI. If you're running on voltar, it actually paralyzes a bit better. 
There's a there. Much faster ways to parallel parallel lives, but this is 
like a super super easy way all right, so we created our learner in the 
usual way. We could use MSA loss here. So that's just going to compare the 
pixels of the output to the pixels. You know that we expected and we can 
run our learning rate finder and we can train it for awhile and here's our 
input and here's our output, and you can see that what we've managed to do 
is to train a very advanced residual convolutional net work. That's learnt 
to blur things. Why is that? Well, because it's what we asked for, we said 
to minimize MSE loss right, an MSA lost between pixels, really, the best 
way to that is just average. The pixels I eat a blur.</p>

<p>So that's why pixel 
lusts, no good! So we want to use our perceptual loss, so let's try 
perceptual us right so with perceptual loss, we're basically going to take 
our vgg network and just like we did last week we're going to find the 
block index just before we get a max ball. Okay. So here are the ends of 
each kind of block of the same grid size and if we just print them out as 
we'd expect, every one of those is a value module, and so in this case, 
these last two blocks are less interesting to us. They're kind of the grid 
size there is small enough. You know kind of coarse enough that it's not as 
useful for super resolution, so we're just going to use the first three and 
so just to save unnecessary computation. We're just going to use those 
first 23 layers or vgg we'll throw a way to look at the rest, we'll stick 
it on the GPU. We're not going to be training! This speech EG model at all, 
we're just using it to compare activations so we'll stick it in 
</p>

<h3>8. <a href="https://youtu.be/1-NYPQw5THU?t=52m">00:52:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Using ‘inplace=True’ in .drop(), &amp; a look at our final ‘feature engineering’ results</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Eval mode and we will set it to not trainable okay, just like last week, we 
will use a save features class to through a forward hook, which saves the 
output activations at each of those layers, and so now we've got everything 
we need to create our perceptual Loss so as I call it here, feature loss 
plus right and so we're going to pass in a list of layer IDs. You know the 
layers where we want the content loss to be calculated and array of weights 
a list of weights for each of those layers. So we can just go through each 
of those layer IDs and create an object which is going to store, which is 
you know, I've got the book function forward, hook, function to store the 
activations and so in our forward. Then we can just go ahead and call the 
forward pass of our model with the target. So the target is the hi, whereas 
image we're trying to create okay, and so the reason we do. That is because 
that's going to then call that book function and store in soft save 
features the activations. We want right now we're going to need to do that 
for our confident output as well right. So we need to clone these, because 
otherwise the confident output is going to go ahead and just plop up what I 
already had. Okay. So now we can do the same thing for the confident 
output, which is the input to the loss function, and so now we've got those 
two things we can zip them all together, along with the weights, so we've 
</p>

<h3>9. <a href="https://youtu.be/1-NYPQw5THU?t=53m40s">00:53:40</a></h3>

<ul style="list-style-type: square;">

<li><b> Starting to feed our NN</b></li>

<li><b>&amp; using ‘pickle.dump()’ for storage encodings</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Got inputs targets and weights, and then we can do the l1 loss between the 
inputs and the targets and multiply by the layer weights. The only other 
thing I do is, I also grab the pixel loss right, but I weight it down quite 
a bit. Okay and most people don't do this - I haven't seen papers that do 
this, but in my opinion it's maybe a little bit better because you've got 
you know the perceptual content lost activation stuff, but you know the 
really finest level. It also cares about the individual pixels. Okay, so 
that's our last function. We create our super resolution, ResNet telling it 
how much to scale up by and then we're going to do our I see in our 
initialization of that pixel shuffle convolution right. So there's really 
like it's. This is very, very boring code. I actually stole it from from 
somebody else. Like literally all it does is just say: okay, you've got 
some weight, tensor X, that you want to initialize, so we're going to treat 
it as if it had shape divided by so number of features divided by scale. 
Squared features in practice so, like you know this might be 2 squared 
before, because we actually want to copy you know we want to just keep set 
with them and then copy them four times. So we divide it by four and we 
create something of that size and we initialize that with by default 
timing, normal initialization, and then we just make scale squared copies 
of it.</p>

<p>Okay and the rest of its just kind of moving axes around a little 
bit. All right, so that's kind of return, a new weight matrix where each 
each initialized sub kernel is repeated R, squared or scale squared times, 
so that details don't matter very much. All that matters here is that I 
just looked through to find what was the actual layer. The cone flower, 
just before the pixel shuffle and stored it away, and then I called I see 
an R on its weight matrix to get my new weight matrix and then I copied 
that new weight matrix back into that layer. Ok, so, as you can see, I went 
to quite a lot of trouble in this exercise to really try to implement all 
the best practices right and I kind of tend to do things a bit one extreme 
or the other. I show you like a really happy version that only slightly 
works or I go to the enth degree to make it work really well right, and so 
this is a version where I'm claiming that this is pretty much a state of 
the art implementation. It's a competition. Winning or at least my 
reimplementation of a competition winning approach and the reason I'm doing 
that is because I think, like this is one of those rare papers where they 
actually get a lot of the details right and I kind of want you to get a 
feel Of what does it feel like to get all the details right and you know 
remember getting the details right - is the difference between this hideous 
blurry mess. You know, and this really pretty exquisite result.</p>

<p>Okay, so so 
we're gon na have a to do potato parallel on that again, we're going to set 
our criterion to be feature loss using our vgg model grab the first few 
blocks, and these are assets of layer weights that I found worked pretty 
well. Do a learning rate finder fit it for a while, and I fiddled around 
for a little while trying to kind of get some of these details right, but 
here's the my favorite part if the paper is what happens next now that 
we've done it for scale equals To progressive resizing right, so 
progressive resizing is the trick that let us get the best single computer 
result for image net training on one bench. It's this idea of starting 
small gradually making bigger. I only know of two papers that have used 
this idea. One is the progressive resizing of gans paper, which allows 
training a very high resolution gains and the other one is. The ideas are, 
and the cool thing about progressive resizing is not only are your earlier. 
Epochs assuming you've got two by two smaller four times faster. You can 
also make the batch size - maybe three or four times bigger, but more 
importantly, they're - going to generalize better because you're feeding 
your model different sized images during training right. So we were able to 
Train like half as many epochs for imagenet as most people. So our epochs 
were faster and they were fewer of them.</p>

<p>So progressive resizing is 
something that you know, particularly if your training from scratch, I'm 
not so sure, if it's useful for fine-tuning transfer learning, but if 
you're training from scratch, you probably want to do nearly all the time. 
So the next step is to go all the way back to the top right and change to 
full scale. Thirty-Two batch size right, like restart so I saved the model 
before I do that, go back and that's why there's a little bit of fussing 
around in here with reloading, because what I needed to do now is I needed 
to load my saved model back in. But there's a slight issue, which is: I now 
have one more up sampling layer than I used to have to go from two by two 
to four by four. My little. My little loop here is now looping through 
twice not once, and therefore it's added an extra convent, an extra pixel 
shuffle. So how am I going to load in weights for a different network? And 
the answer is that I use a very handy thing in pytorch, which is if I call 
that this is what this is. Basically, what learned load calls behind the 
scenes load state kicked if I pass this parameter strict equals false. If I 
pass in this parameter strict equals false that it says. Okay, if you can't 
fill in all of the layers just fill in the lay as you can so after loading 
the model back in this way, we're going to end up with something where it's 
loaded in all the layers that it can and that one comp layer.</p>

<p>That's new is 
going to be randomly initialized, all </p>

<h3>10. <a href="https://youtu.be/1-NYPQw5THU?t=1h45s">01:00:45</a></h3>

<ul style="list-style-type: square;">

<li><b> “Their big mistake” and how they could have won #1</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Right and so then I freeze all my layers and then unfreeze that upsampling 
part right and then use I CNR on my newly added extra layer right and then 
I can go ahead and wear again and so then the rest is the same. So, if 
you're trying to replicate this, don't just run this top to bottom. Okay 
realize it involves a bit of jumping around okay yeah, the longer you 
train, the better it gets. I ended up training it for about 10 hours, but 
you'll still get very good results. Much more quickly if you're, less 
patient, and so we can try it out - and here is the result. Here is my 
pixelated bird and look here: it's like totally random e pixels and here's 
the upsampled version. It's like, it's literally invented color, it 
coloration, but it figured out what kind of bird it is right and it knows 
what the feathers are metal look like, and so it has imagined a set of 
feathers which are compatible with these exact pixels, which is like 
genius. Like saying here at there's no way you can tell what these blue 
dots are meant to represent, but if you know that this kind of bird has an 
array of feathers here, you know that's what they must be right and then 
you can figure out where the Feathers would have to be such that when they 
were pixelated they'd end up in these pots, all right, so it's like 
literally reverse engineered, like given its knowledge of this exact 
species of bird, how it would have to have looked to create this output, 
and so this Is like so amazing, it also knows from all the kind of signs 
around it that this area here was was almost certainly blurred out.</p>

<p>So it's 
actually reconstructed blurred vegetation and you know if it if it hadn't 
have done all of those things it wouldn't have got. Such a good loss 
function right because in the end it had to match. You know the activations 
saying like oh there's a feather over here and it's kind of fluffy looking 
and it's you know in this direction and all that all right. Well, that 
brings us to the end of super resolution. Don't forget to check out the ask 
Jeremy anything's threaded and we will do some Astro me anything after the 
break but see you back here a quarter to eight okay, so we are going to do. 
Ask Jeremy anything Rachel will tell me the most voted up of your 
questions. Yes, Rachel. What are the future plans for fastai in this 
course? Will there be a part three? If there is a part three, I would 
really love to take it. Oh I'm not! Quite sure I always had to guess I hope 
there'll be some kind of follow-up. Last year after part, two one of the 
students started up a weekly book club going through the Ian Goodfellow 
deep learning book and Ian actually came in and presented quite a few of 
the chapters and other people like there's somebody an expert who presented 
every chapter. That was really that was like a really cool part. Three and 
four that extended will depend on I'm you, the community, to come up with 
ideas and help make them happen and yeah, and I'm definitely keen to to 
help. I've got a bunch of ideas, but I'm nervous about saying them because 
I'm not sure which ones will happen and which ones won't, but the more 
support I have in making things happen that you want to happen from you, 
the more likely they are.</p>

<p>What was your experience like starting down the 
path of entrepreneurship? Have you always been an entrepreneur, or did you 
start at it start out at a big company in transition to a startup? Did you 
go from academia to startups or startup stack edenia? No, I was definitely 
not an academia, I'm totally a fake academic. I I </p>

<h3>11. <a href="https://youtu.be/1-NYPQw5THU?t=1h5m30s">01:05:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Splitting into Training and Test, but not randomly</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>I started at McKinsey and company, which is a strategy firm when I was 18, 
which meant I couldn't really go to university, so I didn't really turn up 
and then yeah spent eight years in business, helping really big companies 
on strategic questions. I always wanted to be an entrepreneur planter on 
you spend two years in McKinsey. Only thing I really regret in my life was 
not sticking to that plan and wasting eight years instead. So two years 
would have been perfect, but yeah. Then I went into burner. Ship started 
two companies in Australia and the best part about that was that I didn't 
get any funding. So all the money that I made was mine or the decisions 
were mine and my you know and my partner's you know I focused entirely on 
on profit and product and customer and service, whereas I find in San 
Francisco. I'm glad you know I'm glad I came here, and so the two of us 
from you know came here for cable and EMI and raised. You know ridiculous 
amount of money, eleven million dollars for this really new company. That 
was really interesting, but it's also really distracting. You know trying 
to worry about scaling and VC's wanting to see what your business 
development plans are and also just not having any real need to actually 
make a profit and yeah. So I had a bit of the same problem at analytic 
where I again raised a lot of money: fifteen million dollars pretty quickly 
and yeah a lot of distractions, so yeah.</p>

<p>I think you know trying to 
bootstrap your own company and focus on making money by selling something 
at a better profit, and then you know plowing that back into the company. 
It worked really well right because within like five years, you know we 
were making a profit from three months in and within five years, we're 
making. You know enough for profit, not just to pay all of us in their own 
wages, but also to see my bank account growing and after ten years sold it 
for a big chunk of money, not enough that a VC would be excited. But enough 
that I didn't have to worry that money again, you know, so I think yeah 
bootstrapping a company, something which people in the Bay Area's don't 
seem to appreciate how good our idea, that is, if you were 25 years old 
today and still know what you Know where which of you looking to use AI? 
What are you working on right now are looking to work on in the next two 
years. </p>

<h3>12. <a href="https://youtu.be/1-NYPQw5THU?t=1h8m20s">01:08:20</a></h3>

<ul style="list-style-type: square;">

<li><b> Why they modified their Sales Target with ‘np.log()/max_log_y’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>You should ignore the last part of that I won't even answer it doesn't 
matter where I'm looking like what you should do is leverage your knowledge 
about your domain. So, like one of the main reasons we do, this is to get 
people who have backgrounds in whatever recruiting you know. Oil field 
surveys, journalism, activism, whatever right and solve your problems, 
it'll be really obvious to you. What real problems are and it'll be really 
obvious to you, what data you have and where to find it? Those are all the 
bits that, for everybody else, that's really hard. So people who start out 
with like oh, I know deep learning now I'll, go and find something to apply 
it to basically never succeed. Where else people who are like. Oh, I've 
been spending 25 years doing specialized recruiting for legal firms, and I 
know that the key issue is this thing, and I know that this piece of data 
totally solves it, and so I'm just going to do that now and I already know 
who to call Who actually start selling it to? You know they're the ones who 
who tend to win so yeah and and and you know, if you, if you've done 
nothing but like academic stuff, that it's more, maybe about your your 
hobbies and interests. You know so everybody has hobbies. The main thing, I 
would say is: please don't focus on building tools for data scientists to 
use or for software engineers to use, because every data scientist knows 
about the market of data scientists, whereas only you know about the 
market. For you know, analyzing oil survey world walks, or you know, 
understanding, audiology studies or whatever it is that you do given what 
you've shown us about applying transfer learning from image recognition to 
NLP. There looks to be a lot of value in paying attention to all of the 
developments that happen across the whole machine learning field and that, 
if you were to focus in one area, you might miss out on some great advances 
in other concentrations.</p>

<p>How do you stay aware of all the advancements 
across the field while still having time to dig in deep to your specific 
domains? Yes, awesome. I mean that's kind of the message of this course. 
One of the key messages this course yeah. It's like lots of good works 
being done in different places, and people are so specialized. Most people 
don't know about it like if I can get stated that results in an LP within 
six months of starting to look at NLP, and I think that says more about NLP 
than it does about me. Thankfully, so yeah it's kind of like the 
Entrepreneurship thing. It's like you pick that the areas you see that you 
know about and kind of transfer stuff like. Oh, we could use deep learning 
to solve this problem or, in this case, like </p>

<h3>13. <a href="https://youtu.be/1-NYPQw5THU?t=1h11m20s">01:11:20</a></h3>

<ul style="list-style-type: square;">

<li><b> A look at our basic model</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>We could use you know this idea can compute a vision to solve that problem, 
so things like trendier may transfer learning, i'm sure there's like a 
thousand things opportunities for you to do in other fields to do what 
Sebastian and I did in NLP with NLP classification. So the short answer to 
your question is the way to stay ahead of, what's going on would be to 
follow my feed of Twitter favorites, and my approach is to and follow lots 
and lots of people on Twitter and put them into the Twitter favorites. For 
you, like literally I every time I come across something interesting, I 
click favorite, and there are two reasons I do it. The first is that when 
the next course comes along, I go through my favorites to find which things 
I want to study. The second is so that you know you can do the same thing 
and then you know which do you go deep into it almost doesn't matter like I 
find every time I look at something it turns out to be super interesting 
and important. So tiss like pick something which is like you feel like 
solving, that problem would be actually useful to some reason and it 
doesn't seem to be popular which is kind of the opposite. With what 
everybody else does. Everybody else works on the problems which everybody 
else is already working on, because they're, the ones that seem popular - 
and I don't know - I can't quite understand this chain of thinking - but it 
seems to be very common - is deep learning and overkill to use on tabular 
data When is it better to use deep learning instead of machine learning on 
tabular data? Is that a real question, or did you just put that there so 
that I would point out that Rachel, Thomas just wrote an article? So yes, 
so Rachel's just written about this and and original, and I spent a long 
time talking about it and the short answer is: we think it's great to use 
deep learning on tabular data actually of all the rich, complex, important 
and interesting things that appear in Rachel's, Twitter stream, covering 
everything from the genocide of the Inga through to latest ethics 
violations in AI companies.</p>

<p>The one by far that got the most attention and 
engagement from the community was their question about. Is it called 
tabular data or structured data? So ya ask computer Pires people how to 
name things and you'll, get plenty of interest yeah and there's some really 
good links here to stuff from instacart and pinterest and other folks in 
this area. Any of you that went to the data institute conference will have 
seen Jeremy Stanley's presentation about the really cool work they did and 
instacart. Yes, we're sure, so I relied heavily on lessons 3 and 4 from 
part one and writing this post. So yes, much of that may be familiar to 
you. Yeah Rachel asked me during the post like how to tell whether you 
should use the decision tree ensemble like GBM or random forest or or a 
neural net, and my answer is, I still don't know nobody to my nobody. I'm 
aware of has done that research and any particularly meaningful way, so 
there's a question to be answered there. I guess </p>

<h3>14. <a href="https://youtu.be/1-NYPQw5THU?t=1h14m45s">01:14:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Training our model and questions</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>My approach has been to try to make both of those things as accessible as 
possible through the faster your library, so you can try them both both and 
see what works. Yes, that's what I do, and that is it for the top voted 
questions. Thank you. Okay. So, just quickly to go from super resolution to 
style transfer, it's kind of. Oh, I think I miss the one on reinforcement, 
learning and reinforcement. Learning popularity has been on a gradual rise 
in the recent past. What's your take on reinforcement, learning, which fast 
day I consider covering some ground and popular RL techniques in the 
future? I'm still not a believer in reinforcement learning. I think it's a 
an interesting problem to solve, but it's not at all clear that we have a 
good way of solving this problem. So the problem - it really is the delayed 
credit problem. So you know I want to learn to play pong. I've moved up or 
down and three minutes later I find out whether I won the game of pong 
which actions I took were actually useful and so to me the idea of 
calculating the gradients of those inputs with respect. You know the app so 
the gradient of the output with respect to those inputs. The credit is so 
delayed that those derivatives don't seem very interesting and there's 
been, you know, kind of been. I get this question quite regularly in every 
one of these four courses.</p>

<p>So far, I've always it the same thing I'm rather 
pleased, but finally, recently there's been some results showing that 
actually, basically random search, often does better than reinforcement 
learning. So basically, what's happened is very well-funded. Companies with 
vast amounts of computational power, throw all of it at </p>

<h3>15. <a href="https://youtu.be/1-NYPQw5THU?t=1h16m45s">01:16:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Running the same model with XGBoost</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Reinforcement learning problems and get good results and people then say: 
oh it's because of the reinforcement learning rather than the vast amounts 
of compute power, or they use extremely thoughtful and clever algorithms, 
like a combination of convolutional, neural nets and Monte Carlo tree 
search like they did With the alpha girl stuff to get great results and 
people incorrectly say: oh that's because of reinforcement learning when it 
wasn't really reinforcement learning at all. So I'm very interested like in 
solving these kind of more generic optimization type problems rather than 
just prediction problems and that's what these delay to credit problems 
tend to look like, but I don't think we've yet got good enough best 
practices that I have anything on ready To teach and say, like I've got to 
teach you this thing, because I think it's still going to be useful next 
year, so we'll keep watching and yeah see see what happens. Ok, so we're 
going to now turn the super resolution Network, basically into a style 
transfer network, and what do this pretty quickly? We basically already 
have something so here's my input image and I'm going to have some loss 
function and I've got some neural net again. So, instead of a neural net, 
that does a whole lot of compute and then does up sampling at the end. Our 
input, this time is just as big as our output, so we're going to do some 
down sampling first and then our compute and then our settlement. Okay, so 
that's the first change.</p>

<p>We're going to make is going to add some down 
sampling so since tried to convolution layers to the front of our network. 
The second is, rather than just comparing YC and X, to the same thing here 
right, so we're going to basically say our input. Image should look like 
itself by the end, and so specifically we're going to compare it by 
checking it through vgg and comparing it at one of the content at one of 
the activation layers. And then its style should look like some painting 
which brought to it. Just like we did with the gaddy's approach by looking 
at the grand Matrix correspondence at a number of layers, so that's 
basically it and so that that ought to be super straightforward. It's 
really just combining two things: we've already done, and so all this code 
starts identical, except we don't have high res and low res. We just have 
one size 256. Well, this is the same. My model is the same. One thing I did 
here is I I made I did not do any kind of fancy best practices for this one 
at all, partly because there doesn't seem to be any like there's been very 
little, follow up in this approach compared to the super resolution stuff 
and We'll talk about why in a moment so you'll see this is like much more 
normal looking. You know I've got batch norm layers. I don't have the 
scaling factor here.</p>

<p>You know I don't have a pixel shuffle, that's just 
using a normal up sampling, followed by </p>

<h3>16. <a href="https://youtu.be/1-NYPQw5THU?t=1h20m10s">01:20:10</a></h3>

<ul style="list-style-type: square;">

<li><b> “The really, really, really weird things here !”</b></li>

<li><b>&amp; end of the Rossmann competition;-)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>One by one cons, so it's kind of it's just more normal one thing they 
mentioned in the paper. Is they had a lot of problems with zero, padding, 
creating artifacts and the way they solve? That was by adding 40 pixels of 
reflection padding at the start. So I did the same thing and then they used 
zero padding in there convolutions in there red blocks. Now, if you've got 
zero padding in your convolution in your red blocks, then that means that 
you're, the two parts of your resin, it won't add up anymore, because 
you've lost a pixel from each side on each of your two convolutions. So my 
my red sequential, has become res sequential Center and I've removed the 
last two pixels on each side of those good sus, okay, so other than that. 
This is basically the same as what we had thought. So then we can bring in 
our starry night picture. We can resize it, we can throw it through our 
transformations just to make the method a little bit easier for my brain to 
handle. I took my transformation image which, after transfer our transform 
style in after transformations of three by 256 by 256, and I made a mini 
batch. My batch size is 24 24 copies of it. Now it just makes it a little 
bit easier to do the kind of batch arithmetic without worrying about some 
of the broadcasting they're not really 24. Copies are used, MP, dot 
broadcast to to basically fake 24 piece. Okay, so just like, before we 
create a V Gigi grab the last block this time we're going to use all of 
these layers.</p>

<p>So we keep everything up to the forty-third layer, and so now 
our combined lasts is going to add together a content loss for the third 
block, plus the gramm loss for all of our blocks with different weights. 
And so the gram loss against that kind of going back to everything being is 
like normal as possible. I've gone back to using MSA here. Basically, what 
happened as I had a lot of trouble getting this to train properly, so I 
gradually removed trick after trick and eventually just went. Ok, that's 
good, I believe make it as as bland as possible. Last week's Graham matrix 
was wrong by the way it only worked for a batch size of one, and we only 
had a batch size of one. So that was fine. I was using matrix multiply, 
which meant that every batch was being compared to every other batch. You 
actually need to use batch matrix multiply, which does a matrix multiply 
per batch. Ok, so that's something to be aware of there. Ok, so so I've got 
my grand matrices. I do my MSE loss between the gram matrices. I weight 
them my style weights, so I create that ResNet so create my style. My 
combined loss passing in the vgg Network, passing in the block IDs passing 
in the transformed starry night image, and so you'll see the very start. 
Here. I do a forward pass through both vtg model with that starry night 
image, in order that I can the features for it right now notice.</p>

<p>It's 
really important now that I don't do any data augmentation, because I've 
saved the style features for a particular. You know non Augmented version, 
and so, if I augmented it, it might make some minor problems, but that's 
fine, because I've got all of imagenet to deal with. I don't really need to 
do data augmentation anyway. Okay, so I've got my loss function and I can 
go ahead and fit and there's really nothing flavor here at all. At the end, 
I have my some layers equals false, so I can see what each part looks like 
and see that there is some we balanced and I can finally pop it out. So I 
mentioned that should be pretty easy, and yet it took me about 4 days 
because it just I just found this incredibly fiddly to actually get it to 
work so like when I finally got up in the morning. I said to Rachel guess 
what they're trained correctly Rachel was like. I never thought that was 
going to happen it just it just looked awful all the time and it's really 
about getting the exact right mix of content, lossless, a style loss of the 
mix of the layers of the style loss and that the worst part was it Takes a 
really long time to train the damn CNN, and I don't didn't really know how 
long to train it before before I decided it wasn't doing well like, should 
I just train it for longer or what and I don't know all the little details 
didn't seem To like slightly change it, but just like it would totally fall 
apart all the time.</p>

<p>So I kind of mentioned this partly to say like just 
remember. The final answer you see here is after me, driving myself, crazy 
or weak over nearly always not working until. Finally, the last minute it 
finally does for even for things which just seemed like they couldn't pass 
be difficult, because that is combining two things we already have working. 
The other is like to be careful about how we interpret what authors claim 
yeah, so it was so fiddly getting this style transfer to work and like 
after doing it, it left me thinking. Why did I bother? Because now I've got 
something that takes hours to create a network that can turn any kind of 
photo into one specific </p>

<h3>17. <a href="https://youtu.be/1-NYPQw5THU?t=1h26m30s">01:26:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Taxi Trajectory Prediction (Kaggle) with “another uncool NN” winner</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Style, it just seems very unlikely. I would want that for anything like 
about the only reason I could think that being useful would be to like do 
some art and stuff on a video, rather to turn every frame into some style 
like it's incredibly interesting to what to do. But you know when I looked 
at the paper that you know their tables saying like oh we're a thousand 
times faster than the Gattis approach, which is like it's just such an 
obviously meaningless thing to say in such an incredibly kind of misleading 
thing to say, because It ignores all the hours of training for each 
individual style, and I don't know I find this frustrating because, like a 
groups like this, Stanford group clearly know better or ought to know 
better, but still, I guess the academic community. He kind of encourages 
people to make these ridiculously grand plans and it also completely 
ignores this incredibly sensitive fiddly training process. So you know this 
paper was just so well accepted when it came out. You know I remember 
everybody getting on Twitter and being like wow. You know these Stanford 
people have found this way of doing style, transfer a thousand times faster 
and clearly you know the people saying this would like all like top 
researchers in the field. Clearly, like none of them actually understood 
because nobody said you know, I don't see why this is remotely useful and 
also I tried it and it was incredibly fiddly to get it all to work, and so 
it's not until like what is this they're, like eighteen months Later or 
something that I finally coming back to it and kind of thinking like wait a 
minute, this is kind of stupid, and so so this is the answer I think to the 
question of like well, why haven't people don't follow ups on this to like 
create Really amazing best practices and better approaches like with a 
super resolution, part of the paper, and they I think the answer is because 
it's done so.</p>

<p>I think this part of the paper is clearly not fun. You know 
and it's been improved and improved and improved, and now we have great 
super resolution and I think we can derive from that great noise reduction. 
Great colorization great, you know, slant, removal, great, interactive and 
effective or whatever. So I think, there's a lot of really cool techniques 
here and it's also delivering a lot of stuff that we've been learning and 
getting better and better at okay. So then, finally, let's talk about 
segmentation. This is from the famous some cam vid data set, which is a 
classic example of an academic segmentation data set, and basically you can 
see what we do is we start with a picture they're. Actually video frames in 
this data set like here and we construct. We have some labels, where 
they're not actually colors, that each one has an ID and the IDS of math, 
two colors so like red might be, one purple might be two like pink might be 
three, and so all the buildings. You know one class or the cars or another 
class or the people or another class or the road is another class, and so 
what we're actually doing here is multi-class classification for every 
pixel, okay, and so you can see sometimes that model class specification 
really is quite Tricky there's you know like like these branches. Well, 
though, sometimes the labels are really not that great. You know this is 
very coarse, as you can see so here at traffic lights so forth.</p>

<p>So, but 
that's what we're going to do we're going to do. This is segmentation, and 
so it's a look like bounding boxes. Okay, but you know rather than just 
finding you know a box around each thing: we're actually going to label 
every single pixel with its plus and really that's actually a lot easier 
because it fits our CNN style so nicely that we've. Basically, we can 
create any CNN where the output is an N by M grid containing the integers 
from 0 to C, where there are C categories, and then we can use 
cross-entropy loss with a softmax activation and we're done right so like I 
could actually stop the Class there and you can go and use exactly the 
approaches. You've learnt in like lessons 1 & amp, 2 and you'll get a 
perfectly. Okay result. Ok, so the first thing to say is like this is not 
actually a terribly hard thing to do, but we're going to try and do it 
really well, and so, let's start by doing it, the really simple way and 
we're going to use the CAG or carvanha Competition, so you could go cable 
car Varner to find it. You can download it with the caracal api as per 
usual. Now, basically, there's a train folder containing a bunch of images 
which is the independent variable and a train masks, folder, there's the 
dependent variable and they look like this. Here's the here's one of the 
independent variable and here's one of the dependent variable okay.</p>

<p>So in 
this case, just like cats and dogs we're going simple rather than doing 
multi-class classification we're going to do binary classification, but of 
course multi-class is just the more general version. You know categorical, 
cross-entropy, your binary, cross-entropy, okay, so there's no differences 
conceptually. So we've got this is just you know, zeros and ones where else 
this is a regular image. So, in order to do this well, it would really help 
to know what cars look like right, because you know really we just what to 
do is figure out. This is a car and its orientation and then color, you 
know, put white pixels where we expect the part of e based on the picture 
and their understanding of what cars will plug. The original data set came 
with these CSV files as well. I don't really use them for very much other 
than getting the list of images from them. Each each image after the car ID 
has a 0 1, 0 2, etc of which I've printed out all 16 of them for one car 
and, as you can see, if basically those numbers are the 16 orientations of 
one car. That is, and I don't think anybody in this competition actually 
used this orientation information. I believe they all kick the cars images 
just treated them separately. These images are pretty big, like over a 
thousand by thousand in size, and just opening the JPEGs and resizing them 
is slow, so I processed them all. Also OpenCV can't handle gif files.</p>

<p>So I 
converted them yes, Rachel, the question: how would somebody get these 
masks for training, initially Mechanical Turk or something yeah yeah, just 
a lot of boring work. You know probably some tools that help you with a bit 
of edge, snapping and stuff so that the human can kind of do it roughly and 
then just find you in the bits it gets wrong. Yeah, these kinds of labels 
are expensive. You know, and so one of the things I really want to work on 
is deep learning enhanced interactive labeling tools because you know yeah, 
so I've got a little section here that you can run. If you want to. You 
probably want to which converts the gifts into pngs, so just open it up 
with PIL and then save it as PNG, because open CV doesn't have give support 
and, as per usual for this stuff, I do it with a thread pool. So I can take 
advantage of parallel processing and then also create a separate directory 
train 128 and train masks 128, which contains the 128 by 128 resized 
versions of them, and this is the kind of stuff that keeps you sane. If you 
do it early in the process, so anytime you get a new data set, you know 
seriously think about creating a you know smaller version to make life fast 
anytime. You find yourself waiting on your computer. You know try and think 
of a way to create a smaller version, so yeah after you grab it from 
cowboy. You probably want to run this stuff by way to have lunch come back 
and when you're done you'll have these smaller directories, which we're 
going to use here 128 by 128 pixel versions to start with, so here's a cool 
trick if you use the same access object To plot an image twice and the 
second time you use alpha, which, as you might know, means transparency in 
the computer vision world, then you can actually plot the mask over the top 
of the photo, and so there here's a nice way to see all the masks On top of 
the photos for all of the cars in one group, this is the same match.</p>

<p>Files 
data set we've seen twice already. This is all the same code we used and 
here's something important, though, if we had something that was in the 
training set, go to this image and then the validation had that image. That 
would kind of be cheating, because it's the same Cup. So we use a 
contiguous set of car IDs and since each set is a set of 16, we make sure 
that's evenly divisible by 16, so we make sure that our validation set 
contains different car IDs to our training set. This is the kind of stuff, 
but you've got to be careful of on Kaggle. It's not so bad you'll know 
about it, because you'll submit your result and you'll get a very different 
result on your leaderboard compared to your validation set, but in the real 
world you won't know until you put it in production and send your company 
bankrupt and lose Your job, so you might want to think carefully about your 
validation set in that case, so here we're going to use, transform type 
classification, it's basically the same as transform type dot pixel. But if 
you think about it, we with a pixel version. If we rotate a little bit, 
then we probably want to, like average, the pixels in between the two but 
the classification. Obviously we we don't, we use nearest neighbor, so the 
slight difference there also for classification. You know lighting, doesn't 
kick in normalization to the dependent variable. Okay, they're already 
square images, so we don't have to do any cropping.</p>

<p>So here you can see 
different versions of the Augmented. You know move around a bit when 
they're rotating a bit and so forth. Yeah. I get a lot of questions kind of 
like during our study group and stuff about like how do I debug things and 
fix things that aren't working and like a I </p>

<h3>18. <a href="https://youtu.be/1-NYPQw5THU?t=1h38m">01:38:00</a></h3>

<ul style="list-style-type: square;">

<li><b> “Start with a Conv layer and pass it to an RNN” question and research</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Never have a great answer other than like every time I fix a problem is 
because of stuff like this, that I do all the time you know I just always 
print out everything as I go, and then the one thing that I screw up always 
turns out To be the one thing that I forgot to check along the way so yeah, 
the more of this kind of thing you can do the better if you're, not looking 
at all of your intermediate results, I mean I have troubles okay, so, given 
that we want something That knows what cars look like: we probably want to 
start with a pre trained, imagenet network, so we're going to start with 
ResNet 34 and so with confident builder. We can grab our resin at 34 and we 
can add a custom head, and so the custom head is going to be something that 
up samples, a bunch of plants and we're going to do things really done for 
now, which is we're just going to do a Commons pose to addy batch norm, 
value, okay, and so here's like this is what I'm saying you could any of 
you could have built this without looking at any of this, or at least like 
you, have the information from previous classes. There's nothing new at 
all. Okay and so at the very end, we have a single filter - okay and now 
that's going to give us something which is batch size by 1 by 128 by 128. 
But we want something which is batch sized by 128 by 128. So we have to 
remove that unit axis, so I've got a lambda layer here.</p>

<p>Lambda layers are 
incredibly helpful right because without the lambda layer here, which is 
simply removing that unit axis by just indexing into it at 0, without the 
lambda layer, I would have to have created a custom class with a custom 
forward method and so forth. But by creating a lambda layer that does like 
the one custom bit, I cannotice chocolate from the sequential and so that 
just makes life easier, so the pytorch people are kind of snooty about this 
approach. Lambda is actually something that's part of the first day. I 
library not part of the pad torch library and, like literally people on the 
pad watch discussion board like yes, we could give people this. Yes, it is 
only a single line of code, but they never like encourage them to use 
sequential too often so there you go okay, so so this is that custom head 
right, so we're gon na have a rest at 34 that goes down sample and then a 
Really simple custom here that very quickly up samples, and that hopefully, 
will do something and we're going to use accuracy with a threshold of 0.5 
to print out metrics, and so after a few a pops we've got 96 percent 
accurate. Okay, so is that good is 96 percent, accurate, good and hopefully 
the answer to your question. That question is, it depends, what's it for 
right, and the answer is qivana wanted this, because they wanted to be able 
to take their car images and cut them out and paste them on.</p>

<p>You know 
exotic monte-carlo backgrounds or whatever that's multicolor the place, the 
simulation. So to do that you need a really good mask, but you don't want 
to like leave the rearview mirrors behind or, like you know, kind of, have 
one wheel missing or include a little bit of background or something that 
would look stupid. So you would beat something very good, so only having 96 
percent of the pixels correct doesn't sound great, but we won't really know 
until we look at it. So let's look at it. So there's the correct version 
that we want to cut out. That's the 96 % expert version, okay, so like. 
Where do you look at it? You guys oh yeah, getting 90 %, 96 % of the 
pixels. Accurate is actually easy because, like all the outside bits, not 
care at all the inside bit is car and really really interesting. Bit is the 
edge okay. So we need to do better, so, let's unfreeze, because what we've 
done so far is trained the customer here. Okay, let's do more, and so after 
a bit more, we've got 99.1 percent. Okay, so is that good? I don't know, 
let's take a look, and so actually no, it's totally missed the rearview 
vision mirror here and </p>

<h3>19. <a href="https://youtu.be/1-NYPQw5THU?t=1h42m40s">01:42:40</a></h3>

<ul style="list-style-type: square;">

<li><b> The 100-layers Tiramisu: Fully Convolutional Densenets, for Image Segmentation (Lesson 13 cont.)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Missed a lot of it here and it's clearly got an edge wrong here, and these 
things are totally going to matter when we try to cut it out. So it's still 
not good enough. So let's try upscaling and the nice thing is that when we 
upscale to 512 by 512 make sure you decrease the batch size because you'll 
run out of memory, you know here's the true ones, it's quite a lot more. 
This is all identical. It's quite a lot! More information Everett go on, so 
our accuracy increases to 99.4 % and things keep getting better, but we've 
still got quite a few little black blocky bits. So let's go to 124 by 124 
down to batch size of four. This is pretty high res now and train a bit 
more. Ninety nine point, six ninety-nine point eight, and so now, if we 
look at the masks, they're actually looking, not bad, okay, that's looking 
pretty good right. So can we do better, and the answer is yes, we can so 
we're moving from the carbonyl nut book to the carbonyl unit. Notebook now 
and the unit Network is quite magnificent right. You see with that previous 
approach. Our pre-trained imagenet network was being squished down. All the 
way down to seven by seven and then expand it out all the way back up to 
you know. Well, it's two to four go to seven by seven, but one or two four 
is going quite a bit bigger and then expanded out again or this way, which 
means it has to somehow store all the information about the much bigger 
version in the small version right And actually most of the information 
about the bigger version was really in the original picture anyway, so it 
doesn't seem like a great approach, this squishing and I'm squishing.</p>

<p>So 
the unit idea comes from this fantastic paper where, like it was literally 
invented in this, you know very domain-specific area of biomedical image 
segmentation, but in fact basically, every cattle winner in in anything 
even vaguely related to segmentation has ended up using unit as one of 
These things that, like everybody in Carroll, knows, is the best practice, 
but in more of an academic circles like even now, this has been around for 
a couple of years. At least a lot of people still don't realize. It's like 
this is by far the best approach and here's the basic idea: here's the 
downward path right, where we basically start start at five 72 by 5, 3. 2. 
In this case, and then kind of half the grid size, half the grid, size, 
half the grid, size, half the grid, size right and then here's the upward 
path where we double the grid size, double double double double. No, but 
the thing that we also do is we take. You know at every point where we've 
half the grid size, we actually copy those activations over to the upward 
path and and concatenate the together, and so you can see here these red 
blobs of maps, polling operations, the green blobs are upward sampling and 
then these gray Bits here are copying right, and so we copy and can cat so 
basically, in other words, the input image after a couple of poems is 
copied over to the output concatenated together, and so now we get to use 
all of the informations gone through all the down And all the app plus also 
a slightly modified version of the input, pixels right and a slightly 
modified version of one thing down from the input pixels because they came 
out through yes right.</p>

<p>So we have like all of the richness of going all the 
way down and up, but also like a slightly less coarse version and a 
slightly less cost version, and then this really kind of simple version and 
they can all be combined together. Okay and so that's unit, it's such a 
cool idea, so here we are in the in the carvanha unit notebook all this is 
the same code as before and at the start I've got a simple up sample 
version, just to kind of show you again the the Non unit version this time, 
I'm going to add in something called the dice metric dice is very similar, 
as you see to jacquard or I over you, it's just a minor difference. It's 
basically intersection over Union with a minor tweak and the reason we're 
going to use dice is that's the metric that the caracal competition used 
and it's kind of it's a little bit harder to get a high score than a high 
accuracy. Because it's really looking at. Like what the overlap of the 
correct pixels are with with your pixels, but it's pretty similar so in the 
Carroll competition people that we're doing okay, we're getting about 
ninety nine point: six dice and the winners for about nine nine point: 
seven days. So here's our standard up sample this is all as before, and so 
now we can check our dice metric, and so you can see on dice metric we're 
getting like nine six, eight at 128 by 128, and so that's not great okay. 
So so, let's try unit and I'm calling it unit ish because that's per usual, 
I'm creating my own someone hacky version right kind of trying to keep 
things as similar to what you're used to as possible and doing things that 
I think, makes sense.</p>

<p>And so there should be plenty of opportunity for you 
to at least make this more authentically unit by looking at the exact kind 
of grid sizes and like see how here, the size is going down a little bit so 
they're, obviously not adding any padding and then They're doing here, 
they've got some cropping going on there's a few differences right, but one 
of the things is because I want to take advantage of transfer learning. 
That means I can't quite use unit. So here's another big opportunity is 
what, if you create the unit down path and then add a classifier on the end 
and then train that on imagenet and you've, now got an image net trained 
classifier, which is specifically designed to be a good backbone for unit 
right And then you should be able to now come back and get pretty close to 
winning this old competition is actually not that old. It's fairly recent 
competition, because you know that pre train network didn't exist before. 
But if you think about like what Yolo v3 did it's basically that right, 
they create a darknet, they pre trained it on image net and then they used 
it as the basis for their bounding boxes. So again, this kind of idea of 
free training - things which are designed not just for classification but 
to find for other things, is just something that nobody's nobody's done yet 
and let's weave.</p>

<p>But as we've shown, you know, you can train image net for 
25 bucks in three hours now so and if people in the community are 
interested in doing this, you know hopefully I'll have credits. I can help 
you with as well. So if you do, you know the work to get it set up and give 
me a script. I can probably run it for you so for now, though, we don't 
have that so we're going to use resonate so so we're basically going to 
start with this. Let's see with get base and so base is our base network, 
and that was defined back up for this first section right, so get base is 
going to be something that calls whatever this is, and this is resna 34, so 
we're going to grab our ResNet 34 And cut model is the first thing that our 
confident builder does. It basically removes everything from the adaptive 
pulling onwards, and so that gives us back the backbone of resna, 34. Okay, 
so get base is going to give us okay and then we're going to take that rest 
at 30 for backbone and turn it into a. I call it a unit 34. So what that's 
going to do is it's going to save that ResNet that we passed in and then 
we're going to use a forward hook just like before to save the results at 
the second fourth, fifth and sixth blocks, which, as before, is the 
basically before each Straight to convolution, then we're going to create a 
bunch of these things. We're calling unit blocks and the unit block 
basically says so. These unit blocks are these things.</p>

<p>These are unit 
blocks, so the the unit block tells us. You know we have to tell it how 
many things are coming from the from the kind of previous layer that we're 
up sampling how many are coming across and then how many do we want to come 
out right, and so the amount coming across is entirely defined By whatever 
the base network was right, it's like whatever, whatever the downward path 
was, we need that many layers, and so this is a little bit awkward. 
Actually, one of our master students here kerim, has actually created 
something called dynamic unit that you'll find in fastai unit dynamic unit, 
and it actually calculates this all for you and automatically creates the 
whole unit from your base model. It's got some minor quirks still that I 
want to fix by the time the videos out, it'll definitely be working, and I 
will at least have a notebook showing how to use it and possibly add 
additional video. But for now you know you'll just have to go through and 
do it yourself. You can easily see it just by once, you've got a reson it 
you can just go. You know just type in its name and it'll print out all the 
layers, and you can see how big, how many activations there are in each 
block, or you can even have a printed out for you for each for each block 
automatically anyway. I just did this manually, and so the unit block is 
works like this, so you said: okay right, this penny coming up from the 
previous layer.</p>

<p>I've got this penny coming across this X, I'm using across 
across from the the downward path. This is the amount I want coming out 
now. What I do is they then say: okay, we're going to create a certain 
amount of convolutions from the upward path in a certain amount from the 
cross path, and so I'm going to be concatenated them together. So, let's 
divide the number we want out by two right and so we're going to have our 
cross convolution, take our cross path and create number out /. And then 
the upward path is going to be a common transpose to D right because we 
want to increase up sample and again here, we've got the number n divided 
by two and then at the end I just concatenate those together alright, so 
I've got an upward Sample I've got a cross convolution, I can catenate the 
two together yeah and so that's all a unit block is, and so that's actually 
a pretty easy module to create, and so then, in my forward path, I need to 
pass to the forward of the of the Unit block the upward path and the cross 
plus, so the upward path is just wherever I'm up to so far right, but then 
the cross path is whatever the value is of whatever the activations are 
that I stored on the way down right. So as I come up, it's the last set of 
saved features that I need first and in as I gradually keep going up 
further and further and further.</p>

<p>Eventually it's the first set of features. 
Okay, and so there are some more tricks, we can do to make this a little 
bit better, but this is this is a good stuff right. So if we try this, so 
the simple up, sampling approach, looked horrible right and had a dice of 
nine six. Eight now you net with everything else identical except we've 
been out got these unit blocks, has a dice of nine eight five right, sir. 
That's like we've kind of halved the error with with everything else, 
exactly the same and more the point, you can look at it. This is actually 
looking somewhat car-like compared to our non unit equivalent, which is 
just a block now, because you know try to do this through down and up 
paths. Just it's just asking too much. You know where else when we actually 
provide the downward path pixels at every point, it can actually start to 
create something karush. So at the end of that, we'll go dot close to again 
remove those s. Fs features taking up GPU memory, go to a smaller batch 
size, a higher size, and you can see the dice coefficients really going up. 
This is just so notice here. I'm learning I'm loading in right, the 128 by 
128 version of the network. Okay. So we're doing this progressive resizing 
trick again, so that gets us 99 3 and then unfreeze to get to 99 4, and you 
can see it's now. Looking pretty good.</p>

<p>Okay go down to a batch size of 4, 
so 102 for load-in. What we just did with the 512 Texas, 299 5 unfreeze 
takes us to 99, we'll call that 99 six, five nine nine and, as you can see, 
that actually looks good right in accuracy terms. Ninety-Nine point eight 
two, you know you can see. This is looking like something you could just 
about used to cut out. I think, too, you know at this point there's a 
couple of minor tweaks. We can do to get up to ninety-nine point seven, but 
really the key thing then I think, is just maybe to do a you know a few bit 
of smoothing, maybe or a little bit of post-processing. You can go and have 
a look at the carvanha winners. Blogs and see some of these tricks but, as 
I say, the difference between where we are at ninety nine point: six and 
what the winners got of </p>

<h3>20. <a href="https://youtu.be/1-NYPQw5THU?t=1h58m">01:58:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Building and training the Tiramisu model</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>99.7, you know, is it's not heaps and so really that just the you net on 
its own pretty much pretty much solves our problem. Okay, so that's it so. 
The last thing I wanted to mention is now to come all the way back to 
bounding boxes, because you might remember, I said out, our bounding box 
model was still not doing very well on small objects. So hopefully you 
might be able to guess where I'm going to go with this, which is that for 
the bounding box model, remember how we we had at different grid cells. We 
spat out outputs of their model and it was those earlier ones with the 
small grits sizes that weren't very good, for how do we fix it? You net it 
right. Let's have an upward path with cross connections right and so then 
we're just going to do a unit and then spit them out of that. The now does 
those finer grid cells have all of the information of that path and that 
path and that path and that path for leverage. Now, of course, this is deep 
learning, so that means you can't write a paper saying we just used you net 
for bounding boxes. You have to invent a new word, so this is called 
feature: pyramid networks or fbms, okay and like that literally the paper. 
This is part of the retina net paper, which is actually a it's used in the 
retina net paper. It was you, it was created an earlier paper, specifically 
of Olympians and like if memory serves correctly, they did briefly cite the 
unit paper, but they kind of made it sound like it was this vaguely 
slightly connected thing that maybe some people could consider slightly 
useful, but It really F P ends as units okay.</p>

<p>I don't have an 
implementation of it to show you, but you know it'll be a fun thing, maybe 
for some of us to try and some of us have already some I haven't yet, but I 
know some of the students have been trying so to get it Working well on the 
forums so yeah interesting thing to try. So I think a couple of couple of 
things to look at after this class, as well as the other things I 
mentioned, would be playing around with FP ends and also maybe trying 
caroms dynamic unit. They would both be interesting things to look at all 
right, so so you guys have all been through fourteen lessons of me talking 
at you now. So I'm sorry about that. Thanks for putting up with me, you 
know, I think it's it. It's you're gon na find it hard to find people who 
actually, as know them as much about training, neural networks and 
practice, as you do, it'll be really easy for you to overestimate. How 
capable all these other people are an underestimate, how poor you are, and 
so, like the main thing I'd say is like please practice, please just 
because you don't have this constant thing, getting you to come back here 
every Monday night. Now it's very easy to kind of lose that momentum so 
find ways to keep it. You know you know, organize a study group, you know, 
or a book a reading group or get together some friends and work on a 
project or you know, do something more than just deciding.</p>

<p>I want to keep 
working on X like it's gon na need to involve problem unless you're the 
kind of person who's super motivated, and you know that whenever you decide 
to do something, it happens. That's not me right. It's like, I know 
something to happen. I have to like say yes, David in October. I will 
absolutely teach that course, and then it's like okay, if it actually 
writes a material, let's see only way, I can get stuff to happen. So we've 
got a great community there on the forums. If people have ideas for ways to 
make it better, please tell me you know if you think you can help with you 
know, if you want to create some new forum or moderated in some different 
way or whatever it is. Let me know right: you can always PM me and there's 
a lot of projects going on through github as well lots of stuff so yeah. I 
hope to see you all back here at something else and thanks so much for 
joining me on this journey. [ Applause, ] </p>

<h3>21. <a href="https://youtu.be/1-NYPQw5THU?t=2h2m50s">02:02:50</a></h3>

<ul style="list-style-type: square;">

<li><b> ENet and LINKNet models: better than the Tiramisu ?</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>




<h3>22. <a href="https://youtu.be/1-NYPQw5THU?t=2h4m">02:04:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Part 2: conclusion and next steps</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>









  </body>
</html>
