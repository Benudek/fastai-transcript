<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Lesson 07</title>
    <meta charset="UTF-8">
  </head>
  <body>
  <p style="text-align: right"><a href="http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826">fast.ai: Intro to Machine Learning 1 (v1) (2018)</a></p>
  <h1>Lesson 07</h1>
  <h2>Outline</h2>
<ul>

<li>Motivations for data science</li>
<li>Thinking about the business implications</li>
<li>Tell the story</li>
<li>Review of Confidence in Tree Prediction Variance, Feature importance, Partial Dependence</li>

</ul>


  <h2>Video Timelines and Transcript</h2>


<h3>1. <a href="https://youtu.be/O5F9vR2CNYI?t=1s">00:00:01</a></h3>

<ul style="list-style-type: square;">

<li><b> Review of Random Forest previous lessons,</b></li>

<li><b>Lots of historical/theoritical techniques in ML that we don’t use anymore (like SVM)</b></li>

<li><b>Use of ML in Industry vs Academia, Decision-Trees Ensemble</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Welcome back we're going to be talking today about random forests, we're 
going to finish building our own random forest from scratch. But before we 
do, I wanted to tackle a few things that have come up during the week. A 
few questions that I've had and I want to start with kind of the position 
of random forests in general, so we spent about half of his course doing 
random forests and then, after today, the second half of this course will 
be neural networks broadly defined. This is because these these to 
represent, like the tea, the two key classes of techniques, which cover 
nearly everything that you're likely to need to do random forests belong to 
the class of techniques of decision tree ensembles, along with gradient, 
boosting machines being the other key type And some variants, like 
extremely randomized trees, they have the benefit that they're highly 
interpretive all scalable flexible work well for most kinds of data. They 
have the downside that they don't extrapolate at all to like data. That's 
outside the range that you've seen as we looked at at the end of last 
week's session, but you know they're, there they're a great starting point, 
and so I think you know there's a huge catalogue of machine learning tools 
out there and I got lot of Courses and books, don't attempt to kind of 
curate that down and say like for these kinds of problems, use this for 
these kinds of problems.</p>

<p>Is that finished you know, but they're rather like 
here's, a description of 100 different algorithms and you just don't need 
them. You know like I, don't see why you would ever use in support, vector 
machine today, for instance like not know, no reason at all. I could think 
of doing that. People love studying them in the 90s because they are like 
very theoretically elegant and like you can really write a lot of math 
about support, vector machines and people did. But you know in practice: I 
don't see them as having any place. There's like a lot of techniques that 
you could include in an exhaustive list of every way that people adopt 
machine learning problems. But I would rather tell you like how to actually 
solve machine learning problems in practice. I think they, you know, we've 
we've about to finish today. The first class, which is you know, one type 
of decision tree ensembles in in part to you net, will tell you about the 
other key type there being gradient, boosting and we're about to launch 
next lesson into neural nets, which includes all kinds of GLM Ridge 
regression Elastic net lasso, logistic regression, etc, or all variants of 
neural nets. You know, interestingly, leo breiman, who created random 
forests, did so very late in his life and unfortunately passed away, not 
many years later, so, partly because of that very little has been written 
about them in the academic literature, partly because SVM's were just taken 
over at that Point you know other people didn't look at them and also like 
just because they're like quite hard to grasp at a theoretical level like 
analyze them, theoretically, it's quite hard to write conference papers 
about them or academic papers about them.</p>

<p>So there hasn't been that much 
written about them, but there's been a real resurgence or not resurgence. A 
new wave in recent years of empirical machine learning like what actually 
works, kegels been part of that, but also just in part of it, has just been 
like companies using machine learning to make loads of money like Amazon 
and Google, and so nowadays a lot of People are writing about decision tree 
ensembles in creating better software for decision tree ensembles like like 
GBM and x3 boost and Ranger, 4r and scikit-learn and so forth. But a lot of 
this is being done in industry rather than academia, but you know it's. 
It's encouraging to see. There's certainly more work being done in deep 
learning than in decision tree ensembles, particularly in in academia, but 
but there's a lot of progress being made in both. You know if you look at 
like of the packages being used today for decision tree ensembles like all 
the best ones, the top five or six. I don't know that any of them really 
existed five years ago. You know maybe other than like SK, learn or even 
three years ago. So yet so that's that's been good, but I think there's a 
lot of work still to be done. We talked about, for example, figuring out 
what interactions are the most important last week, and some of you pointed 
out in the forum's that actually there is such a project already for a 
gradient, boosting machine which is great, but it doesn't seem that there's 
anything like that.</p>

<p>Yet for random forests - and you know random forests - 
do have a nice benefit over gbms that they're kind of harder to screw up. 
</p>

<h3>2. <a href="https://youtu.be/O5F9vR2CNYI?t=5m30s">00:05:30</a></h3>

<ul style="list-style-type: square;">

<li><b> How big the Validation Set needs to be ? How much the accuracy of your model matters ?</b></li>

<li><b>Demo with Excel, T-distribution and n&gt;22 observations in every class</b></li>

<li><b>Standard Deviation : n<em>p</em>(1-p), Standard Error (stdev mean): stdev/sqrt(n)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>You know and easier to scale, so hopefully that's something that you know 
this community might help fix. Another question I had during the week was 
about the size of your validations that how big should it be so like to 
answer this question about? How big does your validation set need to be? 
You first need to answer the question: how how accurate do I need help 
recite to know the accuracy of this algorithm right so like if the 
validation set that you have is saying like this is 70 % accurate and if 
somebody said well, is it 75 % Or 65 % or 70 % - and the answer was, I 
don't know anything in that range is close enough like that would be one 
answer where else, if it's like is that 70 percent or 70 point? Oh one 
percent or sixty nine point, nine nine percent. Like then, that's something 
else again right, so you need to kind of start out by saying like how how 
accurate do I need this so like, for example, in the deep learning course, 
we've been looking at dogs versus cats images and the models that we're 
looking at Had about a ninety nine point, four, ninety nine point: five 
percent accuracy on the validation set and a validation set size was 2000. 
Okay. In fact, let's do this in Excel. That'll, be a bit easier, so our 
validation set size was 2000 and was 99.4 % right. So the number of 
incorrect is something around one accuracy times, and so we were getting 
about 12 wrong right and the number of cats we had is 1/2, and so the 
number of wrong cats is about 6. Ok, so then, like we, we run a new model 
and we find instead that the accuracy has gone to 99.2 % right and then 
it's like.</p>

<p>Okay, is this less good at finding cats? That's like! Well, it 
got 2 more cats wrong. So it's like, probably not right so, but then it's 
like well does this matter. There's ninety nine point: four versus ninety 
nine point, two matter and if this was like it wasn't about cats and dogs, 
but it was about finding fraud right then, the difference between a point, 
six percent error rate and a point - eight percent error rate - is like 
twenty Five percent of your cost of fraud so like that can be huge like 
it's really interesting like when imagenet came out earlier this year, the 
new competition results came out and the accuracy had gone down from three 
percent, so the error went down from three percent to Two percent and I saw 
a lot of people on the internet like famous machine learning, researchers 
being like yeah, some Chinese guys got it better from like 97 % to one 
ninety eight percent. It's like, statistically, not even significant, who 
cares kind of a thing, but actually I thought like holy crap, this Chinese 
team just blew away the state-of-the-art. An image recognition like the old 
one was fifty percent less accurate than the new one like. That's that's 
actually the right way to think about it. Isn't it because it's like you 
know we were trying to recognize. You know like which tomatoes were ripe 
and which ones weren't and like our new approach, you know the old 
approach, like fifty percent of the time.</p>

<p>More was like letting in the 
unripe Tomatoes, or you know, 50 percent. More of the time we were like 
accepting fraudulent customers, like that's a really big difference. So, 
just because, like this particular validation set, we can't really see six 
versus eight doesn't mean the 0.2 % different isn't important. It could be 
so. My kind of rule of thumb is that this, like this number of like how 
many observations you actually looking at. I want that generally to be 
somewhere higher than twenty-two. Why 22? Because 22 is the magic number 
where the t-distribution roughly turns into the normal distribution right. 
So, as you may have learned, the T distribution is, is the normal 
distribution for small data sets right, and so, in other words, once we 
have twenty-two of thing or more, it kind of starts to behave kind of 
normally, in both sense of the words like it's Kind of more stable and you 
can kind of understand it better. So that's my magic number when somebody 
says do I have enough of something I kind of start out by saying like do 
you have 22 observations of the thing of interest? So if you were looking 
at like Lyme cancer, you know - and you had a data set that had like a 
thousand people without lung cancer and 20 people with lung cancer. I'd be 
like, I very much doubt we're going to make much progress. You know, 
because we haven't even got 20 of the thing you want so ditto with a 
validation set.</p>

<p>If you don't have twenty of the thing you want, that is 
very unlikely to be useful or, if, like the at the level of accuracy, we 
need it's not plus or minus 20. It's just it's that. That's the point where 
I'm thinking like be a bit careful. So, just to be clear, you want 22 to be 
the number of samples in each set like in the validation, the test and the 
Train. So what I'm saying is like, if there's, if there's less than 22 of a 
class in any of the sets, then it's it's going to get it's getting pretty 
unstable at that point right and so, like that's just like the first rule 
of thumb, but then what I would actually do is like start practicing what 
we learned about the binomial distribution or actually very weak 
distribution. So, what's the what is the mean of the binomial distribution 
of n samples and probability P n times, P? Okay, thank you and times P. Is 
that mean all right? So if you've got a 50 % chance of getting ahead and 
you toss it a hundred times on average you get 50 heads, okay and then 
what's the standard deviation and P 1 minus B? Okay, so these are like two 
numbers: the first number you don't have to remember. It's intuitively 
obvious. The second one is one that try to remember forevermore, because 
not only does it come up all the time, the people that you work with we'll 
all have forgotten it. So you'll be like the one person in the conversation 
who could immediately go. We don't have to run this 100 times. I can tell 
you straight away: it's binomial.</p>

<p>It's going to be NP q, NP 1 minus B. Then 
there's the standard error. The standard error is, if you run a bunch of 
trials each time getting a mean. What is the standard deviation of the 
mean? I don't think you guys are covered this yet. Is that right? No, so 
this is really important, because this means like, if you train a hundred 
models right each time. The validation set accuracy is like the meaning of 
a distribution, and so therefore, the standard deviation of that validation 
set accuracy. It can be calculated with the standard error, and this is 
equal to the standard deviation divided by square root. N all right. So 
this tells you so like one approach to figuring out, like is my validation 
set big enough is train your model five times with exactly the same hyper 
parameters each time and look at the validation set accuracy each time and 
give you know, there's like a mean And a standard deviation of five numbers 
you could use or a maximum and a minimum you can choose, but to save 
yourself some time you can figure out straight away that like okay! Well, I 
I have a point: nine nine accuracy as to you know whether I get the cat 
correct or not correct. So, therefore, the standard deviation is equal to 
0.99 times 0.01. Okay and then I can get the standard error of that right. 
So so, basically the size of the validation set.</p>

<p>You need it's like, 
however big it has to be, such that your insights about accuracy, good 
enough for your particular business problem, and so, like I say, like the 
simple way to do. It is to pick a validation set of like a size of thousand 
trained five models and see how much the validation set accuracy varies and 
if it's like, if they're, if it's they're, all close enough for what you 
need, then you're fine, if it's not. Maybe you should make it bigger, or 
maybe you should consider using cross-validation instead, okay, so like, as 
you can see, it really depends on what it is you're trying to do, how 
common you're less common class is and how accurate your model is. Could 
you pass that back to Melissa, please? Thank you. I have a question about 
the less common classes. If you have less than 22, let's say you have one 
sample of something. Let's say it's a face and I only have one 
representation from that particular country. Do I toss that into the 
training set and it adds variety to I pull it out completely out of the 
data set, or do I put it in a test set instead of the validation set, so he 
certainly couldn't put it in the test of the validation Set because you're, 
asking kind of I mean in general because you're asking can I recognize 
something I've never seen before, but actually this this question of like 
can. I recognize something I've not seen before, there's actually a whole 
class of models specifically for that purpose, it's called either one-shot 
learning, which is you get to see something once and you have to recognize 
it again or zero shot learning, which is where you have to recognize 
Something you've never seen before we're not going to cover them in this 
course, but they can be useful for things like face recognition.</p>

<p>You know 
like is this the same person I've seen before and so generally speaking, 
obviously, for something like that to work, it's not that you've never seen 
a face before it's that you've never seen Melissa's face before you know, 
and so you see Melissa's face once and You have to recognize it again yeah. 
So in general you know your validation, set and test set need to have the 
same mix or frequency observations that you're going to see in production 
in the real world. And then your training set should have an equal number 
in each class. And if you don't just replicate the less common one until it 
is equal, so this is, I think, we've mentioned this paper before a very 
recent paper that came out. They tried lots of different approaches to 
training with unbalanced datasets and found consistently that over 
sampling, the less common class until that is the same size as the more 
common class is always the right thing to do so. You could literally copy 
you know so, like I've. Only got a thousand, you know ten examples of 
people with cancer and 100. Without so I could just copy those 10 and 
other. You know 90 times that's kind of a little memory and efficient. So a 
lot of things, including I think, SK - learns random forests have a class 
weights parameter that says each time your boot strapping or resampling.</p>

<p>I 
want you to sample the less common class with a higher probability or did 
or if you do and doing deep learning you know make sure in your mini batch. 
It's not randomly sampled, but it's a stratified sample, so the less common 
class is picked more often. Okay, okay, so let's get back to finishing off 
our random forests, and so what we're going to do today is </p>

<h3>3. <a href="https://youtu.be/O5F9vR2CNYI?t=18m45s">00:18:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Back to Random Forest from scratch.</b></li>

<li><b>“Basic data structures” reviewed</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>We're going to finish off writing our random forests and then, after day, 
you're after today, your homework will be to take this class and to add to 
it all of the random forest interpretation, algorithms that we've learned. 
Ok, so obviously to be able to do that. You're going to need to totally 
understand how this class works, so, please, you know, ask lots of 
questions as necessary as we go along. So just to remind you, we're doing 
the the bulldozers tackle competition data set again. We split it as before 
into 12,000 validation. The last 12,000 records and then just to make it 
easier for us to keep track of what we're doing we're going to just pick 
two columns out to start with year made and Machine hours on the meter, 
okay, and so what we did last time was. We started out by creating a tree 
ensemble and the tree ensemble had a bunch of trees, which was literally a 
list of entries trees where each time we just called create tree and create 
tree contained a sample size number of random indexes. Okay, this one is 
drawn without replacement, so remember, bootstrapping means sampling with 
replacement, so normally with scikit-learn. If you've got n rows, we grab n 
rows with replacement, which means many of them will appear more than once 
so each time we get a different sample, but it's always the same size as 
the original data set, and then we have our set our F samples, A function 
that we can use, which does with replacement sampling of less than n rows. 
This is doing something again, which is its sampling without replacement 
sample size, rows.</p>

<p>Okay, because we're permuting the numbers from naught to 
self dot y -1 and then grabbing the first self dot sample size of them, 
actually there's a faster way to do this. You can just use NPR and embrace, 
which is a slightly more direct way, but this way it works as well alright. 
So this is our random sample for this one of our entries trees add so then 
we're going to create a decision tree and our decision tree. We don't pass 
it all of X, we pass it. These specific indexes and remember X, is a pandas 
data frame. So if we want to index into it with a bunch of integers, we to 
use iLok integer locations - and that makes it behave - indexing wise just 
like numpy now why vector is numpy, so we can just index into it directly 
and then we're going to keep track About minimum of each size, and so then 
the only other thing we really need an ensemble is somewhere to make a 
prediction, and so we were just going to do the mean of the tree prediction 
for each tree, all right. So that was that and so then, in order to be able 
to run that we need a decision tree class because it's being called here 
and so there we go okay. So that's the starting point. So the next thing we 
need to do is to flesh out our decision tree. So the important thing to 
remember is all of our randomness happened back here in the tree. Ensemble, 
the decision tree class, we're going to create, doesn't have randomness in 
it. Okay, so all right now we are building a random B regressor right.</p>

<p>So 
that's why we're taking the mean of the tree, the outputs? If we were to 
work with classification, do we take the max like the classifier will give 
you either zeros or ones? No, I would still take the mean so the so each 
tree is going to tell you what percentage of that leaf. Node contains cats 
and what percentage to take contains dogs. So then I would average all 
those percentages and say across the trees. On average, there is 19 % cats 
and 81 percent dogs good question, so you know random tree classifiers are 
almost identical or can be almost identical. The random tree regresses the 
technique. We're going to use to build this today will basically exactly 
work for a classification. It's certainly for binary classification. You 
can do with exactly the same code for multi-class classification. You just 
need to change your data structure, but so that, like you, have like a one, 
hot encoded matrix or a list of integers that you treat as a one hot 
encoded matrix okay. So our decision tree so remember our idea here is that 
we're going to like try to avoid thinking so we're going to basically write 
it as if everything we need already exists. Okay, so we know from when we 
created the decision tree, we're kind of pass in the X, the Y and the 
minimum leaf size. So here we need to make sure we've got the X and the y 
and the minimum left sides.</p>

<p>Okay, so then there's one other thing, which 
is, as we split our tree into sub trees, we're going to need to keep track 
of which of the row indexes, went into the left-hand side of the tree, 
which went into the right-hand side of the tree. Okay, so we're going to 
have this thing called indexes as well right, so at first we just didn't, 
bother, passing and indexes at all. So if indexes is not passed in, if it's 
none, then we're just going to set it to everything the entire length of Y 
right, so NP dot. A range is the same as just range in Python, but it 
returns an umpire rate right so that the root of a decision tree contains 
all the roads. That's the definition really of the root of a decision tree. 
So all the rows is Rho naught. Rho 1, Rho, 2, etc up to row y -1. Okay is 
going to store away all that information that we were given we're going to 
keep track of. How many rows are there and how many columns are there? 
Okay, so then the every leaf and every node in a tree has a value. It has a 
prediction that prediction is just equal to the average of the dependent 
variable okay, so every node in the tree Y indexed with the indexes is the 
values of the dependent variable that are in this branch of the tree. And 
so here is the main. Some nodes in a tree also have a score which is like 
how effective was the split here right, but that's only going to be true if 
it's not a leaf node right, a leaf.</p>

<p>Node has no further splits, and at this 
point, when we create a tree, we haven't done any splits yet so it's score 
starts out as being infinity. Okay, so having built that the root of the 
tree, our next job, is to find out, which variable should we split on and 
what level of that variable? Should we split on so let's pretend that 
there's something that does them find bass bit so then we're done. Okay, so 
how do we find a variable to split on so well? We could just go through 
each potential. Variable so C contains the number of columns we have so go 
through each one and see if we can find a better split than we have so far 
on that column, okay, now notice. This is like not the full random forest 
definition. This is assuming that max features they're set to all right. 
Remember we could set max features too, like 0.5, in which case we wouldn't 
check all the numbers should not to see. We would check half the numbers at 
random from not to see so. If you want to turn this into like a random 
forest, that has the max features support, we could easily like add one 
line of code to do that, but we're not going to do it in our implementation 
today. So then we just need to find better split and since we're not 
interested in thinking at the moment for now we're just going to leave that 
empty alright, so there one other thing I like to do with my kind of word 
start: writing a class is I'd Like to have some way to print out, what's in 
that class, all right, and so if you type print followed by an object or if 
it Jupiter notebook, you just type the name of the object.</p>

<p>At the moment, 
it's just printing out underscore underscore main underscore underscore got 
decision tree at blah, blah blah, which is not very helpful right. So if we 
want to replace this with something helpful, we have to define the special 
Python method named dan direct crack to get a representation of this 
object. So when we type when we see please just write the name like this 
behind the scenes that calls that function and the default implementation 
of that method is just to print out this unhelpful stuff. So we can replace 
it by instead saying, let's create a format string where we're going to 
print out N and then show N and then print vowel and then show Val okay. So 
how many? How many rows are in this node and what's the average of the 
dependent variable? Okay, then, if it's not a leaf node, so if it has a 
split, then we should also be able to print out the score. The value we 
split out and the variable that we split on now, you'll notice here self 
dot is leaf, is leaf, is defined as a method, but I don't have any 
parentheses after it. This is a special kind of method, code of property, 
and so a property is something that kind of looks like a regular variable, 
but it's actually calculated on the fly. So when I call is leaf, it 
actually calls this function right, but I've got this special decorator 
property, okay, and what this says is basically, you don't have to include 
the parentheses when you call it okay, and so it's going to say all right. 
Is this a leaf or not so a leaf is something that we don't spit on.</p>

<p>If we 
haven't split on it, then it's score is still set to infinity. So that's my 
logic. That makes sense so this uh, this at notation, is called a 
decorator. It's basically a way of telling Python more information about 
your method. Does anybody here remember where you have seen decorators 
before we pass it over you yeah? Where have you seen that where have you 
seen decorators tell us more about flask and Wow yeah? What is that that no 
words so flasks, so anybody who's done any web programming before with 
something like flask or a similar framework would have had to have said, 
like this method is going to respond to this bit of the URL and either to 
post or to Get and you put it in a special decorator, so behind-the-scenes, 
that's telling Python to treat this method in a special way. So here's 
another decorator, okay, and so you know, if you get more advanced with 
Python, you can actually learn how to write your own decorators, which, as 
was mentioned, you know basically insert some additional code but for now 
just know, there's a bunch of predefined decorators. We can use to change 
how our methods behave, and one of them is a property which basically means 
you don't have to put parentheses anymore, which of course means you can't 
add any more parameters beyond self.y. If it's not belief, why is this for 
infinity? Because infinity mean you're at the root, why no infinity means 
that you're not at the root? It means you're at a leaf.</p>

<p>So the root will 
have a split assuming we find one, but everything will have a split till we 
get all the way to the bottom and leaf, and so the leaves will have a score 
of infinity because they won't split great all right. So that's our 
decision tree, it doesn't do very much, but at least we can like create an 
ensemble right. Ten trees sample size, a thousand right and we can make 
print out. So now, when I go M trees, zero, it doesn't say blah blah blah 
blah blah. It says what we asked it to say: n called the thousand now 10.8. 
Oh wait, okay, and this is the leaf because we haven't spit on it. Yet so 
we've got nothing more to say, okay, so then the indexes are all the 
</p>

<h3>4. <a href="https://youtu.be/O5F9vR2CNYI?t=32m45s">00:32:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Single Branch</b></li>

<li><b>Find the best split given variable with ‘find_better_split’, using Excel demo again</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Numbers from nought to a thousand okay, because the base of the tree has 
everything. This is like everything in the random sample that was passed to 
it because remember by the time we get to the point where it's a decision 
tree where we don't have to worry about any of the randomness in the random 
forest anymore. Okay, all right! So, let's try to write the thing which 
finds a split okay, so we need to implement, find better split, okay, and 
so it's going to take the index of a variable. Variable number one variable 
number three whatever, and it's going to figure out. What's the best blit 
point: is that better than any split we have so far and for the first 
variable the answer will always be yes, because the best one so far is none 
at all, which is infinity bad okay. So, let's start by making sure we've 
got something to compare to so the thing we're going to compare two will be 
scikit-learn, x', random forest, and so we need to make sure that psychic 
learns. Random forest gets exactly the same data that we have. So we start 
out by creating ensemble grab a tree out of it and then find out which 
particular random sample of x and y. Did this tree use okay and we're going 
to store them away so that we can pass them to scikit-learn. So we have 
exactly the same information, so let's go ahead and now create a random 
forest using scikit-learn. So one tree, one decision, no bootstrapping, so 
the whole the whole data set. That's oh.</p>

<p>This should be exactly the same as 
the thing that we're going to create this tree. Okay, so let's try so we 
need to define, find better split. Okay, so fine, better split takes a 
variable; okay, so let's define our x independent variables and say: okay. 
Well, it's everything inside our tree, but only those indexes that are in 
this node right, which at the top of the tree, is everything all right, and 
just this one variable okay and then for our Y's. It's just whatever a 
dependent variable is at the indexes in this node. Okay, so there's our X & 
amp Y. So let's now go through every single value in our independent 
variable and so I'll show you what's going to happen. So let's say our 
independent variable is um ade and not going to be an order right and so 
we're going to go to the very first row and we're going to say: okay, yeah 
mate here is three right, and so what I'm going to do is I'm Going to try 
and calculate the score if we decided to branch on the number three 
alright, so I need to know which rows are greater than three which rows are 
less than equal to three and they're, going to become my left-hand side, my 
right hand, side, but And then we need a score right, so there's lots of 
schools we could use so in random forests. We call this. The information 
gain right. The information gain is like how much better does our score get 
because we split it into these two groups of data. There's lots of ways we 
could calculate it.</p>

<p>Jinni cross-entropy root mean squared error whatever, 
if you think about it, there is an alternative formulation of root, mean 
squared error, which is mathematically the same to within a constant scale, 
but it's a little bit easier to deal with, which is we're gon na try and 
find A split which the causes the two groups to each have as lower 
standard, deviation as possible right so like. I want to find a spirit that 
puts all the cats over here and all the dogs over here right. So if these 
are all cats - and these are all dogs, then this has a standard deviation 
of zero, and this has a standard deviation of zero or else this is like a 
total around a mix of cats and dogs. This is a totally random mix of cats 
and dogs. They're going to have a much higher standard deviation, make 
sense, and so it turns out, if you find a split that minimizes those group 
standard deviations or specifically the weighted average of the true 
standard deviations. It's mathematically. The same as minimizing the root 
mean square error, that's something you can prove to yourself after class. 
If you want to okay, so we're going to need to find first of all split this 
into two groups, so where's all the stuff that is greater than three. So 
greater than three is this one, this one and this one? So we need the 
standard deviation of that.</p>

<p>So let's go ahead and say standard deviation of 
greater than three that one that one and that one okay and then the next 
will be the standard deviation of less than or equal to three. So that 
would be that one that one that one and then we just take the weighted 
average of those two and that's our score. That would be our score if we 
split on three that make sense, and so then the next step would be try to 
spit on four try spitting on one try spitting on six redundantly. Try 
splitting on four again redundantly, try spitting on one again and find out 
which one works best. So that's our code here is we're going to go through 
every row, and so let's say okay left hand. Side is any values in X that 
are less than or equal to this particular value. Our right hand, side is 
every value in X that are greater than this particular value. Okay, so 
what's the data type that's going to be in LHS and RHS? What are they 
actually going to contain they're going to be arrays arrays of what rays of 
erosive audience yeah, which we can treat a zero and one okay, so LHS will 
be an array of false every time, it's not less than or equal to and true 
otherwise, And RHS will be a boolean array of the opposite. Okay and now we 
can't take a standard deviation of an empty set right. So if there's 
nothing that's greater than this number, then these will all be false, 
which means the sum will be zero.</p>

<p>Okay - and in that case, let's not go any 
further with this step because there's nothing to take the standard 
deviation of, and it's obviously not a useful split. Okay. So assuming 
we've got this far, we can now calculate the standard deviation of the 
left-hand side and of the right-hand side and take the weighted average or 
the sums the same thing to us to a scaler right and so there's a score. And 
so we can then check is this better than our best score so far and our best 
score so far we initially initialized it to infinity right so initially 
this is. This is better. So if it's better, let's store away well, as the 
information we need, which variable has found this better split, what was 
the score we found and what was the value that we spit on? Okay, so there 
it is. So if we run that and I'm using time it so what time it does is that 
sees how long this command takes to run and it tries to give you a kind of 
statistically valid measure of that. So you can see here, it's run run at 
ten times to get an average and then it's done that seven times to get a 
mean and standard deviation across runs, and so it's taking me 75 
milliseconds, plus or minus ten okay. So, let's check that this works find 
bladder split, tree zero, so zero is year made one is machine hours current 
meter, so I with one we got back machine hours, current meter, thirty, 
seven, four, four with this score and then we ran it again with zero. 
That's year made and we've got a better score: 658 and split 1974 and so 
1974. Let's compare yeah.</p>

<p>That was what this treated as well. Okay, so 
we've got. We've confirmed that this method is doing is giving the same 
result that, as K loans, random forests did. Okay - and you can also see 
here the value 10 point - oh eight and again matching here the value. Ten 
point: oh eight, okay! So we've got something that can find once bit. Could 
you pass that to your net, please so, Jeremy? Why don't we put a unique on 
the eggs there because I'm not trying to optimize the performance yet, but 
do you see that no like he is doing more yeah, so it's like and you can see 
in the excel? I like checked this one twice. I check this four twice: 
unnecessarily: yeah, okay, so and so you're not already thinking about 
performance, which is good. So tell me: what is the computational 
complexity of this section of the code and and like ever think about it, 
but also like feel free to talk us through it? If you want to kind of think 
and talk at the same time, what's the computational complexity of this 
piece of code, can I pass it over there? Yes, all right, Jay take us 
through your thought process. I think you have to take each different 
values through the column, to calculate it once to see those splits so and 
then compare oh the cup, like all the possible combinations between these 
different values, so that can be expensive like this yours huh.</p>

<p>Can you do, 
somebody else would have tell us the actual computational complexity so 
like yeah, quite high Jayde's thinking how high I think it's great okay. So 
tell me: why is it N squared, Oh because for the full loop it is in yes? 
And I think I guess the standard deviation well ticket in so it's in 
square, okay or um. This one, maybe is even is yet to know like this is 
like which ones are less than X. I I'm gon na have to check every value to 
see if it's less than X, I okay, and so so it's useful to know like how do 
I quickly calculate computational complexity? I can guarantee most of the 
interviews you do are going to ask you to calculate computational 
complexity on the fly, and it's also like when you're coding. You want it 
to be second nature, so the technique is basically. Is there a loop, okay 
with then we're obviously doing this end times? Okay, so there's an N 
involved. It's there a loop inside the loop. If there is, then you need to 
multiply those two together in this case. There's not. Is there anything 
inside the loop? That's not a constant time thing, so you might see a sort 
in there and you just need to know that sort is n. Log n like that, should 
be second nature. If you see a matrix multiply, you need to know what that 
is in this case. There are some things that are doing element wise array 
operations right, so keep an eye out for anything where lump, I is doing 
something to every value of an array in this case is checking every value 
of x against a constant.</p>

<p>So it's going to have to do that n times so to 
flesh this out into a computational complexity. You just take the number of 
things in the loop and you multiply it by the highest computational 
complexity. Inside the loop n times, n is N squared and you pass them in 
this case. Couldn't we just pre sort the list and then do like 1 + log n 
computation? There's lots of things we can do to speed this up. So at this 
stage is just like what is the computational complexity we have, and but 
absolutely it's certainly not as good as it can be. Okay, so and that's 
where we're going to go next, just like alright N squared is not is not 
great. So, let's try and make it </p>

<h3>5. <a href="https://youtu.be/O5F9vR2CNYI?t=45m30s">00:45:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Speeding things up</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Better, so here's my attempt at making it better and the idea is this. Ok, 
who wants to first of all, tell me: what's the equation for standard 
deviation, Masha, can you grab the pose so for the standard deviation, it's 
the difference between the value and its mean. It's we take a square root 
of that, so that we take the the power of two. Then we sum up all of these 
situations and we take the square root out of all this sum. Yeah, you have 
to fight divided by M yep, yep great good. Okay, now, in practice we don't 
normally use that formulation because it kind of requires us calculating. 
You know, X, minus the mean lots of times. Does anybody know the 
formulation that just requires X and x? Squared anybody happen to know that 
one? Yes at the back? Can I pass that back there square root of a mean of 
squares, squared off mean yeah great mean of squares, minus the square of 
the means right. So that's a really good. 1/8. That's a really good one to 
know because, like you can now calculate variances or standard deviations 
of anything, you just have to first of all grab the column as it is the 
column squared right and as long as you've got those stored away somewhere, 
you can immediately Calculate the standard deviation, so the reason this is 
handy for us is that if we first of all sort our data right, let's go ahead 
and sort our data.</p>

<p>Then, if you think about it, as we kind of start going 
down one step at a time right, then each group, it's exactly the same as 
the previous group on the left hand, side with one more thing in it and on 
the right hand, side with one Less thing in it, so, given that we just have 
to keep track of some of X and some of x squared, we can just add one more 
thing to X, one more thing x, squared on the left and remove one thing on 
the right. Okay, so we don't have to go through the whole lot each time, 
and so we can turn this into a order n algorithm. So that's all I do here 
is I sort the data right and they're going to keep track of the count of 
things on the right, the sum of things on the right and the sum of squares 
on the right and initially everything's on the right hand, side. Okay, so 
initially n is the count. Y sum is the sum on the right and Y squared sum 
is the sum of squares on the right, and then nothing is initially on the 
left, so it's zeros, okay and then we just have to loop through each 
observation. Right and add one to the left hand, count subtract one from 
the left right hand, count add the value to the left hand, count subtract 
it from the right hand, count add the value squared, to the left hand, 
subtract it from the right hand. Okay. Now we do need to be careful, 
though, because if we're saying less than or equal to one say we're, not 
stopping here we're stopping here like we have to have everything in that 
group. So the other thing I'm going to do is I'm just going to make sure 
that the next value is not the same as this value.</p>

<p>If it is I'm going to 
skip over it right, so I'm just going to double check that this value and 
the next one aren't the same okay. So as long as they're, not the same, I 
can keep going ahead and calculate my standard deviation. Now passing in 
the count, the sum and the sum squared right and there's that formula: 
okay, the sum is squared divided by the square of the sum. So i minus the 
square of the sum. I do that's the right hand side, and so now we can 
calculate the weighted average score just like before, and all of these 
lames are now the same. Okay, so we've turned our order and square an 
algorithm into an order n algorithm and in general stuff like this, is 
going to get you a lot more value than like pushing something onto a spark 
cluster or ordering faster ram or using more cores and your cpu Or whatever 
right, this is the way you want to be. You know, improving your code and 
specifically write your code right without thinking too much about 
performance run. It is it fast enough for what you need then you're done, 
if not profile it right, so in Jupiter. Instead of seeing percent time at 
you say, % p run and it will tell you exactly where the time was spent in 
your algorithm and then you can go to the bit. That's actually taking the 
time and think about like okay is this: this is algorithmically as 
efficient as a can be okay.</p>

<p>So in this case, we run it and we've gone down 
from 76 milliseconds to less than 2 milliseconds, and now some people that 
are new to programming think like oh great, I've saved 60, something 
milliseconds. But the point is this: is going to get run like tens of 
millions of clients, okay, so the 76 millisecond version is so slow that 
it's got to be impractical for any random forest. You using in practice 
right where else the one millisecond version I found is actually quite 
quite acceptable and then check. The numbers should be exactly the same as 
before, and oh yeah, okay, so now that we have a function, find better 
split. That does what we want. I want to insert it into my decision tree 
class, and this is a really cool Python trick. Python does everything 
dynamically right, so we can actually say the method called find better 
split in decision tree. Is that function I just created and that might 
sticks it inside that class now I'll, tell you what's slightly confusing 
about this. Is that this thing this word here and this word here: they 
actually have no relationship to each other. They just happen to have the 
same letters in the same order right so like I could call this find better 
split, underscore foo right and then I could like call that right and call 
that right so now my function is actually called fine, better split, 
underscore foo, But my method, I'm expecting to call something called 
decision tree dot, fine, better split, all right, so here I could say 
decision tree dot, fine, better split equals, find better split, underscore 
foo! Okay, you see that's the same thing.</p>

<p>Okay, so like it's important to 
understand how namespaces work like in in every language that you use, one 
of the most important things is kind of understanding how how it figures 
out what a name refers to. So this here means find better split as to find 
inside this class right and nowhere else right. Well, I mean there's a 
parent class, but never mind about that. This one here means find better 
split. Fou in the global namespace, a lot of languages, don't have a global 
namespace that Python does okay, and so the two are like, even if they 
happen to have the same letters in the same order. They're not referring in 
any way to the same thing. That makes sense. It's like this family over 
here may. Have somebody called Jeremy and my family has somebody called 
Jeremy and our names happen to be the same, but we're not the same person, 
okay, great! So now that we've stuck the decision tree sorry, I did a fine 
Bettis flip method inside the decision tree with his new definition. When I 
now call the tree ensemble constructor all right. The decision tree 
ensemble instructor called create tree create tree instantiated decision 
tree decision tree called find vas whit, which went through every column to 
see if it could find a better split and we've now defined find better split 
and therefore tree ensemble when we create it has Gone ahead and done this 
wet that makes sense, don't have any anybody have any questions, 
uncertainties about that like we're, only creating one single split so far, 
all right, so this is pretty pretty neat right.</p>

<p>We kind of just do a little 
bit at a time testing everything as we go, and so it's as, as, as you all 
implement the random </p>

<h3>6. <a href="https://youtu.be/O5F9vR2CNYI?t=55m">00:55:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Full single tree</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Forest interpretation techniques you may want to try programming this way 
too, like every step check that you know what you're doing matches up with 
what scikit-learn does or with a test that you've built or whatever. So at 
this point, we should try to go deeper very inception. Right so let's go 
now max depth is two, and so here is what scikit-learn did after breaking 
it in made 74, it then broke at Machine hours later 29:56, so we had this 
thing called find violet right, which just went through every column and 
try to see. If there's a better split there, all right, but actually we 
need to go a bit further than that. Not only do we have to go through every 
column and see if there's a better split in this node, but then we also 
have to see whether there's a better split in the left and the right sides 
that we just created right. In other words, the left right side and the 
right-hand side should become decision, trees themselves right. So there's 
no difference at all between what we do here to create this tree and what 
we do here to create this tree other than this one contains 159 samples, 
and this one contains a thousand. So this row of codes exactly the same as 
we had before. Right and then we check it actually, we could do this a 
little bit easier. We could say if self dot is leaf. Right would be the 
same thing. Hey don't leave it here for now, so it's self dot score. So if 
the score is infinite.</p>

<p>Still, let's write it properly, yes, wait! So: let's 
go back up and just remind ourselves is leaf. Is self that's poor equals in 
okay. So since it's there, we mostly use it. So if it's a leaf node, then 
we have nothing further to do right. So that means we're right at the 
bottom. There's no split. That's been made okay, so we don't have to do 
anything further. On the other hand, if it's not a leaf node, so it's 
somewhere back earlier on, then we need to split it into the left hand. 
Side and the right hand side now earlier on, we created a left hand side in 
the right hand, side, array of bullying's right now, better would be to 
have here. We have an array of indexes and that's because we don't want to 
have a full array of all the volumes in every single node right because 
remember, although it doesn't look like there are many nodes, when you see 
a tree of this size when it's fully expanded, The bottom level, if there's 
a minimum leaf size of one, contains the same number of nodes as the entire 
data set, and so, if every one of those contained a full boolean array of 
size of the whole data set, you've got squared memory requirements which 
would be Bad right, on the other hand, if we just store the indexes there 
for things in this node and that's going to get smaller and smaller, okay, 
so NP non.</p>

<p>Zero is exactly the same as just this thing, which gets the 
boolean array, but it turns it into the indexes of the truths okay, so this 
is now a list of indexes for the left-hand side and indexes to the 
right-hand side. Alright. So now that we have the indexes the left-hand 
side and the right-hand side, we can now just go ahead and create a 
decision tree. Okay, so there's a decision tree for the left and there's 
our decision tree for the right. Okay - and we don't have to do anything 
else - we've already written these - we already have a function of a 
constructor that can create a decision tree so like when you really think 
about what this is doing. It kind of hurts your head right, because the 
reason the whole reason that fine vast bit got called is because find vasp 
lit is called by the decision tree constructor. But then the decision tree 
that then find vast bit itself then causes the decision tree constructor. 
So we actually have circular recursion and I'm not nearly smart enough to 
be able to think through recursion. So I just choose not to write like I 
just write what I mean and then I don't think about it anymore. Right like 
what do I want well to find a variable-speed I've got to go through a few 
column, see if there's something better. It had managed to do a split 
figure out left-hand side of the right-hand side and make them into 
decision trees, okay, but now try to think through how these two methods 
call each other would just drive me crazy, but I don't need to write.</p>

<p>I 
know I have a decision tree constructor that works no. No, no. I have a 
vine up find basket that works. So that's it right. That's how I do 
recursive programming is by pretending. I don't I just just ignore it. 
That's my advice. A lot of you are probably smart enough to be able to 
think through it better than I can. So that's fine! If you can all right so 
now that I've written that again, I can patch it into the decision tree 
class and as soon as I do, the tree ensemble constructor will now use that 
right because pythons dynamic right, that's just happens automatically. So 
now I can check my left-hand side should have 159 samples right and a value 
of nine point. Six six there. It is 159 samples, nine point, six six right 
hand: side, huh, 841, 10.15, the left hand, side of the left hand, side, 
150, samples, nine point, six to 150 samples; nine point: six! Okay, so you 
can see like I'm, because I'm not nearly clever enough to write machine 
learning algorithms like not only can I not write them correctly. The first 
time, often like every single line, I write, will be wrong right. So I 
always start from the assumption that the the line of code I just typed is 
almost certainly wrong and I just have to see why and how right and so 
like. I just make sure, and so eventually I get to the point where, like 
much to my surprise, it's not broken </p>

<h3>7. <a href="https://youtu.be/O5F9vR2CNYI?t=1h1m30s">01:01:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Predictions with ‘predict(self,x)’,</b></li>

<li><b>and ‘predict_row(self, xi)’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Anymore, you know so here I can feel like okay, this. It would be 
surprising if all of these things accidentally happen to be exactly the 
same as scikit-learn, so this is looking pretty good. Okay, so now that we 
have something that can build a whole tree where you want to have something 
that can calculate predictions right and so remind you, we already have 
something that calculates predictions for a tree ensemble by calling 
trade-up predict. But there is nothing called treetop predict, so we're gon 
na have to write that to make this more interesting. Let's start bringing 
up the number of columns that we use. Let's create our tree ensemble again 
and this time let's go to a maximum depth of three okay. So now our tree is 
getting more interesting and, let's now define how do we create a set of 
predictions for a tree, and so a set of predictions for a tree is simply 
the prediction for a row for every row. That's it all right, that's our 
predictions! So the predictions for a tree are every rows predictions in an 
array. Okay, so again we're like skipping thinking, thinking's hard, you 
know so, let's just like keep pushing it back. This is kind of Handy right 
notice that you can do four in array with an umpire array, regardless of 
the rank of the array, regardless of the number of axes in the array and 
what it does is. It will look through the leading axis.</p>

<p>At least these 
concepts are going to be very, very important as we get into more and more 
neural networks, because we're going to be all doing, tensor computations 
all the time. So the leading axis that the vector is the vector itself, the 
leading axis of a matrix, are the rows. The leading access axis of a three 
dimensional tensor, the matrices that represent the slices and so forth 
right. So, in this case, because X is a matrix, this is going to look 
through the rows and if you write your kind of tensor code, this way then 
it'll kind of tend to generalize nicely to higher dimensions like it 
doesn't really mention matter. How many dimensions are in X? This is going 
to loop through each of the leading axis okay, so we can now call that 
decision tree. Do I predict alright? So all I need to do is write, predict 
row right and I've delayed thinking so much, which is great that the actual 
point where I actually have to do the work - it's now basically trivial. 
So, if we're at a leaf, no, then the prediction is just equal to whatever 
that value was which we calculated right back in the original tree. 
Constructor is to assist the average of the Y's right. If it's not a leaf 
node, then we have to figure out whether to go down the left-hand path or 
the right-hand path to get the prediction right. So if this variable in 
this row is less than or equal to that thing, we decided the amount we 
decided to split on. Then we go down the left path.</p>

<p>Otherwise we go down 
the right path: okay and then having figured out what path we want which 
tree we want, then we can just call predict row on that. Right and again, 
we've accidentally created something recursive again. I don't want to think 
about how that works. Control flow wise or whatever, but I don't need to 
because, like I just it just does like, I just told it what I wanted so 
I'll trust it to work right if it's a leaf return, the value otherwise 
return. The prediction for the left, hand, side or the right hand side as 
appropriate. There notice this here this, if has nothing, to do with this, 
if all right this, if is a control flow statement that tells Python to go 
down on that path or that path to do some calculation, this, if is an 
operator that returns a value, so those Of you that have done C or C++ will 
recognize it as being identical to that. It's called the ternary operator. 
All right, if you haven't that's fine, basically, what we're doing is we're 
going to get a value where we're going to say it's this value. If this 
thing is true and that value otherwise - and so you could write it this way 
right, but that would require writing four lines of code to do one thing 
and also require you to code that, if you read it to yourself or to 
somebody else, is Not at all, naturally, the way you would express it 
right, I want to say the tree I going to go down is the left-hand side, if 
the variables less than the split or the right-hand side, otherwise right, 
so I want to write my code the way I Would think about all the way I would 
say my code, okay, so this kind of ternary operator can be quite helpful 
for that.</p>

<p>Alright. So now that I've got a prediction for a row, I can dump 
that into my class, and now I can create calculate predictions, and I can 
now plot my actuals against my predictions. When you do a scatter plot, 
you'll often have a lot of dots sitting. On top of each other, so a good 
trick is to use alpha. Alpha means how transparent the things not just a 
map plot lib but like in every graphics package in the world pretty much, 
and so, if you set alpha to less than 1, then this is saying you would need 
20 dots on top of each other for it To be fully blue, and so this is a good 
way to kind of see how much things are sitting on top of each other. So 
it's a good trick but trick the scatter plots. There's my R squared, not 
bad, and so let's now go ahead and do a random forest with no max mana 
spitting and our tree ensemble had no max amount of spitting. We can 
compare our R squared there are squared and so they're not the same, but 
actually ours is a little better. So I don't know what we did differently, 
but we'll take it okay, so we have now something which, for a forest with a 
single tree in is giving as good accuracy on a validation set using an 
actual real-world data set. You know books for pluto's is compared to 
scikit-learn, so let's go ahead and round this out. So what I would want to 
do now is to create a package that has this coding, and I created it by 
like creating a method here - a method here, a method here and catching 
them together.</p>

<p>So what I did with now is: I went back through in 
</p>

<h3>8. <a href="https://youtu.be/O5F9vR2CNYI?t=1h9m5s">01:09:05</a></h3>

<ul style="list-style-type: square;">

<li><b> Putting it all together,</b></li>

<li><b>Cython an optimising static compiler for Python and C</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>My notebook and collected up all the cells had implemented methods and 
pasted them all together right and I've just pasted them down here. So 
here's this is my original tree ensemble and here is all the cells and the 
decision tree. I just dumped them all into one place without any change, so 
that was it. That was the code we wrote together so now I can go ahead and 
I can create a tree ensemble. I can calculate my predictions. I can do my 
scatter plot. I can get my r-squared right and this is now with five trees 
right and here we are. We have a model of blue dog for bulldozers, with a 
71 %, a squid with a random forest. We wrote entirely from scratch. That's 
pretty cool any questions about that and I know there's like quite a lot to 
get through. So I during the week feel free to ask on the forum about any 
bits of code you come across. Can somebody pass the box to Mercia others? 
Can we get back to the probably to the top, maybe a decision tree when we 
said the score equal to infinity right? Yes, I do a calculator Scott, the 
score for the I mean, like I lost track of that, and specifically, I wonder 
when we implement when we implement, find VAR split, we check for self 
score equal to whether it's equal to infinity or not. It says to me: it 
seems like I'm clear whether we fall out of this. I mean like if we ever 
implement the method, if, if our initial value is infinity, so okay, let's 
talk sure the logic.</p>

<p>So so the decision tree starts out with a score at 
infinity, so in other words, at this point, when we've created the mode, 
there is no split, so it's infinitely bad. Okay, that's why the score is 
infinity and then we try to find a variable and a split that is better and 
do that we look through each column and say: hey column. Do you have a 
split which is better than the best one we have so far, and so then we 
implement that let's do the slow way since it's a bit simpler, find better 
split. We do that by looping through each row and finding out this is the 
current score. If we split here, is it better than the current score? The 
current score is infinitely bad. So, yes, it is, and so now we set the new 
score equal to what we just calculated and we keep track, of which variable 
we chose and the split we spit on. Okay, no worries; okay, great, let's 
take a five-minute break and I'll see you back here at 22. So when I tried 
comparing the performance of this against scikit-learn, this is quite a lot 
slower and the reason. Why is that, although, like a lot of the works being 
done by numpy, which is nicely optimized, c-code think about like the very 
bottom level of a tree? If we've got a million data points and the bottom 
level of the tree has something like 500 thousand decision points with a 
million leaves underneath right, and so that's like five hundred thousand 
split methods being called each one of contained, which contains multiple 
calls to numpy, which Only have like one item that's actually being 
calculated on, and so it's like that's like very inefficient and it's the 
kind of thing that Python is particularly not good at performance.</p>

<p>Wise 
right, like calling lots of functions, lots of times I mean we can see 
it's. It's not bad right, you know, for a kind of a random forest which 15 
years ago would have been considered pretty big. This would be considered 
pretty good performance right, but nowadays this is some hundreds of times 
at least slower than than it should be. So what the scikit-learn folks did 
to avoid this problem was that they wrote their implementation in something 
called siphon and siphon is a superset of Python. So any Python you've 
written pretty much. You can use as siphon right, but then what happens is 
siphon runs it in a very different way, rather than passing it to the kind 
of the Python interpreter it instead converts it to C, can Kyle's that and 
then runs that C code right, which means the First time you run it, it 
takes a little long work, so it has to go through the kind of translation 
and compilation, but then, after that it can be quite a bit faster, and so 
I want to just to quickly show you what that looks like, because You are 
absolutely going to be in a position where siphons going to help you with 
your work and most of the people you're working with will have never used. 
It may not even know it exists, and so this is like a great superpower to 
have so to use siphon in a notebook. You say, load next load, extension 
siphon right and so here's a Python function bit. One here is the same as a 
siphon function is exactly the same thing with percent % at the top.</p>

<p>This 
actually runs about twice as fast as this right, just because it does their 
compilation. Here is the same version again where I've used a special 
siphon extension called C death, which defines the C data type of the 
return value and of each variable right, and so, basically, that's the 
trick that you can use to start making things run quickly right and At that 
point now it knows it's not just some Python object called T. In fact I 
probably should put one here as well. Let's try that so we've got fib 2. We 
call that 53, so 453 yeah. So it's exactly the same as before, but we say 
what the data type of the thing we passed to it was is and then define the 
data types of each of the variables. And so then, if we call that okay, 
we've now got something that's 10 times faster right, so yeah, it doesn't 
really take that much extra and it's just it's just Python with a few 
little bits of markup. So that's like it's it's good to know that that 
exists, because if there's something custom you're trying to do, it's 
actually a find it kind of painful having to go out and you know going to 
see and compile it and whip it back and all that. Where else doing it here 
is pretty easy, can you pass that just your right, please not sure so when 
you're doing like for the second version of it, so in the case, an array 
for an NP array, this is a specific C type of yeah.</p>

<p>So there's like a lot 
of um specific stuff for integrating scythe on with numpy and there's a 
whole page about it yeah. So we won't worry about going over it, but you 
can read that and you can basically see the basic ideas. There's this C 
import which basically imports a certain types of Python library into the 
kind of the C bit of the code, and you can then use it in your siphon yeah. 
It's it's pretty straightforward! Well, good! </p>

<h3>9. <a href="https://youtu.be/O5F9vR2CNYI?t=1h18m1s">01:18:01</a></h3>

<ul style="list-style-type: square;">

<li><b> “Your mission, for next class, is to implement”:</b></li>

<li><b>Confidence based on tree variance,</b></li>

<li><b>Feature importance,</b></li>

<li><b>Partial dependence,</b></li>

<li><b>Tree interpreter.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Question, thank you all right, so your your mission now is to implement 
confidence based on tree variance, feature importance, partial dependence 
in tree interpreter for that random, first, removing redundant features 
doesn't use a random forest at all. So you don't have to worry about that. 
The extrapolation is not an interpretation technique, so you don't have to 
worry about that, so it's just the other ones. So confidence based on tree 
variance, we've already written that code. So I suspect that the exact same 
code we would have in the notebook should continue to work, so you can try 
and make sure it get that working feature. Importance is with the variable 
shuffling technique, and once you have that working partial dependence, it 
will just be a couple of lines of code away rather than you know, rather 
than shuffling a column you're, just replacing it with a constant value 
that it's nearly the same code And then tree interpreter it's going to 
require you writing some code and thinking about that well, ince you've, 
written tree interpreter you're, very close if you want to to creating the 
second approach to feature importance, the one where you add up the 
importance across all of the Rows, which means you would then be very close 
to doing interaction importance, so it turns out that that there are 
actually there's actually a very good library for interaction importance 
for extra boost, but there doesn't seem to be one for random forests, so 
you could, like start By getting it working on our version and if you want 
to do interaction importance and then you could like get it working on the 
original site, scikit-learn version, and that would be a cool contribution. 
All right, like sometimes writing it against your own implementation, is 
kind of nicer because you can see exactly what's going on all right.</p>

<p>So 
that's that's your job! Now you don't have to rewrite the random forest 
feel free to. If you want to you, know practice. So if you get stuck at any 
</p>

<h3>10. <a href="https://youtu.be/O5F9vR2CNYI?t=1h20m15s">01:20:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Reminder: How to ask for Help on Fastai forums</b></li>

<li><b><a href="http://wiki.fast.ai/index.php/How_to_ask_for_Help">http://wiki.fast.ai/index.php/How_to_ask_for_Help</a></b></li>

<li><b>Getting a screenshot, resizing it.</b></li>

<li><b>For lines of code, create a “Gist”, using the extension ‘Gist-it’ for “Create/Edit Gist of Notebook” with ‘nbextensions_configurator’ on Jupyter Notebook, ‘Collapsible Headings’, ‘Chrome Clipboard’, ‘Hide Header’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Point you know ask on the forum right: there is a whole page here on wiki 
dot fast play I about how to ask for help. So when you ask your co-workers 
on slack for help, when you ask people in a technical community on github 
or discourse for help or whatever asking for help, the right way will go a 
long way towards you know having people want to help you and be able To 
help here right so so, like search for your aunt's, like search for the 
arrow you're, getting see if somebody's already asked about it, you know 
how have you tried to fix it already? What do you think's going wrong? What 
kind of computer are you on? How is it set up? What are the software 
versions exactly? What did you type at exactly what happened right now? You 
could do that by taking a screenshot, so you know make sure you've got some 
screenshot software, that's really easy to use. So if I were to take a 
screenshot, I just hit a button. Select the area copy to clipboard, go to 
my forum, paste it in and there we go right. That looks a little bit too 
big. So, let's make it a little smaller all right, and so now I've got a 
screenshot. People can see exactly what happened better still, if there's a 
few lines of code and error messages to look at and create a gist. It just 
is a handy little github thing which basically lets you share code. So if I 
wanted to create a gist of this, I actually have a extension area that 
little extension.</p>

<p>So if I click on here, give it a name say: make public 
okay, and that takes my Jupiter notebook shares it publicly. I can then 
grab that URL copy link, location right and paste it into my forum post 
right and then, when people click on it, then they'll immediately see my 
notebook when it renders okay. So that's a really good way now that 
particular button is an extension. So on Jupiter, you need to click, end 
the extensions and click on just it right while you're there. You should 
also click on collapsible headiness. That's this really handy thing. I use 
that lets me collapse things and open them up. If you go to your Jupiter 
and don't see this MB extensions button, then just Google for Jupiter and B 
extensions it'll show you how to pip, install it and and get it set up 
right where those two extensions are </p>

<h3>11. <a href="https://youtu.be/O5F9vR2CNYI?t=1h23m15s">01:23:15</a></h3>

<ul style="list-style-type: square;">

<li><b> We’re done with Random Forests, now we move on to Neural Networks.</b></li>

<li><b>Random Forests can’t extrapolate, it just averages data that it has already seen, Linear Regression can but only in very limited ways.</b></li>

<li><b>Neural Networks give us the best of both worlds.</b></li>

<li><b>Intro to SGD for MNIST, unstructured data.</b></li>

<li><b>Quick comparison with Fastai/Jeremy’s Deep Learning Course.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Superduper handy alright, so other than that assignment, where we're done 
with random forests and until the next course, when you look at gPMs, we're 
done with decision tree ensembles and so we're going to move on to neural 
networks broadly defined, and so Dero networks are going to Allow us to to 
go beyond just you know: the kind of nearest neighbors approach of random 
forests. You know all around and forests can do - is to average data that 
it's already seen it can't extrapolate it can't. They can't calculate 
right, linear regression can calculate and can extrapolate, but only in 
very limited ways. Neural nets give us the best of both worlds, we're going 
to start by applying them to unstructured data all right, so unstructured 
data means like pixels or the amplitudes of sound waves or words. You know 
data where everything in all the columns are all the same type. You know, 
as opposed to like a database table where you've got like a revenue and a 
cost and a zip code and a state. It should be structured data, we're going 
to use it for structured data as well, but we're going to do that a little 
bit later. So structured data is a little easier and it's also the area 
which more people have been applying deep learning to for longer the. If 
you're doing the deep learning course as well, you know you'll see that 
we're going to be approaching kind of the same conclusion from two 
different directions.</p>

<p>So the deep learning course is starting out with big, 
complicated convolutional neural networks being solved with you know, 
sophisticated optimization schemes and we're going to kind of gradually 
drill down into like exactly how they work. Where else, with the machine 
learning course we're going to be starting out more with like how does 
stochastic gradient descent actually work, what do we do? What can we do is 
like one single layer which would allow us to create things like logistic 
regression when we add regularization to that? How does that give us things 
like Ridge, regression, elastic net lasso and then, as we add additional 
layers to that? How does that? Let us handle more complex problems, and so 
we're not going to we're only going to be looking at fully connected layers 
in this machine learning course, and then I think next semester, with your 
net you're, probably going to be looking at some more sophisticated 
approaches and so Yeah, so this machine learning we're going to be looking 
much more at like what's actually happening with the matrices and how they 
actually calculated and the deep learning it's much more like what are the 
best practices for actually solving. You know at a world-class level, 
real-world deep learning problems right so next week we're going to be 
looking at like the classic Emnes problem, which is like how do we 
recognize digits now, if you're interested you can like skip ahead and like 
try and do this with A random forest and you'll find it's not bad, but it 
given that a random forest is basically a type of nearest neighbors right. 
It's finding like what are the nearest neighbors in entry space, then a 
random forest could absolutely recognize that this nine, those pixels you 
know are similar to pixels we've seen in these other ones and on average 
they were nines as well right so like it can absolutely Solve these kinds 
of problems to an extent using random forests, but we end up being rather 
data limited, because every time we put in another decision point, you know 
we're having our data roughly, and so this is this limitation and the 
amount of calculation that we can Do where else with neural nets, we're 
going to be able to use lots and lots and lots of parameters using these 
tricks? We don't learn about with regularization, and so we're going to be 
able to do lots of computation and there's got to be very little limitation 
on really what we can actually end up calculating as a result.</p>

<p>Great good 
luck with your random forest interpretation, and I will see you next time. 
</p>






  </body>
</html>
