<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Lesson 08</title>
    <meta charset="UTF-8">
  </head>
  <body>
  <p style="text-align: right"><a href="http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826">fast.ai: Intro to Machine Learning 1 (v1) (2018)</a></p>
  <h1>Lesson 08</h1>
  <h2>Outline</h2>
<ul>

<li>Building a Decision Tree from scratch</li>
<li>Optimizing and comparing to SKlearn</li>
<li>How to do 2 levels of decision trees</li>
<li>Fleshing out the RF predict function</li>
<li>Assembling our own decision tree</li>
<li>Cython</li>

</ul>


  <h2>Video Timelines and Transcript</h2>


<h3>1. <a href="https://youtu.be/DzE0eSdy5Hk?t=45s">00:00:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Moving from Decision Trees Ensemble to Neural Nets with Mnist</b></li>

<li><b>lesson4-mnist_sgd.ipynb notebook</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>So I I don't want to embarrass Rachel, but I'm very excited that Rachel's 
here. So this is Rachel for those of you that don't know she's not quite 
back on her feet after her illness, but well enough to at least come to at 
least part. With this lesson so worried if she can't stay for the whole 
thing and I'm really glad she's here, because Rachel actually wrote the 
vast majority of the lesson we're gon na see it's, I think it's a really 
really cool work. So I'm glad she's gon na at least see it being taught, 
even if unfortunately, she's not teaching it herself so good Thanksgiving 
present best for Thanksgiving present. So where, as we discussed at the end 
of last lesson, we're kind of moving from the C decision tree ensembles to 
two neural Nets, broadly defined and as we discussed you know, random 
forests and decision trees are limited by the fact. In the end that 
they're, basically they're basically doing nearest neighbors right, you 
know that all they can do is to get return the average of a bunch of other 
points, and so they can't extrapolate out to you know if you're thinking 
what happens if I'll increase my Prices by 20 % and you've never priced at 
that level before or what's going to happen to sales next year, and 
obviously, we've never seen next year before it's very hard to extrapolate. 
It's also hard, if it needs to you, know like it, can only do around log 
base two n decisions, you know, and so, if there's like a time series, it 
needs to fit too.</p>

<p>That takes like four steps to kind of get to the right 
time area. Then suddenly, there's not many decisions left for it to make. 
So it's kind of this limited amount of computation that it can do so. 
There's a limited complexity of relationship that it can model yes Prince. 
Can I ask about one more drawback of random forests yeah. So if we have a 
data as categorical variable, which are not in sequential order so for 
random forests, we and coat them and treat them as numbers. Let's say we 
have 20 cardinality and 1 to 20. So the result at random for our skills is 
like the spirit and important skills is something like less than 5 less 
than 6. But if the categories are not sequential not in any order, what 
does that mean yeah? So so, if you've got like, oh, let's go back to 
bulldozers, erupts erupts with AC erupts na, I don't know whatever right 
and we arbitrarily label them like this right, and so actually we know that 
all that really mattered was if it had air conditioning. So, what's going 
to happen? Well, it's basically going to say like okay, if I group it into 
those together and those together that, like that's an interesting break 
just because it so happens that the air conditioning ones are going to end 
up in the right hand, side and then having done that. Right, it's then 
going to say: okay well within the group, with the two and three, it's 
going to notice that it's furthermore going to have to split it into two 
more groups.</p>

<p>So eventually it's going to get there it's going to pull out 
that category. It's just! It's going to take more splits than we would 
ideally like, so it's kind of similar to the fact that they get to model a 
line. It can only do it with lots of splits and only approximately let's, 
just fine, with categories that are not sequential, also yeah. So I can't 
do it. It's just like in some way it's sub-optimal because we just need to 
do more breakpoints than we would have liked, but it gets there. It does a 
pretty good job, and so, even although random forests, you know, do you 
have some deficiencies? They're, incredibly powerful, you know, 
particularly because they have so few assumptions they really had to screw 
up, and you know it's kind of hard to actually win Kaggle competition with 
a random first, but it's very easy to get like 10 %. So in like in real 
life, where often that third decimal place doesn't matter, random, forests 
are often like what you end up doing. But for some things like this 
Ecuadorian groceries competition, it's very very hard to get a good result 
with a random forest because, like there's a huge time series component and 
like nearly everything, is these two massively high cardinality categorical 
variables, which is the store and the item And like so this, so there's 
very little there to even throw at a random forest and the you know, the 
difference between every pair of stores is kind of different in different 
ways. And so you know there are some things that are just hard to get.</p>

<p>Even 
relatively good results for the random forest, another example is 
recognizing numbers you can get like okay results with a random forest, but 
in the end, they're kind of the relationship between you know like that. 
The spatial structure turns out to be important right and you kind of want 
to be able to do like computations like finding edges or whatever that kind 
of carry forward through through the computation. So you know just doing a 
kind of a clever nearest neighbors like a random forest. You know turns out 
not to be ideal. So if it's stuff, like this neural networks, turn out that 
they are ideal, neural networks turn out to be something that works 
particularly well. For both things, like the Ecuadorian, groceries, 
competition, so forecasting, sales over time, buy, store and buy item and 
for things like recognizing digits and for things like turning voice into 
speech, and so it's kind of nice between these two things: neural nets and 
random forests. We kind of cover the territory right. I don't. I haven't 
needed to use anything other than these two things for a very long time and 
will actually learn. I don't know them what course exactly, but at some 
point we'll learn also how to combine the two, because you can combine the 
two in really cool ways. So here's a picture from Adam guide key of an 
image. So an image is just a bunch of numbers. Right and each of those 
numbers is not to 255, and the dark ones are too close to 255 white ones 
are close to zero.</p>

<p>All right, so here is an example of a digit from this 
amnesty data set amnesties are really old. It's like a hello world of 
machine of neuro-networks, and so here's an example, and so there are 28 by 
28 pixels if it was color. There would be three of these one for red one 
for green okay, so our job is to look at. You know the array of numbers and 
figure out that this is the number eight which is tricky right. How do we 
do that, so we're going to use a few, a small number of fastai pieces and 
we're gradually going to remove more and more and more until by the end, 
we'll have implemented our own euro network from stretch our own training 
loop from scratch? And our own matrix multiplication from scratch, so we're 
gradually going to dig in further and further alright. So the data for 
amnesty, which is the name of this very famous data, set, is available from 
here, and we have a thing in fast: AI dot, IO called get data which will 
grab it from a URL and store it from your on your computer. Unless it's 
already there, in which case it'll, just go ahead and use it okay and then 
we've got a little function here, called load em nest which simply loads 
</p>

<h3>2. <a href="https://youtu.be/DzE0eSdy5Hk?t=8m20s">00:08:20</a></h3>

<ul style="list-style-type: square;">

<li><b> About Python ‘pickle()’ pros &amp; cons for Pandas, vs ‘feather()’,</b></li>

<li><b>Flatten a tensor</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>It up you'll see that it's zipped, so we could just use pythons gzip to 
open it up, and then it's also pickled. So if you have any kind of Python 
object at all, you can use this built-in etham library called pickle to 
dump it out onto your disk, share it around load it up later and you get 
back the same Python object. You started with so you've already seen this 
something like this with, like pandas, feather format. Right pickle is not 
just for pandas, it's not just for anything. It works for basically nearly 
every object so which might lead to the question. Well, why didn't we use 
pickle for a pandas data frame right and the answer is pickle works for 
nearly every Python object, but it's probably not like optimal for nearly 
any Python object right. So because, like we were looking at pandas 
dataframes with like over a hundred million rows, we really want to save 
that quickly and so feather is a format. That's specifically designed for 
that purpose, and so it's going to do that really fast. If we tried to 
pickle it, it would have been taken a lot longer. Right also note that 
pickle files are only for Python, so you can't give them to somebody else. 
Where else like a feather file, you can hand around yeah. So it's worth 
knowing that pickle exists, because if you've got some dictionary or some 
kind of object floating around that you want to save for later or send to 
somebody else, you can always just pickle it.</p>

<p>Okay. So in this particular 
case, the folks at deep learning network kind enough to provide a pickled 
version. Pickle has changed slightly over time and so old pickle files like 
this one you actually have to this - was a Python to one. So you have to 
tell it that it was encoded using this particular Python, 2 character set, 
but other than that Python, 2 & amp 3. You can normally open each other's 
pickle files all right so once we floated that in we loaded in like so, and 
so this thing, which we're doing here this is called D structuring and so D 
structuring means that load M nest is giving us back a couple Of tuples, 
and so if we have, on the left hand, side of the equal sign a couple of 
tuples, we can fill all these things in so we're given back a couple of 
training data, a couple of validation data and a couple of test data. In 
this case, I don't care about the test data, so I just put it into a 
variable called score, which kind of by like people in pick Python people 
tend to think of underscore as being a special variable, which we put 
things we're going to throw away Into it's actually not special, but it's 
just it's really common. If you see something assigned to underscore, it 
probably means you're, just throwing it away right by the way in a jupiter 
notebook, it does have a special meaning, which is the last cell that you 
calculate is always available in underscore by the way. But that's kind of 
a separate issue.</p>

<p>So then, the first thing in that topple is itself at 
Apple and so we're going to stick that into X & amp Y for our training data 
and then the second one goes into X & amp Y for our validation data. Okay, 
so that's called destructuring and it's pretty common in lots of languages. 
Some languages don't support it, but those that do life becomes a lot 
easier. So as soon as I you know, look at some new data set, I just check 
out. What's what if I got all right so what's its tight? Okay, it's an 
umpire right, what's its shape, it's 50,000 by seven, eight four and then 
what about the dependent variables? That's an array. Its shape is 50,000, 
so this image is not of length. Seven, eight! Four! It's at size, 28 by 28. 
So what happened here? Well, we could guess, and we can check on the 
website. It turns that we would be right that all they did was. They took 
the second row and concatenate it to the first row and the third row and 
concatenate it to that and the fourth row and competitive's of that. So, in 
other words, they took this whole 28 by 28 and flattened it out into a 
single 1d array. That makes sense so it's going to be of size 28 squared. 
This is not like normal by any means. So don't think, like everything you 
see is going to be like this most of the time when people share images they 
share them as JPEGs or PNG s, you load them up.</p>

<p>You get back a nice 2d 
array, but in this particular case, for whatever reason, the thing that 
they pickled was flattened out to be 784, and this word flatten is very 
common with you know: kind of working with tenses. So when you flatten a 
tensor, it just means that you're turning it into a a lower rank tensor 
than you start up with. In this case, we started with a rank two tensor and 
the matrix for each image, and we turned each one into a rank. One tensor 
ie a vector so overall, the whole thing you know is a rank two matrix for 
</p>

<h3>3. <a href="https://youtu.be/DzE0eSdy5Hk?t=13m45s">00:13:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Reminder on the jargon: a vector in math is a 1d array in CS,</b></li>

<li><b>a rank 1 tensor in deep learning.</b></li>

<li><b>A matrix is a 2d array or a rank 2 tensor, rows are axis 0 and columns are axis 1</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>A rank two tensor, rather than a rank three tensor. So just to remind us of 
you know the jargon here this and math. We would call a vector right in 
computer science. We would call it a 1d array, but because deep learning 
have people have to come across as smarter than everybody else, we have to 
call this a rank: 1 tensor, okay, they all mean the same thing more or 
less, unless you're, a physicist, in which case this Means something else 
and you get very angry at the deep learning people, because you say it's 
not a tensor. So there you go, don't blame me. This is just what people 
say. So this is either a matrix or a 2d array or a rank two tensor, and so 
once we start to get into three dimensions, we start to run out of 
mathematical names right, which is why we start to be nice just to say, 
rank three tensor and So there's actually nothing special about vectors and 
matrices that make them in any way more important than rank three tenses or 
rank for tensors or whatever. So I try not to use the terms vector and 
matrix where possible, because I don't really think they're there any more 
special than any other rank of tensor. Okay, so kind of it's good to get 
used to thinking of this as a rank, two tensor, okay and then the the rows 
and columns. If it was a if we're computer science people, we would call 
this dimension zero and dimension one.</p>

<p>But if we're deep learning people, 
we would call this axis zero or access one, okay and then just to be really 
confusing if you're an image person. This is the first axis, and this is 
the second axis all right. So if you think about like TVs, you know 1920 by 
1080, columns by rows everybody else, including deep-learning and 
mathematicians rows by columns. So this is pretty confusing. If you use 
like the Python imaging library you get back columns by rows, pretty much 
everything else rows by columns, so be careful because they hate us because 
they're bad people. I guess I mean there's a lot of just um, particularly 
in deep learning like a whole. Lot of different areas have come together 
like information theory, computer vision, statistics signal, processing and 
you've ended up with this hodgepodge of nomenclature in deep learning, 
often like every version of things will be used. So today we're going to 
hear about something. That's called either negative log likelihood or 
binomial or categorical cross entropy, depending on where you come from, 
we've already seen something that's called either one hot encoding or dummy 
variables, depending on where you come from. It really is just like the 
same concept gets kind of somewhat independently invented in different 
fields and eventually they find their way to machine learning, and then we 
don't know what to call them. So we call them all of the above. Something 
like that.</p>

<p>So I think that's what's happened with with computer vision, 
rows and columns. So there's this idea of normalizing data, which is 
subtracting out the mean and dividing by the standard deviation. So a 
</p>

<h3>4. <a href="https://youtu.be/DzE0eSdy5Hk?t=17m45s">00:17:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Normalizing the data: subtracting off the mean and dividing by stddev</b></li>

<li><b>Important: use the mean and stddev of Training data for the Validation data as well.</b></li>

<li><b>Use the ‘np.reshape()’ function</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Question for you do like often it's important to normalize the data so that 
we can more easily train a model. Do you think it would be important to 
normalize the independent variables for a random forest if we're training a 
random forest, be honest? I don't know why. We don't need to normalize. I 
just know that we've done okay. Does anybody want to think about? Why kara 
it wouldn't matter, because each scaling and transformation we can have 
will be applied to each row and we will be computing means, as we were 
doing like local averages and at the end we will of course want to 
denormalize it back to give. So it wouldn't change the results I'm doing 
about the independent variables, not the dependent variable. I taught to 
us. The world depend: okay, let's have a go Matthew, it might be because we 
just care about the relationship between the independent variables, an 
independent variable, so scale doesn't matter. Okay come on cat. Why? Why? 
Well? Why do we own? We could like, because at each split point we can just 
divide to see which, regardless of what scale you're on what minimizes 
variance, and that would right so really. The key is that, when we're 
deciding where to split all that matters is the order like it all. That 
matters is how they're sorted. So if we divide by the subtract the mean and 
divide by the standard deviation, they're still sorted in the same order. 
So remember when we implemented the random first, we said sort them and 
then we liked it.</p>

<p>Then we completely ignored the values we just said, like 
now add on one thing from the dependent at a time, so so random forests 
only care about the sort order of the independent variables they don't care 
at all about their size. And so that's why they're wonderfully immune to 
outliers, because they totally ignore the fact that it's an outlier they 
only care about which one's higher than what other thing right. So this is 
an important concept. It doesn't just appear in random forests. It occurs 
in some metrics as well, for example, area under the ROC curve. You come 
across a lot that area under the ROC curve completely ignores scale and 
only cares about sort. We saw something else when we did the dendrogram 
Spearman's correlation is a rank. Correlation only cares about order not 
about scale so random forests. One of the many wonderful things about them 
are that we can completely ignore a lot of these statistical distribution 
issues, but we can't for deep learning because for deep learning, we're 
trying to train a parameterised model. So we do need to normalize our data. 
If we don't, then it's going to be much harder to create a network that 
trains effectively. So we grab the mean and the standard deviation of our 
training data and subtract out the mean divided by the standard deviation, 
and that gives us a mean of 0 and a standard deviation of 1. Now, for our 
validation data, we need to use the standard deviation and mean from the 
training data right.</p>

<p>We have to normalize it the same way. Just like 
categorical variables. We had to make sure they had the same indexes, 
mapped to the same levels for a random forest or missing values. We had to 
make sure we have the same median used when we were replacing the missing 
values. You need to make sure anything you do in the training set. You do 
exactly the same thing and the test and validation set. So here I'm 
subtracting out the training set me in the training set standard deviation. 
So this is not exactly zero. This is not exactly one, but it's pretty 
close, and so in general, if you find you try something on a validation set 
or a test set, and it's like much much much worse than your training set. 
That's probably because you normalized in an inconsistent way or encoded 
category is an inconsistent way or something like that all right. So let's 
take a look at some of this data, so we've got 10,000 images in the 
validation set and each one is a rank. One tensor of length: seven, eight, 
four. In order to display it, I want to turn it into a rank: two tensor of 
28 by 28, so there's a dump, a has a reshape function that takes a tensor 
in and reshapes it to whatever size. Tensor. You request. Now, if you think 
about it, you only need to tell it about. If there are d axes, you only 
need to tell it about D minus one of the axes you want, because the last 
one it can figure out for itself right. So in total there are 10,000 by 784 
numbers here altogether right.</p>

<p>So if you say well, I want my last axes to 
be 28 by 28. Then you can figure out that this must be 10,000. Otherwise 
it's not going to fit. It makes sense. So if you put minus 1, it says like 
make it as big or as small as you have to to make it fit, and so you can 
see here it figure it out. It has to be 10,000, so you'll see this used in 
neural net software, pre-processing and stuff, like that, all the time like 
I could have written 10,000 here, but I try to get into a habit of like 
anytime I'm referring to like how many items in my Input I tend to use 
minus one because, like it just means later on, I could like use a 
subsample. This code wouldn't break. I could you know, do some kind of 
stratified sampling. It was unbalanced. This code wouldn't break so by 
using this kind of approach of saying, like minus one here for the size, it 
just makes it more resilient to change us later. It's a good habit to get 
into so this kind of idea of, like being able to take tensors and reshape 
them and and and change axes around and stuff like. That is something you 
need to be like totally do without thinking, because it's going to happen 
all the time so, for example, here's one I tried to read in some images 
they were flattened. I need to unflattering them into a bunch of matrices. 
Okay, reshape thing. I read some: I read some images in with OpenCV and it 
turns out OpenCV orders the channels, blue, green red. Everything else 
expects them to be red, green blue.</p>

<p>I need to reverse the last access. How 
do you do that? I read in some images with place and imaging library. It 
reads them, as you know, rows by columns by channels quiet which expects 
channels by rows by columns. How do I transform that? So these are all 
things you need to be able to do without thinking like straightaway, 
because they just it happens all the time and you never want to be sitting 
there. Thinking about it for ages, so make sure you spend a lot of time 
over the week. Just practicing with things like all the stuff ago to see 
today, reshaping slicing reordering dimensions stuff like that, and so the 
best way is to create some small tensors yourself and start thinking like 
okay. What shall I experiment with so here? Can we pass that over there? A 
backtrack a little bit, of course. I love it so back in normalize. You say 
like you might have gone over this, but I'm still like wrestling with a 
little bit. Yes in many machine learning, algorithms or Milan yeah, but you 
also just said that scale isn't really mattered. So I said it doesn't 
matter for random forests yeah, so random forests are just kind of split 
things based on order, and so we loved them. We love random forests, 
further away there so immune to worrying about distributional assumptions, 
but we're not doing random forests. We're doing deep learning and deep 
learning does care.</p>

<p>Can you pass it over there? We have a parametric, don't 
we should skill? No, not quite right because, like K, nearest neighbors is 
nonparametric and scale matters a helluva lot. So I would say things 
involving trees, generally you're just going to split at a point, and so 
probably you don't care about scale. But you know you probably just need to 
think like. Is this an algorithm that uses order or does it use specific 
numbers, and can you please give us an intuition of why it needs scale? 
Just Bureau says that would make modify simulations not until we get to 
doing SGD, so we're going to get to that yeah. So for now we're just gon na 
say take my word for it: okay, positive Daniel. So this is probably a dumb 
question, but can you like explain a little bit more? What you mean by 
scale, because I guess when I think of scale I'm like? Oh all, the numbers 
should be generally the same size. That's like you mean, but is that, like 
the case like with the cats and dogs that we went over with like the deep 
learning like, you could have a small cat and like a larger cat, but it 
would still know that those were both cats. Oh, I guess you know this is 
one of these problems where language gets overloaded, yeah. So in computer 
vision, when we scale an image, we're actually increasing the size of the 
cat, in this case we're scaling the actual pixel values.</p>

<p>So, in both case, 
Kali means to make something bigger and smaller in this case we're taking 
armors from naught to 255 and making them so that they have an average of 0 
and a standard deviation unit. One Jeremy, could you please explain us? Is 
it by column by row by pixel by pixel, so when you're scaling, I'm just not 
thinking about every picture, but I am kind of an input yeah, so, okay yeah 
sure. So I mean it's a little bit subtle, but in this case I've just got a 
single mean and a single standard deviation right. So it's basically on 
average how how much black is there right and so on average you know we 
have a mean and a standard deviation across all the pixels in computer 
vision. We would normally do it by channel, so we would normally have one 
number for read. One number for green one number for blue in general: you, 
you need a different set of normalization coefficients for each leg. Each 
thing you would expect to behave differently so if we were doing like a 
structured data set where we've got like income distance in kilometers and 
a number of children, but you need three separate normalization 
coefficients for those they're like very different kinds of things. So 
yeah. It's kind of like a bit domain-specific here, it's like in this case. 
All of the pixels are, you know, levels of gray, so we just got a single 
scaling number.</p>

<p>Where else you could imagine if they were red versus cream 
versus blue, you could need to scale those channels in different ways and 
plus they're better, please. So I'm having a little bit of trouble 
imagining what would happen if you do normalize in this case um. So we'll 
get there so for next, oh wait, so this is kind of what your net was saying 
is like. Why would we normalize and for now we're normalizing, because I 
say we have to when we get to looking at stochastic gradient descent will 
basically discover that if you basically to skip ahead a little bit, we're 
going to be doing a matrix multiply by a bunch of Weights, we're going to 
pick those weights in such a way that, when we do the matrix multiply we're 
going to try to keep the numbers at the same scale that they started out as 
and that's going to basically require the initial numbers. We're going to 
have to know what their scale is, so basically, it's much easier to create 
a single kind of neural network architecture that works for lots of 
different kinds of inputs. If we know that they're consistently going to be 
mean, 0 standard deviation, 1. That would be the short answer, but we'll 
learn a lot more about it and if, in a couple of lessons, you're still not 
quite sure why let's come back to it, because it's a really interesting 
thing to talk about. Yes, I'm just trying to visualize the axes.</p>

<p>We're 
working with you so under plots when you, when you write x, valid shape, we 
get ten thousand by seven, eight four yeah that mean that we brought in 
10,000 pictures yeah of that dimension, exactly okay and then in the next 
line. When you choose to reshape it, is there a reason why you put 28 28 on 
azzam Y or z, coordinates, or is there a reason why they're in that order 
yeah there is pretty much all neural network libraries assume that the 
first axis it's like kind of The equivalent of a row, it's like a separate 
thing, it's a sentence or an image, or you know, example of sales or 
whatever. So I want each image you know to be as a separate item of the 
first axis and then so that leaves two more axes for the rows and columns 
of the images and that's pretty standard. That's totally standard yeah. I 
don't think I've ever seen a library that doesn't work. That way. Can you 
pass the table here so, while normalizing the validation data, I saw you 
have used mean of X and standard deviation of X data training data. Only 
yes, so shouldn't we use mean and standard deviation of validation data you 
mean like join them together or separately. Goodwill didn't mean no, 
because you see, then you would be normalizing. The validation set using 
different numbers, and so now the meaning of like this. This pixel has a 
value of 3 in the validation set, has a different meaning to the meaning of 
3 in the in the training set.</p>

<p>It would be like if we had like days of the 
week encoded such that Monday was a 1 in the training set and was a 0 in 
the validation set. We've got now two different sets where the same number 
has a different meaning. So we we want to make sure that we. So let me give 
an example: let's say we were doing like full-color images and our tests. 
Their training set can contained like green frogs, green snakes and gray 
elephants right we're trying to figure out which was which now we 
normalized using you know the each channel mean, and then we have a 
validation set and the test set, which are just green frogs and Green 
snakes, so if we would have normalized by the validation sets statistics, 
we would end up saying things on average a green, and so we would like 
remove all the greenness out, and so we would now fail to recognize the 
green frogs and the green snakes effectively. Right so we actually want to 
use the same normalization coefficients that we were training on and for 
those of you doing the deep learning class we actually go further than that 
when we use a pre tracking network, we have to use the same normalization 
coefficients that the Original authors trained on so the idea is that you 
know that the a number needs to have this consistent, meaning across every 
data set where you use it.</p>

<p>How can you pass the test meter? That means when 
you are looking at the test set, you normalize the test set based on this. 
This meanness, that's right: okay, , so um, here's a you know, so so the 
valid validation y-values. I just rank one tensor of 10,000. Remember, 
there's this kind of weird Python thing where at Apple with just one thing 
in it, there's a trailing comma okay. So this is a rank, one tensor of 
length 10,000 and so here's an example of something from that. It's just 
the number three. So that's our </p>

<h3>5. <a href="https://youtu.be/DzE0eSdy5Hk?t=34m25s">00:34:25</a></h3>

<ul style="list-style-type: square;">

<li><b> Slicing into a tensor, ‘plots()’ from Fastai lib.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Labels so here's another thing: you need to be able to do in your sleep 
slicing into a tensor, so in this case we're slicing into the first axis 
with 0, so that means we're grabbing the first slice so because this is a 
single number, this is going To reduce the rank of the tensor by one, it's 
going to turn it from a 3-dimensional tensor into a two-dimensional tensor 
right. So you can see here. This is now just a matrix and then we're going 
to grab 10 through 14 inclusive rows. 10, through 14 inclusive columns - 
and here it is right. So this is the kind of thing you need to be super 
comfortable, like grabbing pieces out looking at the numbers and looking at 
the picture right. So here's an example of a little piece of that. First 
image - and so you kind of want to get used to this idea - that, if you're 
working with something like pictures or audio, you know this is something 
your brain is really good at interpreting right. So like keep showing 
pictures of what you're doing whenever you can. But also remember behind 
the scenes, they're numbers so like if something's going weird print out a 
few of the actual numbers you might find. Somehow some of them have become 
infinity or they're, all 0 or whatever right so like use this interactive 
environment and to explore the data as you go.</p>

<p>Did you have a question with 
just a quick? I guess semantic question: why, when it's a tensor of Rank 3, 
is it stored as like XYZ, instead of like tamiya, would make more sense to 
store it as like a list of like 2d tensors its let's do it as either right 
so remember the formatting, because, Let's look at this as a 3d okay, so 
here's a 3d right, so a 3d tensor is formatted as showing a list of 2d 
tensors, basically, but when you're extracting it, why isn't it like if 
you're extracting the first one? Why isn't it X images square brackets? 0, 
close square brackets and then a second set of squares, because this has a 
different meaning right. So it's kind of the difference between tenses and 
jagged arrays right. So basically, if you do like something like something 
like that, that says take the second list item and from it grab the third 
list item. And so we tend to use that when we have something called a 
jagged array, which is where each sub array may be of a different length 
right. Where else we have like a single object of three dimensions and so 
we're trying to say like which little piece of it do we want, and so the 
idea is that that is a single slice object to go in and grab that piece 
out. Ok, so here's an example of a few of those images along with their 
labels and this kind of stuff. You want to be able to do pretty quickly 
with matplotlib.</p>

<p>It's it's going to help you a lot in in life in your exam, 
so you can have a look at you know what Rachel wrote here when she wrote 
plots we can use, we can use, add sub plot to basically create those little 
separate plots and you Need to know that I am show is how we basically take 
a numpy array and draw it as a picture. Okay and then we've also added the 
title on top, so there it is all right. So, let's you know, </p>

<h3>6. <a href="https://youtu.be/DzE0eSdy5Hk?t38m20s">00:38:20</a></h3>

<ul style="list-style-type: square;">

<li><b> Overview of a Neural Network</b></li>

<li><b>Michael Nielsen universal approximation theorem: a visual proof that neural nets can compute any function</b></li>

<li><b>Why you should blog (by Rachel Thomas)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Take that data and try to build a neural network with it and so a neural 
network and sorry this is going to be a lot of review. For those of you 
already doing deep learning, a neural network is just a particular 
mathematical function or a class of mathematical functions, but it's a 
really important class because it has the property it supports. What's 
called the universal approximation theorem, which is that which means that 
a neural network can approximate any other function arbitrarily closely 
right, so in other words, it can do in theory. It can do anything as long 
as we make it big enough. So this is very different to a function like 3x 
plus 5 right, which can only do one thing. It's a very specific function or 
the class of functions ax plus B, which can only represent lines of 
different slopes, moving it up and down different amounts, or even the 
function. Ax, squared plus BX, plus C plus sine D. You know again only can 
represent a very specific subset of relationships. The neural network, 
however, is a function that can represent any other function to arbitrarily 
close accuracy right. So what we're going to do is we're going to learn how 
to take a function. So, let's take like ax plus B and we're going to learn 
how to find its parameters, in this case a and B which allow it to fit as 
closely as possible to a set of data.</p>

<p>And so this here is showing example 
from a notebook that we'll be looking at in deep learning course, which 
basically shows what happens when we use something called stochastic 
gradient descent to try and set a and B and basically what happens is we're 
going to pick a Random, a to start with a random B to start with, and then 
we're going to basically figure out. Do I need to increase or decrease a to 
make it closer at the line closer to the dots? Do I need to increase or 
decrease B to make the line closer to the dots and then just keep 
increasing and decreasing a and B lots and lots of times? Okay? So that's 
what we're going to do and to answer the question: do I need to increase or 
decrease a and B we're going to take the derivative right, so the 
derivative of the function with respect to a and B tells us: how will that 
function? Change as we change a and B all right, so that's basically what 
we're going to do, but we're not going to start with just a line. The idea 
is we're to build up to actually having a neural net, and so it's going to 
be exactly the same idea, but because it's an infinitely flexible function, 
we're going to be able to use this exact same technique to fit arbitrarily 
to arbitrarily complex relationships. Now, that's basically the idea. So 
then what you need to know is that a neural net is actually a very simple 
thing. A neural net actually is something which takes as input.</p>

<p>Let's say, 
we've got a vector does a matrix product by that vector right. So this is 
like this is of size. Let's draw this properly so like if this is sighs. 
Ah, this is like our bye see a matrix product will spit out something of 
size C all right, and then we do something called a non-linearity, which is 
basically we're going to throw away all the negative values. So can so it's 
basically max zero, comma X and then we're going to put that through 
another matrix multiply and then we're going to put that through another 
max zero comma X and we're going to put that through another matrix 
multiply and so on. Right until eventually, we end up with the single 
vector that we want, so, in other words, each stage of our neural network 
is the key thing going on is a matrix multiplied so, in other words, a 
linear function. So, basically, deep learning most of their calculation is 
lots and lots of linear functions, but between each one we're going to 
replace the negative numbers with zeros. Can you yes, so why are we 
throwing away the negative numbers? Well, we'll see right. The short answer 
is: if you apply a linear function to a linear function to a linear 
function, it's still just a linear function, so it's totally useless. But 
if you throw away the negatives, that's actually a nonlinear 
transformation, and so it turns out that if you apply a linear function to 
the thing we threw away the negatives.</p>

<p>That applies that to a linear 
function. That creates a neural network and it turns out. That's the thing 
that can approximate any other function arbitrarily closely, so this tiny 
little difference actually makes all the difference and if you're 
interested in it check out the deep learning video where we cover this, 
because I actually show a nice visual intuitive proof, not something that I 
created that's something that Michael Nielsen created or if you want to 
skip straight to his website, you could go to Michael Nielsen universe, or 
I think I spelled his name wrong, never mind Haitian theorem, . We go 
neural networks and deep learning, chapter four and he's got a really nice 
walkthrough, basically with lots of animations, where you can see why this 
works one. I feel like the the hardest thing. I feel like the hardest thing 
with getting started like technical writing on the Internet is just like 
posting, your first thing, so, if you do a search for Rachel, Thomas medium 
blog you'll find, this will put it on the lesson wiki, where she talks 
about. She actually says the top advice she would give to her. Younger self 
would be to start blogging sooner, and she has like both reasons why you 
should do it some examples of things that you know examples of places. 
She's blog has turned out to be great for her and her career, but then some 
tips about how to get started.</p>

<p>I remember when I first suggested to Rachel 
she might think about blogging, because she had so much interesting to say, 
and you know at first he was kind of surprised at the idea that, like she 
could blog you know, and now people come up to us at Conferences and 
they're, like you're, Rachel Thomas, I love your writing. You know so, like 
I've kind of seen that that transition from like wow could I blog to being 
known as a strong technical author, so yeah so check out this article. If 
you still need convincing or if you're wondering how to get started and 
since the first one is the hardest, maybe your first one should be like 
something really easy for you to write. You know so it could be. Like you 
know, here's a summary of the first 15 minutes of lesson. Three of our 
machine learning costs. You know here's why it's interesting, here's what 
we lit, or it could be like here's, a summary of how I used to random 
forests to solve a particular problem. In my practicum I often get 
questions like on my practicum, my organization. We've got like sensitive 
commercial data. That's fine, like you know, just find another data set and 
do it on that stair to show the example or you know, anonymize all of the 
values and change the names of the variables or whatever, like you, can 
talk to your employer or your practicum partner to Make sure that they're 
comfortable with whatever it is you're writing in general, though you know, 
people love it when they're interns and staff blog about what they're 
working on, because it makes them look super cool.</p>

<p>You know it's like hey, 
I'm on. You know intern working at this company and I wrote this post about 
this cool analysis. I did and then other people would be like wow. That 
looks like a great company to work for so generally speaking, you should 
find people are pretty supportive um, besides, which there's lots and lots 
of data sets out there available. So, even if you can't base it on the work 
you're doing, you can find something similar for sure. Alright, 
</p>

<h3>7. <a href="https://youtu.be/DzE0eSdy5Hk?t=47m15s">00:47:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Intro to PyTorch &amp; Nvidia GPUs for Deep Learning</b></li>

<li><b>Website to buy a laptop with a good GPU: <a href="http://xoticpc.com/">xoticpc.com</a></b></li>

<li><b>Using cloud services like <a href="http://crestle.com/">Crestle.com</a> or AWS (and how to gain access EC2 w/ “Request limit increase”)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>So we're going to start building our neural network, we're going to build 
it using something called a torch. pytorch is a library that basically 
looks a lot like numpy, but when you create some code with pytorch, you can 
run it on the GPU rather than the CPU. So the GPU is something which is 
basically going to be probably at least an order of magnitude, possibly 
hundreds of times faster than the code that you might write for the CPU for 
particularly stuff involving lots of linear algebra right so with deep 
learning neural nets. You can, if you, if you don't, have a GPU, you can do 
it on the CPU right, but it's it's going to be frustratingly slow. Your Mac 
does not have a GPU that we can use for this because I'm actually talking 
today, we need an NVIDIA GPU. I would actually much prefer that we could 
use your Mac's, because competition is great right, but in video, we're 
really the first ones to create a GPU which did a good job of supporting 
general purpose: graphics, programming, units GPGPU. So, in other words, 
that means using a GPU for things other than playing computer games. They 
used, they created a framework called CUDA, see UDA. It's it's a very good 
framework. It's pretty much universally used in deep learning. If you don't 
have an NVIDIA GPU, you can't use it. No current max have an NVIDIA GPU, 
most laptops of any kind. Don't have an NVIDIA GPU. If you're interested in 
doing deep learning on your laptop.</p>

<p>The good news is that you need to buy 
one which is really good for playing computer games on there's a place 
called exotic PC gaming laptops where you can go and buy yourself a great 
lap up for doing deep learning. You can tell your parents that you need the 
money to do deep learning, so could you please have yeah so you'll 
generally find a whole bunch of laptops with names like predator and Viper, 
with pictures of robots and stuff, so stealth, pro Raider leopard anyway. 
Having said that, like, I don't know that many people that do much deep 
learning on their laptop most people will log into a cloud environment. By 
far the easiest. I know a few users called cresol with Crestle. You can 
basically sign up and straightaway. The first thing you get is a throne 
straight into a jupiter notebook, backed by a GPU cost 60 cents an hour 
with all of the fastai libraries and data already available. So that makes 
life really easy. It's less flexible and in some ways less fast, then using 
AWS, which is the Amazon Web Services, option, cost a little bit more 
ninety cents, an hour rather than 60 cents, and but it's very likely that 
your employer is already using that. It's like it's good to get to know 
anyway. They've got more different choices around GPUs and it's a good good 
choice. If you're, Google for github student pack, if you're a student, you 
can get a hundred and fifty dollars of credits straight away pretty much, 
and so it's a really good way to get started Daniel. Did you have a 
question yeah? I just wanted to know your opinion on.</p>

<p>I know that until 
recently published like an open source like way of like boosting like 
regular packages, that they claim is equivalent like, if you use the bottom 
tier GPU on your seat, like on your CPU. If you use their boost packages 
like you, can get the same performance, do you know anything about that 
yeah? I do it's a good question, so I'm actually intro makes some great 
numerical programming libraries, particularly this one called mkl, the 
matrix, colonel library. They they definitely make things faster, they're, 
not using those libraries. But if you look at a graph of performance over 
time, GPUs have consistently throughout the last 10 years, including now 
are about 10 times more floating-point operations per second than the 
equivalent CPU and they're generally about a fifth of the price for that 
performance. So yeah it and then because of that, like everybody doing 
anything with deep learning, basically does it on NVIDIA GPUs and therefore 
using anything other than NVIDIA GPUs is currently very annoying, so slower 
more expensive more annoying. I really hope there will be more activity 
around AMD. Gpus, in particular in this area, but AMD's got like literally 
years of catching up to do so it might take a while yeah. So I just wanted 
to point out that you can also buy at things such as, like a GPU extender 
to a laptop yeah.</p>

<p>That's also like kind of making, like maybe a first step 
solution. If you only really want to yeah yeah, I think for like 300 bucks, 
or so you can buy something that plugs into your Thunderbolt port. If you 
have a Mac and then for another five or six hundred bucks, you can buy a 
GPU to plug into that. Having said that, for about a thousand bucks, you 
can actually create a pretty good. You know GPU based desktop, and so, if 
you're, considering that the fastai forums have like lots of threads, where 
people help each other spec out something at a particular price point 
anyway. So to start with out so use cresol, and then you know, when you're 
ready to invest a few extra minutes getting going use AWS to use AWS you 
basically huh yeah yeah yeah, I'm just talking to the folks online as well. 
Okay, so so AWS! When you get there, go to ec2 ac2, like there's lots of 
stuff on AWS ec2 is the bit where we get to like a rent computers by the 
hour right now. We're gon na need a GPU based instance. Unfortunately, when 
you first sign up for AWS, they don't give you access to them, so you have 
to request that access so go to limits up in the top left right and the 
main GPU instance we'll be using is called the p2 so scroll down to p2 And 
here P, 2 dot X large. You need to make sure that that numbers not 0. If 
you've just got a new account, it probably is 0, which means you won't be 
allowed to create one.</p>

<p>You have to go request, limit increase and the trick 
there is when it asks you. Why do you want the limit increase, type faster, 
AI, because AWS knows to look out and they know that faster day I people 
are good people so they'll. Do it quite quickly that takes a day or two 
generally speaking, to go through so once you get the email saying you've 
been approved for p2 instances, you can then go back here and say launch 
instance, and so we've basically set up one. That has everything you need. 
So if you click on community, ama and ami is an Amazon machine image, it's 
basically a completely set up computer right, and so, if you type fastai or 
one word, you'll find here fastai do part one version two for the p2 right. 
So that's all set up ready to go. So if you click on select and it'll say: 
okay, what kind of computer do you want right, and so we have to say all 
right. I want a GPU compute type and specifically I want to pee to 
extra-large right and then you can say, review and launch. I'm assuming you 
already know how to deal with SSH keys and all that kind of stuff. If you 
don't check out the introductory tutorials and workshop videos that we have 
online or Google around for SSH keys, very important skill to know anyway, 
all right, so hopefully you get through all that you have something running 
on a GPU with the past AI repo.</p>

<p>If you use cresol, just CD fastai to the 
the repo is already there get Paul AWS CD, fastai, the repo is already 
there get Paul. If it's your own computer you'll just have to get clone and 
then away. You go all right. So part of all of those is PI, torch is 
pre-installed and so pytorch basically means we can write code. That looks 
a lot like numpy, but it's going to run really quickly on the GPU. 
Secondly, since we need to know like which direction and how much to move 
our parameters to improve our loss, we need to know the derivative of 
functions. pytorch. Has this amazing thing where any code you write using 
the pytorch library, it can automatically take the derivative of that for 
you. So we're not going to look at any capitalist in this course, and I 
don't look at any calculus in any of my courses or at any of my work, 
basically ever in terms of like actually calculating derivatives myself, 
because I've never had to it's done. For me by the library, so as long as 
you write the Python code, it's the derivative, it's done so the only 
calculus you really need to know to be an effective practitioner is like 
what is it? What does it mean to be a derivative, and you also need to know 
the chain rule which will come all right, so we're going to </p>

<h3>8. <a href="https://youtu.be/DzE0eSdy5Hk?t=57m45s">00:57:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Create a Neural Net for Logistic Regression in PyTorch</b></li>

<li><b>‘net = nn.Sequential(nn.Linear(28*28, 10), nn.LogSoftmax()).cuda()’</b></li>

<li><b>‘md = ImageClassifierData.from_arrays(path, (x,y), (x_valid, y_valid))’</b></li>

<li><b>Loss function such as ‘nn.NLLLoss()’ or Negative Log Likelihood Loss or Cross-Entropy (binary or categorical)</b></li>

<li><b>Looking at Loss with Excel</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Start out kind of top-down, create a neural net and we're going to assume a 
whole bunch of stuff and gradually we're going to dig into each piece right 
so to create neural Nets. We need to import the pytorch, neural net 
library, pytorch. Funnily enough is not called play torch, it's called 
torch, okay, so torch NN is the pytorch subsection, that's responsible for 
neural nets. Okay! So we'll call that n N and then we're going to import a 
few bits out of fastai, just to make life a bit easier for us. So here's 
how you create a neural network, n, pytorch by the simplest possible neural 
network. You say, sequential and sequential means I am now going to give 
you a list of the layers that I want in my neural network right. So in this 
case my list has two things in it. The first thing says I want a linear 
layer. Now a linear layer is something that's basically going to do. Y 
equals ax, plus B right but matrix matrix multiply, not not univariate. 
Obviously, so it's going to do a matrix product. Basically, so the input of 
the matrix product is going to be a vector of length. 28 times 28, because 
that's how many pixels we have and the output needs to be of size 10, we'll 
talk about why in a moment, but for now you know this is how we define a 
linear layer and then again we're going to dig into this in Detail, but 
every linear layer, just about in neural nets, has to have a non-linearity 
after it then we're going to learn about this particular non-linearity. In 
a moment it's called the softmax and if you've done, the DL course you've 
already seen this.</p>

<p>So that's how we define a neuron it. This is a two 
layer: neural net there's also kind of an implicit additional first layer, 
which is the input, but with pytorch you don't have to explicitly mention 
the input that normally, we think conceptually like the input. Image is 
kind of also a layer, because we're kind of doing things pretty manually 
with pytorch, we're not taking advantage of any of their convenience, is in 
fastai. For building your stuff, we have to then write CUDA, which tells 
pytorch to copy this neural network across to the GPU. So now, on from now 
on, that network is going to be actually running on the GPU. We didn't say 
that it would run on the CPU, so that gives us back a neuron it very 
simple, neural net. So we're then going to try and fit the neural net to 
some data. So we need some data. So fastai has this concept of a model data 
object, which is basically something that wraps up: training, data, 
validation, data and optionally test data, and so to create a model data 
object. You can just say I want to create some image. Classifier data I'm 
going to grab it from some arrays right and you just say: ok, this is the 
path. Then I'm going to save any temporary files. This is my training data 
arrays, and this is my validation, data, arrays, ok, and so that just 
returns an object. That's going to wrap that all up and so we're going to 
be able to fit to that data.</p>

<p>So now that we have a neural net and we have 
some data - we're going to come back to this in a moment, but we basically 
say what loss function do we want to use what optimizer do we want to use 
and then we say fit, we say Fit this network to this data going over every 
image once using this loss function this optimizer and print out these 
metrics bang. Ok - and this says here - this is 91.8 % accurate. Ok, so 
that's like the simplest possible neuron it. So what that's doing is it's? 
Creating a matrix multiplication followed by a non-linearity and that it's 
trying to find the values for this matrix, which cause which basically that 
fit the data as well as possible that a product that end up predicting this 
is a 1. This is a 9s is a 3, and so we need some definition for as well as 
possible, and so the general term, for that thing is called the loss 
function. So the loss function is the function. That's going to be lower. 
If this is better right. Just like with random forests, we had this concept 
of information gain and we got to like pick what function do you want to 
use to define information gain and we were mainly looking at root, mean 
square error right, most machine learning, algorithms recall something very 
similar to At loss right, so the loss is: how do we score how good we are, 
and so, in the end, we're going to calculate the derivative of the loss 
with respect to the weight matrix that we're multiplying by to figure out 
how to how to update it right? So we're going to use something called 
negative log likelihood loss so negative log likelihood.</p>

<p>Loss is also known 
as cross entropy, they're literally the same thing, there's two versions: 
one called binary cross, entropy or binary negative log likelihood and 
another court categorical cross. Entropy the same thing. One is for when 
you've only got a zero or one dependent, the other is if you've got like 
cat dog airplane or horse, or at 0, 1 through 9 and so forth. So what we've 
got here is the binary version of cross entropy, and so here is the 
definition. I think maybe the easiest way to understand this definition is 
to look at an example. So let's say we're trying to protect cat versus dog. 
One is cat. Zero is dog, so here we've got cat dog dog cat and here are our 
predictions. We said 90 % sure it's a cat, 90 % sure it's a dog, 80 % sure 
it's a dog, 80 % sure it's a cat all right. So we can then calculate the 
binary cross entropy by calling our function. So it's going to say: okay 
for the first one, we've got y equals 1. So it's going to be 1 times log of 
0.9 plus 1 minus y 1. Minus 1 is 0. So that's going to be skipped, okay and 
then the second one is going to be a 0. So it's going to be 0 times 
something, so that's going to be skipped and the second part will be 1 
minus 0. Ah, so this is 1 times log of 1 minus p1. Minus point. 1 is point 
9, so in other words, the first piece and the second piece of this are 
going to give exactly the same number, which makes sense because the first 
one we said we were 90 %, confident it was a cat and it was and the second 
We said we were 90 %, confident it was a dog and it was so in each case. 
The loss is coming from the fact that you know we could have been more 
confident yeah.</p>

<p>So if we said we're a hundred percent confident the loss 
would have been zero right. So let's look at that in Excel. So here's our 
Oh point, nine point. One point two point: eight right and here's our 
predictions 101. So here's one minus the prediction right here is log of 
our prediction: here is log of one minus our prediction, and so then here 
is our sum. Okay, so if you think about it - and I want you to think about 
this during the week - you could replace this with an if statement rather 
than Y, because Y is always 1 or zero right. Then it's only ever going to 
use either this or this. So you could replace this with an if statement. So 
I'd like you during the week to try to rewrite this with an if statement, 
okay and then see if you can then scale it out to be a categorical 
cross-entropy, so categorical cross-entropy works this way. Let's say we 
were trying to predict three and then six and then seven and then two so if 
we were trying to predict three and the actual thing that was predicted was 
like four point: seven right versus like well actually putting it. This way 
we're trying predict three and we actually predicted five or a try to 
predict three, and we accidentally predicted. Nine like Bing. Five, instead 
of three is no better than being mine instead of three. So we're not 
actually going to say like how far away is the actual number we're going to 
express it differently or to put it another way.</p>

<p>What, if we're trying to 
predict cats, dogs, horses and airplanes, you can't like how far away is 
cat from horse, so we're going to express these a little bit differently 
rather than thinking of it as a three, let's think of it as a vector with a 
1. In the third location, and rather than thinking it was a six, let's 
think of it as a vector of zeros for the one in the sixth location, so in 
other words one hot encoding right. So that's one hot in code, a dependent 
variable, and so that way now, rather than predicting trying to predict a 
single number, let's predict ten numbers. Alright, let's predict, what's 
the probability that it's a zero? What's the probability it's a 1: what's 
the probability, it's a 2 and so forth, alright, and so let's say we're 
trying to predict the two right. Then here is our binary: cross-entropy, 
sorry, categorical cross-entropy! So it's just saying: okay did this one 
predict correctly or not? How far off was it and so forth for each one all 
right and so add them all up so categorical cross-entropy is identical to 
binary cross-entropy. We just have to add it up across all of the 
categories, so try and turn the binary cross-entropy function in python 
into a categorical, cross-entropy python and maybe create both the version 
with the if statement and the version with the sum and the product, 
alright. Alright. So that's why, in our pytorch, we had 10 as the output as 
the put dimensionality for this matrix, because when we multiply sup by a 
matrix with 10 columns, we're going to end up with something of length 10, 
which is what we want.</p>

<p>We want to have 10 predictions, ok, so so that's the 
loss function that we're using all right. So </p>

<h3>9. <a href="https://youtu.be/DzE0eSdy5Hk?t=1h9m5s">01:09:05</a></h3>

<ul style="list-style-type: square;">

<li><b> Let’s fit the model then make predictions on Validation set.</b></li>

<li><b>‘fit(net, md, epochs=1, crit=loss, opt=opt, metrics=metrics)’</b></li>

<li><b>Note: PyTorch doesn’t use the word “loss” but the word “criterion”, thus ‘crit=loss’</b></li>

<li><b>‘preds = predict(net, md.val_dl)’</b></li>

<li><b>‘preds.shape’ -&gt; (10000, 10)</b></li>

<li><b>‘preds.argmax(axis=1)[:5]’, argmax will return the index of the value which is the number itself.</b></li>

<li><b>‘np.mean(preds == y_valid)’ to check how accurate the model is on Validation set.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Then we can fit the model and what it does is. It goes through every image 
this many times. So, in this case, it's just looking at every image once 
and go into slightly update the values in that weight, matrix based on 
those gradients, and so once we've trained it. We can then say predict 
using this model on the validation set right and now that spits out 
something of 10,000 by 10 consumers Elmi. Why is this shape these 
predictions? Why are they have shaked 10,000 by 10? Don't fret Chris, it's 
right next to you! Well, it's because we have 10,000 images, we're training 
up 10,000 images training on. So that's what we're validating on north 
coast, but it's the same thing so 10,000 be validating on. So that's the 
first axis as percent. The second axis is because we actually make 10 
predictions per image good good exactly so. Each one of these rows is the 
probabilities that it's Anor, that it's a one that isn't true, that's three 
and so forth: okay, very good! So in math there's a really common 
operation. We do called Arg max notice, though it's common, it's funny you 
like at high school. I never saw odd max first year undergrad. I never saw 
Arg max, but somehow after university everything's about Arg max. So one of 
these things - that's for some reason not really taught at school, but it 
actually turns out to be super critical and so odd.</p>

<p>Max is both something 
that you'll see in math and it's just written out in full Arg max it's in 
numpy. It's in pytorch, it's super important and what it does is it says: 
let's take this array of creds right and let's figure out on this axis. 
Remember access one is columns right so across, as Chris said, the 10 
predictions for each one for each row. Let's find which prediction has the 
highest value and return, not that if it does hit max it would return the 
value. Arg max returns the index with their value, all right. So by saying 
arc max Axess equals 1. It's going to return the index, which is actually 
the number itself right. So let's grab the first 5 okay, so for the first 
one it thinks is a 3. Then it thinks six one's an eight next one's a six, 
the next one's, a nine next one's a six again. Okay. So that's how we can 
convert our probabilities back into predictions all right. So if we I'm 
safe that away call it Preds. We can then say: okay when does Preds equal 
the ground truth right. So that's going to return an array of balls which 
we can treat as ones and zeros and the mean of a bunch of ones and zeroes 
is just the average. So that gives us the accuracy so there's our 91.8 %, 
and so you want to be able to like replicate the numbers you see, and here 
it is there's our 91.8 % all right. So when we train this it tells us.</p>

<p>The 
last thing it tells us is whatever metric we asked for and we asked for 
accuracy, okay, so the last thing it tells us is our metric, which is 
accuracy and then, before that we get the training set loss and the loss is 
again whatever odds we Asked for negative log likelihood, and the second 
thing is the validation set loss. pytorch doesn't use the word loss, they 
use the word criterion, so you'll see here, crit, okay, so that's criterion 
equals loss. This is what loss function. Do we want to use? They call that 
the criterion same thing, okay, so here's how we can recreate that 
accuracy. So now we can go ahead and plot eight of the images along with 
their predictions and we've got three eight six: nine, Oh wrong, five 
wrong! Okay - and you can see like why they are wrong, like this - is 
pretty close to a nine. It's just missing a little cross at the top. This 
is pretty close to a five. It's got a little bit at the extra here right, 
so we've made a start and, and all we've done so far is we haven't actually 
created a deep neural net. We've actually got only one layer, so what we've 
actually done is we've created a logistic regression. Okay, so a logistic 
regression is is literally what we just built and you could try and 
replicate this with scikit-learn logistic regression package.</p>

<p>When I did 
it, I got similar accuracy, but this version ran much faster because this 
is running on the GPU. Where else scikit-learn runs on the CPU okay, so 
even for something like logistic regression, we can, you know, implement it 
very quickly, with pytorch, actually pass that in so when we're. When we're 
creating our net, we have to do dot CUDA. What would be the consequence of 
not doing that? What it does not run? It wouldn't run quickly, yeah it'll 
run on the CPU. Can you pass it to jail, so may build up your network? Why 
is that? We have to do linear and followed by a nonlinear. So the short 
answer is because that's what the universal approximation theorem says is 
the structure which can give you arbitrarily accurate functions for any 
functional form. You know, so the long answer is the details of why the 
universal approximation theorem works. Another version of the short answer 
is that's the definition of a neural network, so the definition of a neural 
network is a linear layer, followed by a activation function, followed by a 
linear layer, followed by an activation function, etc. We go into a lot 
more detail of this in the deep learning, of course, but you know for this 
purpose: it's it's enough to know like that. It works so far. Of course, we 
haven't actually built a deep neural net at all.</p>

<p>We've just built a 
logistic regression, and so at this point, if you think about it, all we're 
doing is we're taking every input, pixel and multiplying it by a weight for 
each possible outcome right. So we're basically saying you know on average 
the number one you know has these pixels turned on the number two has these 
pixels turned on and that's why it's not terribly accurate right. That's 
that's! Not how digit recognition works in real life, but that's that's 
always built. So far, ok, can you pass that to Devon, so you 
</p>

<h3>10. <a href="https://youtu.be/DzE0eSdy5Hk?t=1h16m5s">01:16:05</a></h3>

<ul style="list-style-type: square;">

<li><b> A second pass on “Michael Nielsen universal approximation theorem”</b></li>

<li><b>A Neural Network can approximate any other function to close accuracy, as long as it’s large enough.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Keep saying this Universal approximation theorem yeah. Did you define yeah, 
but let's cover it again, because it's worth talking about so all right. So 
Michael Nielsen has this great website called neural networks and deep 
learning and his chapter 4 is actually kind of famous now and in it he does 
this walkthrough of basically showing that a neural network can can 
approximate any other function to arbitrarily close accuracy. As long as 
it's big enough - and we walk through this in a lot of detail in the deep 
learning course, but the basic trick is that he shows that, with a few 
different numbers, you can basically kind of cause these things to kind of 
create little boxes. You can move the boxes up and down. You can move them 
around. You can join them together to eventually basically create like 
connections of towers, which you can like use to approximate any kind of 
surface right. So, that's you know, that's basically the trick, and so all 
we need to do given given that is to kind of find the parameters for each 
of the linear functions in that neural network. So to find the weights in 
each of the in each of the matrices, and so so far we've got just one 
matrix and so we've just built a simple, logistic regression. So far, just 
a small doubt. I just want to confirm that when you showed images of the 
examples of the images which were misclassified yeah, they look 
rectangular. So it's just that, while rendering pixels are being scale 
differently, so are they still 28 by 28 Square 20? Hyper 20? I think it's 
square. I think I just look like angular cuz they've got titles on the top, 
I'm not sure yeah.</p>

<p>I don't know anyway, they are square and um like 
matplotlib yeah </p>

<h3>11. <a href="https://youtu.be/DzE0eSdy5Hk?t=1h18m15s">01:18:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Defining Logistic Regression ourselves, from scratch, not using PyTorch ‘nn.Sequential()’</b></li>

<li><b>Demo explanation with drawings by Jeremy.</b></li>

<li><b>Look at Excel ‘entropy_example.xlsx’ for Softmax and Sigmoid</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>It does often fiddle around with you know what it considers black versus 
white and you know having different size, axes and stuff. So yeah you do 
have to be careful there, sometimes um, okay. So hopefully this will now 
make more sense, because what we're going to do is like dig in a layer 
deeper and define logistic regression without using an n dot sequential 
without using an end on linear without using an end log softmax. So we're 
going to do nearly all of the layer definition from scratch. Ok! So, to do 
that, we're going to have to define a pytorch module, a pytorch module is 
basically either a neural net or a layer in a neural net which is actually 
kind of a powerful concept of itself. Basically, anything that can kind of 
behave like a neural net can itself be part of another neuron net, and so 
this is like how we can construct particularly powerful architectures, 
combining lots of other pieces so to create a pytorch module just create a 
Python class, but It has to inherit from an end module, so we haven't done 
inheritance before other than that. This is all the same concepts we've 
seen an oo already. Basically, if you put something in parentheses here, 
what it means is that our class gets all of the functionality of this class 
for free. It's called sub classing it. So we're going to get all of the 
capabilities of a neural network module that the pytorch authors have 
provided and then we're going to add additional functionality to it.</p>

<p>When 
you create a sub class, there is one key thing you need to remember to do, 
which is when you initialize your class. You have to first of all 
initialize the superclass right, so the superclass is the NN module, so the 
NN module has to be built before you can start adding your pieces to it, 
and so this is just like something you can copy and paste into every one Of 
your modules, you just say super dot in it. This just means construct the 
superclass first, ok, so having done that, we can now go ahead and define 
our weights and our bias, so our weights is the weight. Matrix is the 
actual matrix that we're going to multiply our data by and as we discuss 
it's going to have 28 times, 28 rows and 10 columns. And that's because if 
we take an image which we flattened out into a 28 by 28 length vector 
right, then we can multiply it by this weight matrix to get back out a 
length 10 vector, which we can then use to consider it as a set of 
Predictions so that's our weight matrix now. The problem is that we don't 
just want y equals ax. We want y equals ax plus B, so the plus B in neural 
nets is called bias, and so as well as defining weights, we're also going 
to find bias, and so since this thing is going to spit out for every image 
something of length 10, that means That we need to create a vector of 
length 10 to be our biases, in other words, for everything naught 1 2 3 up 
to mine, we're going to have a different plus B. That would be adding 
right. So we've got our data matrix here, which is of length 10,000 by 28 
times 28, all right and then we've got our weight matrix, which is 28 by 28 
rose by 10. So if we multiply those together, we get something of size 
10,000 by 10, okay and then we want to add on our bias up run way around 
add on our bias.</p>

<p>Okay, like so, and so when we add on and we're going to 
learn a lot more about this later. But when we add on a vector like this, 
it basically is going to get added to every row. Okay, so that bias is 
going to get added to every road, so we first of all define those and so to 
define them. We've created a tiny little function called get weights, which 
is over here, alright, which basically just creates some normally 
distributed random numbers. So a torch Rand n returns a tensor filled with 
random numbers from a normal distribution. We have to be a bit careful, 
though, when we do deep learning like when we add more linear layers later 
imagine if we have a matrix, which, on average tends to increase the size 
of the inputs we give to it. If we then multiply it by lots of matrices of 
that size, it's going to make the numbers bigger and bigger and bigger like 
exponentially bigger. Well, what if it made them a bit smaller, it's going 
to make them smaller and smaller and smaller exponentially smaller, so 
like, because a deep network applies lots of linear layers if, on average, 
they result in things a bit bigger than they started with or a bit Smaller 
than they started with it's going to like exponentially multiply that 
difference, so we need to make sure that the weight matrix is of an 
appropriate size that the inputs to it, they're kind of the mean of the 
inputs, basically is not going to change.</p>

<p>So it turns out that if you use 
normally distributed random numbers and divided by the number of rows in 
the weight matrix, it turns out that particular random initialization keeps 
your numbers at about the right scale right. So this idea that, like if 
you've done linear algebra, basically if the eigen value the first 
eigenvalue, is like bigger than one or smaller than one, it's cause the 
gradients to like get bigger and bigger or smaller and smaller. That's 
called gradient explosion right. So we'll talk more about this in the deep 
learning course, but if you're interested you can look at climbing her 
initialization and read all about this concept right. But for now you know 
it's probably just enough to know that if you use this type of random 
number generation, you're gon na get random numbers that are and nicely 
behaved, you're going to start out with an input which is mean 0 standard 
deviation. 1. Once you put it through this set of random numbers, you'll 
still have something: that's about mean. 0 standard deviation. 1, that's! 
Basically the goal. Okay, one nice thing about pytorch is that you can play 
with this stuff right, so choice, dot, Randi and like try it out. Every 
time you see a function being used, run it go ahead and take a look and so 
you'll see. It looks a lot like numpy right, but it doesn't return a numpy 
array. It returns a tensor and in fact now I'm GPU programming, okay, like 
put CUDA, and now it's doing it on the GPU so like I just multiplied that 
matrix by 3 very quickly on the GPU right.</p>

<p>So that's how we do GPU 
programming with pay torch. All right, so this this is our weight matrix, 
so we create, as I said, we create one pretty at the 28 by 10. 1 is just 
Rank 1 of 10 for the biases. We have to make them a parameter. This is 
basically telling pytorch, which things to update when it does SGD, that's 
very minor technical detail. So, having created the weight matrices, we 
then define a special method with the name forward. This is a special 
method. The word the name forward has a special meaning in pytorch. A 
method called forward in pytorch is the name of the method that will get 
called when your layer is calculated. Ok, so if you create a neural net or 
a layer, you have to define forward and it's going to get past the data 
from the previous layer. So our definition is to do a matrix multiplication 
of our input data times our weights and add on the biases. So that's it 
that's what happened earlier on when we said n n dot, linear that created 
this this thing for us! Okay! Now, unfortunately, though, we're not getting 
a 28 by 28 long vector we're getting a 28 row by 28 column matrix, so we 
have to flatten it. Unfortunately, in torch high torch, they tend to rename 
things they they spell re sigh reshape. They spell it view. Okay, so view 
means reshape. So you can see here we end up with something where the 
number of images we're going to leave the same and then we're going to 
replace row by column with a single axis again negative one meaning as long 
as required.</p>

<p>Okay. So this is how we flatten something using pytorch, so we 
flatten it do a matrix multiply and then finally, we do our soft maps. So 
softmax is the activation function we use. If you look in the deep learning 
repo you'll find something called entropy example, where you'll see an 
example of softmax, but a soft Mac simply takes the outputs from our final 
layer. So we get our outputs from our from our linear layer and what we do 
is we go e ^ for each output and then we take that number and we divide by 
the sum of the eight of the perils that's called softmax. Why do we do 
that? Well, because we're dividing this by the sum that means that the sum 
of those itself must add to one right and that's what we want, we want the 
probabilities of all the possible outcomes. Add to one, furthermore, 
because we're using e ^. That means we know that everyone, these is between 
zero and one and probabilities we know, should be between zero and one and 
then, finally, because we're using e to the power of it, tends to mean that 
slightly bigger values in the input turn into much bigger values. In the 
output so you'll see, generally speaking my softmax, there was going to be 
one big number and lots of small numbers and that's what we want right, 
because we know that the output is one hot encoded, so in other words a 
softmax activation function.</p>

<p>The softmax non-linearity is something that 
returns things that behave like probabilities and where one of those 
probabilities is more likely to be kind of high and the other ones are more 
likely to be low. And we know that's what we want for a to map to our one 
hot encoding. So a softmax is a great activation function to use to kind of 
help. The neural net make it easier for the neural net to to map to the 
output that you wanted, and this is what we generally want. When we're kind 
of designing neural networks, we try to come up with little architectural 
tweaks that make it as easy for it. As possible to to to match the output 
that we know we want, so that's basically it right like, rather than doing 
sequential, you know and using an end, linear and in dot softmax we've 
defined it from scratch. We can now say just like before our net is equal 
to that plastic CUDA and we can say, dot fetch and we get to within a 
slight random deviation, exactly the same output. </p>

<h3>12. <a href="https://youtu.be/DzE0eSdy5Hk?t=1h31m5s">01:31:05</a></h3>

<ul style="list-style-type: square;">

<li><b> Assignements for the week, student question on ‘Forward(self, x)’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Okay, so I'm what I'd like you to do during the week is to play around 
with, like torture and n to generate some random tensors Torche drop map 
mole to start multiplying them together. Adding them up try to make sure 
that you can rewrite softmax yourself from scratch. You know like try to 
fiddle around a bit with you know reshaping view or that kind of stuff, so 
that by the time you come back next week, you feel like pretty comfortable 
with pytorch, and if you, google, for pytorch tutorial, you'll see there's 
a Lot of great material actually on the pytorch website, to help you along 
basically showing you how to create tensors and modify them and do 
operations on them all right, great. Yes, you had a question: can you pass 
it over? So I see that the forward is the layer that gets applied after 
each of the linear layers. So not quite the forward is just the definition 
of the module, so this is like how it this is how we're implementing 
linear. So does that mean after each linear layer you have to apply the 
same function. Let's say we can't do a log softmax after layer. One and 
then apply some other function after layer two. If we have like a 
multi-layer neural network, so normally way to find neural networks. 
Normally we define neural networks like so we just say here is a list of 
the layers we wander right. We don't you, don't have to write your own 
forward right.</p>

<p>All we did just now is to say like okay, instead of doing 
this, let's not use any of this at all, but write it all by hand ourselves 
all right. So you can. You can write as many layers you see like in what 
any order you like here. The point was that here we're not using any of 
that we've written our own mat mole plus bias our own softmax. So this is 
like this is. This is just Python code. You can write whatever Python code 
inside forward that you like to define your own neural net. So, like you, 
you won't normally do this yourself, normally you'll just use the layers 
that pie chart provides and your use dot, sequential to put them together 
or even more likely. Your download or predefined architecture and use that 
we're just doing this to learn how it works behind the scenes all right, 
great thanks. Everybody </p>






  </body>
</html>
