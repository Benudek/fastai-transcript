<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Lesson 04</title>
    <meta charset="UTF-8">
  </head>
  <body>
  <p style="text-align: right"><a href="http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826">fast.ai: Intro to Machine Learning 1 (v1) (2018)</a></p>
  <h1>Lesson 04</h1>
  <h2>Outline</h2>
<ul>

<li>Forecasting: Grocery Kaggle discussion, Parallel to Rossman stores</li>
<li>Random Forests: Confidence based tree variance</li>
<li>Random Forests: Feature Importance Intro</li>
<li>Random Forests: Decoupled Shuffling</li>

</ul>


  <h2>Video Timelines and Transcript</h2>


<h3>1. <a href="https://youtu.be/0v93qHDqq_g?t=4s">00:00:04</a></h3>

<ul style="list-style-type: square;">

<li><b> How to deal with version control and notebooks ? Make a copy and rename it with “tmp-blablabla” so it’s hidden from Git Pull</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>All right welcome back something to mention somebody asked on the forums. 
Really good question was like: how do I deal with version control and 
notebooks? The question was something like every time I change the 
notebook. Jeremy goes and changes it on git and then I do a git pull and I 
end up with a conflict, and I love that and that's that happens a lot with 
notebooks, because notebooks behind the scenes at JSON files, which, like 
every time you run even a Cell without changing it, it updates that little 
number saying like what numbered cell this is, and so now suddenly there's 
a change and so trying to merge notebook changes is a nightmare. So my 
suggestion, I'd like a simple way to do it is, is when you're looking at 
some notebook like less than 200. If interpretation, you want to start 
playing around with this. The first thing I would do would be to go file, 
make a copy and then in the copy, say, file rename and give it a name that 
starts with TMP. That will hide it from get right, and so now you've got 
your own version of that workbook. That you can that you can play with 
okay, and so, if you now do a git pull and see that the original changed it 
won't conflict with yours, and you can now see there are two different 
versions. There are different ways of kind of dealing with this Jupiter 
notebook get problem like everybody has it one one is there are some hooks 
you can use it like remove all of the cell outputs before you commit to 
get, but in this case I actually want the Outputs to be in the repo, so you 
can read it on github and see it.</p>

<p>So it's a minor issue, but it's a it's 
something which catches everybody. Ah, yes before we move on to 
</p>

<h3>2. <a href="https://youtu.be/0v93qHDqq_g?t=1m50s">00:01:50</a></h3>

<ul style="list-style-type: square;">

<li><b> Summarize the relationship between hyperparameters in Random Forests, overfitting and colinearity.</b></li>

<li><b>‘set_rf_samples()’, ‘oob_score = True’,</b></li>

<li><b>‘min_samples_leaf=’ 8m45s,</b></li>

<li><b>‘max_features=’ 12m15s</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Interpretation of the random forest model. I wonder if we could summarize 
the relationship between the hyper parameters on the random forest and its 
effect on you know, overfitting and dealing with collinearity and yeah. 
That sounds like a question born from experience. Absolutely so I got ta. 
Go back to lesson 1 RF, if you're ever unsure about where I am. You can 
always see my top here courses mly and lesson 1 on earth in terms of the 
hyper parameters that are interesting and I'm ignoring, I'm ignoring like 
pre-processing, but just the actual hyper parameters. The first one of 
interest, I would say, is the set RF samples command, which determines how 
many rows are in each sample, so in each tree, you're created from how many 
rows n each tree. So before we start a new tree, we either bootstrap a 
sample so sampling, with replacement from the whole thing or we pull out a 
subsample of a smaller number of rows, and then we build a tree from there 
so so step. One is: we've got our whole big data set and we grab a few rows 
at random from it and we turn them into a smaller data set and then from 
that we build a tree right. So that's the size of that is set our F 
samples. So when we change that size, let's say this originally had like a 
million rows and we said, set our F samples, twenty thousand right and then 
we're going to grow a tree from there, assuming that the tree remains kind 
of balanced as we grow it.</p>

<p>Can somebody tell me how many layers deep with 
this tree be and assuming we're growing it until every leaf is of size? One 
yes, log base 2 of 20,000 right, okay, so the the depth of the tree doesn't 
actually vary that much depending on the month, samples right, because it's 
it's related to the log of the size. Can somebody tell me at the very 
bottom so once we go all the way down to the bottom, how many leaf nodes 
would there be speak up? What what do you think right, because every single 
leaf node has a single thing in it? So we've got obviously a linear 
relationship between the number of leaf nodes in the size of the sample. So 
when you decrease the sample size, it means that there are less kind of 
final decisions that can be made right. So therefore, the tree is is going 
to be less rich in terms of what it can predict, because it's just making 
less different individual decisions, and it also is making less binary 
choices to get to those decisions. So therefore, setting RS samples lower 
is going to mean that you over fit less, but it also means that you're 
going to have a less accurate, individual tree model right and so remember 
the way Braman the inventor of random first described. This is that you're 
trying to do two things when you build a model when you build a model with 
bagging one.</p>

<p>Is that each individual tree or as SQL you say, each 
individual estimator is as accurate as possible right on the training set. 
So it's like each model is a strong predictive model, but then the across 
the estimators relation between them is as low as possible so that when you 
average them out together, you end up with something that generalizes. So 
by decreasing the set RF samples number. We are actually decreasing the 
power of the estimator and increasing the correlation, and so is that going 
to result in a better or a worse, validation set result for you. It depends 
right. This is the kind of compromise which you have to figure out when you 
do machine learning models. Can you pass that back there? If I wait, if I 
put the Bovie value equal to so it is basically dividing every 30. It 
ensures that the data won't be there in each three right now, our page. 
Second, probably if I put all the equal to two yeah random voice, yeah, so 
is it that make sure that out of my entire data 37 personal data would be 
there in every tree. So all our P equals true does. Is it says whatever 
your sub sample is, it might be a bootstrap sample or it might be a 
subsample take all with the other rows right and put them into a tree tree 
and put them into a different data set and calculate the the error on 
those. So it doesn't actually impact training at all. It just gives you an 
additional metric, which is the oob error.</p>

<p>So if you don't have a 
validation set, then this allows you to get kind of a quasi validation set 
for free. If you want to set out a sample Aref sample so that the default 
is actually, if you say reset our F samples and that causes it to 
bootstrap, so it all sample our new data set as big as the original one, 
but with replacement okay. So obviously the second benefit of set our 
samples is that you can run more quickly and particularly, if you're 
running on a really large data set like a hundred million rows, you know it 
won't be possible to run it on the full data set. So you would either have 
to pick a subsample if yourself before you start or you set our examples. 
The second key parameter that we learnt about was min samples leaf. Okay. 
So if I changed min samples leaf for we assumed that men samples leaf was 
equal to 1, all right, if I set it equal to 2, then what would be my new 
depth? How deep would it be? Yes, log base to 20,000 minus one okay. So, 
each time we double the min samples leaf, we're removing one layer from the 
tree and fine I'll come back to you again since you're doing so well, how 
many leaf nodes would there be in that case, but how many leaf nodes would 
there be? In that case, 10,000, okay, so we're gon na be again dividing the 
number of leaf nodes by that number.</p>

<p>So the result of increasing min 
samples leaf is that now each of our leaf nodes has more than one thing in 
so we're going to get a more stable average that we're calculating in each 
tree. Okay, we've got a little bit less depth. Okay, we've got less 
decisions to make and we've got a smaller number of leaf nodes. So again we 
would expect the result of that would be that each estimator would be less 
predictive, but the estimators would be also less correlated so again. This 
might help us to avoid overfitting. Could you pass the microphone over 
here? Please, oh hi, Jimmy, I'm not sure. If, in that case, every node will 
have exactly two. No, it won't necessarily have exactly two, and I thank 
you for mentioning that, so it might try to do a split and so one reason 
well what would be an example, Chen XI that you wouldn't split, even if you 
had a hundred nodes. What might be a reason for that sorry, 100 items in a 
leaf, node they're, all the same they're, all the same in terms of the 
independent to saw the dependent and it has the dependent right now I mean 
I guess either, but much more likely would be The dependent, so if you get 
to a leaf node where every single one of them has the same option price or 
in classification like every single one of them, is a dog, then there is no 
split that you can do.</p>

<p>That's going to improve your information all right 
and remember, information is the term we use in a kind of a general sense 
in random for us to describe the amount of difference about at that 
additional information we create from a split is like: how much are we 
Improving the model so you'll often see this word, information gain, which 
means like how much better did the model get by adding an additional split 
point and it could be based on our MSC or it could be based on 
cross-entropy or it could be based on how Different to the standard 
deviations or whatever, so that's just a general term, okay, so that's the 
second thing that we can do again. It's going to speed up our training 
because it's like one less set of decisions to make remember, even though 
there's one less set of decisions. Those decisions like have as much data 
again as the previous set so like each layer of the tree can take like 
twice as long as the previous layer, so it could definitely speed up 
training and it could definitely make it generalize better. So then, the 
third one that we had was max features. Who wants to tell me what max 
features does I want to pass that back over there? Okay Vinay, we just 
leave their minds, how many features you're going to use in HP? In this 
case, it's a fraction up so you're going to use up other, be just for each 
three, nearly right. What kind of right can you be more specific, or can 
somebody else be more specific? It's not exactly for each tree.</p>

<p>Can she 
that is that for each tree, randomly simple healthy? So not quite it's not 
for each tree, so the the set don't possibly care them. So the scent are of 
samples picks a picks, a subset of samples subset of rows for each tree, 
but min samples leaf. Sorry that max features doesn't quite do that. It's 
not something different at each set smell ant will yeah right. So it kind 
of sounds like a small difference, but it's actually quite a different way 
of thinking about it, which is we do our set our samples. So we pull out 
our sub sample or a bootstrap sample and that's kept for the whole tree and 
we have all of the columns in there right and then with max features equals 
0.5 at each point. We then, at each split we'd, pick a different half of 
the features and then here what you could pick a different half of the 
features - and here we'll pick a different half of the features, and so the 
reason we do. That is because we want the trees to be as as rich as 
possible right so particularly like if you, if you were only doing a small 
number of trees like you, had only ten trees and you picked the same column 
set all the way through the tree. You're not really getting much variety 
and what kind of things are confined? Okay, so this this way, at least in 
theory, seems to be something which is going to give us a better set of 
trees, picking a different, random subset of features at every decision 
point. So the overall effective max features again, it's the same.</p>

<p>It's 
going to mean that the traits individual tree is probably going to be less 
accurate, but the trees are going to be more buried, and in particular 
here. This can be critical because, like imagine that you've got one 
feature, that's just super predictive. It's so predictive that, like every 
random subsample, you look at always starts out by splitting on that same 
feature, then the trees are going to be very similar in the sense like they 
all have the same initial split right, but there may be some other are 
interesting. Initial splits because they create different interactions of 
variables so by like half the time that feature won't even be available at 
the top of the tree. So half at least half the trees are going to have a 
different initial split. So it definitely can give us more variation and 
therefore again it can help us to create more generalized trees that have 
less correlation with each other, even though the individual trees probably 
won't be as predictive in practice. We actually looked at have a little 
picture of this. That, as as you add, more trees right, if you have max 
features equals none, that's going to use all the features every time. 
Right then, with like very very few trees. That can still give you a pretty 
good error, but, as you create more trees, it's not going to help as much 
because they're all pretty similar, because they're all trying every single 
variable. Where else, if you say max, features, equal square root or max 
pictures equals log.</p>

<p>Two then, as we add more estimators, we see 
improvements. Okay, so there's an interesting interaction between those two 
and this is from the SK loan Docs. This cool little chat. Okay, so then 
things which don't impact out our training at all and jobs simply says how 
many cpu, how many cause do we run on? Okay, so it'll make it faster up to 
a point, generally speaking, making this more than like, eight or so they 
may have diminishing returns. -1 says use all of your cause, so there's 
wrote, there's I don't know why the default is to only use one core. That's 
seems weird to me: you'll definitely get more performance by using more 
cause, because all of you have computers with more than one core nowadays 
and then our base core equals true, simply allows us to see the low B 
score. If you don't say that it doesn't calculate it, and particularly if 
you had set RF samples, pretty small compared to a big data set or B, is 
going to take forever to calculate, hopefully at some point we'll be able 
to fix the library. So that doesn't happen. There's no reason that need be 
that way, but right now, that's that's how the Bible place. Okay, our base, 
Kuhn, okay, basic parameters that we can change. There are more that you 
can see in the docs or shift tab to have a look at them, but the ones 
you've seen are the ones that I've found useful to play with so feel free 
to play with others as well and, generally speaking, you know max Features, 
as I said, max features are like either non means all of them about 0.5 or 
square root or log.</p>

<p>You know kind of those trees seem to work pretty well 
and then some in samples leaf. You know I would generally try kind of 1. 3. 
5. 10. 25. You know 100 and like, as you start doing, that if you notice by 
the time you get to 10, it's already getting worse, then there's no point 
going further. If you get to 100, it's still going better, then you can 
keep trying right but they're. The kind of general amounts that most things 
in to sit in all right, </p>

<h3>3. <a href="https://youtu.be/0v93qHDqq_g?t=18m50s">00:18:50</a></h3>

<ul style="list-style-type: square;">

<li><b> Random Forest Interpretation lesson2-rf_interpretation,</b></li>

<li><b>‘rf_feat_importance()’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>So random forest interpretation is something which you could use to create 
some really cool cattle kernels. Now, obviously, one issue is the faster. I 
library is not available in Cabell kernels, but if you look inside fastai 
dot, structured right, remember you can just use double question. Mark to 
look at the source code for something or you can go into the editor to have 
a look at it, you'll see that most of the methods we're using or a small 
number of lines of code in this library and have no dependencies on 
anything. So you could just copy that little if you need to use one of 
those functions, just copy it into your kernel and and if you do to say 
this is from the first day a library you can link to it on github, because 
it's available on github. As open-source, but you don't need to import the 
whole thing right, so this is a cool trick. Is that because you're, the 
first people to learn how to use these tools, you couldn't start to show 
things that other people haven't seen right. So, for example, this 
confidence based on tree variance is something which doesn't exist anywhere 
else feature importance, definitely does, and that's already in quite a lot 
of cable kernels. If you're, looking at a competition or a data set that 
where nobody's done, feature importance being the first person to do, that 
is always going to win lots of votes because it's like the most important 
thing is like which features are important.</p>

<p>So last time we, let's just 
make sure we put our tree data, so we need to change this to add one extra 
thing, all right, so that's no load, no data, yet there's that data okay. 
So, as I mentioned, when we do mobile interpretation, I tend to set our of 
samples to some subset, something small enough that I can ran a model in 
under 10 seconds or so because there's just no point run running a super 
accurate model. Fifty thousand is more than enough to see you'll basically 
see each time you run an interpretation, you'll get the same results back, 
and so as long as that's true, then you you're already using enough data. 
Okay, so feature importance. We learnt it works by randomly shuffling a 
column, each column, one at a time and then seeing how accurate the model 
the pre trained model the model we've already built is when you pass it in 
all the data as before, but with one column shuffled. So some of the 
questions I got after class kind of reminded me that it's very easy to 
under appreciated and kind of magic. This approach is, and so to explain 
I'll mention a couple of the questions that I heard, and so one question 
was like: why don't we or what, if we just um, we took one column at a time 
and created a tree on just each one column at A time so we've got our data 
set, it's got a bunch of columns, so why don't we just like? We have that 
column and just build a tree from that right and then, like we'll see which 
which columns tree is the most predictive.</p>

<p>Can anybody tell me why what why 
that may give misleading results about feature importance? Okay, we're 
going to lose the interactions between the features yeah. If we just 
shuffle them, it will be a bad randomness and we were able to both capture 
the interactions and the importance of the future. It's great yeah and - 
and so this issue of interactions is not a minor detail. It's like it's 
massively important so like think about this bulldozes data set where, for 
example, where there's one field called year made and there's one field 
called sale date and like if we think about it, it's pretty obvious that 
what matters is the combination of these two, Which, in other words, is 
like how old is the piece of equipment when it got sold? So if we only 
included one of these, we're going to massively underestimate how important 
that feature is now here's a really important point, though, if you it's 
pretty much always possible to create a simple like logistic regression, 
which is as good as pretty much any random first, If you know ahead of time 
exactly what variables you need exactly how they interact exactly how they 
need to be transformed and so forth right. So in this case, for example, we 
could have created a new field which was equal to year made as a sale date 
or sale year year made and we could have fed that to a model and got you 
know, got that interaction for us. But the point is we never know that, 
like you never like, you might have a guess of it.</p>

<p>I think some of these 
things are interacted in this way and I think this thing we need to take 
the log and so forth, but you know the truth. Is that the way the world 
works, the causal structures? You know, they've got many many things 
interacting in many many subtle ways right and so that's why using trees, 
whether it be gradient, boosting machines or random forests works so well. 
So can you pass that to Terrance? Please one thing that bit me years ago 
was also: I tried that doing one variable at a time thinking oh well I'll 
figure out which one's most correlated with the dependent variable, but 
what it doesn't pull apart is that what, if all variables are basically 
copied? The same variable then they're all going to seem equally important, 
but in fact it's really just one factor yeah - and that's also true here. 
So if we had like a column, appeared twice right, then shuffling that 
column isn't going to make the model much worse. Right there'll be, if you 
think about like how it was built some of the times, particularly if we had 
like max features is 0.5 and some of the times we're going to get version a 
of the column, some of the time to get going to get version B of the 
column, so, like half the time shuffling version, a of the column is going 
to make a tree a bit worse. Half the time it's going to make.</p>

<p>You know 
column B, you'll, make it a bit worse and so it'll show that both of those 
features are somewhat important and it'll kind of like share the importance 
between the two features, and so this is why a reco linearity, but 
collinearity literally means that they're linearly Related, so this isn't 
quite right, but this is why, having two variables that are related closely 
related to each other or more variables that are closely related to each 
other means that you will often underestimate their importance using this. 
This random first technique, um, yes, Terrence, and so once we've shuffled 
and we get a new model. What exactly are the units of these importances? Is 
this a change in the R squared yeah? I mean it depends on the library we're 
using. So the units are kind of like I never think about them. I kind of 
know that, like in this particular library, you know 0.005 is often kind of 
a cutoff </p>

<h3>4. <a href="https://youtu.be/0v93qHDqq_g?t=26m50s">00:26:50</a></h3>

<ul style="list-style-type: square;">

<li><b> ‘to_keep = fi[fi.imp&gt;0.005]’ to remove less important features,</b></li>

<li><b>high cardinality variables 29m45s,</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>I would tend to use, but all I actually care about is is this picture 
right, which is the feature importance, ordered for each variable and then 
kind of zooming in turning into a bar plot and I'm kind of like okay, you 
know here, they're all pretty flat And I can see: okay, that's about 0.05, 
and so I removed them at that point and just see like the model. Hopefully 
the validation score didn't get worse and if it did get worse, I'll just 
increase this a little bit. So I decrease this a little bit until it. It 
doesn't get worse, so yeah. The units of measure of this don't matter too 
much and we'll learn later about a second way of doing variable importance. 
By the way, can you pass that over there is one of the goals here to remove 
variables that I guess your state, your score will not get worse. If you 
remove them, so you might as well get rid of them yeah. So that's what 
we're going to do next, so so what having looked at our feature importance 
plot, we said: okay, it looks like the ones like less than 0.005. You know 
that kind of this long tail of boringness. So I said: let's try removing 
them right. So, let's just try grabbing the columns where it's greater than 
0.005, and I said, let's create a new data frame called DF. Keep which is 
DF train with just those cap. Columns create a new training and validation 
set with just those columns created a new random forest, and I looked see 
how the validation set score and the validation set.</p>

<p>Our MSC changed and I 
found they got a tiny bit better. So if they're about the same or a tiny 
bit better than the thinking, my thinking is well. This is just as good a 
model, but it's now simpler and so now, when I redo the feature importance, 
there's less collinearity right and so in this case I saw that year. Maid 
went from being like quite a bit better than the next best thing, which was 
couple system way better than the next best thing. Okay and coupler system 
went from being like quite a bit more important than the next two equally 
important to the next two. So it did seem to definitely change these 
feature importances and hopefully give me some more insight there. So how 
did that help our model in general? Look, what does it mean that your maid 
is no way yeah, so we're going to dig into that kind of now, but basically 
it tells us that, for example, if we're looking for like how we're dealing 
with missing values, is there noise and the data? You know it's a high 
cardinality, categorical variable, they're all different steps we would 
take so, for example, if it was a high cardinality, categorical variable 
that was originally a string right like, for example, I think, like maybe 
fi product class description. I remember one of the ones we looked at the 
other day. He had like first of all, was the type of vehicle and then a 
hyphen and then like the size of the vehicle. We might look at that and be 
like okay. Well, that was an important column.</p>

<p>Let's try like splitting it 
into two on and then take that bit, which is like a size of it and trying 
you know posit and convert convert it into an integer. You know we can try 
and do some feature engineering and basically until you know which ones are 
important, you don't know where to focus that feature engineering time. You 
can talk to your client, you know and say you know, or you know, and if 
you're doing this inside your workplace, you go and talk to the folks that, 
like we're responsible for creating this data, so in this, if you were 
actually working at a bulldozer Auction company - you might now go to the 
actual auctioneers and say I'm really surprised that couply system seems to 
be driving people's pricing decisions so much. Why do you think that might 
be? And they can say to you? Oh, it's actually, because only these classes 
of vehicles have capital systems, or only this manufacturer has couple of 
systems, and so frankly, this is actually not telling you about couple of 
systems, but about something else. And oh hey that reminds me, that's that 
that's something else! We actually have measured that it's in this 
different CSV file I'll go, get it for you, but kind of helps. You focus 
your attention, so I hello little problem this weekend. As you know, I 
introduced a couple of crazy computations in into my random forest and all 
of a sudden they're like, oh my god. These are the most important variables 
ever squashing all of the others.</p>

<p>But then I got a terrible score and then 
is that, because now that I think I have my scores computed correctly, what 
I noticed is that the importance went through the roof, but the validation 
set was still bad or got worse. Is that, because, somehow, that computation 
allow the training to almost like an identifier map, exactly what the 
answer was going to be for training? But </p>

<h3>5. <a href="https://youtu.be/0v93qHDqq_g?t=32m15s">00:32:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Two reasons why Validation Score is not good or getting worse: overfitting, and validation set is not a random sample (something peculiar in it, not in Train),</b></li>

<li><b>The meaning of the five numbers results in ‘print_score(m)’, RMSE of Training &amp; Validation, R² of Train &amp; Valid &amp; OOB.</b></li>

<li><b>We care about the RMSE of Validation set.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Of course, that doesn't generalize to the validation set. Is that what is 
that? What I observed? Okay? So this there's two reasons why your 
validation score might not be very good um, let's go up here, okay, so we 
got these five numbers right. The rmse of the training, validation, 
r-squared of the training validation and the r-squared of the oeob okay. So 
there's two reasons and really in the end, what we care about like for this 
Kaggle competition is the rmse of the validation set. Assuming we've 
created a good validation set, so an Terrance's case he's saying this 
number. Is this thing I care about got worse when I did some feature 
engineering? Why is that? Okay, there's two possible reasons. Reason one is 
that you're overfitting if you're overfeeding, then your mobiie will also 
get worse if you're doing a huge data set with a small set RF sample. So 
you can't use a know. A B then instead create a second validation set, 
which is a random sample. Okay and and do that right so in other words, if 
your OB or your random sample validation set, is has got much worse, then 
you must be overfitting. I think in your case Terrence. It's unlikely: 
that's the problem, because random forests don't over fit that badly like 
it's very hard to get them to overfit that badly. Unless you use some 
really weird parameters, like only one estimator, for example, like once, 
we've got ten trees in there.</p>

<p>There should be enough variation that you're, 
you know you can definitely over fit, but not so much that you're going to 
destroy your validation score by adding a variable. So I'd think you'll 
find that's, probably not the case, but it's easy to check and if it's not 
the case, then you'll see that your oob score or your random sample 
validation score hasn't got worse, okay, so the second reason your 
validation score - can get worse. If your mobiie score hasn't got worse, 
you're, not overfitting, but your validation score is got worse. That means 
you're you're doing something that is true in the training set, but not 
true in the validation set. So this can only happen when your validation 
set is not a random sample. So, for example, in this bulldozes competition 
or in the grocery shopping competition, we've intentionally made a 
validation set. That's for a different date range it's for the most recent 
two weeks right, and so, if something different happened in the last two 
weeks to the previous weeks, then you could totally break your validation 
set. So, for example, if there was some kind of unique identifier which is 
like different in the to date periods, then you could learn to identify 
things using that identifier in the training set, but then, like the last 
two weeks, may have a totally different set of ID's Or the different set of 
behavior could get a lot worse, yeah. What you're describing is not common 
though, and so I'm a bit skeptical.</p>

<p>It might be a bug, but </p>

<h3>6. <a href="https://youtu.be/0v93qHDqq_g?t=35m50s">00:35:50</a></h3>

<ul style="list-style-type: square;">

<li><b> How Feature Importance is normally done in Industry and Academics outside ML: they use Logistic Regression Coefficients, not Random Forests Feature/Variable Importance.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Hopefully, there's enough things you can now use to figure out if it is 
about, will be interested to hear what you learned. Okay, so that's that's 
feature importance, and so I'd like to compare that to how feature 
importance is normally done in industry and in academic communities outside 
of machine learning like in psychology and economics and so forth, and 
generally speaking, people in those kind of environments tend to Use some 
kind of linear regression, logistic regression general linear models, so 
they start with their data set and they basically say that was weird: oh 
okay, so they start with their data set and they say I'm going to assume 
that I know the kind of parametric relationship Between my independent 
variables and my dependent variable, so I'm going to assume that it's a 
linear relationship say or it's a linear relationship with a link function 
like a sigmoid to create logistic regression say, and so assuming that I 
already know that. I can now write this as an equation, so if I've got like 
x1 x2 so forth, right, I can say all right: my Y values are equal to ax 1 
plus BX 2 equals y, and therefore I can find out the feature importance 
easily enough by just Looking at these coefficients and saying like which 
one's the highest, particularly if you've normalized the data first right 
so there's this kind of trope out there, it's it's very common, which is 
that, like this, is somehow more accurate or more pure or in some way 
better way Of doing feature importance, but that couldn't be further from 
the truth right, if you think about it, if you were like, if you were 
missing an interaction right or if you were missing a transformation you 
needed or if you have any way being anything less than a Hundred percent 
perfect in all of your pre-processing, so that your model is the absolute 
correct truth of this situation.</p>

<p>Right unless you've got all of that 
correct, then your coefficients are wrong right. Your coefficients are 
telling you in you're totally wrong model. This is how important those 
things are right, which is basically meaningless. So where else do the 
random forest feature importance? It's telling you in this extremely high 
parameter highly flexible, functional form with few. If any statistical 
assumptions. This is your future importance right. So I would be very 
cautious, you know, and and again I can't stress this enough when you, when 
you leave ence and when you leave this program, you are much more often 
going see. People talk about logistic regression coefficients then you're 
going to see them talk about random first variable importance, and every 
time you see that happen, you should be very, very, very skeptical of what 
you're seeing anytime, you read a paper in economics or in psychology or 
the marketing Department tells you that this regression or whatever every 
single time those coefficients are going to be massively biased by any 
issues in the model. Furthermore, if they've done so much pre-processing 
that actually the model is pretty accurate, then now you're looking at 
coefficients that are going to be of like a coefficient of some principal 
component from a CA or a coefficient of some distance from some cluster or 
something at which Point they're very, very hard to interpret anyway 
they're not actual variables right, so they're kind of the two options I've 
seen when people try to use classic statistical techniques to do recover a 
variable importance, equivalent </p>

<h3>7. <a href="https://youtu.be/0v93qHDqq_g?t=39m50s">00:39:50</a></h3>

<ul style="list-style-type: square;">

<li><b> Doing One-hot encoding for categorical variables,</b></li>

<li><b>Why and how works ‘max_n_cat=7’ based on Cardinality 49m15s, ‘numericalize’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>I think things are starting to change slowly, you know there. There are 
some fields that are starting to realize that this is totally the wrong way 
to do things, but it's been, you know nearly 20 years since random forests 
appeared so it takes a long time. You know people say that the only way 
that knowledge really advances is when the previous generation dies and 
that's kind of true right. Like particularly academics, you know they make 
a career of being good at a particular sub thing, and you know often don't 
if you know it's not until the next generation comes along, that that 
people notice that oh that's actually longer no good way to do things, and 
I think that's what's happened here. Okay, so we've got now a model which 
isn't really any better as a predictive accuracy wise, but it's kind of 
we're getting a good sense that there seems to be like four main important 
things when it was made: the capital system, its size and its product 
classification. Okay, so that's cool. There is something else that we can 
do, however, which is we can do something called one hot encoding, so this 
is going to where we're talking about categorical variables, so remember a 
categorical variable. Let's say we had like a string high and remember the 
order we got was kind of back weird. It was high low medium, so it was in 
alphabetical order by default.</p>

<p>Right was their original category for like 
usage, banned or something, and so we mapped it to 0, 1, 2 right and so by 
the time it gets into our data frame. It's now a number, so the random 
forest doesn't know that it was originally a category. It's just a number 
right, so when the random forest is built, it basically says: oh, is it 
greater than 1 or not, or is it greater than naught or not? You know 
basically the two possible decisions it could have made for for something 
with like five or six bands. You know it could be that just one of the 
levels of a category is actually interesting right so like if it was like 
very high, very low or unknown right. Then we know that, like six levels - 
and maybe the only thing that mattered was whether it was like unknown - 
maybe like not nothing. It's sighs somehow impacts the price. And so, if we 
wanted to be able to recognize that, and particularly if like it just so 
happened that the way that the numbers were coded was it unknown ended up 
in the middle right, then what it's going to do is it's going to say. Okay, 
there is a difference between these two groups. You know less than or equal 
to two versus greater than two and then when it gets into this this leaf. 
Here it's going to say: oh there's a difference between these two between 
less than four and greater than or equal to four, and since going to take 
two splits to get to the point where we can see that it's actually unknown 
that matters.</p>

<p>So this is a little inefficient and we're kind of like 
wasting tree computation and like wasting tree computation, because every 
time we do a split we're having the amount of data, at least that we have 
to do more analysis. So it's going to make our tree less rich, less 
effective. If we're not giving the data in a way that's kind of convenient 
for it to do the work it needs to do so. What we could do instead is create 
six columns. We could create a column, cord is very high, is very low, is 
high, is unknown, is low, is medium and h1 would be ones and zeros right, 
so the one or a zero. So we had six columns this one moment so having added 
six additional columns to our data set, the random first now has the 
ability to pick one of these and say like: oh: let's have a look at is 
unknown. There's one possible fit, I can do, which is one versus zero. 
Let's see if that's any good right, so it actually now has the ability, in 
a single step, to pull out a single category level, and so this, this kind 
of coding is called one-hot encoding and for many many types of machine 
learning model. This is like necessary. Something like this is necessary 
like if you are doing logistic regression. You can't possibly put in a 
categorical variable that goes not through five, because there's obviously 
no written linear relationship between that and anything right. So one part 
encoding a lot of people incorrectly, assume that all machine learning 
requires one pod encoding.</p>

<p>But in this case I'm going to show you how we 
could use it optionally and see whether it might improve things sometimes 
yeah, hi Jeremy. So we have six categories. Like in this case, would there 
would be any problems with adding a column for each of the categories? Oh 
Chris, in linear regression we saw we had to do it like. If there's six 
categories, we should only do it for five of them yeah so um it. You 
certainly can say: oh let's not worry about, adding is medium because we 
can infer it from the other. Five, I would say include it anyway, because 
like rather than otherwise the random forest would have to say is very 
high. Know is very low. Know is high, know, is unknown mode is low, know, 
okay and finally, on there right. So it's like five decisions to get to 
that point. So the reason in linear models that you need to not include one 
is because linear models hate collinearity, but we don't care about about 
that here. So we can do one hot encoding easily enough and the way we do it 
is we pass one extra parameter to proc DF, which is what's the max number 
of categories right. So if we say it's seven, then anything with less than 
seven levels is going to be turned into a one, hot encoded bunch of columns 
right. So in this case this has got six levels, so this would be one hot 
encoded where else like zip code has more than six levels, and so that 
would be left as a number and so, generally speaking, you obviously 
probably wouldn't want a one hot in code.</p>

<p>Zip code right because that's 
just going to create masses of data memory, problems, computation problems 
and so forth right, so so this is like another parameter that you can play 
around with. So if I do that, try it out run the random forest. As per 
usual, you can see what happens to the r-squared of the validation set and 
to the rmse of the validation set, and in this case I found it got a little 
bit worse. This isn't always the case and it's going to depend on your data 
set. You know: do you have a data set where you know single categories tend 
to be quite important or not in this particular case, it didn't make it 
more predictive. How what it did do is that we now have different features 
right, so proc TF puts the name of the variable and then an underscore, and 
then the level name, and so interestingly, it turns out that, where else 
before it said that enclosure was somewhat important when We do it as one 
hot encoded, it actually says. Enclosure erupts with AC is the most 
important thing so for at least the purpose of like interpreting your 
model, you should always try one hot encoding. You know quite a few of your 
variables, and so I often find somewhere around six or seven is pretty 
good. You can try like making that number as high as you can, so that it 
doesn't take forever to compute and the feature importance doesn't include 
like really tiny levels that aren't interesting, so that's kind of up to 
you to play it play around with, but in this Case like this is actually I 
found this very interesting.</p>

<p>It clearly tells me I need to find out what 
enclosure erupts with AC is. Why is it important because, like means 
nothing to me right and but it's the most important thing, so I should go 
figure that out so then I had a question: you plus it. So can you explain 
how changing the max number of categories worse? Because for me, it just 
seems like there's five categories or site categories: oh yeah, sorry! So 
it's it's just like all it's doing is saying like okay, here's, a column 
called zip code, here's a column, called usage band and here's a column sex 
right. I don't know whatever right and so, like zip code has, whatever five 
thousand levels the number of levels in a category we call its cardinality 
okay, so it has a cardinality of five thousand usage banned. Maybe has a 
cardinality of six sex has maybe a cardinality of so when proc TF goes 
through and it says okay, this is a categorical variable. Should i one-hot 
encode it? It checks the cardinality against max and hats and says all five 
thousand is bigger than seven. So I don't one hot encoder and then it goes 
to usage. Band 6 is less than 7. I do one hot encode it goes to. Sex 2 is 
less than 7. I do want to encode it, so it just says for each variable. How 
do I decide whether the one hot encoded or not? We are keeping legal in 
cause? No, once we decide to one hot in code, it does not keep the original 
variable. Maybe the best will be an interval. Well, you don't need a 
labeling code if the.</p>

<p>If so, if the best is an interval, it can approximate 
that with multiple one hot encoding levels. Yeah. So like you know it's a 
the. The truth is that each column is going to have some. You know 
different. You know, should it be label encoded or not, you know which you 
could make on a case-by-case basis. I find in practice it's just not that 
sensitive to this, and so I find like just using a single number for the 
whole data set. Gives me what I need, but you know if you were building a 
model that really had to be as awesome as possible and you had lots and 
lots of time to do it. You can go through men, you know, don't use property 
if you can go through manually and decide which things to use dummies or 
not your you'll see in the code. If you look at the code for property, F, 
Rock D F right like I - never want you to feel like the code that happens 
to be in the fastai library, is the code that you're limited to right. So 
where is that done? You can see that the max n cat gets passed to numerical 
eyes and numerical eyes simply checks. Okay, is that a numeric type and 
it's the number of categories either not in pass to us at all or we've got 
more unique values than there are categories and if so, we're going to use 
the categorical codes. So for any column, where that's where it's skipped 
over that right, so it's remained as a category then at the very end we 
just go. Pandas get dummies, we pass in the whole data frame and so a 
pandas get.</p>

<p>That means you pass in a whole data frame, it checks for 
anything, that's still a categorical variable and it turns it into a dummy 
variable which is another way of saying a one-pot encoding. So you know 
with that kind of approach you can easily override it into your own dummy 
verification. Variable ization did you have a question, so some data has a 
quite obvious order like if you have like a grading system like food, bad 
or whatever things like that. There's an order to that and showing that 
order by doing the dummy variable thing, probably will your benefit, so is 
there a way to just force it to leave alone, one variable just like invert, 
it or yourself, not not in the library, and to remind you like Unless we 
explicitly do something about it, we're not going to get that order. So 
when we, when we import the data, so this is in Lesson one RF: we showed 
how, by default, the categories are ordered alphabetically and we have the 
ability to order them properly. So yeah, if you've actually made an effort 
to turn your ordinal variables into proper ordinals using prop D F, can 
destroy that if you have max MCATs, so the simple thing, the simple way to 
avoid that is, if we know that we always want to use the Codes for usage 
banned rather than the you know like never one hot encoder. You could just 
go ahead and replace it right.</p>

<p>You could just say: okay, let's just go D, F 
dot; u s! -- taband equals DF q, suspend cat codes and it's now an integer 
and so it'll never get page all right. So we kind of already seen how 
variables, which are basically measuring the same thing, can kind of 
confuse our </p>

<h3>8. <a href="https://youtu.be/0v93qHDqq_g?t=55m5s">00:55:05</a></h3>

<ul style="list-style-type: square;">

<li><b> Removing redundant features using a dendogram and '.spearmanr()'for rank correlation, ‘get_oob(df)’, ‘to_drop = []’ variables,  ‘reset_rf_samples()’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Variable importance and there can also make our random forest slightly less 
good, because it requires like more computation to do the same thing. 
There's more columns to check so I'm going to do some more work to try and 
remove redundant features, and the way I do that is to do something called 
a dendrogram and it's a kind of hierarchical clustering. So cluster 
analysis is something where you're trying to look at objects. They can be 
either rows in the data set or columns and find which ones are similar to 
each other. So often you'll see people particularly talking about cluster 
analysis. They normally refer to rows of data and they'll say like oh, 
let's plot it right and like oh there's a cluster and there's a cluster 
right, a common type of cluster analysis time permitting we may get around 
to talking about this in some detail, is called k-means, Which is basically 
where you assume that you don't have any labels at all and you take 
basically a couple of data points at random and you gradually find the ones 
that are near to it and move them closer and closer to centroids. And you 
kind of repeat it again and again, and it's an iterative approach that you 
basically tell how many clusters you want and it'll tell you where it 
thinks that classes are. I really I don't know why, but I really under use 
technique. 20. 30 years ago.</p>

<p>It was much more popular than it is today is 
hierarchical, clustering, hierarchical also known as agglomerated 
clustering and in hierarchical order, agglomerative clustering. We 
basically look at every pair of option up every pair of objects and say: 
okay, which two objects are the closest alright. So in this case, we might 
go okay. Those two objects are the closest and so we've kind of like delete 
them and replace it with the midpoint of the two and then okay here, the 
next two closest if we delete them and replace them with the midpoint of 
the two and you keep doing that Again again right since we're kind of 
removing points and replacing them with their averages, you're gradually 
reducing a number of points by pairwise combining and the cool thing is. 
You can plot that, like so right? So if, rather than looking at points, you 
look at variables, we can say okay, which two variables are the most 
similar. It says: okay, say year and sale elapsed, they're very similar. So 
the kind of horizontal axis here is how similar are the two points that are 
being compared right. So, if they're closer to the right, that means 
they're very similar, so sale year and sale elapsed have been combined and 
they were very similar again, it's like okay, as you know, it'll be like 
correlation coefficient or something like that.</p>

<p>You know in this particular 
case what I actually did, so you get to tell it so in this case I actually 
used Spearman's R, so you guys familiar with correlation coefficients 
already all right, so correlation is cut as almost exactly the same as the 
r-squared right, but It's between two variables, rather than a variable, 
and it's prediction. The problem with a normal correlation is that if the I 
create a new workbook here, if you have data that looks like this, then you 
can do a correlation and you'll get a good result right. But if you've got 
data which looks like this right and you try and do a correlation, it 
assumes linearity. That's not very good right! So there's a thing called a 
rank correlation, a really simple idea: it's replace every point by its 
rank right, so, instead of like so, we basically say: okay, this is the 
smallest, so we'll call that one there's the next one three is next one, 
four five right. So you just replace every number by its rank and then you 
do the same for the y-axis so that 1, 2, 3 and so forth. Right and so then 
you do it like a new plot, where you don't plot the data, but you plot the 
rank of the data and, if you think about it, the rank of this data set is 
going to look an exact line, because every time something was Greater on 
the x-axis, it was also greater on the y-axis. So if we do a correlation on 
the rank, that's called a rank correlation, okay, and so because I want to 
find the columns that are similar in a way that the random forest would 
find them similar.</p>

<p>Random forests, don't care about linearity, they just 
care about ordering. So a rank correlation is the the right way to think 
about that. So Spearman's are is, is the name of the most common rank 
correlation, but you can literally replace the data with its rank and chuck 
it at the regular correlation and you'll get basically the same answer. The 
only difference is in how ties are handled. It's a pretty minor issue if 
you had like a full parabola in that rank, correlation you'll will not 
write right. It has to be has to be monotonic, yeah, yeah, okay, so once 
I've got a correlation matrix, there's, basically a couple of standard 
steps. You do to turn that into a dendogram, which I have to look up on 
stackoverflow each time I do it, you basically turn it into a distance 
matrix, and then you create something that tells you you know which things 
are connected to which other things hierarchically. So this kind of these 
two and this step here, like just three standard steps that you always have 
to do to create a dendogram, and so then you can plot it, and so alright so 
say your and sell a lot soon to be measuring. Basically, the same thing, at 
least in terms of rank, which is not surprising because they elapsed is the 
number of days since the first day in my data set. So obviously, these two 
are nearly entirely correlated with some ties, browser tracks and 
hydraulics flow and coupla system.</p>

<p>All seem to be measuring the same thing, 
and this is interesting because remember coupla system it said was super 
important right, and so this rather supports our hypothesis there's nothing 
to do with whether it's a coupler system, but whether it's whatever kind of 
vehicle it is. It has these kind of features. Product group and product 
groups desk seem to be measuring the same thing. If I base model on fi 
model desk seem to be measuring the same thing, and so once we get past 
that everything else like suddenly, the things are further away. So I'm 
probably going to not worry about those. So we're going to look into these 
one. Two. Three four groups that are very similar: could you pass that over 
there, the citizen that grabbed that the similarity between stick, glint 
and enclosure is higher than with stick lens and anything, that's higher 
yeah? Pretty much I mean it, it's a little hard to interpret, but given 
that stick length and enclosure don't join up until way over here yeah, it 
would strongly suggest that then, that they're a long way away from each 
other. Otherwise you would expect them. We were joined up earlier. I mean 
it's it's possible to construct like a synthetic data set where you kind of 
end up joining things that were close to each other through different 
paths. So you've got to be a bit careful, but I think it's fair to is 
probably assume that stick length or enclosure are probably very different, 
so they are very different, but would they be more similar than, for 
example, stick length and sale day of year? No, which is in a very top, no 
there's nothing to suggest that here because, like the key point, is to 
notice where they sit in this tree right and they both that they sit in 
totally different halves of the tree.</p>

<p>Thank you, but really to actually 
know that the best way would be to actually look at this p.m. and our 
correlation matrix, and if you just want to know how similar is this thing, 
or this thing the Spearman, our correlation matrix, tells you that. Can you 
plus that over there, so today's we are passing the leader cream right? 
Second, we are passing the cream. This is just a data frame, so we're 
passing in DF Cape. So that's the data frame containing the whatever it was 
30 or so features that our random forest thought was interesting. So 
there's no random first being used here, the measure the distance measure 
is being done entirely on rent correlation. So what I then do is I take 
these these groups right and I create a little function that I call get out 
of fans score right, which is it does a random forest for some data frame. 
I make sure that I've taken that data frame and split it into a training 
and validation set, and then I call fit and return the oeob score right. 
So, basically, what I'm going to do is I'm going to try removing each one 
of these one. Two three. Four: five: six, seven and eight nine or so 
variables, one at a time and see which ones I can remove and it doesn't 
make the oob score get worse and each time I run this, I get slightly 
different results. So actually it looks like last time I had seven things, 
not not eight things, so you can see. I just do a loop through each of the 
things that I'm thinking like.</p>

<p>Maybe I could get rid of this because it's 
redundant and I print out the column name and the oeob score of a model 
that is trained after dropping that one column, okay, so the oeob score on 
my whole data frame is point eight, nine and then, after Dropping each one 
of these things they're, basically none of them get much worse. Sale 
elapsed is getting quite a bit worse than say all year, but, like it looks 
like pretty much everything else I can drop with, like only like a third 
decimal place problem. So obviously, though, you've got to remember the 
dendogram, let's take fi model discs and Fi based model right, they're, 
very similar to each other right. So what this says isn't that I can get 
rid of both of them right. I can get rid of one of them because they're 
basically measuring the same thing. Okay. So so then I try it. I say: okay, 
let's try getting rid of one from each group, say: oh yeah, F by based 
model and grouse attracts okay and like let's now have a look. It's like 
okay, I've gone from point. Eight. Nine! Oh two point: eight, eight, eight, 
it's like again so close as to be meaningless, so that sounds good. Simpler 
is better. So I'm now going to drop those columns from my data frame and 
then I can try running the full model again and I can see you know so reset 
our air samples means I'm using my whole data frame.</p>

<p>My whole bootstrap 
sample used for tea estimators and I've got 0.90 seven okay, so I've now 
got a model which is smaller and simpler and I'm getting a good score. 
</p>

<h3>9. <a href="https://youtu.be/0v93qHDqq_g?t=1h7m15s">01:07:15</a></h3>

<ul style="list-style-type: square;">

<li><b> Partial dependence: how important features relate to the dependent variable, ‘ggplot() + stat_smooth()’, ‘plot_pdp()’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>For so at this point, I've now got rid of as many columns as I feel I 
comfortably can ones that either didn't have a good feature importance or 
were highly related to other variables, and the model didn't get worse 
significantly whenever when I removed them. So now, I'm at the point where 
I want to try and really understand my data better by taking advantage of 
the model and we're going to use something called partial dependence. And 
again, this is something that you could like using the Carroll kernel and 
lots of people are going to appreciate this, because almost nobody knows 
about partial dependence and it's a very, very powerful technique. What 
we're going to do is we're going to find out for the features that are 
important. How do they relate to the dependent variable right? So let's 
have a look right. So, let's again since we're doing interpretation, we'll 
set set our samples to 50,000 to run things quickly, we'll take our data 
frame. We'll get our feature importance and notice that we're using Max and 
Kat, because I'm actually pretty interested in terms of interpretation and 
seeing the individual levels, and so here's the top 10. And so let's try 
and learn more about those top 10. So year made is the second most 
important. So one obvious thing we could do would be to plot year made 
against sale elapsed because, as we've talked about already like it just 
seems to make sense that both important, but it seems very likely that they 
kind of combine together to find like how old was The product when it was 
sold, so we could try plotting year, made against sale elapsed to see how 
they relate to each other.</p>

<p>And when we do, we get this very ugly graph, and 
it shows us that year made actually has a whole bunch. That are a thousand 
right. So clearly, you know this is where I would tend to go back to the 
client or whatever and say: ok, I'm guessing that these bulldozers weren't 
actually made in the Year 1000 and they would presumably say to me: oh yes, 
they're ones where we don't know where It was made, you know, maybe before 
1986 we didn't track that or maybe the things that are sold in Illinois. We 
don't have that data provided or or whatever they tell us some reason. So 
in order to understand this plot better, I'm just going to remove them from 
this interpretation section of the analysis. So I'm just going to say: ok, 
let's just grab things where year made is greater than 1930 ok. So, let's 
now look at the relationship between year made and sale, press and there's 
a really great package called GG plot. Gg plot originally was an package G 
G stands for the grammar of graphics, and the grammar of graphics is like 
this very powerful way of thinking about how to produce charts in a very 
flexible way. I'm not going to be talking about it much in this class. 
There's lots of information available online, but I definitely recommend it 
as a great package to use ggplot, which you can pip install. It's part of 
the fastai environment. Already ggplot in python has basically the same 
parameters and API as the R version.</p>

<p>The R version is much better 
documented, so you should read it's documentation to learn how to use it, 
but basically you say: okay, I want to create a plot of this data frame. 
Now, when you create plots most of the datasets you're using, are going to 
be too big to plot, as in like, if you do a scatter plot, it'll create so 
many dots that it's just a big mess. It'll take forever and remember when 
you're plotting things you just you're you're looking at it right, so 
there's no point putting something with a hundred million samples when, if 
you're only used a hundred thousand samples, it's going to be pixel 
identical right. So that's why I call get sample first, so get sample just 
grabs a random sample. Okay, so I'm just going to grab five hundred points 
for now. Okay, so I've got a grab. Five. At a point from my data frame, I 
got a plot a year made against sale price. A EES stands for aesthetic. This 
is the basic way that you set up your columns in ggplot, okay, so this says 
to plot these columns from this data frame and then there's this weird 
thing and GG plot. Where plus means basically add chart elements? Okay, so 
I'm going to add a smoother. So most of the very very often you'll find 
that a scatter plot is very hard to see. What's going on because there's 
too much randomness or else a smoother basically creates a little linear 
regression for every little subset of the graph, and so it kind of joins it 
up and allows you to see a nice smooth curve.</p>

<p>Okay. So this is like the 
main way that I tend to look at univariate relationships and by adding 
standard error equals true. It also shows me the confidence interval of 
this smoother right. So low S stands for locally weighted regression, which 
is this idea of like doing kind of out doing lots of little mini 
regressions. So we can see here. The relationship between year made and 
sale price is kind of all over the place right, which is like not really 
what I would expect. I would. I would have expected that more recent stuff 
it sold more recently would probably be like more expensive because of 
inflation, and because there like more current models and so forth, and the 
problem is that when you look at a univariate relationship like this 
there's a whole lot Of collinearity, going on a whole lot of interactions 
that are being lost, so, for example, why did the price drop yeah? Is it 
actually because, like things made between 1991 and 1997, are less valuable 
or is actually because most of them were also sold during that time? And 
actually there was like maybe a recession then or maybe it was like 
products sold during that time - a lot more people but buying types of 
vehicle that were less expensive, like there's all kinds of reasons for 
that, and so again as data scientists, one of the Things are going to keep 
seeing is that at the companies that you join, people will come to you with 
with these kind of univariate charts where they'll say like.</p>

<p>Oh, my god, 
our sales in Chicago have disappeared. That got really bad or people aren't 
clicking. On. This add anymore and they'll. Show you a chart that looks 
like this and they'll be like what happened and most of the time you'll 
find the answer to the question. What happened is that there's something 
else going on right, so I actually are in Chicago last week. Actually, we 
were doing a new promotion and that's why you know revenue went down it's 
not because people are buying stuff in Chicago anymore. It's because the 
prices were lower, for instance. So what we really want to be able to do is 
say: well, what's the relationship between sale, price and year made all 
other things being equal, so all other things being equal basically means 
if we sold something in 1990 versus 1980, and it was exactly the same Thing 
exactly the same person in exactly the same option so on and so forth. What 
would have been the difference in price, and so to do that we do something 
called a partial dependence plot, and this is a partial dependence plot. 
There's a really nice library which nobody's heard of called PDP, which 
does these partial dependence plots and what happens is this we've got our 
sample of 500 data points right and we're going to do something really 
interesting we're going to take each one of those hundred randomly Chosen 
options and we're going to make a little data set out of it right so, like 
here's, our here's elf come on here's our data set of like 500 options and 
here's our columns, one of which is the thing that we're interested in 
which is year made.</p>

<p>So here's year made okay and what we're going to do is 
we're now going to try and create a chart where we're going to try and say 
all other things being equal in 1960? How much did bulldozers cost? How 
much did things cost in options, and so the way we're going to do that is 
we're going to replace the year made column with 1960 we're going to copy 
in the value 1960 again and again and again, all the way down right. So now 
every row the year made is 1960 and all of the other data is going to be 
exactly the same and we're going to take our random forest. We're going to 
pass all this through our random forest to predict the sale price. So that 
will tell us for everything that was auctioned. How much do we think it 
would have been sold for if that thing was made in 1960, and that's what 
we're going to plot here all right? That's the price we're going to put 
here and then we're going to do the same thing for 1961. Alright, we're 
going to replace all these and do 1961 yeah, so to be clear: we've already 
fit the random forest, yes and then we're just passing a new year and 
seeing what it determines the price should be yeah. So this is a lot like 
the way we did feature importance, but rather than randomly shuffling the 
column, we're going to replace the column with a constant value.</p>

<p>All right, 
so randomly shuffle in the column, tells us how accurate it is when you 
don't use that column anymore, replacing the whole column with a constant 
tells us or estimates for us how much we would have sold that product for 
in that auction on that day. In that place, if that product had been made 
in 1961 right, so we basically then take the average of all of the sale 
prices that we calculate from that random first, and so we drew it in 1961 
and we get this value right. So what the partial dependence plot here shows 
us is each of these light. Blue lines actually is showing us all 500 lions. 
So it says for row number 1 in our data set if we sold it in 1960, we're 
going to index that to 0 right so call that zero right if we sold it in 
1970, that particular auction would have been here if we sold it in 1980. I 
would have been here if he sold in 1990 would have been here, so we 
actually plot all 500 predictions of how much every one of those 500 
auctions would have gone for if we replace it before replacing a year made 
with each of these different values And then then, this dark line here is 
the average right. So this tells us how much would we have sold on average 
all of those options for if all of those products were actually made in 
1985, 1990, 1993, 1994 and so forth, and so you can see.</p>

<p>What's happened 
here is at least in the period where we have a reasonable out of data, 
which is since 1990. This is basically a totally straight line, which is 
what you would expect right, because if it was sold on the same date and it 
was the same kind of tractor, it sold to the same person in the same option 
house, then you would expect more recent vehicles To be more expensive 
because of inflation, and because they're they're newer right, they're, not 
they're, not as secondhand, and you would expect that relationship to be 
roughly linear and that's exactly what we're finding ok. So by removing all 
of these externalities, it often allows us to see the truth much more 
clearly as a question the back. Can you pass that back there you're done. 
Ok, so um this, this partial dependents plot concept is something which is 
using a random forest to get us a more clear interpretation of what's going 
on in our data, and so the steps were to first of all, look at the feature 
importance to tell us like Which things do we think we care about and then 
to use the partial dependence plot to tell us what's going on on average 
right there's another cool thing we can do with PDP as we can use clusters 
and what clusters does is it uses cluster analysis? To look at all of these 
each one of the 500 rows and say to some of those 500 roads kind of move. 
In the same way and like we could kind of see, it seems like there's a 
whole lot of rows, that kind of go down and then up and there seems to be a 
bunch of rows that kind of go up and then go flat like it does Seem like 
there's some kind of different types of behaviors being hidden, and so here 
is the result of doing that.</p>

<p>Cluster analysis right is, we still get the 
same average, but it says here kind of the five most common shapes that we 
see, and this is where you could then go in and say all right. It looks 
like some kinds of vehicle. Actually, after 1990, their prices are pretty 
flat and before that they were pretty linear some kinds of vehicle and of 
exactly the opposite, and so like different kinds of vehicle. Have these 
different shapes right, and so this is something you could dig into. I 
think it was one at the back. Oh you could, okay. So what we're going to do 
with this information? Well, the purpose of interpretation is to learn 
about a data set, and so why do you want </p>

<h3>10. <a href="https://youtu.be/0v93qHDqq_g?t=1h21m50s">01:21:50</a></h3>

<ul style="list-style-type: square;">

<li><b> What is the purpose of interpretation, what to do with that information ?</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>To learn about a data set, it's because you it's, because you want to do 
something with it right. So in this case, it's not so much something. If 
you're trying to win a cogwheel competition, I mean it can be a little bit 
like some of these insights. Might make you realize all I could transform 
this variable or create this interaction or whatever obviously feature 
importance is super important for cowgirl competitions, but this one much 
more for like real life. You know so this is when you're talking to 
somebody and you say to them like okay, those plots you've been showing me 
which actually say that, like there was this kind of dip in prices, you 
know based on, like things made between 1990 and 1997, there wasn't Really 
you know, actually it was they were increasing. There was actually 
something else going on at that time. No, it's basically the thing that 
allows you to say like so. Whatever this outcome, I'm trying to drive in my 
business is: this is how something's driving it all right. So if it's like 
I'm looking at, you know kind of advertising technology. What's driving 
clicks that I'm actually digging into say? Okay, this is actually how 
clicks are being driven. This is actually the variable, that's driving it. 
This is how it's related. So, therefore, we should change our behavior in 
this way. That's really the goal of any model.</p>

<p>I guess there's two possible 
goals: 1 goal of a model is just to get the predictions like if you're 
doing hedge fund trading, you probably want to know what the price of that 
equity is going to be if you're doing insurance. You probably just want to 
know how much claims that guy's going to have, but probably most of the 
time, you're actually trying to change something about how you do business, 
how you do marketing how you do just sticks, so the thing you actually care 
about is how The things are related to each other. All right, I'm sorry. 
Can you explain again when you scroll up and you were looking at the sale, 
pricier may looking at the entire model, and you saw that dip and you said 
something about that dip didn't signify what we thought it did. Can you 
explain why yeah? So this is like a classic boring, univariate plot right, 
so this is basically just taking all of the dots all of the options 
plotting year made against sale, price and we're gon na just fitting a 
rough average through them, and so true that products made between 1992 And 
1997 on average in our data set, are being sold for less so, like very 
often in business. You'll hear somebody look at something like this and 
they'll be like. Oh, we should. We should stop auctioning equipment that is 
made in that year in those years because, like we're getting less money, 
for example, but if the truth actually is that during those years, it's 
just that people were making more small industrial equipment where you 
would expect it to be Sold for less and actually our profit on, it is just 
as high, for instance, or during those years.</p>

<p>It's not that it's not things 
made during those years now would have repeat cheaper it's that during 
those years when we were selling things in those years, they were cheaper 
because, like there was a recession going on. So if you're, trying to like, 
actually take some action based on this, you probably don't just care about 
the fact that things made in those years are cheaper on average. But how 
does that impact today? You know so so this this approach, where we 
actually say, let's try and remove all of these externalities. So if 
something is sold on the same day to the same person of the same kind of 
vehicle, then actually have, as year made impact price, and so this 
basically says, for example, if I am deciding what to buy at an option, 
then this is kind of Saying to me, okay, like getting a more recent vehicle 
on average, really does on average, give you more money, which is not what 
the kind of the naive univariate plot said, that, because it's Tyler,  for 
like this bulldozer bulldozers made in 2010, probably are Not close to the 
type of bulldozers that were made in 1960 right and, if you're taking 
something that would be so very different, like a 2010 bulldozer and then 
trying to just drop it to say. Oh, if it was made in 1960 that may cause 
poor prediction at a point, because it's so you're outside absolutely 
rainy. Absolutely so you know, I think, that's a good point. It's you know 
it's a limitation, however.</p>

<p>Random forest is if you're got a kind of data. 
Point that's like over client, you know which is kind of like in a part of 
the space that it's not seen before, like maybe people didn't put air 
conditioning really in bulldozers in 1960 and you're saying how much would 
this bulldoze over their conditioning have gone for 1960, you don't really 
have any information to know that. So you know you it's a it's it's. This 
is still the best technique I know of, but it's it's not perfect, and you 
know you kind of hope that the trees are still going to find some useful 
truth. Even if, though, it hasn't seen that combination of features before 
but yeah, it's something to be aware of so you can also do the same thing 
in a PDP interaction plot and a PDP interaction plot, which is really what 
I'm trying to get to here is like How to sail elapsed and year made 
together impact price, and so, if I do a PDP interaction plot, it shows me 
sail elapsed versus price. It shows me year made versus price and it shows 
me the combination versus price remember. This is always log of price. 
That's why these prices look weird right, and so you can see that the 
combination of sale elapsed and year made is, as you would expect later 
dates. So more or less time is giving me I'm sorry, it's the other way 
around. Isn't it so the higher crisis? Those where there's the least 
elapsed and the most recent year made so you can see here, there's the 
univariate relationship between sale, elapsed and price, and here is the 
univariate relationship between year made and price.</p>

<p>And then here is the 
combination of the two. It's enough to see like clearly that these two 
things are driving christs together. You can also see these are not like 
simple diagonal lines, so it's kind of some interesting interaction going 
on and so based on. Looking at these plots, it's enough to make me think. 
Oh, we should maybe put in some kind of interaction term and see what 
happens. So, let's come back to that in a moment, but let's just look at a 
couple more remember in this case I did one hot encoding way back at the 
top here I said max and cat equals seven, so I've got like enclosure erupts 
with AC. So if you've got one hot encoded variables, you can pass an array 
of them to pit plot PDP and it'll treat them as a category right, and so in 
this case, I'm going to create a PDP plot of these three categories. I'm 
going to call it enclosure, and I can see here that enclosure erupts with 
AC on average are more expensive than enclosure erupts and enclosure arose. 
It actually looks like enclosure erupts from closure, erupts are pretty 
similar, or else erupts with AC is higher. So this is, you know at this 
point you know I probably being fine to hop into Google and like type 
erupts and erupts and find </p>

<h3>11. <a href="https://youtu.be/0v93qHDqq_g?t=1h30m15s">01:30:15</a></h3>

<ul style="list-style-type: square;">

<li><b> What is EROPS / OROPS ?</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Out what the hell these things are, and here we go so it turns out that 
erupts is enclosed rollover, protective structure, and so it turns out that 
if your your bulldozer is fully enclosed, then optionally, you can also get 
air conditioning. So it turns out that actually this thing is telling us 
whether it's got air conditioning. If it's an open structure, then 
obviously you don't have air conditioning at all. So that's what these 
three levels are, and so we've now learnt all other things being equal. The 
same bulldozer sold at the same time, built at the same time, sold to the 
same person is going to be quite a bit more expensive is if it has air 
conditioning than if it doesn't ok. So again, we're kind of getting this 
nice interpretation ability - and you know now that I spent some time with 
this data set. I'd certainly noticed that this you know knowing this is the 
most important thing you do notice that there's a lot more air conditioned 
bulldozers nowadays and they used to be, and so there's definitely an 
interaction between kind of date and that so based on the earlier 
interaction Analysis I've tried first of all setting everything before 1950 
to 1950s. It seems to be some kind of missing value. I've been set age to 
be equal to sale year year made, and so then I try running a random forest 
on that, and indeed page is now. The single biggest thing sale elapsed is 
way back down here year.</p>

<p>Made is back down here, so we've kind of used this 
to find an interaction, but remember, of course, a random forest can 
create, or it can create an interaction through having multiple split 
points. So we shouldn't assume that this is actually going to be a better 
result and in practice I actually found when I looked at my score and my 
rmse, adding age was actually a little worse and we'll see about that. 
</p>

<h3>12. <a href="https://youtu.be/0v93qHDqq_g?t=1h32m25s">01:32:25</a></h3>

<ul style="list-style-type: square;">

<li><b> Tree interpreter</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Later, probably in the next lesson, ok, so one last thing is tree 
interpreter, so this is also in the category of things that most people 
don't know exists, but it's super important, almost pointless for like 
cattle competitions but super important for real life and here's the idea. 
Let's say you're an insurance company and somebody rings up and you give 
them a quote and they say: oh that's, five hundred dollars more than last 
year. Why? Okay, so in general, you've made a prediction from some model 
and somebody asks why, and so. This is where we use this method called tree 
interpreter and what tree interpreter does is it allows us to take a 
particular row so in this case we're going to pick row number zero right. 
So here here is row: zero right, uh. Presumably this is like year made, I 
don't know what all the codes stand for, but like his is all of the columns 
in row 0. What I can do with a tree interpreter is, I can go t i dot 
predict pass in my random forest pass in my row, so this would be like this 
particular customers, insurance information or this in this case this 
particular option right and it'll. Give me back three things: the first is 
the prediction from the random forest. The second is the bias. The bias is 
basically the average sale price across the whole original data set right 
so, like remember, you know random forest.</p>

<p>We started with single trees. 
Oh, we haven't got to draw in there anymore, but remember we started with a 
single tree in our random forest and we split it once and then we spit that 
once and then we split that one straight. We said like oh: what's the 
average value for the whole data set, then what's the average value for 
those where the first split was true and then what's the average value 
where the next that was also true until eventually you get down to the leaf 
nodes where You've got the average value you predict right, so you can kind 
of think of it. This way, if this for a single tree, if this is our final 
leaf, node right - maybe we're predicting like nine point, one right and 
then maybe the average log sale price for the whole. The whole lot is like 
ten point: two right: that's the average through all the options, and so 
you could kind of like work your way down here. So let's go and create 
this. That's actually go and run this, so I can see it okay. So let's go 
back and redraw this single tree you'll find like in Jupiter notebooks, 
often a lot of the things we create like videos, progress bars and stuff. 
They don't know how to like save themselves to the file. So you'll see just 
like a little string here, and so you actually have to rerun it to create 
the string. So this was the single tree that we created so the whole 
dataset had an average log sale price of 10.2. The data set for those with 
capital system equals true had an average of ten point.</p>

<p>Three, the data set 
for capital system equals true enclosure. Less than point lesson two was 
nine point: nine and then eventually we get all the way up here and also a 
model ID less than forty five. Seventy three, it's ten point two, so you 
could kind of like say, okay. Why did this particular row? Let's say we had 
a row that ended up over in this leaf node. Why did we predict him point 
two? Well, it's because we start with ten point, one nine and then because 
the capitalist system was was was less than point five, so it was actually 
false. We added about point two to that, so we went from ten point one to 
ten point three right. So ten point two to ten point three. So we added a 
little bit because if this one is true and then to go from ten point three 
to nine point: nine so because enclosure is less than two we subtracted 
about 0.4 and then because model ID was less than 45-hundred. We added 
about point seven right, so you can see like with a single tree. You could 
like break down like. Why is it that we predicted ten point two retinas 
like and each one of these decision points we're adding or subtracting a 
little bit from the value. So what we could then do is we could do that for 
all the treats and then we could take the average. So every time we see 
enclosure did we increase or decrease the value and how much, by every time 
we see model ID, did we increase? What decrease the value and how much by, 
and so we could take the average of all of those and that's what ends up in 
this thing called contributions, so here is all of our predictors, and here 
is the value of each, and so this is telling us And I've sorted them here 
that the fact that this thing was made in 1999 was the thing that most 
negatively impacted our prediction and the fact that the age of the vehicle 
was 11 years was more most positively impacted um.</p>

<p>I think you actually 
needs a sort. After you zip them together, they seem to be sort of negative 
point. Five. Well, my bunions are sorted, but then they're just reassigned 
to the columns in the original order, which is what's. Thank you thank you. 
That makes perfect sense. Yes, we need to do an index sort. Okay, thank 
you. We will make sure we fix that by next week, so we need to sort columns 
by the index from contributions. So then there's this thing called bias, 
and so the bias is just the average, but before we start doing any splits 
right. So if you basically start with the average log of value and then we 
went down each tree and each time we saw a year made, we had some impact 
couple systems, some impact product, size, some impact and so forth, right, 
. Okay. So I think what we might do is we might come back to because we 
could have out of time. We might come back to tree interpreter next time, 
but the basic idea. This is the last. This is the last of our key 
interpretation points, and the basic idea is that we want some ability to 
not only tell us about the model as a whole and how it works on average. 
But to look at how the model makes predictions for an individual row and 
that's what we're doing here: okay, great next, everybody see you on this 
way. </p>






  </body>
</html>
