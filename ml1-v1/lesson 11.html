<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Lesson 11</title>
    <meta charset="UTF-8">
  </head>
  <body>
  <p style="text-align: right"><a href="http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826">fast.ai: Intro to Machine Learning 1 (v1) (2018)</a></p>
  <h1>Lesson 11</h1>
  <h2>Outline</h2>
<ul>

<li>Rewriting fit from scratch</li>
<li>Digression of Momentum</li>
<li>Rewriting gradient and step within fit function</li>
<li>NLP</li>
<li>Bag of words / CountVectorizer</li>
<li>LogisticRegression w. Sentiment</li>

</ul>


  <h2>Video Timelines and Transcript</h2>


<h3>1. <a href="https://youtu.be/XJ_waZlJU8g?t=1s">00:00:01</a></h3>

<ul style="list-style-type: square;">

<li><b> Review of optimizing multi-layer functions with SGD</b></li>

<li><b>“d(h(g(f(x)))) / dw = 0,6”</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>So, let's just yeah: let's start by reviewing kind of what we've learnt 
about optimizing, multi-layer functions with SGD, and so the idea is that 
we've got some data and then we do something to that data. For example, we 
multiply it by a weight matrix and then we do something to that. For 
example, we put it through a softmax or a sigmoid, and then we do something 
to that, such as do a cross, entropy loss or a root mean squared error, 
loss. Okay and that's gon na like give us some scalar, so this is gon na, 
have no hidden layers. This has got a linear layer, a nonlinear activation 
being a soft Max and a loss function being a root mean squared error or a 
cross entropy all right and then we've got our input: data, port, linear, 
nonlinear bus. So, for example, if this was sigmoid or softmax, and this 
was cross entropy, then that would be logistic regression. So it's still ya 
cross entropy yeah. Let's do that next short for now. Think of it, like 
think of RIT, means great error. Same thing: some loss function. Okay, now 
we'll look at cross entropy again in the moment. So how do we calculate the 
derivative of that with with respect to our weights right? So really, it 
would probably be better if we said X, comma, W yeah, because it's really a 
function of the weights as well, and so we want the derivative of this with 
respect to our weights. Sorry, I put it in the wrong spot.</p>

<p>Oh G, f of X, 
comma W! That's why that didn't make sense all right, so to do that we just 
basically we do the chain rule. So we just say that this is equal to H of U 
and u equals G! Well, G du equals G of V and V equals f of X, so we can 
just rewrite it like that right and then we can do the chain rule. So we 
can say that's equal to H. The derivative is H: u, by G dash V by F, dash X 
hacker with all that so far, okay. So, in order to take the derivative with 
respect to the weights, therefore, we just have to calculate that 
derivative with respect to W using that exact formula. So if we had in 
there yeah yeah so so d of all that DW would be yep. So then, if, if we, 
you know, went further here and had like another linear layer right, that's 
going to be more room now that linear layer I cover w2 okay. So we have 
another Lydia lair. There's no difference to now calculate the derivative 
with respect to all of the parameters. We can still use the exact same 
chain rule right, so so don't think of the multi-layer network as being 
like things that occur at different times. It's just a composition of 
functions, and so we just use the chain rule to calculate all the 
derivatives at once. You know there's that there just a set of parameters 
that happen to appear in different parts of the function, but look at that. 
The calculus is no.</p>

<p>No different so to calculate this with respect to w1 
and w2. You know it's it's just you just increase, you know W. You can just 
now just call it W and say w1 is all of those weights. So the result. Ah, 
that's a great question. So what you're going to have, then, is a list of 
parameters right so here's w1 and like it's it's it's probably some kind of 
higher rank tensor. You know like if it's a convolutional layer it'll, you 
don't be like a wreck, three tensor or whatever, but we can flatten it out. 
That would just make it a list of parameters. There's w1. Here's w2. Okay! 
It's just another list of parameters right and here's. Our loss, which is a 
single you, know a single number. So therefore, our derivative is just a 
vector of that same length right it's. How much does changing that value of 
W affect the loss? How much does changing that value of W affect the loss 
right? So you can basically think of it as a function like you know, y 
equals ax, 1, plus BX, 2, plus C right and say like oh, what's the 
derivative of that with respect to a B and C, and you would have three 
numbers: the derivative with respect To a and B and C - and that's all, 
this is right, if the derivative with respect to that weight, that weight 
and that weight and that weight that went that way to get there inside the 
chain rule.</p>

<p>We had to calculate and a lot of detail here, but we had to 
calculate like jacobians so like the derivative, when you take a matrix 
product, is you've now got something where you've got like a a weight. 
Matrix and you've got an input vector. These are the activations and the 
previous layer right and you've got some new output, activations right and 
so now you've got to say like okay, for this particular sorry for this 
particular weight. Hablas changing this particular weight change, this 
particular output and how does changing this particular weight change, this 
particular output and so forth? So you kind of end up with these higher 
dimensional tensors showing like for every weight. How does it affect every 
output all right, but then, by the time you get to the last function, the 
last function is going to have like a mean or a sum or or something so 
they're all going to get added up in the end, and so this Kind of thing, 
like I don't know it drives me a bit crazy to try and calculate it out by 
hand or even think of it, step by step, because you tend to have like you 
just have to remember for every input and Aleya for every output. In the 
next layer you know you're going to have to account for every weight for 
every output you're going to have to have a separate grant. Yet one good 
way to look at this is to learn to use pytorches like dot grad attribute 
and got backward method manually and like look up the to tour, the 
pytorched tutorials, and so you can actually start setting up some 
calculations with a vector import In the vector output and then type dot 
backward and then say type grad and like look at it right and then do some 
really small ones, with just two or three items in the input and output 
vectors and let make the make the operation like plus two Or something and 
like see what the shapes are make sure it makes sense yeah, because it's 
kind of like this vector matrix calculus is not like introduces zero new 
concepts to anything you learnt in high school, like strictly speaking, but 
getting a feel for how these shapes Move around I find took a lot of 
practice. You know the good news.</p>

<p>Is you almost never have to worry about 
it? Okay, so we were talking about. Then </p>

<h3>2. <a href="https://youtu.be/XJ_waZlJU8g?t=9m45s">00:09:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Review of Naive Bayes &amp; Logistic Regression for NLP with lesson5-nlp.ipynb notebook</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Using this kind of logistic regression for NLP and before we got to that 
point, we were talking about using naive Bayes for NLP, and the basic idea 
was that we could take a document write. A review like this movie is good 
and turn it into a bag of words, representation consisting of the number of 
times each word appears right and we call this the vocabulary. This is the 
unique list of words. Okay and we used the SK, learn count vectorizer to 
automatically generate both the vocabulary which in SK low and they call 
they call the features and to court, create the bag of words, 
representation, z' and the whole group of them, then, is called a term 
document matrix. Okay and we kind of realized that we could calculate the 
probability that a positive review contains the word this by just 
averaging. The number of time disappears and the positive reviews, and we 
could do the same for the and we could do the same for the negatives. All 
right and then we could take the ratio of them to get something which, if 
it's greater than one was a word that appeared more often in the positive 
reviews or less than one was a word. That appeared more often than the 
negative reviews. Okay and then we realized, you know using using Bayes 
rule that we and taking the logs that we could basically end up with 
something where we could add up the logs of these plus the log of the ratio 
of the probabilities that things are in class.</p>

<p>One versus class zero and 
end up with something we can compare to zero. If it's be greater than zero, 
then we can predict a document is positive or if it's less than zero, we 
can predict the document is negative and that was our Bayes rule all right. 
So we kind of did that from math first principles and I think we agreed 
that the the naive in naive Bayes was a good description because it assumes 
independence when it's definitely not true. But it's an interesting 
starting point and I think it was interesting to observe when we actually 
got to the point where, like okay, now we've, you know calculated the the 
ratio of the probabilities and took the log and now, rather than multiply 
them together. Of course, we have to add them up and when, when we actually 
wrote that down, we realized like. Oh, that is, you know, just a standard 
weight, matrix product plus a bias right, and so then we kind of realized 
like oh okay, so like. If this is not very good accuracy, eighty percent 
accuracy, why not improve it by saying hey, we know other ways to calculate 
a cut. You know a bunch of coefficients and a bunch of biases, which is to 
learn them in a logistic regression. Alright, so in other words this this 
is the meanwhile, we use for a logistic regression, and so why don't we 
just create a logistic regression and fit it okay, and it's going to be 
give us the same thing, but rather than coefficients and biases, which are 
theoretically Correct based on you know this assumption of Independence and 
based on Bayes rule there'll be the coefficients and biases that are 
actually the best in this data right, so that was kind of where we got to, 
and so the kind of key insight here is like just About everything I find a 
machine learning ends up being either like a tree or you know a bunch of 
matrix products and monomi era.</p>

<p>T's right, like it's everything, seems to 
end up kind of coming down to the same thing, including, as it turns out 
Bayes rule. Okay and then it turns out that nearly all of the time, then, 
whatever the parameters are in that function, nearly all the time it turns 
out that they're, better learnt then calculated based on the theory right 
and indeed that's what happened when we actually tried learning those 
Coefficients we got, you know 85 % okay. So then we noticed that we could 
also, rather than take the whole term document matrix. We could instead 
just take them the you know ones and zeros for presence or absence of a 
word, and you know. Sometimes it was, you know this equally as good, but 
then we actually tried something else, which is we tried, adding 
regularization and with regularization the binarized approach turned out to 
be a little better, all right, so then regularization was where we took the 
loss function and again, Let's start with our MSE and then we'll talk about 
cross-entropy loss function was our predictions, our actuals sum that up 
take the average plus a penalty. Okay, and so this specifically, is the l2 
penalty. If this instead was the absolute value of W, then that would be 
the l1 penalty. Okay, we also noted that we don't really care about the 
loss function per se.</p>

<p>We only care about its derivatives, that's actually 
the thing that updates the weights, so we can, because this is a sum we can 
take the derivative of each part separately, and so the derivative of this 
part was just that right, and so we kind of learnt that Even though these 
are mathematically equivalent, they have different names. This version, 
it's called weight decay and it's kind of what's used. That term is used in 
the neural net. </p>

<h3>3. <a href="https://youtu.be/XJ_waZlJU8g?t=16m30s">00:16:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Cross-Entropy as a popular Loss Function for Classification (vs RMSE for Regression)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Literature, okay, so cross-entropy. On the other hand, you know it's just 
another. Loss function like root mean squared error, but it's specifically 
designed for classification, alright, and so here's an example of a binary 
cross-entropy. So let's say this is our you know. Is it a cat or a dog so 
as to say, is cat one or a zero, so Cat Cat Cat - and these are our 
predictions - this is the output of our final layer of our neural net or a 
logistic regression or whatever all right, then all we Do is we say: okay, 
let's take the actual times the log of the prediction, and then we add to 
that 1 minus actual times the log of 1 minus the prediction and then take 
the negative of that whole thing right. So I suggested to to you all that 
you tried to kind of write the if statement version of this, so hopefully 
you've done that by now. Otherwise, I'm about to spoil it for you. So this 
was Y times log y plus 1 minus y times. Log 1. Minus y right and negative 
of that, okay, so here wants to tell me how to write this as an if 
statement can she hit me okay, if y equal to sorry, if y equal to 1 mm-hmm, 
then in return I love Y mm-hmm, otherwise, well um else Return: log. 1, 
minus 1. Okay, oh that's the things at brackets and you take C. My name is 
good, so the key inside Chen she's using is that Y has two possibilities: 1 
or 0. Okay, and so very often the math can hide the key insight which I 
think happens here until you actually think about what the values it can 
take right.</p>

<p>So that's that's all it's doing. It's saying either give me 
that or give me that all right could you pass that to the back? Please 
Jason, all right, I'm missing something, but do you know the two variables 
in that statement? If you got Y soon, it'd be like white hat on the wires 
yeah yeah. Thank you as usual. It's me missing something: okay, okay and so 
then the you know the multi category version is just the same thing, but 
you're saying you know it four different. More than just y equals one or 
zero, but y equals zero. One two three four: five: six, seven, eight nine, 
for instance, okay, and so that you know that loss function has a you, can 
figure it out yourself in particularly simple derivative, and it also you 
know. Another thing you could play with at home. If you like is like 
thinking about how the derivative looks, when you add a sigmoid or a 
softmax before it, you know it turns out, it all turns out very nicely, 
because you've got an X P thing going into a log e thing. So you end up 
with you know very well behaved derivatives. The reason, I guess, there's 
lots of reasons that people use IR MSE for aggression and cross-entropy for 
classification, but most of it comes back to this statistical idea of a 
best, linear, unbiased estimator. You know and based on the likelihood 
function that kind of turns out that these have some nice statistical 
properties it turns out. However, in practice, root means great error.</p>

<p>In 
particular, the properties are perhaps more theoretical than actual, and 
actually nowadays using the the absolute deviation rather than some as 
grads deviation can often work better. So in practice, like everything in 
machine learning, I normally try both for particular data set I'll, try 
both loss functions and see which one works better and us, of course, it's 
a cattle competition, in which case you're told how capital is going to 
judge it, and you Should use the same loss function as caracals evaluation 
metric all right, so yeah, so this is really the key insight is like hey, 
let's, let's not use theory, but instead learn things from the data, and 
you know. We hope that we're going to get better results, particularly with 
regularization we do and then I think the key regularization insight here 
is </p>

<h3>4. <a href="https://youtu.be/XJ_waZlJU8g?t=21m30s">00:21:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Creating more NLP features with Ngrams (bigrams, trigrams)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Hey, let's not like try to reduce the number of parameters in our model, 
but instead like use lots of parameters and then use regularization to 
figure out which are actually useful. And so then we took that a step 
further by saying: hey, given we can do that with wrecker ization. Let's 
create lots more features by adding by grams and trigrams. You know, by 
grams, like by vast and by vengeance and trigrams, like by vengeance and by 
Vera, miles right, and you know just to keep things a little faster. We 
limited it to 800,000 features, but you know, even with the full 70 million 
features it works. Just as well and it's not a hell of a lot slower, so we 
created a term document matrix again using the full set of engrams for the 
training set, the validation set. And so now we can go ahead and say: okay, 
our labels as the training set labels as before our independent variables 
is the binarized term document matrix as before, and then let's fit a 
logistic regression to that and do some predictions and we get 90 % 
accuracy. So this is looking pretty good, okay, so the logistic regression. 
Let's go back to our naive Bayes right in our naive Bayes. We have this 
term document </p>

<h3>5. <a href="https://youtu.be/XJ_waZlJU8g?t=23m1s">00:23:01</a></h3>

<ul style="list-style-type: square;">

<li><b> Going back to Naive Bayes and Logistic Regression,</b></li>

<li><b>then ‘We do something weird but actually not that weird’ with “x_nb = x.multiply®”</b></li>

<li><b>Note: watch the whole 15 mins segment for full understanding.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Matrix and then for every feature, we're calculating the probability of 
that feature occurring if it's class 1 that probability of that feature 
occurring if it's plus 2 and then the ratio of those two all right and in 
the paper that we're actually load basing this off. They call this P, this 
Q and this right, maybe I should just fill that in P here, maybe then we'll 
say probability to make it more obvious, okay and so then we kind of said: 
hey, let's, let's not use these ratios as the coefficients in that. In that 
matrix multiply, but let's instead like try and learn some coefficients. 
You know so maybe start out with some random numbers. You know and then try 
and use stochastic gradient descent to find slightly better ones. So you'll 
notice, you know some important features here. The R vector is a vector of 
Rank 1 and it's length is equal to the number of features and, of course, 
our logistic regression coefficient matrix is also of length. 1. Sorry Rank 
1 and length 1/4. The number of features right - and we know we're saying 
like they're kind of two ways of calculating the same kind of thing right, 
one based on theory, one based on data. So here is like some of the numbers 
in R right. Remember it's using the log. So these numbers, which are less 
than zero, represent things which are more likely to be negative, and these 
ones are here are more likely.</p>

<p>So this one here is more likely to be 
positive, and so here's E ^ that - and so these are the ones we can compare 
to one rather than to zero. So I'm going to do something that hopefully, is 
going to seem weird and so first of all, I'm going to talk about. I got to 
say what we got to do and then I'm gon na try and describe why it's weird 
and then we'll talk about why it may not be as weird as we first thought. 
So here's what we got to do we're going to take our term document matrix 
and we're going to multiply it by. So what that means is we're gon na. I 
can do it here in Excel right, so we're going to say: let's grab everything 
in our term document matrix and multiply it by the equivalent value in the 
vector of our right. So this is like a broadcasted. Element-Wise 
multiplication, not a matrix multiplication. Okay and that's what that does 
okay, so here is the value of the term document matrix times R, in other 
words everywhere, that a zero appears there. A zero appears here and every 
time a one appears here, the equivalent value of R appears here. So we 
haven't really: we haven't really changed much all right, we've just we've 
just kind of changed the ones into something else than two into the into 
the odds from that feature, all right, and so what we're now going to do is 
we're going to use this As our independent variables, instead in our 
logistic regression, okay, so here we are multiply x, x, NB, x, naive, 
Bayes version is x times, R and now, let's do a logistic regression fitting 
using those independent variables.</p>

<p>And let's then do that for the 
validation set. Okay and get the predictions, and lo and behold we have a 
better number okay. So let me explain why this hopefully seem surprising, 
given that we're just multiplying. Oh, I picked out the wrong ones. I 
should have said ah not going. Okay, that's actually uh. I got the wrong 
number okay, so that's our independent variables right and then the the 
logistic regression has come up with some set of coefficients. Let's 
pretend for a moment that these are the coefficients that it happened to 
come up with. Okay, we could now say: well, let's not use this set, let's 
not use this set of independent variables, but let's use the original 
binarized feature matrix right and then divide all of our coefficients by 
the values in R and we're going to get exactly the same result. 
Mathematically, so you know, we've got our X naivebayes version of the 
independent variables and we've got some some set of weights, some some 
sort of coefficients I'll call it W right W one, let's see where it's found 
like this is a good set of coefficients for making Our predictions from 
right that X and B is simply equal to x times as in element wise x, ah 
right so in other words, this is equal to x, times ah times the weights and 
so like. We could just change the weights to be that right and get the same 
number. So this ought to mean that the change that we made to the dependent 
variable shouldn't have made any difference, because we can calculate 
exactly the same thing without making that change.</p>

<p>So there's the question: 
why did it make a difference? So, in order to answer this question, I got 
to try and get you all to try and think about this. In order to answer this 
question, you need to think about like okay. What are the things that 
aren't mathematically the same? Why is why? Is it not identical? What are 
the reasons like come up with some hypotheses? What are some reasons that 
maybe we've actually ended up with a better answer and to figure that out 
we need to first of all start with like well. Why is it even a different 
answer? Why is that different? To that? Is it subtle all right? What do you 
think I just wondering if there was two different kinds of multiplications? 
You said that one is the element wise multiplication. No, they do end up 
mathematically being the same okay, pretty much, there's a minor in 
corporate. Not it's not that it's not some order! Operation, see, let's try 
kimchee, you are on a roll today. So let's see how you go, I feel, like Z 
features, aren't less correlated to each other. I mean I've made a claim 
that these are mathematically equivalent. So so what are you saying? 
Really? You know why are we getting different answers? It's good keep on 
coming up with hypotheses. We need lots of wrong answers before we start 
finding. It's really right ones. It's like that. You know warmer hotter 
colder. You know Ernest you're gon na get as hot.</p>

<p>Oh, does it have anything 
to do with the regularization? Ah, yes, and is it the fact that when you, 
let's start there right so Ernest point here is like okay, Jeremy you've 
said they're equivalent but they're equivalent outcomes right, but you got 
through and through a process to get there and that process included 
regularization and they're. Not necessarily equivalent regularization, like 
our loss function as a penalty, so yeah help us think through Ernest how 
much that might impact things well. This is I'm just noticing that the 
numbers are bigger in the ones that have been weighted by the naive phase: 
mm-hmm our weights, and so these are bigger and some are smaller. Some are 
bigger right, but there are some bigger ones, like the variance between the 
columns is much higher. The variance is bigger. Yeah. I think that's a very 
interesting insight. Okay, that's all yeah! Okay, so build on that prince 
has been on a roll or month. So here's not sure is it also consider like 
considering the dependency of different words instead, why this performing 
better, rather than all but independent of each other? No, really I mean 
it's it's you know again. Theoretically, these are creating mathematically, 
equivalent outputs, so they're not they're, not doing something different, 
except, as Ernest mentioned, they're getting impacted differently by 
regularization. So, what's so, what's regularization right regularization? 
Is we start out with our? That was the weirdest thing.</p>

<p>I forgot to go to 
screenwriting mode, and it just turns out that you can actually write in 
Excel and I had no idea that was true. I still use screenwriting Rosewood's 
could kill up my spreadsheet I'd, never trade, so our loss was equal to 
like our cross, entropy loss. You know based on the predictions of the 
predictions and the actuals right, plus our penalty. So, if your, if your 
weights a large right, then that piece gets bigger right and it drowns out 
that piece right, but that's actually the piece we care about right. We 
actually want it to be a good fit, so we want to have as little 
regularization going on as we can get away with. We want so we want to have 
less weights. So here's the thing right, our value. Yes, can you pass it 
over here? We should let less weights. Did you mean lesser weights? I do 
yeah yeah and I use the two words are level equivalently, which is not 
quite fair. I agree, but the idea is that weights that are pretty close to 
zero were kind of not there. So here's the thing, our values of ah you know 
and I'm not a Bayesian weenie, but I'm still gon na use the word prior 
right, they're kind of like a prior so like. We think that the the 
different levels of importance and positive or negative of these different 
features might be something like that right. We think that, like bad, you 
know might be more correlated with negative, then and good right. So our 
kind of implicit assumption the before was that we have no priors.</p>

<p>So in 
other words, when we'd said squared weights, we're saying a nonzero weight 
is something we don't want to have right. But actually, I think what I 
really want to say is that differing from the naive Bayes expectation is 
something I don't want to do right, like only very from the naive Bayes 
prior, unless you have good reason to believe otherwise right, and so 
that's actually what this Ends up doing right, we end up saying you know 
what we think this value is probably three right, and so, if you're going 
to like make it a lot bigger or a lot smaller right, that's going to create 
the kind of variation in weights. That's going to cause that squared term 
to go up right. So so, if you can, you know just leave all these values 
about similar to where they are now all right, and so that's what the 
penalty term is now doing right. The penalty term, when our inputs is 
already multiplied by our is saying, penalize things where we're burying it 
from our naive Bayes prior. Can you pass it? Why multiply only with our not 
constant like a square or something later, when the variance would be much 
higher deciding because our our prior comes from an actual theoretical 
model right, so I said like I don't like to rely on the theory, but I have, 
if I Have some theory, then you know, maybe we should use that as our 
starting point rather than starting off by assuming everything's equal.</p>

<p>So 
our prior said: hey we've got this model coordinate phase and the naive 
Bayes model said if the naive Bayes assumptions were correct, then R is the 
correct coefficient right in this specific formulation that that's why we 
pick that because our our prior is based on that. That theory, okay, so 
this is a really interesting insight which I never really see covered, 
which is this idea is that we can use these. Like you know, traditional 
machine learning techniques we can imbue them with this kind of Bayesian 
sense by by starting out. You know incorporating our theoretical 
expectations into the data that we give our model right and when we do so 
that then means we don't have to regularize as much and that's good right, 
because if we regularize a lot, let's try it. Let's go back to you know. 
Here's our remember the the way they do it in the eschaton logistic 
regression is. This is the reciprocal of the amount of vectorization 
penalty, so will kind of add lots of regularization by making a small so 
that, like really hurts that really hurts our accuracy, because now it's 
trying really hard to get those weights down the loss function is 
overwhelmed by The need to reduce the weights and the need to make it 
predictive is kind of now seen as totally unimportant right. So so, by kind 
of starting out and saying you know what don't push the weights down so 
that you end up ignoring the the terms, but instead push them down so that 
you try to get rid of.</p>

<p>You know ignore differences from our expectation 
based on the naive Bayes formulation, so that ends up giving us a very nice 
result, which actually was originally this. This </p>

<h3>6. <a href="https://youtu.be/XJ_waZlJU8g?t=39m45s">00:39:45</a></h3>

<ul style="list-style-type: square;">

<li><b> ‘Baselines and Bigrams: Simple, Good Sentiment and Topic Classification’ paper by Sida Wang and Christopher Manning, Stanford U.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Technique was originally presented, I think about 2012 Chris Manning who's, 
a terrific NLP researcher, up at Stanford and CETA Wang, who, I don't know, 
but I assume, is awesome because his paper is awesome. They basically came 
up with this with this idea and what they did was they compared it to a 
number of other approaches on a number of other data sets. So one of the 
things they tried is this one is the IMDB data set that and so here's 
naive, Bayes SVM on diagrams and, as you can see, this approach 
outperformed the other linear based approaches that they looked at and also 
some restricted, Boltzmann machine kind of Neural net based approaches they 
looked at now. Nowadays there are better ways there are. You know there are 
better ways to do this and in fact, in the deep learning course we showed 
new state-of-the-art result that we just developed at first, a a that gets 
well over. Ninety four percent - but still you know like particularly for a 
linear technique - that's easy, fast and intuitive. This is pretty good and 
you'll notice when they, when they did this, they only used by grams, and I 
assume that's because they I looked at their code and it was kind of pretty 
slow and ugly. You know I figured out a way to optimize it. A lot more as 
you saw, and so we were able to use here, try grams, and so we get quite a 
lot better. So we've got ninety one point: eight versus a ninety one point 
two, but other than that. It's identical.</p>

<p>Also, I mean they used a support, 
vector machine which is almost identical to a logistic regression in this 
case. So there's some minor differences right. So I think that's a pretty 
cool result and you know I will mention you know what you get to see here 
in class is the result of like many weeks and often many months of research 
that I do, and so I don't want you to think like This stuff is obvious: 
it's not at all like reading this paper, there's no description in the 
paper of like why they use this model, how it's different, why they thought 
it works. You know it took me a week or two to even realize that it's kind 
of like mathematically equivalent to a normal, logistic regression and then 
a few more weeks to realize that the difference is actually in the 
regularization. You know like this is kind of like machine learning, as I'm 
sure you've noticed from the Carroll competitions you enter. You know, like 
you, come up with a thousand good ideas: 999 of them, no matter how 
confident you are they're going to be great, they always turn out to be you 
know, and then, finally, after four weeks, one of them finally works and 
kind of gives you The enthusiasm to spend another four weeks of misery and 
frustration. This is the norm right and and like for sure that the the best 
practitioners I know in machine learning all share one particular trait in 
common, which they're very, very tenacious.</p>

<p>You know also known as stubborn 
and bloody-minded right, which is definitely a reputation. I seem to have 
probably fare along with another thing, which is that they're all very good 
coders. You know they're very good at turning their ideas into new code, so 
yeah. So you know this was like a really interesting experience for me 
working through this a few months ago to try and like figure out how to at 
least you know how to explain why this at there, at the time kind of 
state-of-the-art result, exists and so </p>

<h3>7. <a href="https://youtu.be/XJ_waZlJU8g?t=43m31s">00:43:31</a></h3>

<ul style="list-style-type: square;">

<li><b> Improving it with PyTorch and GPU, with Fastai Naive Bayes or ‘Fastai NBSVM++’ and “class DotProdNB(nn.Module):”</b></li>

<li><b>Note: this long section includes lots of mathematical demonstration and explanation.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Once I figured that out, I was actually able to build on top of it and make 
it quite a bit better and I'll. Show you what I did, and this is where it 
was very, very handy to have pytorch at my disposal, because I was able to 
kind of create something that was customized. Just the way that I want it 
to be, and also very fast by using the GPU, so here's the kind of fastai 
version of the NBS vm. Actually, my friend Steven marady who's, a terrific 
researcher in NLP, has christened this. The NBS VM plus plus, which I 
thought was lovely, so here's that, even though there is no SVM, it's a 
logistic regression but, as I said, nearly exactly the same thing, so let 
me first of all show you like the code, so this is like we try To like 
once, I figure out like okay, this is like the best way I can come up with 
to do a linear bag of words model. I kind of embed it into fastai, so you 
can just write a couple of lines of code, so the code is basically hey. I 
want to create a data class for text classification. I want to create it 
from a bag of words. All right here is my bag of words. Here are my labels. 
Here is the same thing for the validation set and use up to 2,000 unique 
words per review right, which is plenty so then, from that model data 
construct, a learner which is kind of the faster. I generalization of a 
moral which is based on a dot product of naive, Bayes and then fit that 
model and then do a few epochs and after five epochs.</p>

<p>I was already up to 
ninety two point: two okay, so this is now like. You knowgetting quite well 
above this, this linear based one. So let me show you the code for for 
that, so the code is like horrifyingly short. That's it right and it'll 
also look on the whole extremely familiar right. There's, if there's a few 
tweaks here, pretend this thing that says embedding pretend it actually 
says: linear, okay, I'm going to show you embedding in a moment pretend it 
says linear. So we've got basically a linear layer where the number of 
features coming with the number of features as the rows and remember SK, 
learn, features, means number of words basically and then for each row. 
We're going to create one weight, which makes sense right for like a 
logistic regression, every every sort of for each row for each word, each 
word has one weight and then we're going to be multiplying it by the our 
values. So each word we have one our value per class, so I actually made 
this so this can handle like not just positive versus negative, but maybe 
figuring out, like which author created this work, they're, cooking, five 
or six authors, whatever right and basically we kind of use. Those linear 
layers to to get the the value of the weight and the value of the R, and 
then we take the weight times the R and then sum it up, and so that's just 
a dot product. Okay, so just just a simple dot product.</p>

<p>Just as we would do 
for any logistic regression and then do the softmax, so the very minor 
tweak the we add to get the the better result. Is this the main one really 
is this year, this plus something right and the thing I'm adding is it's a 
parameter, but I pretty much always use this. This version of this value 
2.4. So what does this do so? What this is doing is it's again kind of 
changing the prior right. So, if you think about it, even once we used this 
R times the term document matrix as their independent variables, you really 
want to start with a question. Okay, the penalty terms are still pushing W 
down to 0 right. So what did it mean for W to be 0 all right? So what would 
it mean if we had? You know coefficient 0, 0, 0, 0, 0, all right. So what 
that would do when we go.okay this matrix times these coefficients, we 
still get 0 right, so a weight of 0 still ends up saying. I have no opinion 
on whether this thing is positive or negative. On the other hand, if they 
were all one right, then it's basically says my opinion is that the naive 
Bayes coefficients are exactly right. Okay, and so the idea is that I said 
0 is almost certainly not the right prior right. We shouldn't really be 
saying: if there's no coefficient, it means ignore the naive Bayes 
coefficient. One is probably too high right, because we actually think that 
naive, Bayes is only kind of part of the answer.</p>

<p>All right - and so I 
played around with a few different data - sets where I basically said: take 
the weights and add to them some constant right and so 0 would become in 
this case 0.4. All right so, in other words, the the regularization 
penalty, is pushing the weights not towards zero, but towards this value 
right, and I found that across a number of data sets zero point. Four works 
pretty well, but and it's pretty resilient alright. So again, this is the 
basic idea is to kind of like get the best of both worlds. You know where, 
where we're learning from the data using a simple model but we're 
incorporating you know our prior knowledge as best as we can, and so it 
turns out when you say: okay, let's, let's tell it, you know, as white 
matrix of zeros actually means that You should use about, you know about 
half of the values that ends up that ends up working better than the prior, 
that the weights should all be 0. Yes, is the the weights, the W? Is it 
that the point for the amount of regression required the amount of so we 
have this? You know bad things. We have the term where we reduce the amount 
of error. The prediction error - rmse plus we have the regularization - and 
is it W to the point for denote the amount of visualization required, so W 
are the weights right. So this is calculating our activations okay, so we 
calculate our activations as being equal to the weights times. There are 
some right, so that's just our normal, normal linear function right.</p>

<p>So so 
the the thing which is being penalized is my weight matrix. That's what 
gets penalized so by saying hey, you know what don't just use W use W plus 
0.4. So that's not being penalized, it's not part of the weight matrix. 
Okay, so effectively the weight matrix gets point four for free yeah. So, 
by doing this even after regularization, then every I'm sorry every feature 
is getting some form of fate, some form of weight or something um, not 
necessarily because it could end up choosing a coefficient of negative 0.4 
for a feature, and so that would say you know What even, though, though 
naive Bayes says it's the AH should be whatever for this feature, I think 
you should totally ignore it. Yeah great questions. Okay, we started at 20 
past okay, let's take a break for about eight minutes or so and start back 
about 25. Okay, so a couple of questions at the break. The first was just 
for a kind of reminder or a bit of a summary as to what's going on here 
right and so here we have W plus I'm writing it out. Yeah plus adjusted 
weight of weight adjustment times right, so so normally what we were doing 
so normally what we are doing is saying: hey, logistic regression is 
basically WX right, I'm going to ignore the bias okay and then we were 
changing it to be W dot times X right and then we were kind of saying: 
let's do that bit first right, although in this particular case actually 
now I look at it, I'm doing it in this code.</p>

<p>It doesn't matter, obviously 
in this code, I'm actually doing and during this bit first, and so so this 
thing here, actually I call it W, which is probably pretty bad, it's 
actually W times X right. So so, instead of W times X times, R, I've got W 
times X, plus a constant times R right. So the key idea here is that 
regularization can't draw in yellow that's fair enough. Regularization 
wants the weights to be 0 right because we're trying it's trying to reduce 
that okay, and so what we're saying is like. Ok, we want to push the 
weights towards 0 because we're saying like that's our like default 
starting point expectation is the weights 0, and so we want to be in a 
situation where, if the weights are 0, then we have a model that like makes 
theoretical or Intuitive sense to us right this model if the weights are 0, 
doesn't make intuitive sense to us right, because it's saying hey multiply. 
Everything by 0 gets rid of all of that and gets rid of that as well, and 
we were actually saying no. We actually think our R is useful. We actually 
want to keep that right so so, instead we say you know what let's take that 
piece here and add 0.4 to it all right. So now, if the regularizer is 
pushing the weights towards 0, then it's pushing the value of this sum 
towards 0.4 right, and so therefore, it's pushing a whole model to 0.4 
times R right so, in other words, our kind of default starting point.</p>

<p>If 
you've regularize to all the weights out altogether is to say yeah, you 
know, let's use a bit of our that's, probably a good idea: okay, . So 
that's the idea right! That's the idea is basically, you know what happens 
when when that's zero, but you and you want that to like be something 
sensible, because otherwise regularizing the weights to move in that 
direction, wouldn't be such a good idea. Okay, second question was about 
engrams, so the N in Engram can be uni by try, whatever one two three, 
whatever grounds so so the this movie is good right. It has four unique 
grams. This movie is good. It has three by grams. This movie movie is, is 
good. It has two trigrams. This movie is movie is good, okay doc. Can you 
pass it? Do you mind, go back to the double ad change that zero point four 
stuff yeah. So I was wondering if this adjustment will harm the 
predictability of the model because think of extreme extreme case, if it's 
not zero point four, if it's four thousand and or black coefficients will 
be like right. So so exactly so, so our prior needs to make sense, and so 
our prior here - and you know this is why it's called dot prod NB is there 
prior is that this is something where we think naive. Bayes is a good prior 
right and so naive. Bayes says that our equals p over. That's not how you 
write P P over Q. I have not had much sleep P over Q is a good prayer, and 
not only do we think it's a good prior that we think our times X, plus B is 
a good model.</p>

<p>That's that's the naive Bayes model, so in other words, we 
expect that you know a coefficient of one is a good coefficient, not not 
4,000 yeah. So we think specifically, we don't think we think 0 is probably 
not a good coefficient all right, but we also think that maybe the naive 
Bayes version is a little overconfident, so maybe one's a little high. So 
we're pretty sure that the right number, assuming that our moral only Bayes 
model is appropriate, is between 0 & amp 1. No. But what I was thinking is 
as long as it's not 0. You are pushing those coefficients that are supposed 
to be 0 to something, not zero and make the like high coefficients less 
distinctive from 0 coefficients well, but you see they're not supposed to 
be 0 they're supposed to be are like that's that's what they're supposed to 
be They're supposed to be are right and so and remember this is inside our 
forward function, so this is part of what we're taking the gradient of 
right. So it's basically saying: okay, we're still gon na you know you can 
still set self W to anything you like, but just the regularizer wants it to 
be zero, and so all we're saying is okay. If, if you want it to be zero, 
then I'll try to make 0 B, you know give a sensible answer. That's the 
basic idea and, like yeah, nothing says, point fours perfect.</p>

<p>For every 
data set, I've tried a few different data sets and found various numbers 
between point three and point six that are optimal, but I've never found 
one. Where point four is less good than zero, which is not surprising, and 
I've also never found one where one. It's better right, so the idea is like 
this is a reasonable default, but it's another parameter you can play with 
which I kind of like right. It's another thing you could use grid, search 
or whatever to figure out from your data set what's best, and you know 
really the key here being every model before this one, as far as I know, 
has implicitly assumed it should be zero because they just they don't Have 
this parameter right and you know by the way I've actually got a second 
parameter here as well, which is the same thing I do to R, is actually 
divided by a parameter which I'm not going to worry too much about it. Now 
but again it's another parameter. You can use to kind of adjust what the 
nature of the regularization is. You know, and I mean in the end I'm a 
empiricist, not a theoretician. You know the. I thought this seemed like a 
good idea. Nearly all of my things that seem like a good idea, turn out to 
be stupid. This particular one Dave good results. You know on this data set 
and a few other ones as well.</p>

<p>Okay, could you pass that newest data yep 
yeah, I'm sure a little bit confused about the W plus W? Is it huh, so you 
mentioned that we do W plus W adjusted so that the coefficients don't get 
set to zero, that we place some importance on the priors. But you also said 
that the the effect of learning can be that wget set to a negative value 
which is mentally W, plus W right zero. So if, if we are, we are allowing 
the learning process to indeed set the priors to zero. So why is that in 
any way different from just having W because yeah great question, because 
of regularization, because we're penalizing it by that right? So, in other 
words, we're saying you know what, if you're, if the best thing to do is to 
ignore the value of R that'll cost you you're going to have to set W to a 
negative number right. So only do that, if that's fairly a good idea. 
Unless it's clearly a good idea, then you should leave, leave it where it 
is that that's the only reason like all of this stuff we've done today is 
basically entirely about. You know maximizing the advantage we get from 
regularization and saying regularization pushes us towards some default 
assumption and nearly all of the machine learning literature assumes that 
default assumption is everything zero and I'm saying like it turns out. You 
know it makes sense theoretically and turns out empirically that, actually 
you should decide what your default assumption is and that'll give you 
better results.</p>

<p>So would it be right to say that, in a way you are putting 
an additional hurdle in the along the way towards getting all coefficients 
to zero, so it will be able to do that if it is really worth it yeah. 
Exactly so I'd say like the default herd or without this is, is making a 
coefficient non. Zero is the heck hurdle, and now I'm saying no, the 
coefficient not be equal to 0.40. So this is some of the W squared in to 
see some of into some lambda or C penalty. Constant yeah yeah times 
something yeah, so the beta K should also depend on the value of C. If it 
is very less like. If C is, I say to you, hey yeah, so if a is point one, 
then the wage might not go towards zero yeah. Then we might not need weight 
decay so well that the whatever this value I mean, if the if the value of 
this is zero, then there is no recordation right, but if this value is 
higher than zero, then there is some penalty right and - and presumably 
we've Set it to non zero because we're overfitting, so he wants some 
penalty, and so if there is some penalty, then then my assertion is that we 
should penalize things that are different to our prior, not that we should 
penalize things that are different to zero and prior Is that things should 
be, you know around about equal to our ok, let's move on thanks for the 
great questions I want to talk about. Embedding I said pretend it's linear, 
and indeed we can pretend it's linear.</p>

<p>Let me show you how much we can 
pretend it's! Linear, as in n n dot, linear create a linear layer here is 
our data matrix. Alright, here are our coefficients. If we're in the R 
version here are coefficients are right. So if we were to put those into a 
column vector like so right, then we could do a matrix multiply of that by 
that right and so we're going to end up with so here's our matrix, here's 
our vector right so we're going to end up with 1 times 1 plus 1 times 1, 1 
times 1, 1 times 3 right, 0 times, 1, 0 times, point 3, all right and then 
the next one, 0 times 1, 1 times 1, so forth. Ok, so like that, the matrix 
multiply you know of this independent variable matrix by this coefficient 
matrix is going to give us an answer. Ok, so that's that is just a matrix 
multiply. So the question is like ok. Well, why didn't Jeremy right and n 
minion? Why did Jeremy right and n dot embedding and the reason is because 
if you recall, we don't actually store it like this, because this actually 
of whit's 800,000 and of height 25,000 right. So, rather than storing it 
like this, we actually store it as 0. 1. 2. 3 right, 1, 2. 3. 4. 0. 1. 2. 
5. 1. 2, 4, 5. Okay, that's actually how we store it, that is this bag of 
words contains, which word indexes. That makes sense. Ok, so that's like 
this is like a sparse way of storing it right.</p>

<p>It's just list out the 
indexes in each sentence, so, given that I want to now do that matrix 
multiply that I just showed you to create that same outcome right, but I 
want to do it from this representation. So if you think about it, all this 
is actually doing. Is it saying a 1 hot? You know this is basically one hot 
encoded right. It's kind of like a dummy dummy matrix version. Does it have 
the word this doesn't have? The word movie doesn't have the word is and so 
forth. So if we took the simple version of like doesn't have the word this 
100 right and we multiplied that by that right. Then that's just going to 
return the first item. That makes sense so in general, a one hot encoded 
vector times a matrix, is identical to looking up that matrix to find the 
end row in it all right. So this is identical to saying, find the zero 
first, second and fifth coefficients right, so they're they're the same 
they're exactly the same thing and like it doesn't like in this case, I 
only have one coefficient per feature right, but actually the way I did 
this was To have one coefficient per feature for each class right, so in 
this case is both positive and negative, so I actually had kind of like an 
AA, positive and a negative, so our negative would be just the opposite 
right equals that divided by that now, in the Binary case, obviously it's 
redundant to have both, but what if it was like? What's the author of this 
text? Is it Jeremy or Savannah or Terrence right now, we've got three 
categories. We want three values of R right, so the nice thing is doing 
this sparse version.</p>

<p>You know you can just look up. You know the 0th and 
the first and the second and the fifth. Alright and again it's identical 
mathematically, identical to a multiplying by a one Haughton coded matrix. 
But when you have sparse inputs, it's obviously much much more efficient. 
So this computational trick, which is mathematically identical to not 
conceptually analogous to mathematically identical to multiplying by a one 
hot encoded matrix, is called an embedding right. So I'm sure you've all 
heard - or most of you probably heard about embeddings, like word 
embeddings word to their core glove or whatever and people love to make 
them sound like this amazing, new, complex, neural net thing right, they're 
not embedding means make a multiplication by a One hot encoded matrix 
faster by replacing it with a simple array: cup. Okay, so that's why I said 
you can think of this, as if it said self, W equals n n dot, linear and F, 
plus one by one right, because it actually does the same thing right. It 
actually is a matrix with those dimensions. This actually is a matrix with 
those dimensions right. It's a linear layer, but it's expecting that the 
input we're going to give it is not actually one hot encoded matrix, but is 
actually a list of integers right, the indexes for each word of each item. 
So you can see that the forward function in fastai automatically gets for 
this Werner for feature indexes right, so they come from the sparse matrix 
automatically numpy makes it very easy to just grab those those indexes. 
Okay, so in other words there we've got here.</p>

<p>We've got a list of H, word 
index of a of the 800-thousand that are in this document, and so then this 
here says look up each of those in our embedding matrix, which is got 
800,000 rows and return. Each thing that you find okay, so mathematically, 
identical to multiplying by the one hunting coded matrix so make sense. So 
that's all an embedding is, and so what that means is we can now handle 
building any kind of model. Like a you know, whatever kind of neural 
network, where we have potentially very high cardinality categorical 
variables as our inputs, we can then just turn them into a numeric code 
between zero and the number of levels. And then we can learn a you know, a 
linear layer from that, as if we had one hot encoded it without ever 
actually constructing the one hot encoded version and without ever actually 
doing that matrix model play okay. Instead, we will just store the index 
version and simply do the array lookup, okay and so the gradients that are 
flowing back. You know, basically, in the one hot encoded version, 
everything that was a zero has no gradient, so the gradients flowing back 
is best go to update the particular row of the embedding matrix that we 
used okay, and so that's fundamentally important for NLP. Just like here. 
Like you know, I wanted to create a pytorch model that would implement 
this. This ridiculously simple little equation, alright, and to do what, 
without this trick, would have meant.</p>

<p>I was feeding in a twenty five 
thousand by adherence to 800,000 element array, which would have been kind 
of crazy right, and so this this trick allowed me to write. You know you 
know: I've just replaced the word linear with embedding replace the thing 
that feeds the one-hot encodings in with something to dispense the indexes 
in, and that was it that then it kept working, and so this now trains you 
know in about a minute per Epoch, okay, so what we can now do is we can now 
take this idea and apply it not just to language but to anything right, for 
example, predicting the sales of items at a grocery. Yes, where's that 
asset, just a quick question, so you're not actually looking up anything 
right. We are just seeing that now that array with the indices, that is the 
representation so the represent, so we are doing a lookup right. The 
representation that's being stored it for the book, but for the bag of 
words is now not one one 100 one, but oh one, two five right, and so then 
we actually have to do our matrix product right, but rather than doing the 
matrix product, we look Up the zero thing, and the first thing, and the 
second thing and the fifth thing so that means we are still retaining the 
one hard encoded matrix. No, we didn't there's no one hot encoded matrix 
used here this here's, the one who decoded matrix, which is not currently 
highlighted. We've currently highlighted the list of indexes and the list 
of coefficients from the weight matrix.</p>

<p>So it says: okay, okay, so what 
we're going to do now is we're kind of go to just go to go a step further 
and saying, like let's not use a linear model at all. Let's use a 
multi-layer neural network right and let's have the input to that 
potentially be include some categorical variables and those categorical 
variables we will just have as numeric indexes and so the first layer for 
those won't be a normal linear layer. There'll be an embedding layer which 
we know behaves exactly like a linear layer. </p>

<h3>8. <a href="https://youtu.be/XJ_waZlJU8g?t=1h17m30s">01:17:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Deep Learning: Structured and Time-Series data with Rossmann Kaggle competition, with the 3rd winning solution ‘Entity Embeddings of Categorical Variables’ by Guo/Berkhahn.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Mathematically and so then our hope will be that we can now use this to 
create a neural network for any kind of data. All right, and so there was a 
competition on Kaggle a few years ago - called rossmann, which is a German 
grocery chain, where they asked to predict the sales of items in in their 
stores right, and that included the mixture of categorical and continuous 
variables. And in this paper by Gordon Burke and they described their 
third-place winning entry, which was much simpler than the first placed 
winning entry, but nearly as good but much much simpler because they took 
advantage of this idea of what they call ng embeddings in the paper. They 
they thought, I think that they had invented this actually had been written 
before earlier by yoshua, bengio and his co-authors in another cackle 
competition, which was predicting taxi destinations. Although I will say, I 
feel, like war went a lot further in describing how this can be used in 
many other ways, and so will will talk about that as well. So the so, this 
one is actually in the is in the deep learning one repo. Okay deal one 
lesson: three: okay, because we talked about some of the deep learning 
specific aspects in the deep learning course, where else in this course 
we're going to be talking mainly about the feature, engineering and we're 
also going to be talking about. You know kind of this, this embedding idea. 
So let's start with the data right.</p>

<p>So the data was, you, know, store 
number one on the 31st of July 2015 was open, they had a promotion going 
on, there was a school holiday, it was not a state holiday and they sold 
five thousand two hundred and sixty three items. So that's the key data 
they provided, and so the goal is obviously to predict sales in a test set 
that has the same information without sales. They also tell you that for 
each store it's of some particular type, it sells some particular 
assortment of goods. Its nearest competitor competitor is some distance 
away. The competitor opened in September 2008 and there's some more 
information about promos. I don't know the details of what that means like 
in many Carroll competitions. They let you download external data sets if 
you wish, as long as you share them with other competitors, so people - oh, 
they also told you what's date. Each store is, in so people downloaded a 
list of the names of the different states of Germany. They downloaded a 
file for each state in Germany for each week some kind of Google trend 
data. I don't know what specific Google trend they got, but there was that 
for each date they downloaded a whole bunch of temperature information, , 
that's it and then here's the test set. Okay, so I mean one interesting 
insight here. Is that the it was probably a mistake in some ways for 
Russman to design this competition as being one where you could use 
external data, because in reality you don't actually get to find out next 
week's weather or next week's Google Trends? You know, but you know, when 
you're competing in category you don't care about that.</p>

<p>You just want to 
win, so you use whatever you can get. So, let's talk first of all about 
</p>

<h3>9. <a href="https://youtu.be/XJ_waZlJU8g?t=1h21m30s">01:21:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Rossmann Kaggle: data cleaning &amp; feature engineering.</b></li>

<li><b>Using Pandas to join tables with ‘Left join’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Data cleaning, you know that there wasn't really much feature engineering 
done in this third place, winning entry like by particularly by cattle 
standards, where normally every last thing counts. This is a great example 
of how far you can get with with a neural net, and it certainly reminds me 
of the claims prediction competition we talked about yesterday, where the 
winner did no feature engineering and entirely relied on deep learning, the 
laughter in the room. I guess is from people who did a little bit more than 
no feature engineering in that competition. So you know I should mention by 
the way, like I find that bit where, like you, work hard at a competition 
and then it closes and you didn't win and the winner comes out and says 
this is how I won like that's the bit where you learn. The most right, like 
sometimes that's, happened to me and it's been like. Oh I thought of that. 
I thought I tried that and then I go back and I realize I like had a bug 
there. I didn't test properly and I learn like oh okay, like I really need 
to learn to like test this thing in this away. Sometimes it's like. Oh, I 
thought of that, but I assumed it wouldn't work. I've really got to 
remember to check everything before I make any assumptions, and you know 
sometimes it's just like. Oh I I did not think of that technique.</p>

<p>Wow now I 
know it's better than everything I just tried because, like otherwise 
somebody says like hey, you know: here's a really good technique, you're 
like okay, great right, but when you spent months trying to do something 
and like somebody else, did it better by using that Technique, that's 
pretty convincing right and so like it's kind of hard, like I'm standing up 
in front of you saying here's a bunch of techniques that I've I've used and 
I've won some capital competitions and I've got some state of the art 
results. But it's like that's kind of second-hand information by the time 
it hits you right, so it's really great to yeah, try things out and and 
also like it's been kind of nice to see, particularly I've noticed in the 
deep learning course. Quite a few of my students have you know. I've said 
like this technique works really well and they've tried it and they've got 
into the top ten of a Keagle competition the next day and they're like 
okay, that that counts is working really well, so so yeah caryl 
competitions are helpful for lots and lots of Reasons, but you know one of 
the best ways is what happens after it finishes, and so definitely like for 
the ones that you that are now finishing up, make sure you, you know, watch 
the forums, see what people are sharing in terms of their solutions, and 
you Know if you want to learn more about them like don't feel free to ask 
the winners like hey.</p>

<p>Could you tell me more about this or that people are 
normally pretty pretty good about explaining and then ideally try and 
replicate it yourself right and that can turn into a great blog post? You 
know, or a great kernel is to be able to say: okay such-and-such said that 
they use this technique. Here's a really short explanation of what that 
technique is, and here's a little bit of code showing how it's implemented, 
and you know, here's the results showing you. You can get the same result 
that can be a really interesting write-up as well. Okay, so you know it's, 
it's always nice to kind of have your data reflect like. Oh no bee is kind 
of easy to understand as possible. So in this case the data that came from 
Kegel used various you know, integers for the holidays. We can just use a 
boolean if like was it a holiday or not so like just clean that up? We've 
got quite a few different tables. We need to join them all together right. 
I have a standard way of joining things together with pandas. I just used 
the pandas merge function, and specifically, I always do a left joint. So, 
who wants to tell me what a left join is since it's there go ahead? You 
retain all the rows in the left table and you take so you have a key 
column. You match that with a key column in the right side table and you 
just merge the rules that are also present in the right side table yeah. 
That's a great explanation, good job. I don't have much to add to that.</p>

<p>The 
key reason that I always do a left join is that after I do the join, I 
always then check if there were things in the right-hand side that a noun 
no right, because if so it means that I some things yeah, I haven't shown 
it here, But I also check that the number of rows hasn't varied before and 
after, if it has, that means that the right hand side table wasn't unique. 
Okay, so even when I'm sure something's true, I always also assume that 
I've screwed it up. So I always check so I could go ahead and merge the 
state names into the whether I can also, if you look at the Google Trends 
table, it's got this week range which I need to turn into a date in order 
to join it right, and so The nice thing about doing this in pandas is that 
pandas gives us access to. You know all of Python right and so, for 
example, inside the the series object, Str attribute that gives you access 
to all the string. Processing functions not just like cat gives you access 
to the categorical functions. Dt gives you access to the date/time 
functions, so I can now split everything in that column and it's really 
important to try and use these pandas functions because they, you know 
they're going to be vectorized accelerated through you know, often 
threesome D, at least through you know, C code, so that runs nice and 
quickly, and then you know, as per usual, let's add date metadata to our 
dates. In the end, we are basically denormalizing all these tables we're 
going to put them all into one table.</p>

<p>So in the Google trend table there 
was also, though they were mainly trends by state, but there was also 
trends for the whole of Germany, so we kind of put the Germany on you know 
the whole of Germany ones into a separate data frame so that we Can join 
that so we're going to have that Google trend for this state and Google 
trend for the whole of Germany, and so now we can go ahead and start 
joining both for the training set and for the test set and then which both 
check that we Don't have zeros my merge function. I set the suffix. If 
there are two columns are the same. I set their suffix on the left to be 
nothing at all, so it doesn't screw around with the name and the right hand 
side to be underscore Y, and in this case I didn't want any of that. You 
look at ones, so I just went through and deleted them. Okay and then we're 
gon na in a moment we're going to try to create a competition. You know the 
the the main competitor for this store has been open since some date right, 
and so you can just use pandas to date. I'm passing in the year the month 
and the day right, and so that's going to give us an error unless they all 
have years and months so so we're going to fill in the missing ones with 
the 1900 and a1 okay and then what we really know. It we didn't want to 
know is like how long is this store been open for at the time of this 
particular record all right, so we can just do a date.</p>

<p>Subtract! Okay! Now, 
if you think about it, sometimes the competition you know open later than 
this particular row, so sometimes it's going to be negative and it doesn't 
probably make sense to have negative spending like it's going to open in X 
days time now. Having said that, I would never put in something like this 
without first of all running a model with it in and without it in right, 
because, like our assumptions about about the data very often turned out 
not to be true. Now, in this case, I didn't invent any of these 
pre-processing steps. I wrote all the code, but it's all based on the 
third-place winners, github repo right, so knowing what it takes to get 
third place in the cable competition, I'm pretty sure they would have 
checked every one of these pre-processing steps and made sure it actually 
improved their Their validation set score okay, so what we're going to be 
doing is creating a neural network where some of the inputs to it are 
continuous and some of them are categorical, and so what that means in the 
in the neural net that you know we have we're. Basically going to have you 
know this kind of initial weight matrix right and we're going to have this. 
This input feature vector right, and so some of the inputs are just going 
to be plain. Continuous numbers, like you know, what's the maximum 
temperature here or what's the number of kilometers to the nearest store, 
and some of them are going to be one HUD encoded effectively right, but 
we're not actually going to store it as one hot encoded.</p>

<p>We're actually 
going to store it as the index right, and so the neural net model is going 
to need to know which of these columns should you should you basically 
create an embedding, for which ones should you treat you know as if they 
were kind of one Hot encoded, and which ones should you just you feed 
directly into the linear layer right and so we're going to tell the model 
when we get there, which is which, but we actually need to think ahead of 
time about, like which ones do we want to treat? As categorical, and which 
ones are continuous, in particular things that were going to treat it as 
categorical, we don't want to create more categories than we need right, 
and so let me show you what I mean. The the the third-place getters in this 
competition decided that the number of months that the competition was open 
was something that they were going to use as a categorical variable, all 
right, and so in order to avoid having more categories than they needed. 
They truncated it at 24 months. They said anything more than 24 months old 
truncate to 24. So here are the unique values of competition months open 
and it's all the numbers from naught to 24 right. So what that means is 
that there's going to be? You know an embedding matrix, that's going to 
have basically an embedding vector for things that aren't open yet for 
things that are open a month for things that are open two months and so 
forth. Now they absolutely could have done that as a continuous variable 
right. They could have just had a number here, which is just a single 
number of how many months has it been open and they could have treated it 
as continuous and fed it straight into the initial weight matrix.</p>

<p>What I 
found, though, and obviously what these competitors found is, where 
possible it's best to treat things as categorical variables right and the 
reason for that is that, like when you feed some things through an 
embedding matrix, you basically mean it means every level can be treated 
Like totally differently right, and so for example, in this case, whether 
something's been open for zero months or one months, is right, really 
different right. And so, if you fed that in as a continuous variable, it 
would be kind of difficult for the neural net to try and find a functional 
form. That kind of has that that big difference as possible, because neural 
Nets can do anything right, but you're not making it easy for it. Where 
else, if you used an embedding treated it as categorical, then it will have 
a totally different vector for zero versus one right. So it seems like 
particularly as long as you've got enough data that the treating columns as 
categorical variables, where possible, is a better idea, and so I say when 
I say we're possible that kind of basically means like where the 
cardinalities not too high. You know. So if this was like, you know the 
sales ID number that was like uniquely different on every row, you can't 
treat that as a categorical variable right, because you know it would be a 
huge embedding matrix and everything only appears once or ditto for, like 
kilometers away From the nearest store to two decimal places, you wouldn't 
make a categorical variable all right, so that's kind of that's kind of the 
rule of thumb that they both used in this competition.</p>

<p>In fact, if we 
scroll down to their choices here is how they did it right, they're 
continuous variables, with things that were genuinely continuous, like 
number of kilometers away to the competitor, the temperature stuff right, 
the number you know the specific number in the Google trend right, Where 
else everything else, basically, they treated as categorical. Okay, so 
that's it for today, so yeah next time, we'll we'll finish this off we'll 
see we'll see how to turn this into a neural network and yeah kind of wrap 
things up so see. Then </p>








  </body>
</html>
