<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Lesson 02</title>
    <meta charset="UTF-8">
  </head>
  <body>
  <p style="text-align: right"><a href="http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826">fast.ai: Intro to Machine Learning 1 (v1) (2018)</a></p>
  <h1>Lesson 02</h1>
  <h2>Outline</h2>
<ul>

<li>Python basics</li>
<li>Git, Symlink, AWS</li>
<li>Python notebook basics</li>
<li>Crash course on pandas</li>
<li>FastAI introduction</li>
<li>add_datepart</li>
<li>train_cats</li>
<li>Feather Format</li>
<li>Run your first Random Forest</li>

</ul>


  <h2>Video Timelines and Transcript</h2>


<h3>1. <a href="https://youtu.be/blyXCk4sgEg?t=3m30s">00:03:30</a></h3>

<ul style="list-style-type: square;">

<li><b> simlink sim link to fastai directory</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>So from here the next two or three lessons we're going to be really diving 
deep into random forests. So so far all we've learned is there's a thing 
called random forests for some particular datasets. They seem to work 
really really well without too much trouble, but we don't really know yet 
like well. How do they actually work? What do we do if they don't work 
properly? What are their pros and cons? What are the can we tune and so 
forth? So we're gon na look at all that and then after that, we're going to 
look at how do we interpret the results of random forests to get not just 
predictions but to actually deeply understand our data in a model driven 
way. So that's where we're going to go from here, so, let's just review 
where we're up to so we learned that there's this library called fastai and 
the fastai library is basically it's a highly opinionated library, which is 
to say, we've spent a lot of time. Researching what are the best techniques 
to get like state-of-the-art results, and then we take those techniques and 
package them into pieces of code so that you can use the state-of-the-art 
results yourself, and so, where possible, we wrap or provide things on top 
of existing code, and so In particular for the kind of structured data 
analysis, we're doing, scikit-learn has a lot of really great code, so most 
of the stuff that we're showing you from fastai is stuff to help us get 
stuff into scikit-learn and then interpret stuff out from scikit-learn. The 
fastai library, the way it works in our environment here, is that we've got 
out. Our notebooks are inside fastai, repo / courses and in / ml 1, + dl, 1 
and then inside there there's a symlink to the parent of the parent, 
fastai. So this is a sim link to a directory containing a bunch of modules. 
So if you want to use the fastai library in your own code, there's a number 
of things you can do, one is to put your notebooks or scripts in the same 
directory as ml 1 or 0 1, whether it's already this simile and just import 
it Just like I do, you could copy this directory dot, dot, slash, dot, dot, 
slash birthday I into somewhere else and use it or you could sim link it 
just like I have from here to wherever you want to use it so notice, it's 
mildly, confusing.</p>

<p>There's a github repo called fastai and inside the 
github repo caught fastai, which looks like this. There is a folder called 
fastai, okay and so the FASTA, a folder in the FASTA, a repo contains the 
FASTA, a library and it's that library. When we go from fastai dot imports 
to import star, then that's looking inside the farsi, a folder for a file 
called inputs, imports, py and importing everything from that. Okay, yes, 
Danielle, okay, and just like as a clarifying question for this Bentley. 
It's just that anything! That's just the Ellen thing that you talked about 
yeah, so a symlink is something you can create by typing: Ln minus s and 
then the path to the source, which in this case would be dot, dot, dot. 
First, AI could be relative or it could be absolute and then the name of 
the destination. If you just put the current directory at the destination, 
it'll use the same name as it comes from like a alias on the Mac or a 
shortcut on Windows, and when you do the important system yep got it 
imports this and then append that relative link. That also creates the same 
link and I don't think I've created the same link anywhere in the 
workbooks. The symlink actually lives inside the github repo okay. I 
created some symlinks in the deep learning notebook to some data that was 
different at the top of Tim Lee's workbook from the last class. There was 
important, oh yeah, don't do that? Probably I mean you you can, but I think 
this is.</p>

<p>I think this is better like this way you can go from fastai 
imports and regardless of kind of how you got it there, it's it's going to 
work. You know okay, okay, so then we had all of our data for blue books 
for bulldozers, competition in data, slash bulldozers, and here it is 
right, so we were able to read that CSV file. The only thing we really had 
to do was to say which columns were dates and having done that, we were 
able to take a look at a few earth examples of the rows of the data, and so 
we also noted that it's very important to deeply understand The evaluation 
metric for this project, and so if a cattle, they tell you what the 
evaluation metric is, and in this case it </p>

<h3>2. <a href="https://youtu.be/blyXCk4sgEg?t=6m15s">00:06:15</a></h3>

<ul style="list-style-type: square;">

<li><b> understand the RMSLE relation to RMSE, and why use np.log(‘SalePrice’) with RMSE as a result</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Was the root mean squared log error, so that is some of the actuals the 
predictions now, but it's the log of the actuals, the log, the predictions 
squared right. So if we replace actuals with log actuals and replace 
predictions with log predictions, then it's just the same as root mean 
squared error. So that's what we did was we replaced sale price with log of 
sale, price and so now, if, if we optimize for root, mean squared error, 
we're actually optimizing for the root mean squared error of the logs okay. 
So then we learnt that we need all of our columns to be numbers, and so the 
first way we did, that was to take the date column and remove it and 
instead replace it with a whole bunch of different columns such as is that 
date. The start of a quarter is at the end of a year. How many days are 
elapsed since generally, the first 1970, what's a year, what's the month or 
the day of weeks and so forth? Okay, so they're all numbers, then we learnt 
that we can use train underscore katz to replace all of the strings with 
categories. Now, when you do that, it doesn't look like you've done 
anything different. They still look like strings all right, but if you 
actually take a deeper look, you'll see that the datatype now is not 
string, but category and category is a pandas class where you can then go 
dark cat dot and find a whole bunch of different attributes such As cat 
categories to find a list of all of the possible categories - and this says 
hi is going to be 0 low - will become 1. Medium will become 2, so we can 
then get codes to actually get the numbers.</p>

<p>So then, what we need to do to 
actually use this data set to turn it into numbers, is take every 
categorical column and replace it with cap codes, and so we did that using 
</p>

<h3>3. <a href="https://youtu.be/blyXCk4sgEg?t=9m1s">00:09:01</a></h3>

<ul style="list-style-type: square;">

<li><b> proc_df, numericalize</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Proc DF, okay. So how do I get the source code for proc DF question: 
question: mark? Okay, all right! So if I scroll down, I go through each 
column and I numerical eyes it. Okay, that's actually the one I want. So 
I'm going to now have to look up numerical eyes so tab to complete it. If 
it's not numeric, then replace the data frames filled with that columns cat 
codes last one because otherwise unknown is minus one. We went unknown to 
be zero. Okay, so that's how we turn the strings into numbers right. They 
get replaced with a unique, basically arbitrary index. It's actually based 
on the alphabetical order of the feature names. The other thing property f 
did remember, was continuous columns that had missing values. The missing 
got replaced with the median and we added an additional column called 
column name underscore na, which is a boolean column, told you if that 
particular item was missing or not so once we did that we were able to call 
random forest regressor dot fit and Get the dots poor and turns out, we 
have an r-squared of 0.98. Can anybody tell me what an r-squared is? Listen 
um? Can we </p>

<h3>4. <a href="https://youtu.be/blyXCk4sgEg?t=11m1s">00:11:01</a></h3>

<ul style="list-style-type: square;">

<li><b> rsquare root square of mean errors RMSE,</b></li>

<li><b>What the formula rsquare (and others in general) does and understand it</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>You so r-squared essentially shows how much variance is explained by the 
modal. This is the yeah. This is the relation of this is ssro, which is 
like trying to trying to remember the exact formal, but I don't roughly a 
curative way, yeah intuitively. It's how much the model explains the how 
much it accounts for the variance in the data. Okay, good. So let's talk 
about the formula, and so with formulas the idea is not to learn the 
formula and remember it, but to learn what the formula does and understand 
it right. So here's the formula: it's 1 minus something divided by 
something else. So, what's this something else from the bottom, SS taught 
okay. So what this is saying is we've got some actual data, so my eyes 
right, we've got some actual data, 3e two-for-one, okay and then we've got 
some average okay. So our top bit, this SS tot is the sum of each of these 
that so, in other words, it's telling us. How much does this data bury? 
Perhaps more interestingly, is I remember when we talked about like last 
week. What's the simplest non stupid model you could come up with and I 
think the simplest non stupid model we came up with was create a column of 
the mean just copy the mean a bunch of times and submit that to cable. If 
you did that, then your root mean squared error would be this, so this is 
the root mean squared error of the most naive non stupid model, where the 
model is just predict mean on the top.</p>

<p>We have SS res, which is here, which 
is that we're now going to add a column of predictions, okay, and so then 
what we do is, rather than taking the. Why I, why mean we're going to take? 
Why I Fi right - and so now, instead of saying, what's the root mean 
squared error of our naive model, we're saying: what's the root mean 
squared error of the actual model that we're interested in and then we take 
the ratio, so in other words, if we actually Were exactly as effective as 
just predicting the mean, then this would be top and bottom would be the 
same. That would be 1. 1 minus 1 will be 0. If we were perfect so fi minus 
y, I was always zero. Then that's zero, divided by something one. Minus 
that is 1 ok, so what is the possible range of values of R squared? Okay? I 
heard a lot of 0 to 1. Does anybody want to give me an alternative negative 
1 to 1 anything less than one there's the right answer, let's find out way 
through the boss? Okay, so why is it any number less than one which you can 
make a model? Basically, as crappy as you want, and just I guess like the 
arrows as you want you're just subtracting from one in the formula exactly 
so. Interestingly, I was talking to our computer science, professor 
Terrence this morning, who was talking to where a statistics professor told 
him that the possible range of values was asked where it was zero to one. I 
said that is totally not true.</p>

<p>If you predict infinity for every column, 
sorry for every row, then you're going to have infinity for every residue 
and so you're going to have one minus infinity. Okay, so the possible range 
of values is less than one. That's all we know, and this will happen you 
will get negative values, sometimes in your r-squared and when that 
happens, it's not a mistake. It's not it's not like a bug. It means your 
moral is worse than predicting the main okay, which is suggest it's not 
great. So that's R, squared it's not it's not necessarily what you're 
actually trying to optimize right, but it's it's it's the nice thing about 
it is that it's a number that you can use kind of for every model, and so 
you can kind of start try to get A feel of like what does point-eight look 
like what does point-9 look like so like something I find interesting is to 
like create some different. Synthetic data sets just to two dimensions, 
with kind of different amounts of random noise and like see what they look 
like on. A scatterplot and see what they are squared are just gon na get a 
feel for like what does it ask for it? You know, is it a spur to point line 
close or not about 0.7 closed or not? Okay, so I think r-squared is a 
useful number to have a familiarity with, and you don't need to remember 
the formula if you remember the meaning, which is: what's the ratio between 
how good your model is, it means bit error versus how good is the naive 
mean Model for it, </p>

<h3>5. <a href="https://youtu.be/blyXCk4sgEg?t=17m30s">00:17:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Creating a good validation set, ‘split_vals()’ explained</b></li>

<li><b>“I don’t trust ML, we tried it, it looked great, we put it in production, it didn’t work” because the validation set was not representative !</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Squared error, okay and LK is 0.98. It's saying it's a very good model. 
However, it might be a very good model because it looks like this all 
right. This would be called overfitting, so we may well have created a 
model which is very good at running through the points that we gave it. But 
it's not going to be very good at running through points that we didn't 
give it. So that's why we always want to have a validation set. Creating 
your validation set is the most important thing that I think you need to do 
when you're doing a machine learning project, at least in terms of in the 
actual modeling, because what you need to do is come up with a data set 
where the scroller of Your model on that data set is going to be 
representative of how well your model is going to do in the real world, 
like in cattle on the leaderboard or off Kaggle like when you actually use 
it in production. I very very very often hear people in industries they, I 
don't trust machine loading. I tried modeling once it looked great, we put 
it in production, it didn't work now, whose fault is that right? That means 
their validation set was not representative right. So here's a very simple 
thing, which, generally speaking, cattle, is pretty good about doing. If 
your data has a time piece in it right, as happens in Blue Book for 
bulldozers in Blue Book for bulldozers, we're talking about the sale price 
of a piece of industrial equipment on a particular date.</p>

<p>So the start up 
during this competition wanted to create a model that wouldn't predict last 
February's prices. That would predict next month's prices, so what they did 
was they gave us data representing a particular date range in the training 
set, and then the test set represented a future set of dates that wasn't 
represented in the training set right. So that's pretty good right. That 
means that, if we're doing well on this model, we've built something which 
can actually predict the future, or at least it could predict the future. 
Then assuming things haven't changed dramatically. So that's the test set. 
We have so we need to create a validation set. That has the same 
properties, so the test set had 12,000 rows in so let's create a validation 
set that has 12,000 rows right and then, let's split the data set into the 
first n minus 12 thousand rows for the training set and the last 12,000 
rows. For the validation set, and so we've now got something which 
hopefully looks like cackles test set, plus enough that when we actually 
try and use this validation set we're going to get some reasonably accurate 
scores and the reason we want this is because on cattle, you can Only 
submit so many times and if you submit too often you'll end up fitting to 
the leaderboard anyway and in real life you actually want to build a model. 
That's going to work in.</p>

<p>Why did you have a question? Can we help the green 
box? Can you explain the difference between a validation set and 
</p>

<h3>6. <a href="https://youtu.be/blyXCk4sgEg?t=21m1s">00:21:01</a></h3>

<ul style="list-style-type: square;">

<li><b> overfitting over-fitting underfitting ‘don’t look at test set !’,</b></li>

<li><b>Example of failed methodology in sociology, psychology,</b></li>

<li><b>Hyperparameters,</b></li>

<li><b>Using PEP8 (or not) for ML prototyping models</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>A test set absolutely so. What we're going to learn today is how to set 
what other things alone. It's how to set high parameters. Hyper parameters 
are like tuning parameters that are going to change how your model behaves 
now, if you just have one holdout set, so one set of data that you're not 
using to train with, and we use that to decide which set of hyper 
parameters to use. If we try a thousand different sets of hyper parameters, 
we may end up overfitting to that holdout set, that is to say, well, find 
something which only accidentally worked. So what we actually want to do is 
we want to have a second holdout set where we can say: okay, I'm finished 
okay. I've done the best I can and now just once right. At the end, I'm gon 
na see whether it works, and so this is something which almost nobody in 
industry does correctly. You really actually need to remove that holdout 
set. That's called the test set, remove it from the data, give it to 
somebody else and tell them do not. Let me look at this data until I 
promise you I'm finished like it's so hard, otherwise, not to look at it 
and, for example, in the world of psychology and sociology you might have 
heard about this replication crisis. This is basically because people in 
these fields have accidentally or intentionally maybe been peed hacking, 
which means they've been basically trying lots of different variations 
until they find something that works, and then it turns out and they try to 
replicate it.</p>

<p>In other words, it's like somebody creates a test set. 
Somebody says okay, this study, which shows you know the impact of whether 
you eat marshmallows, on your tenacity later in life, I'm going to rerun it 
and like over half the time they're finding the effect turns out. Not to 
exist, so that's why we want to have a test set, get that next door yeah. 
So for handling categorical data. You can market those to numerix two 
numbers order. Numbers I've seen a lot of models where we convert 
categorical data into different columns using one for encoding, yes, so 
which approach to use in which model yeah we're kind of track. For that 
today, yeah. It's a great question: okay, so so I'm splitting my my data 
into validation and training sets, and so you can see now that my 
validation set is twelve thousand about 66. Where else my training set is 
three hundred ninety nine thousand sixty-six okay. So we're going to use 
this set of data to train a model and this set of data to see how well it's 
working. So when we then tried that last week we found out just a moment. 
We found out that our model, which had point nine eight two R squared on 
the training, set only had point eight, eight, seven on the validation set, 
which makes us think that we're overfitting quite badly, but it turned out 
it wasn't too badly because the root mean Squared error on the logs of the 
prices actually would have caught us in the top 25 percent of the 
competition anyway.</p>

<p>So, even although we're overfitting, it wasn't the end 
of the world, could you pass the microphone to Marsha? Please? Ah, in terms 
of you dividing the set into training and validation, it seems, like you, 
simply take the first and train observations of the data status and set 
them aside. Why don't you like? Why don't you randomly pick up the 
observations? Because if I did that, I wouldn't be replicating the test 
set, so cattle has a test set that when you actually look at the dates in 
the test set, they are a set of dates that are more recent than any date in 
the training set. So if we used a validation set, that was a random sample. 
That is much easier because we're predicting options like what's the value 
of this piece of industrial equipment. On this day, when we add, we already 
have some observations from that day, so in general, anytime, you're 
building a model that has a time element. You want your test set to be a 
separate time period and therefore you really need your validation set to 
be observer time period as well, and in this case the data was already 
sorted. That's why this works. So let's say we have our test. The training 
set by which we in the data and then we have the validation set against 
which we are trying to find the r-square in in case of r-squared, turns out 
to be really bad. We would want to cheer to not parameters and run it 
again. Yes, so wouldn't that be eventually overfitting on the overall 
training set yeah.</p>

<p>So, actually, that's that's the issue, so that would 
eventually have the possibility of overfitting on the validation set and 
then, when you try it on the test set or we submit it to kaggle, it turns 
out not to be very good, and this happens in careful competitions. All the 
time careful actually has a fourth data set, which is called the private 
leader board set and every time you submit to Kaggle, you actually only get 
feedback on how well it does on something called the public leader board 
set, and you don't know which rows They are, and at the end of the 
competition you actually get judged on a different data set entirely called 
the private leader board set. So the only way to avoid this is to actually 
be a good machine learning practitioner and know how to set these 
parameters as effectively as possible, which we're going to be doing gladly 
today and over the next few weeks. Can you get that actually watching? Is 
it too early or late to ask? What's the difference between a paper 
parameter and a parameter? Okay? So, let's start tracking things on written 
in spread era. So here is root, mean square error in a line of code, and 
you can see here like this - is one of these examples where I'm not writing 
this. The way a proper software engineer would write this right. So a 
proper software engineer would be a number of things differently. They 
would have it on a different line.</p>

<p>They would use longer variable names, 
they would have documentation blah blah blah right, but I really think like 
for me. I really think that being able to look at something in one go with 
your eyes and like over time, learn to immediately see what's going on, has 
a lot of value and also to like consistently use, like particular letters, 
to have many particular things or abbreviations. I think works really well 
in data science, if you're doing it like a take-home interview test or 
something you should write your code according to pet eight standards 
right. So pep eight is the the style guide for python code and you should 
know it and use it, because a lot of software engineers are super anal 
about this kind of thing, but for your own work, you know, I think this is. 
I think this works well for me, you know, so I just wanted to make you 
aware a that. You shouldn't necessarily use this as a role model for 
dealing with software engineers, but B that I actually think this is not. 
This is a reasonable approach. Okay, so there's a root mean </p>

<h3>7. <a href="https://youtu.be/blyXCk4sgEg?t=29m1s">00:29:01</a></h3>

<ul style="list-style-type: square;">

<li><b> RMSE function and RandomForestRegressor,</b></li>

<li><b>Speeding things up with a smaller dataset (subset = ),</b></li>

<li><b>Use of ‘_’ underscore in Python</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Squared error and then from time to time we're just going to print out the 
score which will give us the RMS C of the predictions on the training 
versus the actual. There are predictions on the valid versus the actual 
RMS, see the R squared for the training and the R squared of the Bell and 
we'll come back to over in a moment. So when we ran that we found that this 
rmse was in the top 25 %, and it's like okay there's a good start now. This 
took eight seconds of wall time, so eight actual seconds, if you put 
percent time it'll, tell you how long things talk, and luckily I've got 
quite a few calls quite a few CPUs in this computer because it actually 
took over a minute a compute time. So I paralyzed that across cause, if 
your dataset was bigger or you had less cause, you know you could well find 
that this took a few minutes to run or even a few hours, and my rule of 
thumb is that if something takes more 10 seconds to Run it's too long for 
me to do like interactive analysis with it right. I want to be able to like 
run something wait a moment and then continue. So what we do is we try to 
make sure that things can run in a reasonable time and then, when we're 
when we're finished at the end of the day, we can then say: ok, this 
feature engineering, these hyper parameters, whatever these are all working 
well and I'll now rerun it, you know, that's the big, slow, precise way.</p>

<p>So 
one way to speed things up is to pass in the subset parameter to proc DF 
and that will randomly sample my data right, and so here I'm got a randomly 
sample 30,000 rows. Now, when I do that, I still need to be careful to make 
sure that my validation set doesn't change and that my training set doesn't 
overlap with the dates, otherwise I'm cheating. So I call split bells again 
to again do this split by dates and you'll also see I'm using rather than 
putting it into a validation set, I'm putting it into a variable called 
underscore. This is kind of a standard approach in Python is to use a 
variable called underscore. If you want to throw something away, because I 
don't want to change my validation set like no matter what different models 
I build, I want to be able to compare them all to each other, so I want to 
keep my validation set. The same all the time. Okay, so all I'm doing here 
is I'm resampling. My training set into a 20, the first 20,000 out of a 
30,000 subsets, so I now could run that and it runs it 621 milliseconds. So 
I can like really zip through things now. Try things out. Okay, so with 
that, let's use this subset to build a model that is so simple that we can 
actually take a look at it, and so </p>

<h3>8. <a href="https://youtu.be/blyXCk4sgEg?t=32m1s">00:32:01</a></h3>

<ul style="list-style-type: square;">

<li><b> Single Tree model and visualize it,</b></li>

<li><b>max_depth=3,</b></li>

<li><b>bootstrap=False</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>We're going to build a forest is made of trees, and so before we look at 
the forest, we look at the trees in scikit-learn. They don't call them 
trees, they call them estimators. So we're going to pass in the parameter 
number of estimators equals one. The Creator forest with just one tree and 
then we're going to make a small tree, so we pass in maximum depth, equals 
three and a random forest as we're going to learn randomizes a whole bunch 
of things. We want to turn that off so to turn that off you say, bootstrap 
equals false. So if I pass you in these parameters that creates a small 
deterministic tree okay, so if I fit it and say print score, my a squared 
has gone down from point. Eight. Five, two point four: so this is not a 
good model, it's better than the mean model. This is better than zero 
right, it's not a good model, but it's a model that we can draw all right. 
So, let's learn about what it's built. So a tree consists of a sequence of 
binary decisions of binary splits, so at first of all decided to split on 
coupla system greater than or less than point five. That's boolean 
variable, so it's actually true or false, and then within the group we're a 
couple system was true: we decided to split into year, made greater than or 
less than 1987 and then we're a couple system was true, and year made was 
less than or equal To 1986 it used fi product class desk is less than or 
equal to 0.75 and so forth. Right so right at the top.</p>

<p>We have 20,000 
samples, 20,000 rows right, and the reason for that is because that's what 
we asked for here when we split our data in the sample. I just want to 
double-check that for your decision tree, that you had there that the 
coloration was whether it's true or false, not so like it, it gets darker, 
it's true for the next one, not the darker is a higher value, we'll get to 
that. In a moment, okay, so let's look at these numbers here. So in the 
whole data set well our sample that we're using there are 20,000 rows. The 
meter, the average of the log of Christ is 10.1, and if we built a model 
where we just used that average all the time, then the mean squared error 
would be 0.477 okay, so this is, in other words, the denominator of an R 
squared all right. This is like the most basic model is a tree with zero 
splits right, which is just predict the average. So the best single binary 
split we can make it turns out to be splitting by where the coupler system 
is greater than or equal to sorry less than or equal to or greater than 0.5 
know, whether it's true or false and turns out. If we do that the mean 
squared error of couple system is less than 0.5, so it's false goes down 
from 0.477 to 0.1 one right. So it's really improved the error a lot in the 
other group. It's only improved at a bit, so I'm from 0.47 2.41, and so we 
can see that the coupler system equals false group has a pretty small 
percentage. It's only got twenty two hundred of the twenty thousand all 
right. Where else this other group has a much larger percentage, but it 
hasn't improved it as much so let's say you wanted to create a tree with 
just one split, so you're just trying to find like what is the very best 
single binary decision you can make for Your data, how might you be able to 
do that? How could you do it? I'm gon na give it to foot, specify the max 
depth of one, but I mean your writing.</p>

<p>You don't have a random first write. 
How are you going to? How are you going to like write? What's an algorithm, 
a simple algorithm which you could use sure, so we want to start building a 
random forest from scratch, so the first step is to create a tree. The 
first step to create a tree is to create the first binary decision. How are 
you gon na? Do it I'm gon na give it to Chris, maybe in two steps. So isn't 
this simply trying to find the best predictor based on maybe a linear 
regression? You could use a linear regression, but could you do something 
much simpler and more complete we're trying not to use any statistical 
assumptions here? I can't see your name, sorry and, of course, your friends 
anything can we just do like take just one variable if it is true give it 
like the true thing and if it is false so which variable are we gon na 
choose so at each binary point? We have to choose a variable and something 
to split on. How are we going to do that? Then I pass it over there. How do 
I pronounce your name chicken, so the variability tools could be like which 
divides population into two groups, which kind of heterogeneous to each 
other and homogeneous within themselves like having the same quality within 
themselves, and they are very different. Could you be more specific, like 
in terms of the target variable, maybe yeah right? Let's say we have two 
groups of tested, so one has a different price altogether from the second 
group. Yes, internally, they have similar prices.</p>

<p>Okay, that's good, so 
like to simplify things. A little bit when we're saying find a variable 
that we could split into such that the two groups are as different to each 
other as possible and okay, how do you? How would you pick which variable 
and which split point? That's the question: yeah. What's your first cut, 
which variable and which split point we don't like we're making the trade 
from scratch. We want to create our own tree. That makes sense. We've got 
somebody amazing. Can we test all of the possible split and see which one 
has this? Ah, small and rmse that sounds good okay. So let's dig into this 
so when you say test all of the possible splits, what does that mean? How 
do we enumerate all the possible splits, oh and think of that? But if you 
want for each variable, you could put one aside and then put a second aside 
and compare the two and if it was better good okay, so for each variable 
for each possible value of that variable see whether it's better now coming 
back to Maslak. So I want to dig into the better when you said see if the 
RMS see is better. What does that mean, though, because after a split 
you've got two rmse, so you've got two two groups, so you just gon na fit 
what that one variable comparing to the other, it's not so so. What I mean 
here is that before we decided to speed on coupler system, we had a root, 
mean square root of 0.477 and after we've got two groups, one were the mean 
square error point one another with a mean squared error of 0.4, so you 
treat each Individual model separately so for the first player you're, just 
gon na compare between each variable in himself and then you move on to the 
next node with the remaining, but but even the first node like so.</p>

<p>The 
model with zero splits has a single root, mean squared error, the model 
with one split. So the very first thing we tried. We've now got two groups 
with two mean square errors. You don't give it to Daniel. Do you pick the 
one that gets them as different as they can be? Well we're truck? Well, 
okay, that would be one idea, get the two mean squared errors as different 
as possible, but why might that not work? What might be a problem with that 
sample size? Come on because you could just literally leave one point out 
yeah, so we could have like year made is less than 1950 and it might have a 
single sample with a low price and like that's, not a great split. Is it? 
You know because the other group is actually not going to be very 
interesting at all. Can you improve it? A bit. Can Jason improve it a bit? 
Could you take a weighted average yeah, a weighted average, so we could 
take point four one times: 17,000 plus 0.1 times 2,000. That's good right, 
and that would be the same as actually saying I've got a model. The model 
is a single binary decision and I'm going to say for everybody with year 
made less than ninety six point: five, I'm going to fill in point ten point 
two for everybody else are going to fill in nine point two and then I'm 
going to calculate The root mean squared error of this crappy model, and 
that would give exactly the same right as the weighted average that you're 
suggesting okay good.</p>

<p>So we now have a single number that represents how 
good a split is, which is the weighted average of the mean squared errors 
of the two groups that creates okay, and thanks to, I think it was. Was it 
Jake? We have a way to find the best split, which is to try every variable 
and to try every possible value of that variable and see which variable and 
which value gives us a split with the best score that makes sense. Okay, 
what's your name, sir okay and somebody give Natalie the box when you see 
every possible number for every possible variable link? Are you saying like 
here? We have 0.5 as like our criteria, to split the tree. So are you? Are 
you saying we're trying out a reasonable number for every possible value 
right? So cupola system only has two values, true and false. So there's 
only one way of splitting, which is trues and falses ear made, is an 
integer which varies between like O'donnell, 1960 and 2010. So we can just 
say what are all the possible unique values of year made and and try them 
all so we're trying all the possible spec points. Can you pass a better 
annual pass to me and I so I just want to clarify again for the first 
split. Why did we split on Cutler system, true or false sister, because 
what we did was we used Jake's technique? We tried every variable for every 
variable. We tried every possible split, so each one we noted down.</p>

<p>I think 
it was Jason's idea, which was the weighted average mean squared error of 
the two groups are created, we found which one had the best means grid 
error and we picked it and it turned out. It was cutlass system. True or 
false. Does that make sense? I guess my question is more like so coupler 
system is like one of them like best indicators. I guess it's the best. We 
tried every variable and every possible level everything else at try. It 
wasn't as good okay and then you do that right. So now that we've done 
that, we now take this group here, everybody who's got coupler system 
equals true, and we do it again for every possible variable for every 
possible level for people where coupler system equals true, what's the best 
possible split and then are there circumstances When it's not just like 
binaries, like you split it into like three groups for like example, you're 
made, so I'm gon na make a claim, and then I'm gon na see if you can 
justify it, I'm gon na claim that it's never necessary to do more than One 
split at a level because you can just cuz: you can just split them again 
exactly so, you can get exactly the same result by submitting twice okay 
good. So that is the entirety of creating a decision tree. You stop either 
when you hit some limit. That was requested, so we had a limit where we 
said max depth equals three, so that's one one way to stop would be you ask 
to stop at some point, and so we stopped otherwise you stop when you're 
your leaf nodes.</p>

<p>These things at the end of court leaf nodes when your leaf 
nodes only have one thing in them: okay, that's a decision tree. That is 
how we grow a decision tree, and this decision tree is not very good 
because it's got a validation r-squared of 0.4. So we could try to make it 
better by removing next steps, equals three right and creating a deeper 
tree. So it's going to go all the way down they're going to keep splitting 
these things further until every leaf. Node only has one thing in it and if 
we do that, the training r-squared is, of course one because we can exactly 
predict every training element, because it's in a leaf node or on its own, 
but the validation r-squared is not one. It's actually better than our 
really really shallow tree, but it's not as good as we'd like okay. So we 
want to find some other way. </p>

<h3>9. <a href="https://youtu.be/blyXCk4sgEg?t=47m1s">00:47:01</a></h3>

<ul style="list-style-type: square;">

<li><b> Bagging of little Boostraps, ensembling</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Of making these trees better and the way we're going to do it is to create 
a forest. So what's the forest to create a forest, we're going to use a 
statistical technique called bagging and you can bag any kind of model. In 
fact, Michael Jordan, who is one of the speakers at the recent recent data 
Institute conference here at university of san francisco, developed a 
technique called the bag of little bootstraps and which he shows how to use 
bagging for absolutely any kind of model to make it More robust, and also 
to give you confidence intervals, the random forest is simply a way of 
bagging trees. So what is bagging managing is a really interesting idea, 
which is what, if we created five different models, each of which was only 
somewhat predictive but the models weren't at all correlated with each 
other. They gave predictions that weren't correlated with each other. That 
would mean that the five models would have profound different insights into 
the relationships in the data, and so, if you took the average of those 
five models right, then you're effectively bringing in the insights from 
each of them. And so this idea of averaging models is a is: is a technique 
for ensemble right, which is really important? Now, let's come up with a 
more specific idea of how to do this. Ensemble. What if we created a whole 
lot of these trees, big, deep massively overfit trees right, but each one? 
Let's say we only pick a random one-tenth of the data.</p>

<p>So we pick one out 
of every 10 rows at random, build a deep tree right, which is perfect on 
that subset and kind of crappy. On the rest, all right, let's say we do 
that a hundred times so different random sample every time. So all of the 
trees are going to be better than nothing right because they do actually 
have a real random subset of the data, and so they found some insight but 
they're also overfitting terribly, but they're all using different random 
samples. So they all over fit in different ways on different things, so in 
other words, they all have errors, but the errors are random. What is the 
average of a bunch of random errors? Zero, so in other words, if we take 
the average of these trees, each of which have been trained on a different 
random subset, the errors will average out to zero and, what's left is the 
true relationship and that's the random forest right. So there's the 
technique right. We've got a whole bunch of rows of data. We grab a few at 
random, all right put them into a smaller data set and build a tree based 
on that. Okay - and then we put that tree aside and do it again with a 
different random subset and do it again with a different random subset. Do 
that a whole bunch of times and then for each one. We can then make 
predictions by running our test data through the tree to get to the leaf 
mode. Take the average in that leaf, node for all the trees and average 
them all together.</p>

<p>So, to do that, we simply call random forests regressor 
and by default it creates n what scikit-learn cords estimators an estimator 
is a tree right, so this is going to create ten treats, and so we go ahead 
and train it. I can't remember if I remember, to split okay, so create our 
ten trees and we're just doing this on our little random subset of 20,000, 
and so, let's take a look at one example, can you pass the box to Devin so 
just to make sure I understand This so you're saying like we take ten kind 
of crappy models. We average ten crappy models and we get a good model 
exactly because the crappy models are based on different random subsets, 
and so their errors are not correlated with each other. If the error errors 
work, are they with other this isn't going to work okay? So the key insight 
here is to construct multiple models which are better than nothing and 
where the errors are as much as possible, not correlated with each other. 
So is there like a certain number of trees that, like we need that, in 
order to be this sort of the things like valid or invalid, there's like has 
a good validation set, our MSC or not, you know, and so that's what we're 
going to look at Is how to make that metric higher, and so this is the 
first of our hyper parameters, then we're going to learn about how to tune 
hyper parameters and the first one is going to be the number of trees and 
we're about to us. Look at that now.</p>

<p>Yes, mostly you're selecting, aren't 
they exclusive? Yes, so I mentioned you know. One approach would be pick 
out like attempt at random, but actually what scikit-learn does by default 
is for n rows. It picks out n rows with replacement, okay and that's called 
bootstrapping, and if memory serves me correctly, that gets you on average, 
63.2 % of the rows will be represented, and you know a bunch of them will 
be represented multiple times yeah. It sure so, rather than just picking 
out like a tenth of the rows at random, instead we're going to pick out of 
an n row data set we're going to pick out n rows with replacement, which, 
on average, gets about 63. I think 63.2 % of the rows will be represented. 
Many of those rows will appear multiple times. I think there's a question 
behind you. In essence, what this model is doing is, if I understand Craig, 
is just picking out data points that look very similar to the one you're 
looking at yeah, that's a great insight. So what a tree is kind of doing, 
there's no quite complicated way of doing about doing that. I mean there 
would be other ways of assessing similarity. There are other ways of 
assessing similarity, but what's interesting about this way, is it's doing 
it in in tree space? Right so we're basically saying what are in this case 
like for this little tree.</p>

<p>What are the 593 samples you know closest to 
this one and what's their average closest in tree space, so other ways of 
doing that would be like and we'll learn later on. In this course about K, 
nearest neighbors, you could use like Euclidean distance C right, but 
here's a thing. The whole point of machine learning is to identify which 
variables actually matter the most and how do they relate to each other and 
to your dependent variable together right. So if you've like imagine a 
synthetic data set where you create five variables that add together to 
create your independent to create your dependent variable and ninety five 
variables, which are entirely random and don't impact your dependent 
variable. And then, if you do like a K, nearest neighbors in Euclidean 
space, you're going to get meaningless nearest neighbors, because most of 
your columns are actually meaningless or imagine your actual relationship 
is that your dependent variable equals x1 times x2. Then you'll actually 
need to find this interaction right, so you don't actually care about how 
close it is to x1 and how close to x2, but how close to the product. So the 
entire purpose of modeling in machine learning is to find a model which 
tells you which variables are important and how do they interact together 
to drive your dependent variable and so you'll find in practice the 
difference between using like tree space or random forest space? To find 
your nearest neighbors versus like Euclidean space is the difference 
between a model that makes good predictions in the model.</p>

<p>That makes many 
most predictions elicit here. I did, but I feel, like we've got only 35 
minutes, so yeah great, so so in general, a machine learning model which is 
effective is one which is accurate. When you look at the training data, 
it's it's accurate at predicting at actually finding the relationships in 
that training data, and then it generalizes well to new data, and so in 
bagging. That means that each of your individual estimators at your 
individual trees, you want to be as predictive as possible, but for the 
predictions of your individual trees to be as uncorrelated as possible, and 
so the inventor of random forests talks about this at length. In his 
original paper that introduced this in the late 90s, this idea of trying to 
come up with predictive but poorly correlated trees. The the research 
community in recent years has generally found that the more important thing 
seems to be creating uncorrelated trees rather than more accurate trees. So 
more recent </p>

<h3>10. <a href="https://youtu.be/blyXCk4sgEg?t=57m1s">00:57:01</a></h3>

<ul style="list-style-type: square;">

<li><b> scikit-learn ExtraTreeRegressor randomly tries variables</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Advances tend to create trees which are less predictive on their own, but 
also less correlated with each other. So, for example, in scikit-learn 
there's another class you can use called extra trees regress on your extra 
trees, classifier with exactly the same API. You can try it tonight. Just 
replace my random prose regressor with that that's called an extremely 
randomized trees model and what that does is the same as what we just 
discussed, but rather than trying every speck of every variable. It 
randomly tries a few splits of a few variables right. So it's much faster, 
the Train, it has more randomness, okay, but then you've got time. You can 
build more trees and therefore get better generalization. So in practice, 
if you've got crappy individual models, you just need more trees to get a 
good end up model Melissa. Could you pass that over to Devin? Could you 
talk a little bit more about what you mean by like uncorrelated trees, 
yeah? If I build a thousand trees h1 on just ten data points, then it's 
quite likely that the ten data points for every tree are going to be 
totally different, and so it's quite likely that those ten trees are going 
to trees are going to give totally different Answers to each other, so the 
correlation between the predictions of tree, one and three two is going to 
be very small between tree one and three, three, very small and so forth. 
On the other hand, if I create a thousand trees where each time I use the 
entire data set with just one element removed, all those trees are going to 
be nearly identical.</p>

<p>Ie, their predictions will be highly correlated, and 
so, in the latter case, it's probably not going to generalize very well 
where else in the former case the individual trees we're not going to be 
very predictive. So I need to find some nice in-between, so yes, Danielle 
and is there a case where you want to use one over the other like any 
particular times, yeah so again, hyper parameter tuning so driven in terms 
of like random, random forests versus extremely randomized phase yeah. So 
again, a hyper parameter, walk tree architecture. Do we use so we're going 
to talk about that now? Can you pass that through? Do you know you know, I 
was just trying to understand how this random forest actually makes sense 
for continuous variables. I mean I'm assuming that you build a tree 
structure and the last final notes you'd be seeing like maybe the small 
node represents, maybe a category a or a category B. But how does it make 
sense for your continuous storage? So this is actually what we have here, 
and so the value here is the average. So this is the average log of price 
for this subgroup and that's what we do. The prediction is the average of 
the value of the dependent variable in that leaf. Node means, finally, if 
you have just like dengue nodes, you just have 10 values. Yes, that's 
right! Well, if it was only one tree right, so a couple things to remember. 
The first is that by default, we're actually going to train the tree all 
the way down until the leaf nodes were size 1, which means for a data set 
with n rows.</p>

<p>We're gon na have n leaf nodes and then we're going to have 
multiple trees, which we average together right. So in practice, we're gon 
na have a you know: lots of different possible values. It's a question 
behind you! So far, the continuous variable. How do we decide like which 
value to split out? Because there could be many values? We try every 
possible value of that in the training set, one did be computationally 
computational expensive, and this is where it's very good to remember that 
your CPUs performance is measured in gigahertz, which is billions of clock 
cycles per second, and it has multiple cores and each Core has something 
called Simbi single instruction, multiple data, where it can direct up to 
eight computations per core at once, and then, if you do it on the GPU, the 
performance is measured in teraflops, so trillions of floating-point 
operations per second, and so this is where, when It comes to designing 
algorithms, it's very difficult for us mere humans to realize how stupid 
algorithms should be given how fast today's computers are so yeah, it's 
quite a few operations, but that trillions per second you hardly notice it 
Russia. I have a question so essentially at each remote we make a decision 
like which category T or which variable to use and which pinpoint yes, so 
we have MSE a calculated for each node.</p>

<p>So this is kind of our one of the 
decision criteria, but please MSE it is calculated for which model like 
which model underlies the model is the model is for the initial root mode 
is what if we just predicted the average right, which is here is ten Point, 
oh nine, eight and just the average, and then the next model is what, if we 
predicted the average of those people with capitalist system equals false 
and for those people with couple of system is true and then the next is 
what if we predicted the average Of couple systems equals true yeah made 
less than 1986. Is it always average sure we can use median or we can even 
run linear regression? There's all kinds of things we could do and practice 
the average works really well. There are. There are types of they're not 
called random forests, but there are kinds of trees where the leaf nodes 
are independent, linear, regressions, they're not terribly widely used, but 
there are certainly researchers who I have worked on them. Okay, thank you 
and pass it back over that afford and then to check. So this tree has a 
depth of three yeah and then I on one of the next commands we get rid of 
the max depth yeah the tree without the max depth. Does that contain the 
tree with with the depth of three yeah, so that is that, like by 
definition, it's yeah well, except in this case we've added randomness. But 
if you turned bootstrapping off then yeah the the deeper tree.</p>

<p>Will you 
know the the the the less deep tree would be, how it start and then it goes 
keep spitting okay. So you have many trees, you're gon na </p>

<h3>11. <a href="https://youtu.be/blyXCk4sgEg?t=1h4m1s">01:04:01</a></h3>

<ul style="list-style-type: square;">

<li><b> m.estimators_,</b></li>

<li><b>Using list comprehension</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Have different leaf nodes across streets? Hopefully, so we want um. So how 
do you average leaf nodes across different trees? So we just take the first 
row in the validation set. We run it through the first tree. We find its 
average nine point. Two eight then do it through the next tree find its 
average in the second tree, nine point: nine, five and so forth, and we're 
about to do that. So you'll see it okay. So let's try it right. So after 
you've built a random forest, each tree is stored in this attribute called 
estimators underscore okay, so one of the things that you guys need to be 
very, very comfortable with is using list comprehensions. Okay, so I hope 
you've all been practicing okay, so here I'm using a list comprehension to 
go through each tree in my model, I'm going to call predict on it with my 
validation set, and so that's going to give me a list of arrays of 
predictions. So H array will be all of the predictions for that tree and I 
have ten trees. Mp, dot stack, concatenates them together on a new axis. So 
after I run this and called shape, you can see. I now have the first axis 
ten means I have my ten different sets of predictions and for each one my 
validation set as a side of twelve thousand. So here are my twelve thousand 
predictions for each of the ten trees.</p>

<p>Alright. So let's take the first row 
of that and print it out, and so here are what weirdest thing here are ten 
predictions one from each tree: okay, and so then, if we say take the mean 
of that here is the mean of those ten predictions and then What was the 
actual? The actual was nine point. One. Our prediction was nine point O 
seven, so you see how like none of our individual trees had very good 
predictions, but the mean of them was actually pretty good, and so, when I 
talk about experimenting like Jupiter, notebook is great for experimenting. 
This is the kind of stuff I mean dig inside these objects and like look at 
them and plot them. Take your own averages cross check to make sure that 
they work the way you thought they did write your own implementation of 
r-squared make sure it's the same as a psychic learn version plot it like 
here's an interesting plot. I did let's go through each Taffet entries 
right and then take the mean of all of the predictions up to the ithe tree 
right. So, let's start by predicting just based on the first tree than the 
first two trees and the first three trees. And let's then plot the 
r-squared, so here's the r-squared of just the first tree is the r-squared 
of the first two trees, three trees, four trees, blah blah blah blah up to 
ten trees, and so not surprisingly, a script keeps improving right because 
the more estimators we Have the more bagging that we're doing the more 
it's? Well, it's going to generalize all right and you should find that 
that number there bit under point.</p>

<p>Eight six should match this number here: 
okay, let's rerun that yeah, okay, so they're actually slightly above what 
it says. All right so again, these are all like the cross checks. You can 
do the things you can visualize to deepen your understanding. Okay, so, as 
we add more trees, our r-squared improves, it seems to flatten out after a 
while. So we might guess that if we increase the number of estimators to 
twenty right, it's maybe not going to be that much better. So, let's see 
we've got point. Eight. Six. Two first is point: eight, six, oh yeah, so 
doubling the number of trees didn't help very much, but double it again. 
Eight six, seven double it again, eight six! Nine! So you can see like 
there's some point at which you're going to you know not want to add more 
trees, not because it's never going to get worse, because every tree is, 
you know, giving you more semi-random models to bag together right, but 
it's going to stop Improving things much okay, and so this is like the 
first hyper parameter. If you learn to set, is number of estimators and the 
method for setting is as many as you have time to fit, and that actually 
seemed to be hopping. Okay, now, in practice, we're going to learn to set a 
few more hyper parameters, adding more trees, slows it down, but with less 
trees. You can still get the same insights, so I've build most of my models 
in practice with like 20 to 30 trees, and it's only like then at the end of 
the project, or maybe at the end of the day's work.</p>

<p>I'll then try doing 
like. I don't know a thousand trees and run it overnight. Was there a 
question? Yes, can we pass that Prince, so each tree might have different 
estimators different combination of estimators HQ is an estimator, so this 
is a synonym. So in scikit-learn, when I say estimator, they name three, so 
I mean each tree would have different break points on differently on 
different columns. But if, at the end, we want to locate the important 
features and we'll get to that yeah, so I'm after we finished with kind of 
setting hyper parameters. The next stage of the course will be learning 
about what it tells us about the data. If you need to know it now, you 
know, for your projects feel free to look ahead. There's a lesson to our F 
interpretation. </p>

<h3>12. <a href="https://youtu.be/blyXCk4sgEg?t=1h10m">01:10:00</a></h3>

<ul style="list-style-type: square;">

<li><b> Out-of-bag (OOB) score</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Them is where we can see it. Okay, so that's our first type of parameter 
um. I want to talk next about out-of-bag score. Sometimes your data set 
will be kind of small and you won't want to pull out a validation set 
because doing so means you now. Don't have enough data to build a good 
model. What do you do? There's a cool trick which is pretty much unique to 
random forests, and it's this. What we could do is recognize that some of 
our in our first tree some of our columns - sorry some of our rows - didn't 
get used. So what we could do would be to pass those rows through the first 
tree and treat it as a validation set and then for the second tree. We 
could pass through the rows that weren't used for the second tree through 
it to create a validation set for that and so effectively. We would have a 
different validation set for each tree and so now to calculate our 
prediction. We would average all of the trees where that row is not used 
for training right so for tree number one we would have the ones I've 
marked in blue here and then maybe for tree number. Two it turned out. It 
was like this one, this one, this one and this one and so forth right so as 
long as you've got enough trees, every rows going to appear in the out of 
bag sample from one of them, at least so you'll be averaging.</p>

<p>You know, 
hopefully, a few trees, so if you've got a hundred trees, it's very likely 
that all of the rows are going to appear many times in these other bag 
samples. So what you can do is you can create an out of bag prediction by 
averaging. All of the trees, you didn't use to train each individual row 
and then you can calculate your root, mean square error, R, squared, etc. 
On that, if you pass low B score equals true to scikit-learn, it will do 
that for you and it will create an attribute called oob score underscore, 
and so my little print score function here. If that attribute exists, it it 
adds it to the print. So if you take a look here, hoby square equals. True, 
we've now got one extra number and it's R squared that is the R squared for 
the oeob sample. It's a square. It is very similar, the R squared and the 
validation set, which is what we hoped for. Can we plus it? Is it the case 
that the prediction for the obese core has to be must be mathematically 
lower than the one for our entire forest um? Certainly, it's not true that 
the prediction is lower, it's possible for accuracy, yeah um. It's not 
mathematically necessary that it's true, but it's gon na be true on 
average, because your average for each row appears in less trees in the 
oeob samples and it does in the full set of trees. So, as you see here, 
it's a little less good.</p>

<p>So I'm in general, it's a great insight Chris in 
general, the OOBE r-squared will slightly underestimate how generalizable 
the model is the more trees. You add the less serious that underestimation 
is and for me in practice. I I find it's totally good enough. You know in 
practice. Okay, so this </p>

<h3>13. <a href="https://youtu.be/blyXCk4sgEg?t=1h13m45s">01:13:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Automate hyperparameters hyper-parameters with grid-search gridsearch</b></li>

<li><b>Randomly subsample the dataset to reduce overfitting with ‘set_rf_samples()’, code detail at 1h18m25s</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Our B score is, is, is super handy and one of the things that super handy 
for is you're gon na see, there's quite a few hyper parameters that were 
going to set, and we would like to find some automated way to set them. And 
one way to do that is to do what's called a grid search. A grid search is 
where, if there's a scikit-learn function, called grid search and you pass 
in the list of all of the parameters, all of the hyper parameters that you 
want to tune. You pass in for each one a list of all of the values of that 
hyper parameter. You want to try and it runs your model on every possible 
combination of all of those hyper parameters and tells you which one is the 
best, and our B score is a great like choice for to forgetting it to Tory, 
which one is best in terms of Our V score, like that's an example of 
something you can do with a which works well now, if you think about it, I 
kind of did something pretty dumb earlier, which is. I took a subset of 
30,000 rows of the data and it built all my models of that, which means 
every tree in my random forest is a different subset of that subset of 
30,000. Why do that? Why not take a different? I, like a totally different 
subset of 30,000 each time, so in other words, let's leave the entire 300 
thousand records, as is right, and if I want to make things faster right, 
pick a different subset of 30,000 each time so, rather than bootstrapping 
the entire set of Rows, that's just randomly sample a subset of the data, 
and so we can do that.</p>

<p>So let's go back and recall property, yet without 
the subset parameter to get all of our data again and so to remind you that 
is okay. 400,000. I, in the whole data frame of which we have three 89,000 
in our training set, and instead we're going to go, set our F samples 
20,000. Remember that was the site that off the 30,000 we use 20,000 of 
them in our training set. If I do this there now, when I run a random 
forest, it's not going to bootstrap an entire set of 391 thousand rows. 
It's going to just grab a subset of 20,000 rows, all right, and so now, if 
I run this, it will still run just as quickly as if I had like originally 
done a random sample of 20,000. But now every tree can have access to the 
whole data set right. So if I do enough estimators enough trees, eventually 
it's going to see everything right. So in this case, with 10 trees, which 
is the default. I get an R squared of 0.8 6, which is actually about the 
same as my R squared with the with the 20,000 subsets, and that's because I 
haven't used many estimators yet right. But if I increase the number of 
estimators, it's got to make more of a difference right. So if I increase 
the number of estimators two-forty, alright, it's going to take a little 
bit longer to run, but it's going </p>

<h3>14. <a href="https://youtu.be/blyXCk4sgEg?t=1h17m20s">01:17:20</a></h3>

<ul style="list-style-type: square;">

<li><b> Tip for Favorita Grocery competition,</b></li>

<li><b>‘set_rf_samples()’,</b></li>

<li><b>‘reset_rf_samples()’,</b></li>

<li><b>‘min_samples_leaf=’,</b></li>

<li><b>‘max_features=’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>To be able to see a larger subset of the data set, and so, as you can see, 
the r-squared is gone up from point. Eight six two point: eight, seven, 
six: okay, so this is actually a great approach and for those of you who 
would doing the groceries competition, that's got something like 120 
million roads, but there's no way you would want to create a random forest 
using 128 million roads. In every tree, like it's going to take forever, so 
what you could do is use this set area of samples to do like, I don't know 
a hundred thousand or a million I'll play around with it. So the trick here 
is that, with a random forest using this technique, no data set is too big. 
I don't care. If it's got a hundred billion rows right, you can create a 
bunch of trees, each one of the different random subsets. Can somebody pass 
the actual accuracy, so my question was for the albey scores and these ones 
does it take the only like for the ones from the sample or take from all 
the that's a great question, so unfortunately, scikit-learn does not 
support this functionality. Out of the box, so I had to write this and it's 
kind of a horrible hack right, because we're much rather be passing in, 
like a sample size parameter rather than doing this kind of setting up 
here. So what I actually do is, if you look at the source code, is I'm 
actually. This is a internal. This is the internal function.</p>

<p>I looked at 
their source code that they call and I've replaced it with a with a lambda 
function that has the behavior. We want, unfortunately, the current version 
is not changing how our B is calculated, so yeah, so currently low, B, 
scores and set our F samples are not compatible with each other, so you 
need to turn our B equals false. If you use this approach, which I hope to 
fix, but at this stage it's it's not fixed, so if you want to turn it off, 
you just call reset our F samples, okay, and that returns it back to what 
it was okay. So in practice, when I'm like doing interactive machine 
learning using random forests in order to like explore my model explore 
hyper parameters, the stuff we're going to learn in the future lesson where 
we actually analyze like feature importance and partial dependence and so 
forth. I generally use subsets and reasonably small forests, because all 
the insights that I'm going to get are exactly the same as the big ones, 
but I can run it in, like you know three or four seconds rather than hours 
alright. So this is one of the biggest tips I can give you and very very 
few people in industry or academia actually do this. Most people run all of 
their models on all of the data all of the time, using their best possible 
parameters, which is just pointless right if you're trying to find out like 
which features are important and how are they related to each other and so 
forth? Having that fourth decimal place of accuracy isn't going to change 
any of your insights at all, okay, so I would say, like do most of your 
models on you know a large enough sample size that your accuracy is.</p>

<p>You 
know reasonable when I say reasonable. It's like within a reasonable 
distance of the best accuracy you can get and it's taking you know a small 
number of seconds to train so that you can interactively. Do your analysis, 
so there's a couple more parameters. I wanted to talk about so I'm going to 
call reset RF samples to get back to our full data set because in this case 
at least on this computer, it's actually running in less than 10 seconds. 
So here's our baseline we're going to do a baseline with 40 estimators, 
okay, and so each of those 40 estimators is going to Train all the way down 
to all the leaf nodes. Just have one sample in them. So that's going to 
take a few seconds to run here we go so that gets us a point: eight nine, 
eight, a sweat on the validation set or 0.90 eight on the oeob. Now this 
case the OB is better. Why is it better? Well, that's because remember, our 
validation set is not a random sample. Our validation set is a different 
time period. Okay, so it's actually much harder to predict a different time 
period than this one, which is just predicting random. Okay. So that's why 
this is not the way around we expected so the next. The first parameter we 
can try fiddling with is min samples leaf and so min samples leaf says, 
stop training the tree further. When your leaf node has three or less 
samples in there are going all the way down until there's one we're going 
to go down until there's three.</p>

<p>So in practice this means there's going to 
be like one or two less levels of decision being made, which means we've 
got like half the number of actual decision criteria we have to do. Is it's 
going to train more quickly? It means that when we look at an individual 
tree rather than just taking one point, we're taking the average of at 
least three points, that's who would expect the trees to generalize each 
one to generalize a little bit better okay, but each tree is probably going 
to Be slightly less powerful on its own, so let's try training that so 
possible values of min samples leaf. I find ones which work well are kind 
of 1. 3. 5. 10. 25. You know like I find that kind of range seems to work 
well but like sometimes, if you've got a really big data set and you're, 
not using the small samples. You know you might need a mint samples leaf of 
hundreds or thousands. So it's you kind of got to think about like how 
bigger your sub-samples going through and try things out now in this case, 
going from the default of one to three has increased. Our validation set 
our squared from 898 to 902. So it's a slight improvement. Okay and it's 
going to train a little faster as well as something else you can try, which 
is, and since this worked, I'm going to leave that in I'm going to add in 
max features equals point five. Why does max features do well? The idea is 
that the less correlated your trees are with each other, the better.</p>

<p>Now 
imagine you had one column that was so much better than all of the other 
columns of being predictive that every single tree you built, regardless of 
like which subset of rows, always started with that column. So the trees 
are all going to be pretty similar right. But you can imagine there might 
be some interaction of variables where that interaction is more important 
than that individual column. So if every tree always fits on the first 
thing, the same thing, the first time you're not going to get much 
variation in those trees. So what we do is, in addition to just taking a 
subset of rows. We then, at every single split point, take a different 
subset of columns. So it's slightly different to the row sampling for the 
row. Sampling H, nu tree is based on a random set of rows or columns 
sampling, every individual binary split. We choose from a different subset 
of columns, so in other words, rather than looking at every possible level 
of every possible column, we look at every possible level of a random 
subset of columns. Okay and each time each decision point each binary 
split. We use a different random subset. How many well you get to pick 0.5 
means randomly choose half of them. The default is to use all of them. 
There's also a couple of special values you can use here.</p>

<p>As you can see in 
max features, you can also pass in square root to get square root of 
features or log two to get log two in features so in practice, good values. 
I found our range from 1.5 log to or square root. That's going to give you 
a nice bit of variation right. Can somebody positive Danielle and so just 
to clarify? Does that just like break it up smaller each time it goes 
through the tree or is it just taking half of what's left over or like 
hasn't been touched each time? There's no such thing as, what's left over 
after you've split on year, made less than or greater than 1984. You made 
still there right so later on. You might then spit on your maid left Center 
greater than 1989. So so it's just each time, rather than checking every 
variable to see where it's best fit. Is you just check half of them and so 
the next time you check a different hops. The next time you took a 
different path, but I mean, like terms is, as you got like further to like 
the leaf you're gon na have less options. No you're not you'd, never remove 
the variables okay, you can use them again and again and again, because 
you've got lots of different spit points. So imagine, for example, that the 
relationship was just entirely linear between year made and price right, 
then, in practice to actually model that you know your real relationship is 
year made versus price right, but the best we could do would be this kind 
of. First of all, split here right and then just split here and here right 
and like spitting, spitting, split so yeah, even if the binary most random 
forest libraries don't do anything special about that they just kind of go. 
Okay, we'll try this variable! Oh it turns out.</p>

<p>There's only one level 
left, you know so yeah that didn't they don't do any kind of clever 
bookkeeping. Okay. Okay, so if we add next features equals 0.5, it goes up 
from 901, the 906, so that's better still, and so, as we've been doing, 
this we've also hopefully have noticed that our root mean squared error. 
Log price has been dropping on a validation set as well, and so it's now 
down to 0.2 to 86. So how good is that right? So, like our totally untrue 
and random florists got us in about the top 25 percent? Now remember, our 
validation set, isn't identical to the Kaggle test set right, and this 
competition unfortunately, is old enough that you can't even put in a kind 
of after this after the time entry to find out how you would have gone. So 
we can only approximate how we could have gone, but you know, generally 
speaking, is going to be a pretty good approximation. So 2:286 here is the 
competition. Here's, the public leaderboard two, two eight six here we go 
14th or 15th place. So you know, roughly speaking, looks like we would be 
about in the top 20 of this competition with basically totally brainless 
random forest with some totally brainless minor, hyper parameter tuning, 
and so this is kind of why the random forest is such an important, not just 
first Step but often only step from the Xin learning, because it's kind of 
hard to screw it up like even when we didn't tune the hyper parameters, we 
still got a good result and then a small amount of type of parameter tuning 
got us a much better result And so any kind of model so, and I'm 
particularly thinking of like linear type models which have a whole bunch 
of statistical assumptions, and you have to get a whole bunch of things 
right before they start to work at all, can really throw you off track 
right Because they give you like totally wrong answers about how accurate 
the predictions can be, but also the random forest, you know, generally 
speaking, they tend to work on.</p>

<p>Most data sets most of the time with most 
sets of hyper parameters, so for </p>

<h3>15. <a href="https://youtu.be/blyXCk4sgEg?t=1h30m20s">01:30:20</a></h3>

<ul style="list-style-type: square;">

<li><b> Looking at ‘fiProductClassDesc’ column with <code>.cat.categories</code> and <code>.cat.codes</code></b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - please proofread to improve - remove this note once proofread)</p>


<p>Example, we did this thing with it with our categorical variables. In fact, 
let's take a look at our tree. A single tree. Look at this right, fi 
product class desc less than 7.5. What does that mean? So f, I product plus 
desc, here's some examples of that column all right. So what does it mean 
to be less than or equal to 7? Well, we'd have to look at dark cat, dark 
categories to find out, okay, and so it's 0. 1. 2. 3. 4. 5. 6. 7. So what 
it's done is its created, a split where all of the backhoe loaders and 
these three types of hydraulics enter in one group and everything else is 
in the other group. So, like that's like weird, you know like like these 
aren't, even in order we could have made them in order if we had, you know, 
bother to say the categories have this order, but we hadn't right. So how 
come this even works like because when we turn it into codes, it's 
actually, this is actually what the random forest sees, and so imagine to 
think about this. Imagine like the only thing that mattered was whether, as 
a hydraulic excavator at zero to two metric tons, and nothing else 
mattered. Imagine that right, so it has to pick out this this single level. 
Well, it can do that because, first of all, it could say: okay, let's pick 
out everything less than seven versus greater than seven to create.</p>

<p>You 
know this is one group, and this is another group right and then within 
this group they could then pick out everything less than six versus greater 
than six we're just going to pick out this one item right. So, with two 
split points: we can pull out a single category, so this is why it works. 
Right is because the tree is like infinitely flexible, even with a 
categorical variable, if there's particular categories which have different 
levels of price, it can like gradually zoom in on those groups by using 
multiple splits right now, you can help it by telling it the order of Your 
categorical variable, but even if you don't it's okay, it's just going to 
take a few more decisions to get there, and so you can see here it's 
actually using this product class desk quite a few times right and and as 
you go deeper down the tree. You'll see it used more and more right where 
else in a linear model or almost any kind of other model, certainly any any 
non tree model pretty much encoding. A categorical variable like this won't 
work at all, because there's no linear relationship to train totally 
arbitrary identifiers and anything right. So so these are the kinds of 
things that make Brennan forests very easy to use and and very resilient, 
and so by using that you know, we've gotten ourselves a model which is 
clearly you know world class.</p>

<p>At this point already, it's like you know, 
probably will, in the top 20 of this cackle competition and then in our 
next lesson, we're going to learn about how to analyze that model, to learn 
more about the data, to make it even better right. So this week try and 
like really experiment right, have a look inside, look, try and draw the 
trees, try and plot the different errors. Try maybe using different data, 
sets to see how they work really experiment, to try to get a sense and 
maybe try to like replicate things like write your own r-squared, you know, 
write your own versions or some of these functions. See if ya see how much 
you can really learn about your data set about the random person. Great see 
you on Thursday,  </p>






  </body>
</html>
