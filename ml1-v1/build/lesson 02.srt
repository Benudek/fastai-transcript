1
00:00:00,030 --> 00:00:06,000
so from here the next two or three

2
00:00:03,540 --> 00:00:08,849
lessons we're going to be really diving

3
00:00:06,000 --> 00:00:11,039
deep into random forests so so far all

4
00:00:08,849 --> 00:00:13,949
we've learned is there's a thing called

5
00:00:11,039 --> 00:00:16,559
random forests for some particular

6
00:00:13,949 --> 00:00:18,390
datasets they seem to work really really

7
00:00:16,559 --> 00:00:21,059
well without too much trouble

8
00:00:18,390 --> 00:00:24,060
but we don't really know yet like well

9
00:00:21,059 --> 00:00:25,679
how do they actually work what do we do

10
00:00:24,059 --> 00:00:28,018
if they don't work properly what are

11
00:00:25,679 --> 00:00:29,550
their pros and cons what are the can we

12
00:00:28,018 --> 00:00:31,919
tune and so forth so we're gonna look at

13
00:00:29,550 --> 00:00:33,840
all that and then after that we're going

14
00:00:31,920 --> 00:00:35,640
to look at how do we interpret the

15
00:00:33,840 --> 00:00:38,340
results of random forests to get not

16
00:00:35,640 --> 00:00:40,649
just predictions but to actually deeply

17
00:00:38,340 --> 00:00:42,570
understand our data in a model driven

18
00:00:40,649 --> 00:00:44,210
way so that's where we're going to go

19
00:00:42,570 --> 00:00:47,929
from here

20
00:00:44,210 --> 00:00:51,600
so let's just review where we're up to

21
00:00:47,929 --> 00:00:54,840
so we learned that there's this library

22
00:00:51,600 --> 00:00:58,079
called fast AI and the fast AI library

23
00:00:54,840 --> 00:01:01,800
is basically it's a highly opinionated

24
00:00:58,079 --> 00:01:03,960
library which is to say we've spent a

25
00:01:01,799 --> 00:01:05,368
lot of time researching what are the

26
00:01:03,960 --> 00:01:07,769
best techniques to get like

27
00:01:05,368 --> 00:01:09,629
state-of-the-art results and then we

28
00:01:07,769 --> 00:01:12,780
take those techniques and package them

29
00:01:09,629 --> 00:01:14,339
into pieces of code so that you can use

30
00:01:12,780 --> 00:01:21,930
the state-of-the-art results yourself

31
00:01:14,340 --> 00:01:25,228
and so where possible we wrap or provide

32
00:01:21,930 --> 00:01:26,640
things on top of existing code and so in

33
00:01:25,228 --> 00:01:29,150
particular for the kind of structured

34
00:01:26,640 --> 00:01:32,400
data analysis we're doing scikit-learn

35
00:01:29,150 --> 00:01:33,659
has a lot of really great code so most

36
00:01:32,400 --> 00:01:36,600
of the stuff that we're showing you from

37
00:01:33,659 --> 00:01:40,020
fast AI is stuff to help us get stuff

38
00:01:36,599 --> 00:01:44,399
into scikit-learn and then interpret

39
00:01:40,019 --> 00:01:47,118
stuff out from scikit-learn the fast AI

40
00:01:44,399 --> 00:01:47,118
library

41
00:01:48,790 --> 00:01:59,500
the way it works in our environment here

42
00:01:52,599 --> 00:02:02,919
is that we've got out our notebooks are

43
00:01:59,500 --> 00:02:07,629
inside fast AI repo / courses and in /

44
00:02:02,920 --> 00:02:12,280
ml 1 + dl 1 and then inside there

45
00:02:07,629 --> 00:02:15,849
there's a symlink to the parent of the

46
00:02:12,280 --> 00:02:20,800
parent fast AI so this is a sim link to

47
00:02:15,849 --> 00:02:24,069
a directory containing a bunch of

48
00:02:20,800 --> 00:02:28,660
modules so if you want to use the fast

49
00:02:24,069 --> 00:02:30,759
AI library in your own code there's a

50
00:02:28,659 --> 00:02:33,099
number of things you can do one is to

51
00:02:30,759 --> 00:02:35,679
put your notebooks or scripts in the

52
00:02:33,099 --> 00:02:37,539
same directory as ml 1 or 0 1 whether

53
00:02:35,680 --> 00:02:38,849
it's already this simile and just import

54
00:02:37,539 --> 00:02:43,060
it just like I do

55
00:02:38,849 --> 00:02:46,650
you could copy this directory dot dot

56
00:02:43,060 --> 00:02:49,569
slash dot dot slash birthday I into

57
00:02:46,650 --> 00:02:52,360
somewhere else and use it or you could

58
00:02:49,569 --> 00:02:55,209
sim link it just like I have from here

59
00:02:52,360 --> 00:02:57,610
to wherever you want to use it so notice

60
00:02:55,209 --> 00:03:00,819
it's mildly confusing there's a github

61
00:02:57,610 --> 00:03:04,030
repo called fast AI and inside the

62
00:03:00,819 --> 00:03:06,789
github repo caught fast AI which looks

63
00:03:04,030 --> 00:03:10,719
like this there is a folder called fast

64
00:03:06,789 --> 00:03:13,150
AI okay and so the FASTA a folder in the

65
00:03:10,719 --> 00:03:17,159
FASTA a repo contains the FASTA a

66
00:03:13,150 --> 00:03:20,379
library and it's that library when we go

67
00:03:17,159 --> 00:03:22,810
from fast AI dot imports to import star

68
00:03:20,379 --> 00:03:26,650
then that's looking inside the farsi a

69
00:03:22,810 --> 00:03:31,180
folder for a file called inputs imports

70
00:03:26,650 --> 00:03:36,060
py and importing everything from that

71
00:03:31,180 --> 00:03:36,060
okay yes Danielle

72
00:03:37,740 --> 00:03:43,930
okay and just like as a clarifying

73
00:03:42,669 --> 00:03:47,309
question for this Bentley

74
00:03:43,930 --> 00:03:47,310
it's just that anything

75
00:03:48,818 --> 00:03:53,359
that's just the Ellen thing that you

76
00:03:51,110 --> 00:03:56,930
talked about yeah so a symlink is

77
00:03:53,360 --> 00:04:00,019
something you can create by typing Ln

78
00:03:56,930 --> 00:04:01,790
minus s and then the path to the source

79
00:04:00,019 --> 00:04:04,340
which in this case would be dot dot dot

80
00:04:01,789 --> 00:04:06,469
first AI could be relative or it could

81
00:04:04,340 --> 00:04:08,209
be absolute and then the name of the

82
00:04:06,469 --> 00:04:09,590
destination if you just put the current

83
00:04:08,209 --> 00:04:12,189
directory at the destination

84
00:04:09,590 --> 00:04:17,720
it'll use the same name as it comes from

85
00:04:12,189 --> 00:04:20,228
like a alias on the Mac or a shortcut on

86
00:04:17,720 --> 00:04:20,229
Windows

87
00:04:29,500 --> 00:04:39,040
and when you do the important system yep

88
00:04:35,829 --> 00:04:41,319
got it imports this and then append that

89
00:04:39,040 --> 00:04:47,110
relative link that also creates the same

90
00:04:41,319 --> 00:04:49,060
link and I don't think I've created the

91
00:04:47,110 --> 00:04:51,310
same link anywhere in the workbooks the

92
00:04:49,060 --> 00:04:57,430
symlink actually lives inside the github

93
00:04:51,310 --> 00:04:59,740
repo okay I created some symlinks in the

94
00:04:57,430 --> 00:05:04,168
deep learning notebook to some data that

95
00:04:59,740 --> 00:05:06,960
was different at the top of Tim Lee's

96
00:05:04,168 --> 00:05:09,959
workbook from the last class there was

97
00:05:06,959 --> 00:05:09,959
important

98
00:05:10,120 --> 00:05:16,269
oh yeah don't do that probably I mean

99
00:05:12,728 --> 00:05:18,849
you you can but I think this is I think

100
00:05:16,269 --> 00:05:22,389
this is better like this way you can go

101
00:05:18,850 --> 00:05:24,010
from fast AI imports and regardless of

102
00:05:22,389 --> 00:05:29,829
kind of how you got it there it's it's

103
00:05:24,009 --> 00:05:31,959
going to work you know okay okay so then

104
00:05:29,829 --> 00:05:34,209
we had all of our data for blue books

105
00:05:31,959 --> 00:05:40,449
for bulldozers competition in data slash

106
00:05:34,209 --> 00:05:44,199
bulldozers and here it is right so we

107
00:05:40,449 --> 00:05:45,550
were able to read that CSV file the only

108
00:05:44,199 --> 00:05:49,149
thing we really had to do was to say

109
00:05:45,550 --> 00:05:51,819
which columns were dates and having done

110
00:05:49,149 --> 00:05:53,560
that we were able to take a look at a

111
00:05:51,819 --> 00:06:02,288
few earth examples of the rows of the

112
00:05:53,560 --> 00:06:07,199
data and so we also noted that it's very

113
00:06:02,288 --> 00:06:10,959
important to deeply understand the

114
00:06:07,199 --> 00:06:13,389
evaluation metric for this project and

115
00:06:10,959 --> 00:06:16,060
so if a cattle they tell you what the

116
00:06:13,389 --> 00:06:21,069
evaluation metric is and in this case it

117
00:06:16,060 --> 00:06:23,610
was the root mean squared log error so

118
00:06:21,069 --> 00:06:23,610
that is

119
00:06:25,649 --> 00:06:37,849
some of the actuals - the predictions

120
00:06:38,430 --> 00:06:50,728
now but it's the log of the actuals -

121
00:06:42,269 --> 00:06:55,859
the log the predictions squared right so

122
00:06:50,728 --> 00:06:57,418
if we replace actuals with log actuals

123
00:06:55,860 --> 00:06:59,520
and replace predictions with log

124
00:06:57,418 --> 00:07:03,810
predictions then it's just the same as

125
00:06:59,519 --> 00:07:06,599
root mean squared error so that's what

126
00:07:03,810 --> 00:07:10,500
we did was we replaced sale price with

127
00:07:06,600 --> 00:07:12,660
log of sale price and so now if if we

128
00:07:10,500 --> 00:07:15,870
optimize for root mean squared error

129
00:07:12,660 --> 00:07:17,570
we're actually optimizing for the root

130
00:07:15,870 --> 00:07:21,930
mean squared error of the logs

131
00:07:17,569 --> 00:07:26,550
okay so then we learnt that we need all

132
00:07:21,930 --> 00:07:29,129
of our columns to be numbers and so the

133
00:07:26,550 --> 00:07:32,728
first way we did that was to take the

134
00:07:29,129 --> 00:07:34,348
date column and remove it and instead

135
00:07:32,728 --> 00:07:43,949
replace it with a whole bunch of

136
00:07:34,348 --> 00:07:46,050
different columns such as is that date

137
00:07:43,949 --> 00:07:49,160
the start of a quarter is at the end of

138
00:07:46,050 --> 00:07:52,020
a year how many days are elapsed since

139
00:07:49,160 --> 00:07:53,699
generally the first 1970 what's a year

140
00:07:52,019 --> 00:07:58,069
what's the month or the day of weeks and

141
00:07:53,699 --> 00:07:58,069
so forth okay so they're all numbers

142
00:07:58,158 --> 00:08:03,810
then we learnt that we can use train

143
00:08:01,019 --> 00:08:08,370
underscore katz to replace all of the

144
00:08:03,810 --> 00:08:11,399
strings with categories now when you do

145
00:08:08,370 --> 00:08:12,840
that it doesn't look like you've done

146
00:08:11,399 --> 00:08:16,109
anything different they still look like

147
00:08:12,839 --> 00:08:22,679
strings all right but if you actually

148
00:08:16,110 --> 00:08:25,020
take a deeper look you'll see that the

149
00:08:22,680 --> 00:08:32,879
datatype now is not string but category

150
00:08:25,019 --> 00:08:35,639
and category is a panda's class where

151
00:08:32,879 --> 00:08:37,708
you can then go dark cat dot and find a

152
00:08:35,639 --> 00:08:41,069
whole bunch of different attributes such

153
00:08:37,708 --> 00:08:43,109
as cat categories to find a list of all

154
00:08:41,070 --> 00:08:45,540
of the possible categories and this says

155
00:08:43,110 --> 00:08:48,120
hi is going to be 0 low will become 1

156
00:08:45,539 --> 00:08:49,528
medium will become 2 so we can then get

157
00:08:48,120 --> 00:08:53,578
codes

158
00:08:49,528 --> 00:08:55,740
to actually get the numbers so then what

159
00:08:53,578 --> 00:08:58,019
we need to do to actually use this data

160
00:08:55,740 --> 00:09:00,209
set to turn it into numbers is take

161
00:08:58,019 --> 00:09:06,139
every categorical column and replace it

162
00:09:00,208 --> 00:09:06,138
with cap codes and so we did that using

163
00:09:07,639 --> 00:09:15,269
proc DF okay

164
00:09:11,730 --> 00:09:28,289
so how do I get the source code for proc

165
00:09:15,269 --> 00:09:30,889
DF question question mark okay all right

166
00:09:28,289 --> 00:09:33,870
so if I scroll down I go through each

167
00:09:30,889 --> 00:09:35,698
column and I numerical eyes it okay

168
00:09:33,870 --> 00:09:37,230
that's actually the one I want so I'm

169
00:09:35,698 --> 00:09:45,628
going to now have to look up numerical

170
00:09:37,230 --> 00:09:48,808
eyes so tab to complete it if it's not

171
00:09:45,629 --> 00:09:53,490
numeric then replace the data frames

172
00:09:48,808 --> 00:09:55,588
filled with that columns cat codes last

173
00:09:53,490 --> 00:09:58,519
one because otherwise unknown is minus

174
00:09:55,589 --> 00:10:03,240
one we went unknown to be zero okay so

175
00:09:58,519 --> 00:10:06,089
that's how we turn the strings into

176
00:10:03,240 --> 00:10:09,539
numbers right they get replaced with a

177
00:10:06,089 --> 00:10:11,069
unique basically arbitrary index it's

178
00:10:09,539 --> 00:10:13,289
actually based on the alphabetical order

179
00:10:11,068 --> 00:10:16,110
of the feature names

180
00:10:13,289 --> 00:10:20,519
the other thing property f did remember

181
00:10:16,110 --> 00:10:22,649
was continuous columns that had missing

182
00:10:20,519 --> 00:10:24,778
values the missing got replaced with the

183
00:10:22,649 --> 00:10:27,990
median and we added an additional column

184
00:10:24,778 --> 00:10:30,240
called column name underscore na which

185
00:10:27,990 --> 00:10:34,850
is a boolean column told you if that

186
00:10:30,240 --> 00:10:39,680
particular item was missing or not

187
00:10:34,850 --> 00:10:43,490
so once we did that we were able to call

188
00:10:39,679 --> 00:10:46,278
random forest regressor dot fit and get

189
00:10:43,490 --> 00:10:51,919
the dots poor and turns out we have an

190
00:10:46,278 --> 00:11:03,190
r-squared of 0.98 can anybody tell me

191
00:10:51,919 --> 00:11:03,189
what an r-squared is listen um can we

192
00:11:07,080 --> 00:11:09,139
you

193
00:11:12,548 --> 00:11:20,149
so r-squared essentially shows how much

194
00:11:16,068 --> 00:11:27,288
variance is explained by the modal this

195
00:11:20,149 --> 00:11:33,470
is the yeah this is the relation of this

196
00:11:27,288 --> 00:11:36,318
is ssro which is like trying to trying

197
00:11:33,470 --> 00:11:37,759
to remember the exact formal but I don't

198
00:11:36,318 --> 00:11:40,098
roughly a curative way

199
00:11:37,759 --> 00:11:43,369
yeah intuitively it's how much the model

200
00:11:40,099 --> 00:11:46,329
explains the how much it accounts for

201
00:11:43,369 --> 00:11:50,749
the variance in the data okay good so

202
00:11:46,328 --> 00:11:52,969
let's talk about the formula and so with

203
00:11:50,749 --> 00:11:55,189
formulas the idea is not to learn the

204
00:11:52,970 --> 00:11:58,339
formula and remember it but to learn

205
00:11:55,188 --> 00:12:04,639
what the formula does and understand it

206
00:11:58,339 --> 00:12:08,359
right so here's the formula it's 1 minus

207
00:12:04,639 --> 00:12:09,859
something divided by something else so

208
00:12:08,359 --> 00:12:14,119
what's this something else from the

209
00:12:09,859 --> 00:12:18,668
bottom SS taught okay so what this is

210
00:12:14,119 --> 00:12:18,668
saying is we've got some actual data

211
00:12:20,070 --> 00:12:28,839
so my eyes right we've got some actual

212
00:12:22,929 --> 00:12:37,359
data 3e two-for-one okay and then we've

213
00:12:28,839 --> 00:12:43,380
got some average okay so our top bit

214
00:12:37,360 --> 00:12:47,409
this SS tot is the sum of each of these

215
00:12:43,379 --> 00:12:50,019
- that

216
00:12:47,409 --> 00:12:52,868
so in other words it's telling us how

217
00:12:50,019 --> 00:12:55,778
much does this data bury perhaps more

218
00:12:52,869 --> 00:12:58,298
interestingly is I remember when we

219
00:12:55,778 --> 00:13:01,389
talked about like last week what's the

220
00:12:58,298 --> 00:13:03,428
simplest non stupid model you could come

221
00:13:01,389 --> 00:13:05,859
up with and I think the simplest non

222
00:13:03,428 --> 00:13:08,318
stupid model we came up with was create

223
00:13:05,859 --> 00:13:09,759
a column of the mean just copy the mean

224
00:13:08,318 --> 00:13:13,868
a bunch of times and submit that to

225
00:13:09,759 --> 00:13:19,239
cable if you did that then your root

226
00:13:13,869 --> 00:13:22,298
mean squared error would be this so this

227
00:13:19,239 --> 00:13:26,709
is the root mean squared error of the

228
00:13:22,298 --> 00:13:32,348
most naive non stupid model where the

229
00:13:26,708 --> 00:13:37,028
model is just predict mean on the top we

230
00:13:32,349 --> 00:13:38,908
have SS res which is here which is that

231
00:13:37,028 --> 00:13:41,908
we're now going to add a column of

232
00:13:38,908 --> 00:13:41,908
predictions

233
00:13:48,240 --> 00:13:55,870
okay and so then what we do is rather

234
00:13:51,730 --> 00:14:01,870
than taking the why I - why mean we're

235
00:13:55,870 --> 00:14:04,210
going to take why I - Fi right and so

236
00:14:01,870 --> 00:14:06,429
now instead of saying what's the root

237
00:14:04,210 --> 00:14:07,690
mean squared error of our naive model

238
00:14:06,429 --> 00:14:09,250
we're saying what's the root mean

239
00:14:07,690 --> 00:14:13,089
squared error of the actual model that

240
00:14:09,250 --> 00:14:14,909
we're interested in and then we take the

241
00:14:13,089 --> 00:14:21,270
ratio

242
00:14:14,909 --> 00:14:24,779
so in other words if we actually were

243
00:14:21,269 --> 00:14:27,539
exactly as effective as just predicting

244
00:14:24,779 --> 00:14:29,490
the mean then this would be top and

245
00:14:27,539 --> 00:14:34,409
bottom would be the same that would be 1

246
00:14:29,490 --> 00:14:38,159
1 minus 1 will be 0 if we were perfect

247
00:14:34,409 --> 00:14:40,199
so fi minus y I was always zero then

248
00:14:38,159 --> 00:14:46,860
that's zero divided by something one

249
00:14:40,200 --> 00:14:52,160
minus that is 1 ok so what is the

250
00:14:46,860 --> 00:14:52,159
possible range of values of R squared

251
00:14:52,759 --> 00:14:58,370
okay I heard a lot of 0 to 1 does

252
00:14:55,230 --> 00:14:58,370
anybody want to give me an alternative

253
00:14:58,669 --> 00:15:02,000
negative 1 to 1

254
00:15:05,429 --> 00:15:09,189
anything less than one there's the right

255
00:15:07,450 --> 00:15:11,340
answer let's find out way through the

256
00:15:09,190 --> 00:15:11,340
boss

257
00:15:22,980 --> 00:15:25,930
okay

258
00:15:24,100 --> 00:15:27,820
so why is it any number less than one

259
00:15:25,929 --> 00:15:29,859
which you can make a model basically as

260
00:15:27,820 --> 00:15:31,210
crappy as you want and just I guess like

261
00:15:29,860 --> 00:15:33,070
the arrows as you want you're just

262
00:15:31,210 --> 00:15:37,990
subtracting from one in the formula

263
00:15:33,070 --> 00:15:40,570
exactly so interestingly I was talking

264
00:15:37,990 --> 00:15:42,430
to our computer science professor

265
00:15:40,570 --> 00:15:44,710
Terrence this morning who was talking to

266
00:15:42,429 --> 00:15:46,479
where a statistics professor told him

267
00:15:44,710 --> 00:15:48,460
that the possible range of values was

268
00:15:46,480 --> 00:15:51,190
asked where it was zero to one I said

269
00:15:48,460 --> 00:15:54,009
that is totally not true if you predict

270
00:15:51,190 --> 00:15:56,220
infinity for every column sorry for

271
00:15:54,009 --> 00:15:58,809
every row then you're going to have

272
00:15:56,220 --> 00:16:01,240
infinity for every residue and so you're

273
00:15:58,809 --> 00:16:03,419
going to have one minus infinity okay so

274
00:16:01,240 --> 00:16:06,730
the possible range of values is less

275
00:16:03,419 --> 00:16:08,829
than one that's all we know and this

276
00:16:06,730 --> 00:16:10,810
will happen you will get negative values

277
00:16:08,830 --> 00:16:13,450
sometimes in your r-squared and when

278
00:16:10,809 --> 00:16:16,299
that happens it's not a mistake it's not

279
00:16:13,450 --> 00:16:19,120
it's not like a bug it means your moral

280
00:16:16,299 --> 00:16:23,049
is worse than predicting the main okay

281
00:16:19,120 --> 00:16:29,250
which is suggest it's not great so

282
00:16:23,049 --> 00:16:29,250
that's R squared it's not

283
00:16:29,649 --> 00:16:35,259
it's not necessarily what you're

284
00:16:32,619 --> 00:16:37,509
actually trying to optimize right but

285
00:16:35,259 --> 00:16:39,730
it's it's it's the nice thing about it

286
00:16:37,509 --> 00:16:43,509
is that it's a number that you can use

287
00:16:39,730 --> 00:16:45,369
kind of for every model and so you can

288
00:16:43,509 --> 00:16:46,839
kind of start try to get a feel of like

289
00:16:45,369 --> 00:16:48,699
what does point-eight look like what

290
00:16:46,839 --> 00:16:51,249
does point-9 look like so like something

291
00:16:48,698 --> 00:16:54,248
I find interesting is to like create

292
00:16:51,249 --> 00:16:56,860
some different synthetic data sets just

293
00:16:54,249 --> 00:16:59,050
to two dimensions with kind of different

294
00:16:56,860 --> 00:17:00,818
amounts of random noise and like see

295
00:16:59,049 --> 00:17:02,649
what they look like on a scatterplot and

296
00:17:00,818 --> 00:17:05,259
see what they are squared are just gonna

297
00:17:02,649 --> 00:17:06,459
get a feel for like what does it ask for

298
00:17:05,259 --> 00:17:11,398
it you know is it a spur to point line

299
00:17:06,459 --> 00:17:11,399
close or not about 0.7 closed or not

300
00:17:11,759 --> 00:17:17,828
okay so I think r-squared is a useful

301
00:17:15,369 --> 00:17:20,759
number to have a familiarity with and

302
00:17:17,828 --> 00:17:23,859
you don't need to remember the formula

303
00:17:20,759 --> 00:17:26,078
if you remember the meaning which is

304
00:17:23,859 --> 00:17:28,389
what's the ratio between how good your

305
00:17:26,078 --> 00:17:30,759
model is it means bit error versus how

306
00:17:28,390 --> 00:17:34,899
good is the naive mean model for it

307
00:17:30,759 --> 00:17:38,769
squared error okay and LK is 0.98 it's

308
00:17:34,898 --> 00:17:40,389
saying it's a very good model however it

309
00:17:38,769 --> 00:17:43,148
might be a very good model because it

310
00:17:40,390 --> 00:17:46,120
looks like this all right this would be

311
00:17:43,148 --> 00:17:48,250
called overfitting so we may well have

312
00:17:46,119 --> 00:17:49,839
created a model which is very good at

313
00:17:48,250 --> 00:17:51,759
running through the points that we gave

314
00:17:49,839 --> 00:17:53,918
it but it's not going to be very good at

315
00:17:51,759 --> 00:17:56,589
running through points that we didn't

316
00:17:53,919 --> 00:18:01,809
give it so that's why we always want to

317
00:17:56,589 --> 00:18:05,230
have a validation set creating your

318
00:18:01,808 --> 00:18:08,980
validation set is the most important

319
00:18:05,230 --> 00:18:11,640
thing that I think you need to do when

320
00:18:08,980 --> 00:18:14,288
you're doing a machine learning project

321
00:18:11,640 --> 00:18:20,020
at least in terms of in the actual

322
00:18:14,288 --> 00:18:23,470
modeling because what you need to do is

323
00:18:20,019 --> 00:18:25,569
come up with a data set where the

324
00:18:23,470 --> 00:18:27,940
scroller of your model on that data set

325
00:18:25,569 --> 00:18:30,788
is going to be representative of how

326
00:18:27,940 --> 00:18:33,009
well your model is going to do in the

327
00:18:30,788 --> 00:18:35,259
real world like in cattle on the

328
00:18:33,009 --> 00:18:37,869
leaderboard or off Kaggle like when you

329
00:18:35,259 --> 00:18:43,058
actually use it in production

330
00:18:37,869 --> 00:18:46,029
I very very very often hear people in

331
00:18:43,058 --> 00:18:48,849
industries they I don't trust machine

332
00:18:46,029 --> 00:18:50,980
loading I tried modeling once it looked

333
00:18:48,849 --> 00:18:54,699
great we put it in production it didn't

334
00:18:50,980 --> 00:18:57,099
work now whose fault is that right that

335
00:18:54,700 --> 00:19:00,370
means their validation set was not

336
00:18:57,099 --> 00:19:02,379
representative right so here's a very

337
00:19:00,369 --> 00:19:04,209
simple thing which generally speaking

338
00:19:02,380 --> 00:19:08,890
cattle is pretty good about doing if

339
00:19:04,210 --> 00:19:11,230
your data has a time piece in it right

340
00:19:08,890 --> 00:19:13,179
as happens in Blue Book for bulldozers

341
00:19:11,230 --> 00:19:16,420
in Blue Book for bulldozers we're

342
00:19:13,179 --> 00:19:18,190
talking about the sale price of a piece

343
00:19:16,420 --> 00:19:21,759
of industrial equipment on a particular

344
00:19:18,190 --> 00:19:23,590
date so the start up during this

345
00:19:21,759 --> 00:19:26,019
competition wanted to create a model

346
00:19:23,589 --> 00:19:28,509
that wouldn't predict last February's

347
00:19:26,019 --> 00:19:31,389
prices that would predict next month's

348
00:19:28,509 --> 00:19:33,460
prices so what they did was they gave us

349
00:19:31,390 --> 00:19:35,620
data representing a particular date

350
00:19:33,460 --> 00:19:38,710
range in the training set and then the

351
00:19:35,619 --> 00:19:40,629
test set represented a future set of

352
00:19:38,710 --> 00:19:43,029
dates that wasn't represented in the

353
00:19:40,630 --> 00:19:45,070
training set right so that's pretty good

354
00:19:43,029 --> 00:19:47,500
right that means that if we're doing

355
00:19:45,069 --> 00:19:50,500
well on this model we've built something

356
00:19:47,500 --> 00:19:51,630
which can actually predict the future or

357
00:19:50,500 --> 00:19:53,769
at least it could predict the future

358
00:19:51,630 --> 00:19:57,370
then assuming things haven't changed

359
00:19:53,769 --> 00:19:59,230
dramatically so that's the test set we

360
00:19:57,369 --> 00:20:02,678
have so we need to create a validation

361
00:19:59,230 --> 00:20:05,589
set that has the same properties so the

362
00:20:02,679 --> 00:20:07,660
test set had 12,000 rows in so let's

363
00:20:05,589 --> 00:20:11,019
create a validation set that has 12,000

364
00:20:07,660 --> 00:20:16,390
rows right and then let's split the data

365
00:20:11,019 --> 00:20:20,129
set into the first n minus 12 thousand

366
00:20:16,390 --> 00:20:22,660
rows for the training set and the last

367
00:20:20,130 --> 00:20:24,520
12,000 rows for the validation set and

368
00:20:22,660 --> 00:20:28,720
so we've now got something which

369
00:20:24,519 --> 00:20:31,269
hopefully looks like cackles test set

370
00:20:28,720 --> 00:20:33,308
plus enough that when we actually try

371
00:20:31,269 --> 00:20:36,190
and use this validation set we're going

372
00:20:33,308 --> 00:20:39,129
to get some reasonably accurate scores

373
00:20:36,190 --> 00:20:41,558
and the reason we want this is because

374
00:20:39,130 --> 00:20:43,690
on cattle you can only submit so many

375
00:20:41,558 --> 00:20:45,460
times and if you submit too often you'll

376
00:20:43,690 --> 00:20:47,590
end up fitting to the leaderboard anyway

377
00:20:45,460 --> 00:20:49,480
and in real life you actually want to

378
00:20:47,589 --> 00:20:51,668
build a model that's going to work in

379
00:20:49,480 --> 00:21:00,400
why did you have a question can we help

380
00:20:51,669 --> 00:21:02,200
the green box can you explain the

381
00:21:00,400 --> 00:21:05,230
difference between a validation set and

382
00:21:02,200 --> 00:21:07,419
a test set absolutely so what we're

383
00:21:05,230 --> 00:21:08,620
going to learn today is how to set what

384
00:21:07,419 --> 00:21:10,299
other things alone it's how to set high

385
00:21:08,619 --> 00:21:12,099
parameters hyper parameters are like

386
00:21:10,298 --> 00:21:15,369
tuning parameters that are going to

387
00:21:12,099 --> 00:21:18,189
change how your model behaves now if you

388
00:21:15,369 --> 00:21:19,959
just have one holdout set so one set of

389
00:21:18,190 --> 00:21:22,058
data that you're not using to train with

390
00:21:19,960 --> 00:21:24,519
and we use that to decide which set of

391
00:21:22,058 --> 00:21:25,869
hyper parameters to use if we try a

392
00:21:24,519 --> 00:21:29,230
thousand different sets of hyper

393
00:21:25,869 --> 00:21:30,969
parameters we may end up overfitting to

394
00:21:29,230 --> 00:21:33,308
that holdout set that is to say well

395
00:21:30,970 --> 00:21:35,980
find something which only accidentally

396
00:21:33,308 --> 00:21:38,500
worked so what we actually want to do is

397
00:21:35,980 --> 00:21:41,589
we want to have a second holdout set

398
00:21:38,500 --> 00:21:45,190
where we can say okay I'm finished

399
00:21:41,589 --> 00:21:47,949
okay I've done the best I can and now

400
00:21:45,190 --> 00:21:52,120
just once right at the end I'm gonna see

401
00:21:47,950 --> 00:21:54,789
whether it works and so this is

402
00:21:52,119 --> 00:21:59,409
something which almost nobody in

403
00:21:54,789 --> 00:22:01,869
industry does correctly you really

404
00:21:59,410 --> 00:22:03,460
actually need to remove that holdout set

405
00:22:01,869 --> 00:22:05,500
that's called the test set remove it

406
00:22:03,460 --> 00:22:08,380
from the data give it to somebody else

407
00:22:05,500 --> 00:22:10,269
and tell them do not let me look at this

408
00:22:08,380 --> 00:22:12,640
data until I promise you I'm finished

409
00:22:10,269 --> 00:22:14,440
like it's so hard otherwise not to look

410
00:22:12,640 --> 00:22:16,600
at it and for example in the world of

411
00:22:14,440 --> 00:22:18,970
psychology and sociology you might have

412
00:22:16,599 --> 00:22:20,889
heard about this replication crisis this

413
00:22:18,970 --> 00:22:22,870
is basically because people in these

414
00:22:20,890 --> 00:22:25,480
fields have accidentally or

415
00:22:22,869 --> 00:22:27,209
intentionally maybe been peed hacking

416
00:22:25,480 --> 00:22:29,740
which means they've been basically

417
00:22:27,210 --> 00:22:31,539
trying lots of different variations

418
00:22:29,740 --> 00:22:33,130
until they find something that works and

419
00:22:31,539 --> 00:22:34,869
then it turns out and they try to

420
00:22:33,130 --> 00:22:36,700
replicate it in other words it's like

421
00:22:34,869 --> 00:22:39,219
somebody creates a test set somebody

422
00:22:36,700 --> 00:22:40,480
says okay this study which shows you

423
00:22:39,220 --> 00:22:42,789
know the impact of whether you eat

424
00:22:40,480 --> 00:22:46,539
marshmallows on your tenacity later in

425
00:22:42,789 --> 00:22:48,609
life I'm going to rerun it and like over

426
00:22:46,539 --> 00:22:50,798
half the time they're finding the effect

427
00:22:48,609 --> 00:22:52,699
turns out not to exist so that's why we

428
00:22:50,798 --> 00:22:57,240
want to have a test set

429
00:22:52,700 --> 00:22:59,789
get that next door yeah so for handling

430
00:22:57,240 --> 00:23:02,579
categorical data you can market those to

431
00:22:59,789 --> 00:23:05,639
numerix two numbers order numbers I've

432
00:23:02,579 --> 00:23:08,099
seen a lot of models where we convert

433
00:23:05,640 --> 00:23:10,169
categorical data into different columns

434
00:23:08,099 --> 00:23:12,990
using one for encoding yes

435
00:23:10,169 --> 00:23:14,429
so which approach to use in which model

436
00:23:12,990 --> 00:23:19,319
yeah we're kind of track for that today

437
00:23:14,429 --> 00:23:24,780
yeah it's a great question okay so so

438
00:23:19,319 --> 00:23:26,909
I'm splitting my my data into validation

439
00:23:24,779 --> 00:23:29,279
and training sets and so you can see now

440
00:23:26,910 --> 00:23:32,880
that my validation set is twelve

441
00:23:29,279 --> 00:23:33,960
thousand about 66 where else my training

442
00:23:32,880 --> 00:23:37,020
set is three hundred ninety nine

443
00:23:33,960 --> 00:23:39,720
thousand sixty-six okay so we're going

444
00:23:37,019 --> 00:23:41,160
to use this set of data to train a model

445
00:23:39,720 --> 00:23:44,789
and this set of data to see how well

446
00:23:41,160 --> 00:23:48,419
it's working so when we then tried that

447
00:23:44,789 --> 00:23:50,668
last week we found out just a moment we

448
00:23:48,419 --> 00:23:52,200
found out that our model which had point

449
00:23:50,669 --> 00:23:55,320
nine eight two R squared on the training

450
00:23:52,200 --> 00:23:57,750
set only had point eight eight seven on

451
00:23:55,319 --> 00:24:00,808
the validation set which makes us think

452
00:23:57,750 --> 00:24:02,220
that we're overfitting quite badly but

453
00:24:00,808 --> 00:24:04,440
it turned out it wasn't too badly

454
00:24:02,220 --> 00:24:06,929
because the root mean squared error on

455
00:24:04,440 --> 00:24:09,029
the logs of the prices actually would

456
00:24:06,929 --> 00:24:10,740
have caught us in the top 25 percent of

457
00:24:09,029 --> 00:24:12,928
the competition anyway so even although

458
00:24:10,740 --> 00:24:14,460
we're overfitting it wasn't the end of

459
00:24:12,929 --> 00:24:20,250
the world could you pass the microphone

460
00:24:14,460 --> 00:24:23,179
to Marsha please ah in terms of you

461
00:24:20,250 --> 00:24:26,609
dividing the set into training and

462
00:24:23,179 --> 00:24:29,309
validation it seems like you simply take

463
00:24:26,609 --> 00:24:32,490
the first and train observations of the

464
00:24:29,308 --> 00:24:35,428
data status and set them aside why don't

465
00:24:32,490 --> 00:24:38,789
you like why don't you randomly pick up

466
00:24:35,429 --> 00:24:41,460
the observations because if I did that I

467
00:24:38,789 --> 00:24:43,980
wouldn't be replicating the test set so

468
00:24:41,460 --> 00:24:45,269
cattle has a test set that when you

469
00:24:43,980 --> 00:24:48,720
actually look at the dates in the test

470
00:24:45,269 --> 00:24:51,119
set they are a set of dates that are

471
00:24:48,720 --> 00:24:53,910
more recent than any date in the

472
00:24:51,119 --> 00:24:56,069
training set so if we used a validation

473
00:24:53,910 --> 00:24:58,290
set that was a random sample that is

474
00:24:56,069 --> 00:25:00,869
much easier because we're predicting

475
00:24:58,289 --> 00:25:02,428
options like what's the value of this

476
00:25:00,869 --> 00:25:03,599
piece of industrial equipment on this

477
00:25:02,429 --> 00:25:05,548
day when we add

478
00:25:03,599 --> 00:25:09,959
we already have some observations from

479
00:25:05,548 --> 00:25:12,298
that day so in general anytime you're

480
00:25:09,960 --> 00:25:16,200
building a model that has a time element

481
00:25:12,298 --> 00:25:18,298
you want your test set to be a separate

482
00:25:16,200 --> 00:25:19,769
time period and therefore you really

483
00:25:18,298 --> 00:25:21,538
need your validation set to be observer

484
00:25:19,769 --> 00:25:23,220
time period as well and in this case the

485
00:25:21,538 --> 00:25:33,269
data was already sorted that's why this

486
00:25:23,220 --> 00:25:35,160
works so let's say we have our test the

487
00:25:33,269 --> 00:25:37,589
training set by which we in the data and

488
00:25:35,160 --> 00:25:39,538
then we have the validation set against

489
00:25:37,589 --> 00:25:42,720
which we are trying to find the r-square

490
00:25:39,538 --> 00:25:45,839
in in case of r-squared turns out to be

491
00:25:42,720 --> 00:25:48,058
really bad we would want to cheer to not

492
00:25:45,839 --> 00:25:50,668
parameters and run it again yes so

493
00:25:48,058 --> 00:25:53,460
wouldn't that be eventually overfitting

494
00:25:50,669 --> 00:25:55,140
on the overall training set yeah so

495
00:25:53,460 --> 00:25:57,150
actually that's that's the issue so that

496
00:25:55,140 --> 00:25:59,038
would eventually have the possibility of

497
00:25:57,150 --> 00:26:01,200
overfitting on the validation set and

498
00:25:59,038 --> 00:26:03,269
then when you try it on the test set or

499
00:26:01,200 --> 00:26:05,490
we submit it to kaggle it turns out not

500
00:26:03,269 --> 00:26:06,990
to be very good and this happens in

501
00:26:05,490 --> 00:26:09,538
careful competitions all the time

502
00:26:06,990 --> 00:26:11,779
careful actually has a fourth data set

503
00:26:09,538 --> 00:26:14,970
which is called the private leader board

504
00:26:11,779 --> 00:26:17,369
set and every time you submit to Kaggle

505
00:26:14,970 --> 00:26:18,990
you actually only get feedback on how

506
00:26:17,369 --> 00:26:21,298
well it does on something called the

507
00:26:18,990 --> 00:26:23,130
public leader board set and you don't

508
00:26:21,298 --> 00:26:24,690
know which rows they are and at the end

509
00:26:23,130 --> 00:26:26,549
of the competition you actually get

510
00:26:24,690 --> 00:26:29,029
judged on a different data set entirely

511
00:26:26,548 --> 00:26:32,009
called the private leader board set so

512
00:26:29,029 --> 00:26:34,379
the only way to avoid this is to

513
00:26:32,009 --> 00:26:37,019
actually be a good machine learning

514
00:26:34,380 --> 00:26:38,990
practitioner and know how to set these

515
00:26:37,019 --> 00:26:41,099
parameters as effectively as possible

516
00:26:38,990 --> 00:26:45,019
which we're going to be doing gladly

517
00:26:41,099 --> 00:26:45,019
today and over the next few weeks

518
00:26:45,720 --> 00:26:49,069
can you get that actually watching

519
00:26:53,259 --> 00:26:58,629
is it too early or late to ask what's

520
00:26:57,069 --> 00:27:12,849
the difference between a paper parameter

521
00:26:58,630 --> 00:27:14,410
and a parameter okay so let's start

522
00:27:12,849 --> 00:27:18,009
tracking things on written in spread era

523
00:27:14,410 --> 00:27:19,900
so here is root mean square error in a

524
00:27:18,009 --> 00:27:22,599
line of code and you can see here like

525
00:27:19,900 --> 00:27:25,180
this is one of these examples where I'm

526
00:27:22,599 --> 00:27:26,799
not writing this the way a proper

527
00:27:25,180 --> 00:27:28,180
software engineer would write this right

528
00:27:26,799 --> 00:27:29,829
so a proper software engineer would be a

529
00:27:28,180 --> 00:27:32,140
number of things differently they would

530
00:27:29,829 --> 00:27:40,720
have it on a different line they would

531
00:27:32,140 --> 00:27:44,610
use longer variable names they would

532
00:27:40,720 --> 00:27:47,610
have documentation blah blah blah right

533
00:27:44,609 --> 00:27:47,609
but

534
00:27:47,819 --> 00:27:57,119
I really think like for me I really

535
00:27:50,640 --> 00:27:59,400
think that being able to look at

536
00:27:57,119 --> 00:28:02,549
something in one go with your eyes and

537
00:27:59,400 --> 00:28:05,759
like over time learn to immediately see

538
00:28:02,549 --> 00:28:09,319
what's going on has a lot of value and

539
00:28:05,759 --> 00:28:11,190
also to like consistently use like

540
00:28:09,319 --> 00:28:13,529
particular letters to have many

541
00:28:11,190 --> 00:28:16,940
particular things or abbreviations I

542
00:28:13,529 --> 00:28:21,539
think works really well in data science

543
00:28:16,940 --> 00:28:25,710
if you're doing it like a take-home

544
00:28:21,539 --> 00:28:28,289
interview test or something you should

545
00:28:25,710 --> 00:28:32,880
write your code according to pet eight

546
00:28:28,289 --> 00:28:34,740
standards right so pep eight is the the

547
00:28:32,880 --> 00:28:36,390
style guide for python code and you

548
00:28:34,740 --> 00:28:38,339
should know it and use it because a lot

549
00:28:36,390 --> 00:28:41,490
of software engineers are super anal

550
00:28:38,339 --> 00:28:46,289
about this kind of thing but for your

551
00:28:41,490 --> 00:28:48,539
own work you know I think this is I

552
00:28:46,289 --> 00:28:50,339
think this works well for me you know so

553
00:28:48,539 --> 00:28:52,319
I just wanted to make you aware a that

554
00:28:50,339 --> 00:28:54,899
you shouldn't necessarily use this as a

555
00:28:52,319 --> 00:28:57,179
role model for dealing with software

556
00:28:54,900 --> 00:28:59,430
engineers but B that I actually think

557
00:28:57,180 --> 00:29:01,529
this is not this is a reasonable

558
00:28:59,430 --> 00:29:03,180
approach okay so there's a root mean

559
00:29:01,529 --> 00:29:04,649
squared error and then from time to time

560
00:29:03,180 --> 00:29:07,529
we're just going to print out the score

561
00:29:04,650 --> 00:29:09,090
which will give us the RMS C of the

562
00:29:07,529 --> 00:29:10,950
predictions on the training versus the

563
00:29:09,089 --> 00:29:13,559
actual there are predictions on the

564
00:29:10,950 --> 00:29:14,940
valid versus the actual RMS see the R

565
00:29:13,559 --> 00:29:16,769
squared for the training and the R

566
00:29:14,940 --> 00:29:19,110
squared of the Bell and we'll come back

567
00:29:16,769 --> 00:29:22,289
to over in a moment so when we ran that

568
00:29:19,109 --> 00:29:24,359
we found that this rmse was in the top

569
00:29:22,289 --> 00:29:31,170
25% and it's like okay there's a good

570
00:29:24,359 --> 00:29:33,629
start now this took eight seconds of

571
00:29:31,170 --> 00:29:35,850
wall time so eight actual seconds if you

572
00:29:33,630 --> 00:29:38,850
put percent time it'll tell you how long

573
00:29:35,849 --> 00:29:40,799
things talk and luckily I've got quite a

574
00:29:38,849 --> 00:29:43,139
few calls quite a few CPUs in this

575
00:29:40,799 --> 00:29:45,869
computer because it actually took over a

576
00:29:43,140 --> 00:29:50,100
minute a compute time so I paralyzed

577
00:29:45,869 --> 00:29:53,279
that across cause if your dataset was

578
00:29:50,099 --> 00:29:55,559
bigger or you had less cause you know

579
00:29:53,279 --> 00:29:57,329
you could well find that this took a few

580
00:29:55,559 --> 00:30:00,509
minutes to run or even a few hours and

581
00:29:57,329 --> 00:30:01,099
my rule of thumb is that if something

582
00:30:00,509 --> 00:30:05,660
takes more

583
00:30:01,099 --> 00:30:09,230
10 seconds to run it's too long for me

584
00:30:05,660 --> 00:30:10,548
to do like interactive analysis with it

585
00:30:09,230 --> 00:30:12,740
right I want to be able to like run

586
00:30:10,548 --> 00:30:17,359
something wait a moment

587
00:30:12,740 --> 00:30:19,819
and then continue so what we do is we

588
00:30:17,359 --> 00:30:22,519
try to make sure that things can run in

589
00:30:19,819 --> 00:30:24,439
a reasonable time and then when we're

590
00:30:22,519 --> 00:30:27,048
when we're finished at the end of the

591
00:30:24,440 --> 00:30:28,640
day we can then say ok this feature

592
00:30:27,048 --> 00:30:30,259
engineering these hyper parameters

593
00:30:28,640 --> 00:30:33,620
whatever these are all working well and

594
00:30:30,259 --> 00:30:37,158
I'll now rerun it you know that's the

595
00:30:33,619 --> 00:30:39,558
big slow precise way so one way to speed

596
00:30:37,159 --> 00:30:42,620
things up is to pass in the subset

597
00:30:39,558 --> 00:30:46,819
parameter to proc DF and that will

598
00:30:42,619 --> 00:30:48,949
randomly sample my data right and so

599
00:30:46,819 --> 00:30:53,869
here I'm got a randomly sample 30,000

600
00:30:48,950 --> 00:30:55,220
rows now when I do that I still need to

601
00:30:53,869 --> 00:30:58,879
be careful to make sure that my

602
00:30:55,220 --> 00:31:01,220
validation set doesn't change and that

603
00:30:58,880 --> 00:31:03,020
my training set doesn't overlap with the

604
00:31:01,220 --> 00:31:06,048
dates otherwise I'm cheating

605
00:31:03,019 --> 00:31:10,369
so I call split bells again to again do

606
00:31:06,048 --> 00:31:12,918
this split by dates and you'll also see

607
00:31:10,369 --> 00:31:14,298
I'm using rather than putting it into a

608
00:31:12,919 --> 00:31:16,340
validation set I'm putting it into a

609
00:31:14,298 --> 00:31:18,650
variable called underscore this is kind

610
00:31:16,339 --> 00:31:20,119
of a standard approach in Python is to

611
00:31:18,650 --> 00:31:22,009
use a variable called underscore if you

612
00:31:20,119 --> 00:31:23,599
want to throw something away because I

613
00:31:22,009 --> 00:31:25,940
don't want to change my validation set

614
00:31:23,599 --> 00:31:27,529
like no matter what different models I

615
00:31:25,940 --> 00:31:29,360
build I want to be able to compare them

616
00:31:27,529 --> 00:31:30,859
all to each other so I want to keep my

617
00:31:29,359 --> 00:31:32,808
validation set the same all the time

618
00:31:30,859 --> 00:31:36,769
okay so all I'm doing here is I'm

619
00:31:32,808 --> 00:31:41,178
resampling my training set into a 20 the

620
00:31:36,769 --> 00:31:44,929
first 20,000 out of a 30,000 subsets so

621
00:31:41,179 --> 00:31:47,240
I now could run that and it runs it 621

622
00:31:44,929 --> 00:31:49,850
milliseconds so I can like really zip

623
00:31:47,240 --> 00:31:55,329
through things now try things out okay

624
00:31:49,849 --> 00:31:59,240
so with that let's use this subset to

625
00:31:55,329 --> 00:32:01,339
build a model that is so simple that we

626
00:31:59,240 --> 00:32:04,460
can actually take a look at it and so

627
00:32:01,339 --> 00:32:07,129
we're going to build a forest is made of

628
00:32:04,460 --> 00:32:09,950
trees and so before we look at the

629
00:32:07,130 --> 00:32:12,409
forest we look at the trees in

630
00:32:09,950 --> 00:32:14,080
scikit-learn they don't call them trees

631
00:32:12,409 --> 00:32:15,610
they call them estimators

632
00:32:14,079 --> 00:32:18,699
so we're going to pass in the parameter

633
00:32:15,609 --> 00:32:21,329
number of estimators equals one the

634
00:32:18,700 --> 00:32:23,650
Creator forest with just one tree and

635
00:32:21,329 --> 00:32:26,230
then we're going to make a small tree

636
00:32:23,650 --> 00:32:28,870
so we pass in maximum depth equals three

637
00:32:26,230 --> 00:32:31,019
and a random forest as we're going to

638
00:32:28,869 --> 00:32:34,029
learn randomizes a whole bunch of things

639
00:32:31,019 --> 00:32:36,369
we want to turn that off so to turn that

640
00:32:34,029 --> 00:32:38,349
off you say bootstrap equals false so if

641
00:32:36,369 --> 00:32:43,059
I pass you in these parameters that

642
00:32:38,349 --> 00:32:46,509
creates a small deterministic tree okay

643
00:32:43,059 --> 00:32:48,789
so if I fit it and say print score my a

644
00:32:46,509 --> 00:32:51,369
squared has gone down from point eight

645
00:32:48,789 --> 00:32:54,009
five two point four so this is not a

646
00:32:51,369 --> 00:32:55,899
good model it's better than the mean

647
00:32:54,009 --> 00:32:58,059
model this is better than zero right

648
00:32:55,900 --> 00:33:02,140
it's not a good model but it's a model

649
00:32:58,059 --> 00:33:07,259
that we can draw all right so let's

650
00:33:02,140 --> 00:33:10,270
learn about what it's built so a tree

651
00:33:07,259 --> 00:33:15,250
consists of a sequence of binary

652
00:33:10,269 --> 00:33:18,329
decisions of binary splits so at first

653
00:33:15,250 --> 00:33:20,319
of all decided to split on coupla system

654
00:33:18,329 --> 00:33:22,119
greater than or less than point five

655
00:33:20,319 --> 00:33:25,509
that's boolean variable so it's actually

656
00:33:22,119 --> 00:33:27,250
true or false and then within the group

657
00:33:25,509 --> 00:33:29,529
we're a couple system was true we

658
00:33:27,250 --> 00:33:32,109
decided to split into year made greater

659
00:33:29,529 --> 00:33:34,329
than or less than 1987 and then we're a

660
00:33:32,109 --> 00:33:36,369
couple system was true and year made was

661
00:33:34,329 --> 00:33:39,699
less than or equal to 1986

662
00:33:36,369 --> 00:33:43,389
it used fi product class desk is less

663
00:33:39,700 --> 00:33:47,350
than or equal to 0.75 and so forth right

664
00:33:43,390 --> 00:33:50,920
so right at the top we have 20,000

665
00:33:47,349 --> 00:33:54,099
samples 20,000 rows right and the reason

666
00:33:50,920 --> 00:33:56,230
for that is because that's what we asked

667
00:33:54,099 --> 00:33:58,619
for here when we split our data in the

668
00:33:56,230 --> 00:33:58,620
sample

669
00:34:02,460 --> 00:34:06,750
I just want to double-check that for

670
00:34:04,769 --> 00:34:08,489
your decision tree that you had there

671
00:34:06,750 --> 00:34:11,219
that the coloration was whether it's

672
00:34:08,489 --> 00:34:14,339
true or false not so like it it gets

673
00:34:11,219 --> 00:34:17,309
darker it's true for the next one not

674
00:34:14,340 --> 00:34:18,389
the darker is a higher value we'll get

675
00:34:17,309 --> 00:34:21,329
to that in a moment okay

676
00:34:18,389 --> 00:34:22,500
so let's look at these numbers here so

677
00:34:21,329 --> 00:34:25,409
in the whole data set

678
00:34:22,500 --> 00:34:26,840
well our sample that we're using there

679
00:34:25,409 --> 00:34:30,690
are 20,000 rows

680
00:34:26,840 --> 00:34:34,460
the meter the average of the log of

681
00:34:30,690 --> 00:34:36,960
Christ is 10.1 and if we built a model

682
00:34:34,460 --> 00:34:39,300
where we just used that average all the

683
00:34:36,960 --> 00:34:44,909
time then the mean squared error would

684
00:34:39,300 --> 00:34:49,050
be 0.477 okay so this is in other words

685
00:34:44,909 --> 00:34:50,579
the denominator of an R squared all

686
00:34:49,050 --> 00:34:53,700
right this is like the most basic model

687
00:34:50,579 --> 00:34:57,900
is a tree with zero splits right which

688
00:34:53,699 --> 00:35:02,039
is just predict the average so the best

689
00:34:57,900 --> 00:35:03,750
single binary split we can make it turns

690
00:35:02,039 --> 00:35:06,449
out to be splitting by where the coupler

691
00:35:03,750 --> 00:35:07,440
system is greater than or equal to sorry

692
00:35:06,449 --> 00:35:09,529
less than or equal to or greater than

693
00:35:07,440 --> 00:35:12,840
0.5 know whether it's true or false and

694
00:35:09,530 --> 00:35:15,900
turns out if we do that the mean squared

695
00:35:12,840 --> 00:35:20,100
error of couple system is less than 0.5

696
00:35:15,900 --> 00:35:23,340
so it's false goes down from 0.477 to

697
00:35:20,099 --> 00:35:26,429
0.1 one right so it's really improved

698
00:35:23,340 --> 00:35:28,650
the error a lot in the other group it's

699
00:35:26,429 --> 00:35:30,598
only improved at a bit so I'm from 0.47

700
00:35:28,650 --> 00:35:33,479
2.41

701
00:35:30,599 --> 00:35:35,910
and so we can see that the coupler

702
00:35:33,478 --> 00:35:38,338
system equals false group has a pretty

703
00:35:35,909 --> 00:35:41,429
small percentage it's only got twenty

704
00:35:38,338 --> 00:35:43,650
two hundred of the twenty thousand all

705
00:35:41,429 --> 00:35:45,629
right where else this other group has a

706
00:35:43,650 --> 00:35:50,489
much larger percentage but it hasn't

707
00:35:45,630 --> 00:35:54,390
improved it as much so let's say you

708
00:35:50,489 --> 00:35:56,068
wanted to create a tree with just one

709
00:35:54,389 --> 00:36:01,288
split so you're just trying to find like

710
00:35:56,068 --> 00:36:05,400
what is the very best single binary

711
00:36:01,289 --> 00:36:06,930
decision you can make for your data how

712
00:36:05,400 --> 00:36:09,259
might you be able to do that how could

713
00:36:06,929 --> 00:36:09,259
you do it

714
00:36:09,539 --> 00:36:13,699
I'm gonna give it to foot

715
00:36:13,750 --> 00:36:18,699
specify the max depth of one but I mean

716
00:36:17,019 --> 00:36:21,789
your writing you don't have a random

717
00:36:18,699 --> 00:36:23,199
first write how are you going to how are

718
00:36:21,789 --> 00:36:25,090
you going to like write what's an

719
00:36:23,199 --> 00:36:29,259
algorithm a simple algorithm which you

720
00:36:25,090 --> 00:36:32,140
could use sure so we want to start

721
00:36:29,260 --> 00:36:34,870
building a random forest from scratch so

722
00:36:32,139 --> 00:36:37,000
the first step is to create a tree the

723
00:36:34,869 --> 00:36:40,119
first step to create a tree is to create

724
00:36:37,000 --> 00:36:42,210
the first binary decision how are you

725
00:36:40,119 --> 00:36:45,659
gonna do it I'm gonna give it to Chris

726
00:36:42,210 --> 00:36:45,659
maybe in two steps

727
00:36:48,820 --> 00:36:54,530
so isn't this simply trying to find the

728
00:36:52,429 --> 00:36:57,679
best predictor based on maybe a linear

729
00:36:54,530 --> 00:37:00,280
regression you could use a linear

730
00:36:57,679 --> 00:37:03,649
regression but could you do something

731
00:37:00,280 --> 00:37:05,660
much simpler and more complete we're

732
00:37:03,650 --> 00:37:08,059
trying not to use any statistical

733
00:37:05,659 --> 00:37:09,829
assumptions here I can't see your name

734
00:37:08,059 --> 00:37:14,059
sorry

735
00:37:09,829 --> 00:37:16,429
and of course your friends anything can

736
00:37:14,059 --> 00:37:20,779
we just do like take just one variable

737
00:37:16,429 --> 00:37:24,019
if it is true give it like the true

738
00:37:20,780 --> 00:37:25,850
thing and if it is false so which

739
00:37:24,019 --> 00:37:27,259
variable are we gonna choose so at each

740
00:37:25,849 --> 00:37:31,009
binary point we have to choose a

741
00:37:27,260 --> 00:37:33,320
variable and something to split on how

742
00:37:31,010 --> 00:37:36,970
are we going to do that then I pass it

743
00:37:33,320 --> 00:37:42,320
over there how do I pronounce your name

744
00:37:36,969 --> 00:37:44,959
chicken so the variability tools could

745
00:37:42,320 --> 00:37:47,780
be like which divides population into

746
00:37:44,960 --> 00:37:50,630
two groups which kind of heterogeneous

747
00:37:47,780 --> 00:37:52,910
to each other and homogeneous within

748
00:37:50,630 --> 00:37:55,070
themselves like having the same quality

749
00:37:52,909 --> 00:37:55,519
within themselves and they are very

750
00:37:55,070 --> 00:37:59,150
different

751
00:37:55,519 --> 00:38:01,309
could you be more specific like in terms

752
00:37:59,150 --> 00:38:03,889
of the target variable maybe yeah right

753
00:38:01,309 --> 00:38:06,469
let's say we have two groups of tested

754
00:38:03,889 --> 00:38:09,079
so one has a different price altogether

755
00:38:06,469 --> 00:38:11,059
from the second group yes internally

756
00:38:09,079 --> 00:38:11,449
they have similar prices okay that's

757
00:38:11,059 --> 00:38:14,090
good

758
00:38:11,449 --> 00:38:16,819
so like to simplify things a little bit

759
00:38:14,090 --> 00:38:19,160
when we're saying find a variable that

760
00:38:16,820 --> 00:38:21,460
we could split into such that the two

761
00:38:19,159 --> 00:38:26,480
groups are as different to each other as

762
00:38:21,460 --> 00:38:28,309
possible and okay how do you how would

763
00:38:26,480 --> 00:38:31,809
you pick which variable and which split

764
00:38:28,309 --> 00:38:31,809
point that's the question

765
00:38:37,039 --> 00:38:43,489
yeah what's your first cut which

766
00:38:39,300 --> 00:38:43,490
variable and which split point

767
00:38:46,550 --> 00:38:50,180
we don't like we're making the trade

768
00:38:48,500 --> 00:38:53,150
from scratch we want to create our own

769
00:38:50,179 --> 00:38:55,839
tree that makes sense we've got somebody

770
00:38:53,150 --> 00:38:55,840
amazing

771
00:39:03,030 --> 00:39:08,080
can we test all of the possible split

772
00:39:05,769 --> 00:39:10,929
and see which one has this ah small and

773
00:39:08,079 --> 00:39:11,650
rmse that sounds good okay so let's dig

774
00:39:10,929 --> 00:39:14,589
into this

775
00:39:11,650 --> 00:39:17,170
so when you say test all of the possible

776
00:39:14,590 --> 00:39:20,730
splits what does that mean how do we

777
00:39:17,170 --> 00:39:20,730
enumerate all the possible splits

778
00:39:23,340 --> 00:39:32,110
oh and think of that but

779
00:39:26,329 --> 00:39:36,679
if you want for each variable you could

780
00:39:32,110 --> 00:39:38,300
put one aside and then put a second

781
00:39:36,679 --> 00:39:41,139
aside and compare the two and if it was

782
00:39:38,300 --> 00:39:45,100
better good okay so for each variable

783
00:39:41,139 --> 00:39:48,559
for each possible value of that variable

784
00:39:45,099 --> 00:39:49,759
see whether it's better now coming back

785
00:39:48,559 --> 00:39:52,219
to Maslak so I want to dig into the

786
00:39:49,760 --> 00:39:54,620
better when you said see if the RMS see

787
00:39:52,219 --> 00:39:56,619
is better what does that mean though

788
00:39:54,619 --> 00:40:01,039
because after a split you've got two

789
00:39:56,619 --> 00:40:03,889
rmse so you've got two two groups so you

790
00:40:01,039 --> 00:40:06,619
just gonna fit what that one variable

791
00:40:03,889 --> 00:40:08,089
comparing to the other it's not so so

792
00:40:06,619 --> 00:40:10,789
what I mean here is that before we

793
00:40:08,090 --> 00:40:13,250
decided to speed on coupler system we

794
00:40:10,789 --> 00:40:16,070
had a root mean square root of 0.477 and

795
00:40:13,250 --> 00:40:18,289
after we've got two groups one were the

796
00:40:16,070 --> 00:40:22,250
mean square error point one another with

797
00:40:18,289 --> 00:40:25,369
a mean squared error of 0.4 so you treat

798
00:40:22,250 --> 00:40:26,539
each individual model separately so for

799
00:40:25,369 --> 00:40:28,579
the first player you're just gonna

800
00:40:26,539 --> 00:40:30,980
compare between each variable in himself

801
00:40:28,579 --> 00:40:32,690
and then you move on to the next node

802
00:40:30,980 --> 00:40:37,010
with the remaining but but even the

803
00:40:32,690 --> 00:40:39,440
first node like so the model with zero

804
00:40:37,010 --> 00:40:42,800
splits has a single root mean squared

805
00:40:39,440 --> 00:40:44,659
error the model with one split so the

806
00:40:42,800 --> 00:40:47,530
very first thing we tried we've now got

807
00:40:44,659 --> 00:40:52,339
two groups with two mean square errors

808
00:40:47,530 --> 00:40:54,140
you don't give it to Daniel do you pick

809
00:40:52,340 --> 00:40:56,420
the one that gets them as different as

810
00:40:54,139 --> 00:40:59,869
they can be well we're truck well okay

811
00:40:56,420 --> 00:41:01,659
that would be one idea get the two mean

812
00:40:59,869 --> 00:41:04,819
squared errors as different as possible

813
00:41:01,659 --> 00:41:08,809
but why might that not work what might

814
00:41:04,820 --> 00:41:10,730
be a problem with that sample size come

815
00:41:08,809 --> 00:41:12,559
on because you could just literally

816
00:41:10,730 --> 00:41:15,710
leave one point out yeah so we could

817
00:41:12,559 --> 00:41:18,739
have like year made is less than 1950

818
00:41:15,710 --> 00:41:21,079
and it might have a single sample with a

819
00:41:18,739 --> 00:41:23,119
low price and like that's not a great

820
00:41:21,079 --> 00:41:25,279
split is it you know because the other

821
00:41:23,119 --> 00:41:28,429
group is actually not going to be very

822
00:41:25,280 --> 00:41:32,440
interesting at all can you improve it a

823
00:41:28,429 --> 00:41:32,440
bit can Jason improve it a bit

824
00:41:33,369 --> 00:41:39,309
could you take a weighted average yeah a

825
00:41:37,179 --> 00:41:42,339
weighted average so we could take point

826
00:41:39,309 --> 00:41:45,730
four one times 17,000 plus 0.1 times

827
00:41:42,340 --> 00:41:49,059
2,000 that's good right and that would

828
00:41:45,730 --> 00:41:51,730
be the same as actually saying I've got

829
00:41:49,059 --> 00:41:53,739
a model the model is a single binary

830
00:41:51,730 --> 00:41:55,179
decision and I'm going to say for

831
00:41:53,739 --> 00:41:57,339
everybody with year made less than

832
00:41:55,179 --> 00:42:00,460
ninety six point five I'm going to fill

833
00:41:57,340 --> 00:42:02,350
in point ten point two for everybody

834
00:42:00,460 --> 00:42:04,240
else are going to fill in nine point two

835
00:42:02,349 --> 00:42:07,750
and then I'm going to calculate the root

836
00:42:04,239 --> 00:42:09,369
mean squared error of this crappy model

837
00:42:07,750 --> 00:42:11,440
and that would give exactly the same

838
00:42:09,369 --> 00:42:14,619
right as the weighted average that

839
00:42:11,440 --> 00:42:18,400
you're suggesting okay good so we now

840
00:42:14,619 --> 00:42:21,009
have a single number that represents how

841
00:42:18,400 --> 00:42:22,389
good a split is which is the weighted

842
00:42:21,010 --> 00:42:25,260
average of the mean squared errors of

843
00:42:22,389 --> 00:42:28,599
the two groups that creates okay and

844
00:42:25,260 --> 00:42:30,730
thanks to I think it was was it Jake we

845
00:42:28,599 --> 00:42:32,829
have a way to find the best split which

846
00:42:30,730 --> 00:42:34,329
is to try every variable and to try

847
00:42:32,829 --> 00:42:36,759
every possible value of that variable

848
00:42:34,329 --> 00:42:40,469
and see which variable and which value

849
00:42:36,760 --> 00:42:42,910
gives us a split with the best score

850
00:42:40,469 --> 00:42:50,108
that makes sense

851
00:42:42,909 --> 00:42:59,558
okay what's your name sir okay and

852
00:42:50,108 --> 00:43:01,568
somebody give Natalie the box when you

853
00:42:59,559 --> 00:43:04,450
see every possible number for every

854
00:43:01,568 --> 00:43:08,159
possible variable link are you saying

855
00:43:04,449 --> 00:43:12,659
like here we have 0.5 as like our

856
00:43:08,159 --> 00:43:14,889
criteria to split the tree so are you

857
00:43:12,659 --> 00:43:18,278
are you saying we're trying out a

858
00:43:14,889 --> 00:43:20,739
reasonable number for every possible

859
00:43:18,278 --> 00:43:23,920
value right so cupola system only has

860
00:43:20,739 --> 00:43:25,899
two values true and false so there's

861
00:43:23,920 --> 00:43:29,470
only one way of splitting which is trues

862
00:43:25,900 --> 00:43:31,450
and falses ear made is an integer which

863
00:43:29,469 --> 00:43:33,489
varies between like O'Donnell 1960 and

864
00:43:31,449 --> 00:43:35,318
2010 so we can just say what are all the

865
00:43:33,489 --> 00:43:38,258
possible unique values of year made and

866
00:43:35,318 --> 00:43:40,480
and try them all so we're trying all the

867
00:43:38,259 --> 00:43:56,798
possible spec points can you pass a

868
00:43:40,480 --> 00:43:59,380
better annual pass to me and I so I just

869
00:43:56,798 --> 00:44:04,420
want to clarify again for the first

870
00:43:59,380 --> 00:44:06,220
split why did we split on Cutler system

871
00:44:04,420 --> 00:44:09,068
true or false sister because what we did

872
00:44:06,219 --> 00:44:12,368
was we used Jake's technique we tried

873
00:44:09,068 --> 00:44:15,130
every variable for every variable we

874
00:44:12,369 --> 00:44:17,650
tried every possible split so each one

875
00:44:15,130 --> 00:44:20,588
we noted down I think it was Jason's

876
00:44:17,650 --> 00:44:22,180
idea which was the weighted average mean

877
00:44:20,588 --> 00:44:24,960
squared error of the two groups are

878
00:44:22,179 --> 00:44:28,358
created we found which one had the best

879
00:44:24,960 --> 00:44:30,548
means grid error and we picked it and it

880
00:44:28,358 --> 00:44:32,219
turned out it was cutlass system true or

881
00:44:30,548 --> 00:44:35,369
false

882
00:44:32,219 --> 00:44:39,449
does that make sense I guess my question

883
00:44:35,369 --> 00:44:43,289
is more like so coupler system is like

884
00:44:39,449 --> 00:44:46,108
one of them like best indicators I guess

885
00:44:43,289 --> 00:44:51,570
it's the best we tried every variable

886
00:44:46,108 --> 00:44:53,549
and every possible level everything else

887
00:44:51,570 --> 00:44:56,190
at try it wasn't as good okay and then

888
00:44:53,550 --> 00:44:58,170
you do that right so now that we've done

889
00:44:56,190 --> 00:45:00,300
that we now take this group here

890
00:44:58,170 --> 00:45:03,570
everybody who's got coupler system

891
00:45:00,300 --> 00:45:05,670
equals true and we do it again for every

892
00:45:03,570 --> 00:45:07,950
possible variable for every possible

893
00:45:05,670 --> 00:45:10,139
level for people where coupler system

894
00:45:07,949 --> 00:45:13,199
equals true what's the best possible

895
00:45:10,139 --> 00:45:16,440
split and then are there circumstances

896
00:45:13,199 --> 00:45:18,629
when it's not just like binaries like

897
00:45:16,440 --> 00:45:20,849
you split it into like three groups for

898
00:45:18,630 --> 00:45:22,950
like example you're made so I'm gonna

899
00:45:20,849 --> 00:45:24,780
make a claim and then I'm gonna see if

900
00:45:22,949 --> 00:45:28,289
you can justify it I'm gonna claim that

901
00:45:24,780 --> 00:45:31,019
it's never necessary to do more than one

902
00:45:28,289 --> 00:45:32,969
split at a level because you can just

903
00:45:31,019 --> 00:45:35,639
cuz you can just split them again

904
00:45:32,969 --> 00:45:42,230
exactly so you can get exactly the same

905
00:45:35,639 --> 00:45:45,449
result by submitting twice okay good so

906
00:45:42,230 --> 00:45:49,320
that is the entirety of creating a

907
00:45:45,449 --> 00:45:52,529
decision tree you stop either when you

908
00:45:49,320 --> 00:45:54,599
hit some limit that was requested so we

909
00:45:52,530 --> 00:45:56,970
had a limit where we said max depth

910
00:45:54,599 --> 00:45:58,830
equals three so that's one one way to

911
00:45:56,969 --> 00:46:01,319
stop would be you ask to stop at some

912
00:45:58,829 --> 00:46:05,069
point and so we stopped otherwise you

913
00:46:01,320 --> 00:46:06,480
stop when you're your leaf nodes these

914
00:46:05,070 --> 00:46:08,250
things at the end of court leaf nodes

915
00:46:06,480 --> 00:46:12,449
when your leaf nodes only have one thing

916
00:46:08,250 --> 00:46:15,329
in them okay that's a decision tree that

917
00:46:12,449 --> 00:46:16,889
is how we grow a decision tree and this

918
00:46:15,329 --> 00:46:19,789
decision tree is not very good because

919
00:46:16,889 --> 00:46:22,529
it's got a validation r-squared of 0.4

920
00:46:19,789 --> 00:46:25,710
so we could try to make it better by

921
00:46:22,530 --> 00:46:27,780
removing next steps equals three right

922
00:46:25,710 --> 00:46:29,070
and creating a deeper tree so it's going

923
00:46:27,780 --> 00:46:30,440
to go all the way down they're going to

924
00:46:29,070 --> 00:46:33,420
keep splitting these things further

925
00:46:30,440 --> 00:46:37,200
until every leaf node only has one thing

926
00:46:33,420 --> 00:46:41,400
in it and if we do that the training

927
00:46:37,199 --> 00:46:43,559
r-squared is of course one

928
00:46:41,400 --> 00:46:45,059
because we can exactly predict every

929
00:46:43,559 --> 00:46:49,710
training element because it's in a leaf

930
00:46:45,059 --> 00:46:52,110
node or on its own but the validation

931
00:46:49,710 --> 00:46:53,760
r-squared is not one it's actually

932
00:46:52,110 --> 00:46:56,910
better than our really really shallow

933
00:46:53,760 --> 00:47:01,230
tree but it's not as good as we'd like

934
00:46:56,909 --> 00:47:06,659
okay so we want to find some other way

935
00:47:01,230 --> 00:47:08,849
of making these trees better and the way

936
00:47:06,659 --> 00:47:13,170
we're going to do it is to create a

937
00:47:08,849 --> 00:47:14,940
forest so what's the forest to create a

938
00:47:13,170 --> 00:47:19,050
forest we're going to use a statistical

939
00:47:14,940 --> 00:47:23,190
technique called bagging and you can bag

940
00:47:19,050 --> 00:47:24,930
any kind of model in fact Michael Jordan

941
00:47:23,190 --> 00:47:26,610
who is one of the speakers at the recent

942
00:47:24,929 --> 00:47:28,919
recent data Institute conference here at

943
00:47:26,610 --> 00:47:32,090
university of san francisco developed a

944
00:47:28,920 --> 00:47:35,940
technique called the bag of little

945
00:47:32,090 --> 00:47:38,610
bootstraps and which he shows how to use

946
00:47:35,940 --> 00:47:41,820
bagging for absolutely any kind of model

947
00:47:38,610 --> 00:47:44,849
to make it more robust and also to give

948
00:47:41,820 --> 00:47:48,090
you confidence intervals the random

949
00:47:44,849 --> 00:47:52,860
forest is simply a way of bagging trees

950
00:47:48,090 --> 00:47:55,590
so what is bagging managing is a really

951
00:47:52,860 --> 00:47:59,970
interesting idea which is what if we

952
00:47:55,590 --> 00:48:02,789
created five different models each of

953
00:47:59,969 --> 00:48:04,859
which was only somewhat predictive but

954
00:48:02,789 --> 00:48:07,259
the models weren't at all correlated

955
00:48:04,860 --> 00:48:08,490
with each other they gave predictions

956
00:48:07,260 --> 00:48:10,680
that weren't correlated with each other

957
00:48:08,489 --> 00:48:13,739
that would mean that the five models

958
00:48:10,679 --> 00:48:16,859
would have profound different insights

959
00:48:13,739 --> 00:48:18,569
into the relationships in the data and

960
00:48:16,860 --> 00:48:21,660
so if you took the average of those five

961
00:48:18,570 --> 00:48:23,519
models right then you're effectively

962
00:48:21,659 --> 00:48:26,969
bringing in the insights from each of

963
00:48:23,519 --> 00:48:30,059
them and so this idea of averaging

964
00:48:26,969 --> 00:48:34,819
models is a is is a technique for

965
00:48:30,059 --> 00:48:37,019
ensemble right which is really important

966
00:48:34,820 --> 00:48:40,440
now let's come up with a more specific

967
00:48:37,019 --> 00:48:44,690
idea of how to do this ensemble what if

968
00:48:40,440 --> 00:48:49,679
we created a whole lot of these trees

969
00:48:44,690 --> 00:48:52,980
big deep massively overfit trees right

970
00:48:49,679 --> 00:48:53,989
but each one let's say we only pick a

971
00:48:52,980 --> 00:48:57,869
random

972
00:48:53,989 --> 00:49:00,269
one-tenth of the data so we pick one out

973
00:48:57,869 --> 00:49:04,219
of every 10 rows at random build a deep

974
00:49:00,269 --> 00:49:07,369
tree right which is perfect on that

975
00:49:04,219 --> 00:49:09,659
subset and kind of crappy on the rest

976
00:49:07,369 --> 00:49:12,150
all right let's say we do that a hundred

977
00:49:09,659 --> 00:49:15,029
times so different random sample every

978
00:49:12,150 --> 00:49:16,769
time so all of the trees are going to be

979
00:49:15,030 --> 00:49:18,540
better than nothing right because they

980
00:49:16,769 --> 00:49:20,340
do actually have a real random subset of

981
00:49:18,539 --> 00:49:22,079
the data and so they found some insight

982
00:49:20,340 --> 00:49:24,000
but they're also overfitting terribly

983
00:49:22,079 --> 00:49:26,009
but they're all using different random

984
00:49:24,000 --> 00:49:28,469
samples so they all over fit in

985
00:49:26,010 --> 00:49:31,890
different ways on different things so in

986
00:49:28,469 --> 00:49:36,809
other words they all have errors but the

987
00:49:31,889 --> 00:49:41,159
errors are random what is the average of

988
00:49:36,809 --> 00:49:43,110
a bunch of random errors zero so in

989
00:49:41,159 --> 00:49:45,449
other words if we take the average of

990
00:49:43,110 --> 00:49:47,579
these trees each of which have been

991
00:49:45,449 --> 00:49:49,889
trained on a different random subset the

992
00:49:47,579 --> 00:49:52,549
errors will average out to zero and

993
00:49:49,889 --> 00:49:55,650
what's left is the true relationship and

994
00:49:52,550 --> 00:49:58,320
that's the random forest right so

995
00:49:55,650 --> 00:50:01,829
there's the technique right we've got a

996
00:49:58,320 --> 00:50:05,550
whole bunch of rows of data we grab a

997
00:50:01,829 --> 00:50:08,699
few at random all right put them into a

998
00:50:05,550 --> 00:50:13,019
smaller data set and build a tree based

999
00:50:08,699 --> 00:50:16,679
on that okay and then we put that tree

1000
00:50:13,019 --> 00:50:18,659
aside and do it again with a different

1001
00:50:16,679 --> 00:50:20,849
random subset and do it again with a

1002
00:50:18,659 --> 00:50:23,789
different random subset do that a whole

1003
00:50:20,849 --> 00:50:26,819
bunch of times and then for each one we

1004
00:50:23,789 --> 00:50:29,759
can then make predictions by running our

1005
00:50:26,820 --> 00:50:32,039
test data through the tree to get to the

1006
00:50:29,760 --> 00:50:34,590
leaf mode take the average in that leaf

1007
00:50:32,039 --> 00:50:39,449
node for all the trees and average them

1008
00:50:34,590 --> 00:50:42,059
all together so to do that we simply

1009
00:50:39,449 --> 00:50:45,359
call random forests regressor and by

1010
00:50:42,059 --> 00:50:48,659
default it creates n what scikit-learn

1011
00:50:45,360 --> 00:50:51,300
cords estimators an estimator is a tree

1012
00:50:48,659 --> 00:50:53,519
right so this is going to create ten

1013
00:50:51,300 --> 00:50:59,519
treats

1014
00:50:53,519 --> 00:51:10,889
and so we go ahead and train it I can't

1015
00:50:59,519 --> 00:51:12,719
remember if I remember to split okay

1016
00:51:10,889 --> 00:51:14,579
so create our ten trees and we're just

1017
00:51:12,719 --> 00:51:19,349
doing this on our little random subset

1018
00:51:14,579 --> 00:51:22,980
of 20,000 and so let's take a look at

1019
00:51:19,349 --> 00:51:26,880
one example can you pass the box to

1020
00:51:22,980 --> 00:51:28,679
Devin so just to make sure I understand

1021
00:51:26,880 --> 00:51:31,950
this so you're saying like we take ten

1022
00:51:28,679 --> 00:51:33,599
kind of crappy models we average ten

1023
00:51:31,949 --> 00:51:36,779
crappy models and we get a good model

1024
00:51:33,599 --> 00:51:39,420
exactly because the crappy models are

1025
00:51:36,780 --> 00:51:41,190
based on different random subsets and so

1026
00:51:39,420 --> 00:51:43,740
their errors are not correlated with

1027
00:51:41,190 --> 00:51:45,360
each other if the error errors work are

1028
00:51:43,739 --> 00:51:48,449
they with other this isn't going to work

1029
00:51:45,360 --> 00:51:51,090
okay so the key insight here is to

1030
00:51:48,449 --> 00:51:53,099
construct multiple models which are

1031
00:51:51,090 --> 00:51:55,200
better than nothing and where the errors

1032
00:51:53,099 --> 00:51:59,190
are as much as possible not correlated

1033
00:51:55,199 --> 00:52:00,629
with each other so is there like a

1034
00:51:59,190 --> 00:52:03,690
certain number of trees that like we

1035
00:52:00,630 --> 00:52:05,760
need that in order to be this sort of

1036
00:52:03,690 --> 00:52:08,849
the things like valid or invalid there's

1037
00:52:05,760 --> 00:52:12,510
like has a good validation set our MSC

1038
00:52:08,849 --> 00:52:14,309
or not you know and so that's what we're

1039
00:52:12,510 --> 00:52:17,400
going to look at is how to make that

1040
00:52:14,309 --> 00:52:20,130
metric higher and so this is the first

1041
00:52:17,400 --> 00:52:21,599
of our hyper parameters then we're going

1042
00:52:20,130 --> 00:52:22,980
to learn about how to tune hyper

1043
00:52:21,599 --> 00:52:24,239
parameters and the first one is going to

1044
00:52:22,980 --> 00:52:30,150
be the number of trees and we're about

1045
00:52:24,239 --> 00:52:35,519
to us look at that now yes mostly you're

1046
00:52:30,150 --> 00:52:37,889
selecting aren't they exclusive yes so I

1047
00:52:35,519 --> 00:52:41,009
mentioned you know one approach would be

1048
00:52:37,889 --> 00:52:42,779
pick out like attempt at random but

1049
00:52:41,010 --> 00:52:46,619
actually what scikit-learn does by

1050
00:52:42,780 --> 00:52:49,859
default is for n rows it picks out n

1051
00:52:46,619 --> 00:52:51,930
rows with replacement okay and that's

1052
00:52:49,858 --> 00:52:54,000
called bootstrapping and if memory

1053
00:52:51,929 --> 00:52:57,599
serves me correctly that gets you on

1054
00:52:54,000 --> 00:52:59,730
average 63.2% of the rows will be

1055
00:52:57,599 --> 00:53:03,279
represented and you know a bunch of them

1056
00:52:59,730 --> 00:53:07,210
will be represented multiple times yeah

1057
00:53:03,280 --> 00:53:09,090
it sure so rather than just picking out

1058
00:53:07,210 --> 00:53:12,280
like a tenth of the rows at random

1059
00:53:09,090 --> 00:53:14,050
instead we're going to pick out of an n

1060
00:53:12,280 --> 00:53:17,560
row data set we're going to pick out n

1061
00:53:14,050 --> 00:53:20,950
rows with replacement which on average

1062
00:53:17,559 --> 00:53:24,429
gets about 63 I think 63.2% of the rows

1063
00:53:20,949 --> 00:53:27,489
will be represented many of those rows

1064
00:53:24,429 --> 00:53:33,399
will appear multiple times I think

1065
00:53:27,489 --> 00:53:35,439
there's a question behind you in essence

1066
00:53:33,400 --> 00:53:37,680
what this model is doing is if I

1067
00:53:35,440 --> 00:53:39,909
understand Craig is just picking out

1068
00:53:37,679 --> 00:53:42,129
data points that look very similar to

1069
00:53:39,909 --> 00:53:44,739
the one you're looking at yeah that's a

1070
00:53:42,130 --> 00:53:46,840
great insight so what a tree is kind of

1071
00:53:44,739 --> 00:53:49,149
doing there's no quite complicated way

1072
00:53:46,840 --> 00:53:50,140
of doing about doing that I mean there

1073
00:53:49,150 --> 00:53:52,210
would be other ways of assessing

1074
00:53:50,139 --> 00:53:54,639
similarity there are other ways of

1075
00:53:52,210 --> 00:53:56,260
assessing similarity but what's

1076
00:53:54,639 --> 00:54:00,069
interesting about this way is it's doing

1077
00:53:56,260 --> 00:54:01,810
it in in tree space right so we're

1078
00:54:00,070 --> 00:54:03,039
basically saying what are in this case

1079
00:54:01,809 --> 00:54:06,219
like for this little tree what are the

1080
00:54:03,039 --> 00:54:08,559
593 samples you know closest to this one

1081
00:54:06,219 --> 00:54:11,230
and what's their average closest in tree

1082
00:54:08,559 --> 00:54:12,730
space so other ways of doing that would

1083
00:54:11,230 --> 00:54:14,679
be like and we'll learn later on in this

1084
00:54:12,730 --> 00:54:17,019
course about K nearest neighbors you

1085
00:54:14,679 --> 00:54:22,329
could use like Euclidean distance C

1086
00:54:17,019 --> 00:54:25,659
right but here's a thing the whole point

1087
00:54:22,329 --> 00:54:28,329
of machine learning is to identify which

1088
00:54:25,659 --> 00:54:30,969
variables actually matter the most and

1089
00:54:28,329 --> 00:54:33,340
how do they relate to each other and to

1090
00:54:30,969 --> 00:54:35,259
your dependent variable together right

1091
00:54:33,340 --> 00:54:37,840
so if you've like imagine a synthetic

1092
00:54:35,260 --> 00:54:39,850
data set where you create five variables

1093
00:54:37,840 --> 00:54:41,470
that add together to create your

1094
00:54:39,849 --> 00:54:43,449
independent to create your dependent

1095
00:54:41,469 --> 00:54:45,549
variable and ninety five variables which

1096
00:54:43,449 --> 00:54:47,679
are entirely random and don't impact

1097
00:54:45,550 --> 00:54:49,330
your dependent variable and then if you

1098
00:54:47,679 --> 00:54:51,579
do like a K nearest neighbors in

1099
00:54:49,329 --> 00:54:53,049
Euclidean space you're going to get

1100
00:54:51,579 --> 00:54:54,519
meaningless nearest neighbors because

1101
00:54:53,050 --> 00:54:57,039
most of your columns are actually

1102
00:54:54,519 --> 00:54:58,659
meaningless or imagine your actual

1103
00:54:57,039 --> 00:55:03,489
relationship is that your dependent

1104
00:54:58,659 --> 00:55:05,379
variable equals x1 times x2 then you'll

1105
00:55:03,489 --> 00:55:07,209
actually need to find this interaction

1106
00:55:05,380 --> 00:55:09,400
right so you don't actually care about

1107
00:55:07,210 --> 00:55:12,010
how close it is to x1 and how close to

1108
00:55:09,400 --> 00:55:14,470
x2 but how close to the product so the

1109
00:55:12,010 --> 00:55:15,510
entire purpose of modeling in machine

1110
00:55:14,469 --> 00:55:18,239
learning is

1111
00:55:15,510 --> 00:55:20,040
to find a model which tells you which

1112
00:55:18,239 --> 00:55:21,449
variables are important and how do they

1113
00:55:20,039 --> 00:55:24,929
interact together to drive your

1114
00:55:21,449 --> 00:55:27,869
dependent variable and so you'll find in

1115
00:55:24,929 --> 00:55:30,779
practice the difference between using

1116
00:55:27,869 --> 00:55:32,730
like tree space or random forest space

1117
00:55:30,780 --> 00:55:35,250
to find your nearest neighbors versus

1118
00:55:32,730 --> 00:55:37,230
like Euclidean space is the difference

1119
00:55:35,250 --> 00:55:39,000
between a model that makes good

1120
00:55:37,230 --> 00:55:43,789
predictions in the model that makes many

1121
00:55:39,000 --> 00:55:43,789
most predictions elicit here

1122
00:55:43,949 --> 00:55:58,189
I did but I feel like we've got only 35

1123
00:55:47,639 --> 00:56:00,659
minutes so yeah great so so in general a

1124
00:55:58,190 --> 00:56:07,590
machine learning model which is

1125
00:56:00,659 --> 00:56:09,389
effective is one which is accurate when

1126
00:56:07,590 --> 00:56:12,289
you look at the training data it's it's

1127
00:56:09,389 --> 00:56:14,339
accurate at predicting at actually

1128
00:56:12,289 --> 00:56:16,230
finding the relationships in that

1129
00:56:14,340 --> 00:56:21,059
training data and then it generalizes

1130
00:56:16,230 --> 00:56:23,869
well to new data and so in bagging that

1131
00:56:21,059 --> 00:56:27,690
means that each of your individual

1132
00:56:23,869 --> 00:56:30,659
estimators at your individual trees you

1133
00:56:27,690 --> 00:56:32,309
want to be as predictive as possible but

1134
00:56:30,659 --> 00:56:34,739
for the predictions of your individual

1135
00:56:32,309 --> 00:56:36,989
trees to be as uncorrelated as possible

1136
00:56:34,739 --> 00:56:38,549
and so the inventor of random forests

1137
00:56:36,989 --> 00:56:40,289
talks about this at length in his

1138
00:56:38,550 --> 00:56:42,660
original paper that introduced this in

1139
00:56:40,289 --> 00:56:45,929
the late 90s this idea of trying to come

1140
00:56:42,659 --> 00:56:50,369
up with predictive but poorly correlated

1141
00:56:45,929 --> 00:56:53,579
trees the the research community in

1142
00:56:50,369 --> 00:56:56,969
recent years has generally found that

1143
00:56:53,579 --> 00:57:00,420
the more important thing seems to be

1144
00:56:56,969 --> 00:57:02,759
creating uncorrelated trees rather than

1145
00:57:00,420 --> 00:57:05,130
more accurate trees so more recent

1146
00:57:02,760 --> 00:57:08,160
advances tend to create trees which are

1147
00:57:05,130 --> 00:57:10,230
less predictive on their own but also

1148
00:57:08,159 --> 00:57:13,349
less correlated with each other so for

1149
00:57:10,230 --> 00:57:16,139
example in scikit-learn there's another

1150
00:57:13,349 --> 00:57:18,119
class you can use called extra trees

1151
00:57:16,139 --> 00:57:20,190
regress on your extra trees classifier

1152
00:57:18,119 --> 00:57:22,139
with exactly the same API you can try it

1153
00:57:20,190 --> 00:57:23,010
tonight just replace my random prose

1154
00:57:22,139 --> 00:57:25,679
regressor with that

1155
00:57:23,010 --> 00:57:29,340
that's called an extremely randomized

1156
00:57:25,679 --> 00:57:31,049
trees model and what that does is

1157
00:57:29,340 --> 00:57:33,210
the same as what we just discussed but

1158
00:57:31,050 --> 00:57:36,720
rather than trying every speck of every

1159
00:57:33,210 --> 00:57:39,539
variable it randomly tries a few splits

1160
00:57:36,719 --> 00:57:42,980
of a few variables right so it's much

1161
00:57:39,539 --> 00:57:45,719
faster the Train it has more randomness

1162
00:57:42,980 --> 00:57:48,000
okay but then you've got time you can

1163
00:57:45,719 --> 00:57:52,559
build more trees and therefore get

1164
00:57:48,000 --> 00:57:54,630
better generalization so in practice if

1165
00:57:52,559 --> 00:57:56,730
you've got crappy individual models you

1166
00:57:54,630 --> 00:57:58,200
just need more trees to get a good end

1167
00:57:56,730 --> 00:58:05,190
up model Melissa could you pass that

1168
00:57:58,199 --> 00:58:06,689
over to Devin could you talk a little

1169
00:58:05,190 --> 00:58:11,820
bit more about what you mean by like

1170
00:58:06,690 --> 00:58:15,240
uncorrelated trees yeah if I build a

1171
00:58:11,820 --> 00:58:20,130
thousand trees h1 on just ten data

1172
00:58:15,239 --> 00:58:21,719
points then it's quite likely that the

1173
00:58:20,130 --> 00:58:23,519
ten data points for every tree are going

1174
00:58:21,719 --> 00:58:25,589
to be totally different and so it's

1175
00:58:23,519 --> 00:58:27,659
quite likely that those ten trees are

1176
00:58:25,590 --> 00:58:30,450
going to trees are going to give totally

1177
00:58:27,659 --> 00:58:33,149
different answers to each other so the

1178
00:58:30,449 --> 00:58:34,799
correlation between the predictions of

1179
00:58:33,150 --> 00:58:36,360
tree one and three two is going to be

1180
00:58:34,800 --> 00:58:38,519
very small between tree one and three

1181
00:58:36,360 --> 00:58:40,050
three very small and so forth on the

1182
00:58:38,519 --> 00:58:43,590
other hand if I create a thousand trees

1183
00:58:40,050 --> 00:58:46,830
where each time I use the entire data

1184
00:58:43,590 --> 00:58:47,820
set with just one element removed all

1185
00:58:46,829 --> 00:58:50,940
those trees are going to be nearly

1186
00:58:47,820 --> 00:58:53,160
identical ie their predictions will be

1187
00:58:50,940 --> 00:58:54,990
highly correlated and so in the latter

1188
00:58:53,159 --> 00:58:57,059
case it's probably not going to

1189
00:58:54,989 --> 00:58:59,519
generalize very well where else in the

1190
00:58:57,059 --> 00:59:01,230
former case the individual trees we're

1191
00:58:59,519 --> 00:59:09,059
not going to be very predictive so I

1192
00:59:01,230 --> 00:59:12,420
need to find some nice in-between so yes

1193
00:59:09,059 --> 00:59:14,759
Danielle and is there a case where you

1194
00:59:12,420 --> 00:59:17,250
want to use one over the other like any

1195
00:59:14,760 --> 00:59:19,140
particular times yeah so again hyper

1196
00:59:17,250 --> 00:59:20,730
parameter tuning so driven in terms of

1197
00:59:19,139 --> 00:59:23,849
like random random forests versus

1198
00:59:20,730 --> 00:59:26,219
extremely randomized phase yeah so again

1199
00:59:23,849 --> 00:59:27,989
a hyper parameter walk tree architecture

1200
00:59:26,219 --> 00:59:30,119
do we use so we're going to talk about

1201
00:59:27,989 --> 00:59:32,269
that now can you pass that through do

1202
00:59:30,119 --> 00:59:32,269
you know

1203
00:59:32,550 --> 00:59:36,760
you know I was just trying to understand

1204
00:59:34,869 --> 00:59:38,920
how this random forest actually makes

1205
00:59:36,760 --> 00:59:40,600
sense for continuous variables I mean

1206
00:59:38,920 --> 00:59:42,730
I'm assuming that you build a tree

1207
00:59:40,599 --> 00:59:44,469
structure and the last final notes you'd

1208
00:59:42,730 --> 00:59:47,050
be seeing like maybe the small node

1209
00:59:44,469 --> 00:59:48,819
represents maybe a category a or a

1210
00:59:47,050 --> 00:59:51,100
category B but how does it make sense

1211
00:59:48,820 --> 00:59:52,480
for your continuous storage so this is

1212
00:59:51,099 --> 00:59:56,920
actually what we have here and so the

1213
00:59:52,480 --> 01:00:00,639
value here is the average so this is the

1214
00:59:56,920 --> 01:00:03,460
average log of price for this subgroup

1215
01:00:00,639 --> 01:00:04,929
and that's what we do the prediction is

1216
01:00:03,460 --> 01:00:07,530
the average of the value of the

1217
01:00:04,929 --> 01:00:10,449
dependent variable in that leaf node

1218
01:00:07,530 --> 01:00:13,890
means finally if you have just like

1219
01:00:10,449 --> 01:00:16,149
dengue nodes you just have 10 values yes

1220
01:00:13,889 --> 01:00:18,400
that's right well if it was only one

1221
01:00:16,150 --> 01:00:19,720
tree right so a couple things to

1222
01:00:18,400 --> 01:00:21,430
remember the first is that by default

1223
01:00:19,719 --> 01:00:23,889
we're actually going to train the tree

1224
01:00:21,429 --> 01:00:27,549
all the way down until the leaf nodes

1225
01:00:23,889 --> 01:00:29,500
were size 1 which means for a data set

1226
01:00:27,550 --> 01:00:31,539
with n rows we're gonna have n leaf

1227
01:00:29,500 --> 01:00:33,940
nodes and then we're going to have

1228
01:00:31,539 --> 01:00:37,860
multiple trees which we average together

1229
01:00:33,940 --> 01:00:40,059
right so in practice we're gonna have a

1230
01:00:37,860 --> 01:00:45,670
you know lots of different possible

1231
01:00:40,059 --> 01:00:47,500
values it's a question behind you so far

1232
01:00:45,670 --> 01:00:49,329
the continuous variable how do we decide

1233
01:00:47,500 --> 01:00:51,489
like which value to split out because

1234
01:00:49,329 --> 01:00:54,819
there could be many values we try every

1235
01:00:51,489 --> 01:00:57,509
possible value of that in the training

1236
01:00:54,820 --> 01:00:59,590
set one did be computationally

1237
01:00:57,510 --> 01:01:01,570
computational expensive and this is

1238
01:00:59,590 --> 01:01:04,240
where it's very good to remember that

1239
01:01:01,570 --> 01:01:06,940
your CPUs performance is measured in

1240
01:01:04,239 --> 01:01:09,159
gigahertz which is billions of clock

1241
01:01:06,940 --> 01:01:12,400
cycles per second and it has multiple

1242
01:01:09,159 --> 01:01:14,529
cores and each core has something called

1243
01:01:12,400 --> 01:01:16,300
Simbi single instruction multiple data

1244
01:01:14,530 --> 01:01:19,660
where it can direct up to eight

1245
01:01:16,300 --> 01:01:22,300
computations per core at once and then

1246
01:01:19,659 --> 01:01:24,549
if you do it on the GPU the performance

1247
01:01:22,300 --> 01:01:27,010
is measured in teraflops

1248
01:01:24,550 --> 01:01:29,080
so trillions of floating-point

1249
01:01:27,010 --> 01:01:32,080
operations per second and so this is

1250
01:01:29,079 --> 01:01:34,539
where when it comes to designing

1251
01:01:32,079 --> 01:01:37,059
algorithms it's very difficult for us

1252
01:01:34,539 --> 01:01:40,659
mere humans to realize how stupid

1253
01:01:37,059 --> 01:01:43,719
algorithms should be given how fast

1254
01:01:40,659 --> 01:01:45,269
today's computers are so yeah it's quite

1255
01:01:43,719 --> 01:01:48,269
a few operations

1256
01:01:45,269 --> 01:01:53,929
but that trillions per second you hardly

1257
01:01:48,269 --> 01:01:56,550
notice it Russia I have a question so

1258
01:01:53,929 --> 01:02:00,179
essentially at each remote we make a

1259
01:01:56,550 --> 01:02:04,820
decision like which category T or which

1260
01:02:00,179 --> 01:02:04,819
variable to use and which pinpoint yes

1261
01:02:05,929 --> 01:02:13,230
so we have MSE a calculated for each

1262
01:02:10,110 --> 01:02:16,380
node so this is kind of our one of the

1263
01:02:13,230 --> 01:02:18,570
decision criteria but please MSE it is

1264
01:02:16,380 --> 01:02:22,170
calculated for which model like which

1265
01:02:18,570 --> 01:02:27,690
model underlies the model is the model

1266
01:02:22,170 --> 01:02:30,630
is for the initial root mode is what if

1267
01:02:27,690 --> 01:02:32,010
we just predicted the average right

1268
01:02:30,630 --> 01:02:35,849
which is here is ten point oh nine eight

1269
01:02:32,010 --> 01:02:37,830
and just the average and then the next

1270
01:02:35,849 --> 01:02:40,409
model is what if we predicted the

1271
01:02:37,829 --> 01:02:43,199
average of those people with capitalist

1272
01:02:40,409 --> 01:02:45,868
system equals false and for those people

1273
01:02:43,199 --> 01:02:47,250
with couple of system is true and then

1274
01:02:45,869 --> 01:02:49,470
the next is what if we predicted the

1275
01:02:47,250 --> 01:02:52,559
average of couple systems equals true

1276
01:02:49,469 --> 01:02:54,868
yeah made less than 1986 is it always

1277
01:02:52,559 --> 01:02:58,199
average sure we can use median or we can

1278
01:02:54,869 --> 01:02:59,910
even run linear regression there's all

1279
01:02:58,199 --> 01:03:03,919
kinds of things we could do and practice

1280
01:02:59,909 --> 01:03:06,779
the average works really well there are

1281
01:03:03,920 --> 01:03:08,190
there are types of they're not called

1282
01:03:06,780 --> 01:03:09,990
random forests but there are kinds of

1283
01:03:08,190 --> 01:03:12,240
trees where the leaf nodes are

1284
01:03:09,989 --> 01:03:14,219
independent linear regressions they're

1285
01:03:12,239 --> 01:03:16,319
not terribly widely used but there are

1286
01:03:14,219 --> 01:03:19,709
certainly researchers who I have worked

1287
01:03:16,320 --> 01:03:23,030
on them okay thank you and pass it back

1288
01:03:19,710 --> 01:03:23,030
over that afford and then to check

1289
01:03:25,079 --> 01:03:30,720
so this tree has a depth of three yeah

1290
01:03:27,690 --> 01:03:33,420
and then I on one of the next commands

1291
01:03:30,719 --> 01:03:36,389
we get rid of the max depth yeah the

1292
01:03:33,420 --> 01:03:38,729
tree without the max depth does that

1293
01:03:36,389 --> 01:03:40,978
contain the tree with with the depth of

1294
01:03:38,728 --> 01:03:42,899
three yeah so that is that like by

1295
01:03:40,978 --> 01:03:44,368
definition it's yeah well except in this

1296
01:03:42,900 --> 01:03:47,400
case we've added randomness but if you

1297
01:03:44,369 --> 01:03:50,729
turned bootstrapping off then yeah the

1298
01:03:47,400 --> 01:03:53,039
the deeper tree will you know the the

1299
01:03:50,728 --> 01:03:54,449
the the less deep tree would be how it

1300
01:03:53,039 --> 01:04:02,460
start and then it goes keep spitting

1301
01:03:54,449 --> 01:04:05,399
okay so you have many trees you're gonna

1302
01:04:02,460 --> 01:04:07,889
have different leaf nodes across streets

1303
01:04:05,400 --> 01:04:11,099
hopefully so we want um so how do you

1304
01:04:07,889 --> 01:04:14,248
average leaf nodes across different

1305
01:04:11,099 --> 01:04:16,798
trees so we just take the first row in

1306
01:04:14,248 --> 01:04:19,768
the validation set we run it through the

1307
01:04:16,798 --> 01:04:21,538
first tree we find its average nine

1308
01:04:19,768 --> 01:04:23,399
point two eight then do it through the

1309
01:04:21,539 --> 01:04:26,009
next tree find its average in the second

1310
01:04:23,400 --> 01:04:27,809
tree nine point nine five and so forth

1311
01:04:26,009 --> 01:04:32,728
and we're about to do that so you'll see

1312
01:04:27,809 --> 01:04:35,819
it okay so let's try it right so after

1313
01:04:32,728 --> 01:04:37,978
you've built a random forest each tree

1314
01:04:35,818 --> 01:04:40,588
is stored in this attribute called

1315
01:04:37,978 --> 01:04:42,659
estimators underscore okay

1316
01:04:40,588 --> 01:04:44,548
so one of the things that you guys need

1317
01:04:42,659 --> 01:04:47,998
to be very very comfortable with is

1318
01:04:44,548 --> 01:04:49,949
using list comprehensions okay so I hope

1319
01:04:47,998 --> 01:04:51,358
you've all been practicing okay so here

1320
01:04:49,949 --> 01:04:54,358
I'm using a list comprehension to go

1321
01:04:51,358 --> 01:04:56,909
through each tree in my model I'm going

1322
01:04:54,358 --> 01:04:59,009
to call predict on it with my validation

1323
01:04:56,909 --> 01:05:05,578
set and so that's going to give me a

1324
01:04:59,009 --> 01:05:07,739
list of arrays of predictions so H array

1325
01:05:05,579 --> 01:05:12,528
will be all of the predictions for that

1326
01:05:07,739 --> 01:05:15,179
tree and I have ten trees MP dot stack

1327
01:05:12,528 --> 01:05:22,170
concatenates them together on a new axis

1328
01:05:15,179 --> 01:05:24,808
so after I run this and called shape you

1329
01:05:22,170 --> 01:05:27,599
can see I now have the first axis ten

1330
01:05:24,809 --> 01:05:29,249
means I have my ten different sets of

1331
01:05:27,599 --> 01:05:30,809
predictions and for each one my

1332
01:05:29,248 --> 01:05:33,028
validation set as a side of twelve

1333
01:05:30,809 --> 01:05:35,509
thousand so here are my twelve thousand

1334
01:05:33,028 --> 01:05:40,259
predictions for each of the ten trees

1335
01:05:35,509 --> 01:05:44,509
alright so let's take the first row of

1336
01:05:40,259 --> 01:05:46,858
that and print it out and so here are

1337
01:05:44,509 --> 01:05:49,858
what weirdest thing here are ten

1338
01:05:46,858 --> 01:05:52,639
predictions one from each tree okay and

1339
01:05:49,858 --> 01:05:54,900
so then if we say take the mean of that

1340
01:05:52,639 --> 01:05:58,170
here is the mean of those ten

1341
01:05:54,900 --> 01:06:01,079
predictions and then what was the actual

1342
01:05:58,170 --> 01:06:03,150
the actual was nine point one our

1343
01:06:01,079 --> 01:06:05,190
prediction was nine point O seven so you

1344
01:06:03,150 --> 01:06:08,039
see how like none of our individual

1345
01:06:05,190 --> 01:06:09,619
trees had very good predictions but the

1346
01:06:08,039 --> 01:06:11,190
mean of them was actually pretty good

1347
01:06:09,619 --> 01:06:13,960
and so

1348
01:06:11,190 --> 01:06:15,639
when I talk about experimenting like

1349
01:06:13,960 --> 01:06:17,470
Jupiter notebook is great for

1350
01:06:15,639 --> 01:06:20,230
experimenting this is the kind of stuff

1351
01:06:17,469 --> 01:06:22,480
I mean dig inside these objects and like

1352
01:06:20,230 --> 01:06:24,369
look at them and plot them take your own

1353
01:06:22,480 --> 01:06:25,539
averages cross check to make sure that

1354
01:06:24,369 --> 01:06:27,608
they work the way you thought they did

1355
01:06:25,539 --> 01:06:29,739
write your own implementation of

1356
01:06:27,608 --> 01:06:31,779
r-squared make sure it's the same as a

1357
01:06:29,739 --> 01:06:34,328
psychic learn version plot it like

1358
01:06:31,780 --> 01:06:37,720
here's an interesting plot I did let's

1359
01:06:34,329 --> 01:06:40,240
go through each Taffet entries right and

1360
01:06:37,719 --> 01:06:44,858
then take the mean of all of the

1361
01:06:40,239 --> 01:06:47,379
predictions up to the ithe tree right so

1362
01:06:44,858 --> 01:06:49,568
let's start by predicting just based on

1363
01:06:47,380 --> 01:06:52,059
the first tree than the first two trees

1364
01:06:49,568 --> 01:06:55,509
and the first three trees and let's then

1365
01:06:52,059 --> 01:06:57,970
plot the r-squared so here's the

1366
01:06:55,510 --> 01:07:00,040
r-squared of just the first tree is the

1367
01:06:57,969 --> 01:07:01,959
r-squared of the first two trees three

1368
01:07:00,039 --> 01:07:04,389
trees four trees blah blah blah blah up

1369
01:07:01,960 --> 01:07:07,568
to ten trees and so not surprisingly a

1370
01:07:04,389 --> 01:07:11,289
script keeps improving right because the

1371
01:07:07,568 --> 01:07:13,150
more estimators we have the more bagging

1372
01:07:11,289 --> 01:07:16,179
that we're doing the more it's well it's

1373
01:07:13,150 --> 01:07:19,298
going to generalize all right and you

1374
01:07:16,179 --> 01:07:22,539
should find that that number there bit

1375
01:07:19,298 --> 01:07:25,750
under point eight six should match this

1376
01:07:22,539 --> 01:07:28,539
number here okay

1377
01:07:25,750 --> 01:07:30,429
let's rerun that yeah okay so they're

1378
01:07:28,539 --> 01:07:31,838
actually slightly above what it says all

1379
01:07:30,429 --> 01:07:33,519
right so again these are all like the

1380
01:07:31,838 --> 01:07:34,779
cross checks you can do the things you

1381
01:07:33,519 --> 01:07:37,269
can visualize to deepen your

1382
01:07:34,780 --> 01:07:39,819
understanding okay so as we add more

1383
01:07:37,269 --> 01:07:42,969
trees our r-squared improves it seems to

1384
01:07:39,818 --> 01:07:44,500
flatten out after a while so we might

1385
01:07:42,969 --> 01:07:47,169
guess that if we increase the number of

1386
01:07:44,500 --> 01:07:50,190
estimators to twenty right it's maybe

1387
01:07:47,170 --> 01:07:50,190
not going to be that much better

1388
01:07:52,769 --> 01:07:59,949
so let's see we've got point eight six

1389
01:07:56,048 --> 01:08:01,929
two first is point eight six oh yeah so

1390
01:07:59,949 --> 01:08:05,500
doubling the number of trees didn't help

1391
01:08:01,929 --> 01:08:09,159
very much but double it again eight six

1392
01:08:05,500 --> 01:08:12,429
seven double it again eight six nine so

1393
01:08:09,159 --> 01:08:14,048
you can see like there's some point at

1394
01:08:12,429 --> 01:08:16,000
which you're going to you know not want

1395
01:08:14,048 --> 01:08:18,909
to add more trees not because it's never

1396
01:08:16,000 --> 01:08:23,020
going to get worse because every tree is

1397
01:08:18,909 --> 01:08:24,818
you know giving you more semi-random

1398
01:08:23,020 --> 01:08:27,339
models to bag together

1399
01:08:24,819 --> 01:08:29,650
right but it's going to stop improving

1400
01:08:27,338 --> 01:08:31,210
things much okay and so this is like the

1401
01:08:29,649 --> 01:08:33,939
first hyper parameter if you learn to

1402
01:08:31,210 --> 01:08:38,198
set is number of estimators and the

1403
01:08:33,939 --> 01:08:41,528
method for setting is as many as you

1404
01:08:38,198 --> 01:08:45,250
have time to fit and that actually

1405
01:08:41,529 --> 01:08:46,960
seemed to be hopping okay now in

1406
01:08:45,250 --> 01:08:49,539
practice we're going to learn to set a

1407
01:08:46,960 --> 01:08:54,279
few more hyper parameters adding more

1408
01:08:49,539 --> 01:08:57,009
trees slows it down but with less trees

1409
01:08:54,279 --> 01:08:59,500
you can still get the same insights so

1410
01:08:57,009 --> 01:09:03,158
I've build most of my models in practice

1411
01:08:59,500 --> 01:09:05,170
with like 20 to 30 trees and it's only

1412
01:09:03,158 --> 01:09:06,818
like then at the end of the project or

1413
01:09:05,170 --> 01:09:08,770
maybe at the end of the day's work I'll

1414
01:09:06,819 --> 01:09:11,139
then try doing like I don't know a

1415
01:09:08,770 --> 01:09:13,089
thousand trees and run it overnight was

1416
01:09:11,139 --> 01:09:15,389
there a question yes can we pass that

1417
01:09:13,088 --> 01:09:15,389
Prince

1418
01:09:18,130 --> 01:09:21,850
so each tree might have different

1419
01:09:20,229 --> 01:09:24,309
estimators different combination of

1420
01:09:21,850 --> 01:09:26,530
estimators HQ is an estimator so this is

1421
01:09:24,310 --> 01:09:31,030
a synonym so in scikit-learn when I say

1422
01:09:26,529 --> 01:09:33,219
estimator they name three so I mean each

1423
01:09:31,029 --> 01:09:35,769
tree would have different break points

1424
01:09:33,220 --> 01:09:37,359
on differently on different columns but

1425
01:09:35,770 --> 01:09:40,240
if at the end we want to locate the

1426
01:09:37,359 --> 01:09:43,120
important features and we'll get to that

1427
01:09:40,239 --> 01:09:45,789
yeah so I'm after we finished with kind

1428
01:09:43,119 --> 01:09:48,789
of setting hyper parameters the next

1429
01:09:45,789 --> 01:09:52,329
stage of the course will be learning

1430
01:09:48,789 --> 01:09:54,100
about what it tells us about the data if

1431
01:09:52,329 --> 01:09:56,279
you need to know it now you know for

1432
01:09:54,100 --> 01:10:00,760
your projects feel free to look ahead

1433
01:09:56,279 --> 01:10:05,500
there's a lesson to our F interpretation

1434
01:10:00,760 --> 01:10:08,650
them is where we can see it okay so

1435
01:10:05,500 --> 01:10:11,039
that's our first type of parameter um I

1436
01:10:08,649 --> 01:10:14,079
want to talk next about out-of-bag score

1437
01:10:11,039 --> 01:10:16,630
sometimes your data set will be kind of

1438
01:10:14,079 --> 01:10:19,600
small and you won't want to pull out a

1439
01:10:16,630 --> 01:10:21,250
validation set because doing so means

1440
01:10:19,600 --> 01:10:24,670
you now don't have enough data to build

1441
01:10:21,250 --> 01:10:26,680
a good model what do you do there's a

1442
01:10:24,670 --> 01:10:30,430
cool trick which is pretty much unique

1443
01:10:26,680 --> 01:10:35,890
to random forests and it's this what we

1444
01:10:30,430 --> 01:10:38,289
could do is recognize that some of our

1445
01:10:35,890 --> 01:10:42,510
in our first tree some of our columns

1446
01:10:38,289 --> 01:10:46,449
sorry some of our rows didn't get used

1447
01:10:42,510 --> 01:10:49,840
so what we could do would be to pass

1448
01:10:46,449 --> 01:10:53,359
those rows through the first tree and

1449
01:10:49,840 --> 01:10:55,760
treat it as a validation set

1450
01:10:53,359 --> 01:10:57,439
and then for the second tree we could

1451
01:10:55,760 --> 01:10:59,659
pass through the rows that weren't used

1452
01:10:57,439 --> 01:11:01,639
for the second tree through it to create

1453
01:10:59,659 --> 01:11:03,199
a validation set for that and so

1454
01:11:01,640 --> 01:11:07,010
effectively we would have a different

1455
01:11:03,199 --> 01:11:10,429
validation set for each tree and so now

1456
01:11:07,010 --> 01:11:14,449
to calculate our prediction we would

1457
01:11:10,430 --> 01:11:18,500
average all of the trees where that row

1458
01:11:14,449 --> 01:11:21,019
is not used for training right so for

1459
01:11:18,500 --> 01:11:23,270
tree number one we would have the ones

1460
01:11:21,020 --> 01:11:26,030
I've marked in blue here and then maybe

1461
01:11:23,270 --> 01:11:29,360
for tree number two it turned out it was

1462
01:11:26,029 --> 01:11:31,789
like this one this one this one and this

1463
01:11:29,359 --> 01:11:34,729
one and so forth right so as long as

1464
01:11:31,789 --> 01:11:37,729
you've got enough trees every rows going

1465
01:11:34,729 --> 01:11:39,379
to appear in the out of bag sample from

1466
01:11:37,729 --> 01:11:43,179
one of them at least so you'll be

1467
01:11:39,380 --> 01:11:46,850
averaging you know hopefully a few trees

1468
01:11:43,180 --> 01:11:50,180
so if you've got a hundred trees it's

1469
01:11:46,850 --> 01:11:51,590
very likely that all of the rows are

1470
01:11:50,180 --> 01:11:53,990
going to appear many times in these

1471
01:11:51,590 --> 01:11:55,760
other bag samples so what you can do is

1472
01:11:53,989 --> 01:11:58,340
you can create an out of bag prediction

1473
01:11:55,760 --> 01:12:00,710
by averaging all of the trees you didn't

1474
01:11:58,340 --> 01:12:02,960
use to train each individual row and

1475
01:12:00,710 --> 01:12:06,890
then you can calculate your root mean

1476
01:12:02,960 --> 01:12:10,310
square error R squared etc on that if

1477
01:12:06,890 --> 01:12:13,880
you pass low B score equals true to

1478
01:12:10,310 --> 01:12:21,260
scikit-learn it will do that for you and

1479
01:12:13,880 --> 01:12:23,810
it will create an attribute called oob

1480
01:12:21,260 --> 01:12:25,579
score underscore and so my little print

1481
01:12:23,810 --> 01:12:29,840
score function here if that attribute

1482
01:12:25,579 --> 01:12:32,930
exists it it adds it to the print so if

1483
01:12:29,840 --> 01:12:36,909
you take a look here hoby square equals

1484
01:12:32,930 --> 01:12:40,340
true we've now got one extra number and

1485
01:12:36,909 --> 01:12:43,220
it's R squared that is the R squared for

1486
01:12:40,340 --> 01:12:45,199
the oeob sample it's a square it is very

1487
01:12:43,220 --> 01:12:48,260
similar the R squared and the validation

1488
01:12:45,199 --> 01:12:50,800
set which is what we hoped for can we

1489
01:12:48,260 --> 01:12:50,800
plus it

1490
01:12:51,579 --> 01:12:57,880
is it the case that the prediction for

1491
01:12:55,569 --> 01:12:59,829
the obese core has to be must be

1492
01:12:57,880 --> 01:13:02,289
mathematically lower than the one for

1493
01:12:59,829 --> 01:13:04,720
our entire forest um certainly it's not

1494
01:13:02,289 --> 01:13:09,579
true that the prediction is lower it's

1495
01:13:04,720 --> 01:13:11,619
possible for accuracy yeah um it's not

1496
01:13:09,579 --> 01:13:13,510
mathematically necessary that it's true

1497
01:13:11,619 --> 01:13:15,630
but it's gonna be true on average

1498
01:13:13,510 --> 01:13:18,699
because your average for each row

1499
01:13:15,630 --> 01:13:20,650
appears in less trees in the oeob

1500
01:13:18,699 --> 01:13:24,429
samples and it does in the full set of

1501
01:13:20,649 --> 01:13:27,309
trees so as you see here it's a little

1502
01:13:24,430 --> 01:13:29,920
less good so I'm in general it's a great

1503
01:13:27,310 --> 01:13:33,130
insight Chris in general the OOBE

1504
01:13:29,920 --> 01:13:35,680
r-squared will slightly underestimate

1505
01:13:33,130 --> 01:13:38,829
how generalizable the model is the more

1506
01:13:35,680 --> 01:13:40,539
trees you add the less serious that

1507
01:13:38,829 --> 01:13:43,149
underestimation is and for me in

1508
01:13:40,539 --> 01:13:51,340
practice I I find it's totally good

1509
01:13:43,149 --> 01:13:55,059
enough you know in practice okay so this

1510
01:13:51,340 --> 01:13:57,840
our B score is is is super handy and one

1511
01:13:55,060 --> 01:13:59,650
of the things that super handy for is

1512
01:13:57,840 --> 01:14:01,210
you're gonna see there's quite a few

1513
01:13:59,649 --> 01:14:04,960
hyper parameters that were going to set

1514
01:14:01,210 --> 01:14:08,680
and we would like to find some automated

1515
01:14:04,960 --> 01:14:10,149
way to set them and one way to do that

1516
01:14:08,680 --> 01:14:11,980
is to do what's called a grid search a

1517
01:14:10,149 --> 01:14:14,679
grid search is where if there's a

1518
01:14:11,979 --> 01:14:17,169
scikit-learn function called grid search

1519
01:14:14,680 --> 01:14:18,700
and you pass in the list of all of the

1520
01:14:17,170 --> 01:14:20,770
parameters all of the hyper parameters

1521
01:14:18,699 --> 01:14:23,079
that you want to tune you pass in for

1522
01:14:20,770 --> 01:14:24,700
each one a list of all of the values of

1523
01:14:23,079 --> 01:14:27,850
that hyper parameter you want to try and

1524
01:14:24,699 --> 01:14:29,829
it runs your model on every possible

1525
01:14:27,850 --> 01:14:32,110
combination of all of those hyper

1526
01:14:29,829 --> 01:14:36,760
parameters and tells you which one is

1527
01:14:32,109 --> 01:14:39,399
the best and our B score is a great like

1528
01:14:36,760 --> 01:14:41,199
choice for to forgetting it to Tory

1529
01:14:39,399 --> 01:14:42,909
which one is best in terms of our V

1530
01:14:41,199 --> 01:14:44,529
score like that's an example of

1531
01:14:42,909 --> 01:14:45,800
something you can do with a which works

1532
01:14:44,529 --> 01:14:48,489
well

1533
01:14:45,800 --> 01:14:48,489
now

1534
01:14:52,090 --> 01:14:58,210
if you think about it I kind of did

1535
01:14:55,149 --> 01:15:01,629
something pretty dumb earlier which is I

1536
01:14:58,210 --> 01:15:04,989
took a subset of 30,000 rows of the data

1537
01:15:01,630 --> 01:15:08,020
and it built all my models of that which

1538
01:15:04,989 --> 01:15:11,170
means every tree in my random forest is

1539
01:15:08,020 --> 01:15:16,570
a different subset of that subset of

1540
01:15:11,170 --> 01:15:18,399
30,000 why do that why not take a

1541
01:15:16,569 --> 01:15:22,299
different I like a totally different

1542
01:15:18,399 --> 01:15:24,219
subset of 30,000 each time so in other

1543
01:15:22,300 --> 01:15:27,070
words let's leave the entire 300

1544
01:15:24,220 --> 01:15:29,500
thousand records as is right and if I

1545
01:15:27,069 --> 01:15:31,989
want to make things faster right pick a

1546
01:15:29,500 --> 01:15:33,909
different subset of 30,000 each time so

1547
01:15:31,989 --> 01:15:36,899
rather than bootstrapping the entire set

1548
01:15:33,909 --> 01:15:40,949
of rows that's just randomly sample a

1549
01:15:36,899 --> 01:15:44,019
subset of the data and so we can do that

1550
01:15:40,949 --> 01:15:46,359
so let's go back and recall property yet

1551
01:15:44,020 --> 01:15:48,850
without the subset parameter to get all

1552
01:15:46,359 --> 01:15:56,289
of our data again and so to remind you

1553
01:15:48,850 --> 01:16:01,600
that is okay 400,000 I in the whole data

1554
01:15:56,289 --> 01:16:05,529
frame of which we have three 89,000 in

1555
01:16:01,600 --> 01:16:09,070
our training set and instead we're going

1556
01:16:05,529 --> 01:16:11,409
to go set our F samples 20,000 remember

1557
01:16:09,069 --> 01:16:12,969
that was the site that off the 30,000 we

1558
01:16:11,409 --> 01:16:16,090
use 20,000 of them in our training set

1559
01:16:12,970 --> 01:16:18,460
if I do this there now when I run a

1560
01:16:16,090 --> 01:16:22,449
random forest it's not going to

1561
01:16:18,460 --> 01:16:24,880
bootstrap an entire set of 391 thousand

1562
01:16:22,449 --> 01:16:29,019
rows it's going to just grab a subset of

1563
01:16:24,880 --> 01:16:32,800
20,000 rows all right and so now if I

1564
01:16:29,020 --> 01:16:35,560
run this it will still run just as

1565
01:16:32,800 --> 01:16:38,020
quickly as if I had like originally done

1566
01:16:35,560 --> 01:16:41,620
a random sample of 20,000 but now every

1567
01:16:38,020 --> 01:16:44,440
tree can have access to the whole data

1568
01:16:41,619 --> 01:16:46,840
set right so if I do enough estimators

1569
01:16:44,439 --> 01:16:50,879
enough trees eventually it's going to

1570
01:16:46,840 --> 01:16:54,130
see everything right so in this case

1571
01:16:50,880 --> 01:16:57,970
with 10 trees which is the default I get

1572
01:16:54,130 --> 01:17:00,699
an R squared of 0.8 6 which is actually

1573
01:16:57,970 --> 01:17:03,320
about the same as my R squared with the

1574
01:17:00,699 --> 01:17:04,670
with the 20,000 subsets

1575
01:17:03,319 --> 01:17:06,920
and that's because I haven't used many

1576
01:17:04,670 --> 01:17:09,710
estimators yet right but if I increase

1577
01:17:06,920 --> 01:17:11,389
the number of estimators it's got to

1578
01:17:09,710 --> 01:17:14,380
make more of a difference right so if I

1579
01:17:11,389 --> 01:17:14,380
increase the number of estimators

1580
01:17:16,079 --> 01:17:22,869
two-forty alright it's going to take a

1581
01:17:19,420 --> 01:17:25,270
little bit longer to run but it's going

1582
01:17:22,869 --> 01:17:27,309
to be able to see a larger subset of the

1583
01:17:25,270 --> 01:17:28,660
data set and so as you can see the

1584
01:17:27,310 --> 01:17:32,020
r-squared is gone up from point eight

1585
01:17:28,659 --> 01:17:33,970
six two point eight seven six okay so

1586
01:17:32,020 --> 01:17:35,290
this is actually a great approach and

1587
01:17:33,970 --> 01:17:37,300
for those of you who would doing the

1588
01:17:35,289 --> 01:17:39,850
groceries competition that's got

1589
01:17:37,300 --> 01:17:41,949
something like 120 million roads but

1590
01:17:39,850 --> 01:17:45,010
there's no way you would want to create

1591
01:17:41,949 --> 01:17:46,988
a random forest using 128 million roads

1592
01:17:45,010 --> 01:17:50,440
in every tree like it's going to take

1593
01:17:46,988 --> 01:17:52,299
forever so what you could do is use this

1594
01:17:50,439 --> 01:17:54,460
set area of samples to do like I don't

1595
01:17:52,300 --> 01:17:56,710
know a hundred thousand or a million

1596
01:17:54,460 --> 01:17:58,689
I'll play around with it so the trick

1597
01:17:56,710 --> 01:18:01,510
here is that with a random forest using

1598
01:17:58,689 --> 01:18:03,129
this technique no data set is too big I

1599
01:18:01,510 --> 01:18:06,489
don't care if it's got a hundred billion

1600
01:18:03,130 --> 01:18:08,020
rows right you can create a bunch of

1601
01:18:06,488 --> 01:18:10,988
trees each one of the different random

1602
01:18:08,020 --> 01:18:13,350
subsets can somebody pass the actual

1603
01:18:10,988 --> 01:18:13,349
accuracy

1604
01:18:26,819 --> 01:18:33,039
so my question was for the albey scores

1605
01:18:29,739 --> 01:18:37,000
and these ones does it take the only

1606
01:18:33,039 --> 01:18:38,859
like for the ones from the sample or

1607
01:18:37,000 --> 01:18:40,140
take from all the that's a great

1608
01:18:38,859 --> 01:18:45,009
question

1609
01:18:40,140 --> 01:18:46,690
so unfortunately scikit-learn does not

1610
01:18:45,010 --> 01:18:50,949
support this functionality out of the

1611
01:18:46,689 --> 01:18:52,419
box so I had to write this and it's kind

1612
01:18:50,949 --> 01:18:55,029
of a horrible hack right because we're

1613
01:18:52,420 --> 01:18:57,159
much rather be passing in like a sample

1614
01:18:55,029 --> 01:18:58,960
size parameter rather than doing this

1615
01:18:57,159 --> 01:19:03,010
kind of setting up here so what I

1616
01:18:58,960 --> 01:19:05,230
actually do is if you look at the source

1617
01:19:03,010 --> 01:19:06,820
code is I'm actually this is a internal

1618
01:19:05,229 --> 01:19:08,229
this is the internal function I looked

1619
01:19:06,819 --> 01:19:10,509
at their source code that they call and

1620
01:19:08,229 --> 01:19:13,409
I've replaced it with a with a lambda

1621
01:19:10,510 --> 01:19:16,239
function that has the behavior we want

1622
01:19:13,409 --> 01:19:21,489
unfortunately the current version is not

1623
01:19:16,239 --> 01:19:25,750
changing how our B is calculated so yeah

1624
01:19:21,489 --> 01:19:28,389
so currently low B scores and set our F

1625
01:19:25,750 --> 01:19:30,460
samples are not compatible with each

1626
01:19:28,390 --> 01:19:35,590
other so you need to turn our B equals

1627
01:19:30,460 --> 01:19:38,350
false if you use this approach which I

1628
01:19:35,590 --> 01:19:40,900
hope to fix but at this stage it's it's

1629
01:19:38,350 --> 01:19:45,220
not fixed so if you want to turn it off

1630
01:19:40,899 --> 01:19:48,869
you just call reset our F samples okay

1631
01:19:45,220 --> 01:19:48,869
and that returns it back to what it was

1632
01:19:51,449 --> 01:20:02,309
okay so in practice when I'm like

1633
01:19:59,000 --> 01:20:04,079
doing interactive machine learning using

1634
01:20:02,310 --> 01:20:06,870
random forests in order to like explore

1635
01:20:04,079 --> 01:20:07,800
my model explore hyper parameters the

1636
01:20:06,869 --> 01:20:10,109
stuff we're going to learn in the future

1637
01:20:07,800 --> 01:20:11,640
lesson where we actually analyze like

1638
01:20:10,109 --> 01:20:14,750
feature importance and partial

1639
01:20:11,640 --> 01:20:19,760
dependence and so forth I generally use

1640
01:20:14,750 --> 01:20:21,779
subsets and reasonably small forests

1641
01:20:19,760 --> 01:20:23,310
because all the insights that I'm going

1642
01:20:21,779 --> 01:20:25,500
to get are exactly the same as the big

1643
01:20:23,310 --> 01:20:28,770
ones but I can run it in like you know

1644
01:20:25,500 --> 01:20:31,319
three or four seconds rather than hours

1645
01:20:28,770 --> 01:20:34,740
alright so this is one of the biggest

1646
01:20:31,319 --> 01:20:37,380
tips I can give you and very very few

1647
01:20:34,739 --> 01:20:40,260
people in industry or academia actually

1648
01:20:37,380 --> 01:20:42,180
do this most people run all of their

1649
01:20:40,260 --> 01:20:43,829
models on all of the data all of the

1650
01:20:42,180 --> 01:20:46,380
time using their best possible

1651
01:20:43,829 --> 01:20:47,729
parameters which is just pointless right

1652
01:20:46,380 --> 01:20:49,140
if you're trying to find out like which

1653
01:20:47,729 --> 01:20:51,319
features are important and how are they

1654
01:20:49,140 --> 01:20:53,550
related to each other and so forth

1655
01:20:51,319 --> 01:20:55,170
having that fourth decimal place of

1656
01:20:53,550 --> 01:20:57,360
accuracy isn't going to change any of

1657
01:20:55,170 --> 01:20:59,640
your insights at all okay so I would say

1658
01:20:57,359 --> 01:21:02,069
like do most of your models on you know

1659
01:20:59,640 --> 01:21:06,060
a large enough sample size that your

1660
01:21:02,069 --> 01:21:08,789
accuracy is you know reasonable when I

1661
01:21:06,060 --> 01:21:10,500
say reasonable it's like within a

1662
01:21:08,789 --> 01:21:13,350
reasonable distance of the best accuracy

1663
01:21:10,500 --> 01:21:15,930
you can get and it's taking you know a

1664
01:21:13,350 --> 01:21:18,800
small number of seconds to train so that

1665
01:21:15,930 --> 01:21:21,300
you can interactively do your analysis

1666
01:21:18,800 --> 01:21:22,289
so there's a couple more parameters I

1667
01:21:21,300 --> 01:21:24,329
wanted to talk about so I'm going to

1668
01:21:22,289 --> 01:21:27,149
call reset RF samples to get back to our

1669
01:21:24,329 --> 01:21:28,350
full data set because in this case at

1670
01:21:27,149 --> 01:21:32,489
least on this computer it's actually

1671
01:21:28,350 --> 01:21:36,690
running in less than 10 seconds so

1672
01:21:32,489 --> 01:21:40,380
here's our baseline we're going to do a

1673
01:21:36,689 --> 01:21:42,539
baseline with 40 estimators okay and so

1674
01:21:40,380 --> 01:21:44,670
each of those 40 estimators is going to

1675
01:21:42,539 --> 01:21:50,779
Train all the way down to all the leaf

1676
01:21:44,670 --> 01:21:50,779
nodes just have one sample in them

1677
01:21:51,038 --> 01:21:56,920
so that's going to take a few seconds to

1678
01:21:54,038 --> 01:22:00,698
run here we go so that gets us a point

1679
01:21:56,920 --> 01:22:04,118
eight nine eight a sweat on the

1680
01:22:00,698 --> 01:22:07,149
validation set or 0.90 eight on the oeob

1681
01:22:04,118 --> 01:22:07,479
now this case the OB is better why is it

1682
01:22:07,149 --> 01:22:08,649
better

1683
01:22:07,479 --> 01:22:10,598
well that's because remember our

1684
01:22:08,649 --> 01:22:13,420
validation set is not a random sample

1685
01:22:10,599 --> 01:22:15,880
our validation set is a different time

1686
01:22:13,420 --> 01:22:19,510
period okay so it's actually much harder

1687
01:22:15,880 --> 01:22:21,038
to predict a different time period than

1688
01:22:19,510 --> 01:22:23,980
this one which is just predicting random

1689
01:22:21,038 --> 01:22:27,578
okay so that's why this is not the way

1690
01:22:23,979 --> 01:22:29,259
around we expected so the next the first

1691
01:22:27,578 --> 01:22:32,109
parameter we can try fiddling with is

1692
01:22:29,260 --> 01:22:36,340
min samples leaf and so min samples leaf

1693
01:22:32,109 --> 01:22:42,880
says stop training the tree further when

1694
01:22:36,340 --> 01:22:44,739
your leaf node has three or less samples

1695
01:22:42,880 --> 01:22:46,569
in there are going all the way down

1696
01:22:44,738 --> 01:22:50,169
until there's one we're going to go down

1697
01:22:46,569 --> 01:22:51,759
until there's three so in practice this

1698
01:22:50,170 --> 01:22:54,719
means there's going to be like one or

1699
01:22:51,760 --> 01:22:57,789
two less levels of decision being made

1700
01:22:54,719 --> 01:22:59,800
which means we've got like half the

1701
01:22:57,788 --> 01:23:01,059
number of actual decision criteria we

1702
01:22:59,800 --> 01:23:03,610
have to do is it's going to train more

1703
01:23:01,059 --> 01:23:05,619
quickly it means that when we look at an

1704
01:23:03,609 --> 01:23:08,018
individual tree rather than just taking

1705
01:23:05,618 --> 01:23:08,859
one point we're taking the average of at

1706
01:23:08,019 --> 01:23:10,840
least three points

1707
01:23:08,859 --> 01:23:12,819
that's who would expect the trees to

1708
01:23:10,840 --> 01:23:15,279
generalize each one to generalize a

1709
01:23:12,819 --> 01:23:17,109
little bit better okay but each tree is

1710
01:23:15,279 --> 01:23:20,559
probably going to be slightly less

1711
01:23:17,109 --> 01:23:23,799
powerful on its own so let's try

1712
01:23:20,559 --> 01:23:26,380
training that so possible values of min

1713
01:23:23,800 --> 01:23:32,949
samples leaf I find ones which work well

1714
01:23:26,380 --> 01:23:35,529
are kind of 1 3 5 10 25 you know like I

1715
01:23:32,948 --> 01:23:38,379
find that kind of range seems to work

1716
01:23:35,529 --> 01:23:40,809
well but like sometimes if you've got a

1717
01:23:38,380 --> 01:23:43,510
really big data set and you're not using

1718
01:23:40,809 --> 01:23:46,420
the small samples you know you might

1719
01:23:43,510 --> 01:23:48,309
need a mint samples leaf of hundreds or

1720
01:23:46,420 --> 01:23:50,050
thousands so it's you kind of got to

1721
01:23:48,309 --> 01:23:51,909
think about like

1722
01:23:50,050 --> 01:23:53,349
how bigger your sub-samples going

1723
01:23:51,908 --> 01:23:56,348
through and try things out now in this

1724
01:23:53,349 --> 01:23:59,920
case going from the default of one to

1725
01:23:56,349 --> 01:24:02,679
three has increased our validation set

1726
01:23:59,920 --> 01:24:04,328
our squared from 898 to 902 so it's a

1727
01:24:02,679 --> 01:24:07,328
slight improvement okay and it's going

1728
01:24:04,328 --> 01:24:09,519
to train a little faster as well as

1729
01:24:07,328 --> 01:24:10,988
something else you can try which is and

1730
01:24:09,520 --> 01:24:13,420
since this worked I'm going to leave

1731
01:24:10,988 --> 01:24:14,319
that in I'm going to add in max features

1732
01:24:13,420 --> 01:24:18,099
equals point five

1733
01:24:14,319 --> 01:24:21,729
why does max features do well the idea

1734
01:24:18,099 --> 01:24:24,880
is that the less correlated your trees

1735
01:24:21,729 --> 01:24:30,218
are with each other the better now

1736
01:24:24,880 --> 01:24:31,270
imagine you had one column that was so

1737
01:24:30,219 --> 01:24:33,309
much better than all of the other

1738
01:24:31,270 --> 01:24:35,349
columns of being predictive that every

1739
01:24:33,309 --> 01:24:37,779
single tree you built regardless of like

1740
01:24:35,349 --> 01:24:39,969
which subset of rows always started with

1741
01:24:37,779 --> 01:24:42,309
that column so the trees are all going

1742
01:24:39,969 --> 01:24:45,279
to be pretty similar right but you can

1743
01:24:42,309 --> 01:24:47,829
imagine there might be some interaction

1744
01:24:45,279 --> 01:24:50,198
of variables where that interaction is

1745
01:24:47,828 --> 01:24:54,308
more important than that individual

1746
01:24:50,198 --> 01:24:56,649
column so if every tree always fits on

1747
01:24:54,309 --> 01:24:58,449
the first thing the same thing the first

1748
01:24:56,649 --> 01:25:01,210
time you're not going to get much

1749
01:24:58,448 --> 01:25:05,408
variation in those trees so what we do

1750
01:25:01,210 --> 01:25:09,429
is in addition to just taking a subset

1751
01:25:05,408 --> 01:25:13,049
of rows we then at every single split

1752
01:25:09,429 --> 01:25:15,940
point take a different subset of columns

1753
01:25:13,050 --> 01:25:19,210
so it's slightly different to the row

1754
01:25:15,939 --> 01:25:21,759
sampling for the row sampling H nu tree

1755
01:25:19,210 --> 01:25:25,899
is based on a random set of rows or

1756
01:25:21,760 --> 01:25:28,659
columns sampling every individual binary

1757
01:25:25,899 --> 01:25:32,920
split we choose from a different subset

1758
01:25:28,658 --> 01:25:35,618
of columns so in other words rather than

1759
01:25:32,920 --> 01:25:38,230
looking at every possible level of every

1760
01:25:35,618 --> 01:25:40,899
possible column we look at every

1761
01:25:38,229 --> 01:25:42,729
possible level of a random subset of

1762
01:25:40,899 --> 01:25:46,719
columns okay

1763
01:25:42,729 --> 01:25:48,729
and each time each decision point each

1764
01:25:46,719 --> 01:25:53,109
binary split we use a different random

1765
01:25:48,729 --> 01:25:56,738
subset how many well you get to pick 0.5

1766
01:25:53,109 --> 01:26:00,000
means randomly choose half of them

1767
01:25:56,738 --> 01:26:02,529
the default is to use all of them

1768
01:26:00,000 --> 01:26:07,960
there's also a couple of special values

1769
01:26:02,529 --> 01:26:10,119
you can use here as you can see in max

1770
01:26:07,960 --> 01:26:12,189
features you can also pass in square

1771
01:26:10,119 --> 01:26:15,399
root to get square root of features or

1772
01:26:12,189 --> 01:26:19,629
log two to get log two in features so in

1773
01:26:15,399 --> 01:26:24,579
practice good values I found our range

1774
01:26:19,630 --> 01:26:26,230
from 1.5 log to or square root that's

1775
01:26:24,579 --> 01:26:28,600
going to give you a nice bit of

1776
01:26:26,229 --> 01:26:33,849
variation right can somebody positive

1777
01:26:28,600 --> 01:26:36,400
Danielle and so just to clarify does

1778
01:26:33,850 --> 01:26:38,230
that just like break it up smaller each

1779
01:26:36,399 --> 01:26:40,179
time it goes through the tree or is it

1780
01:26:38,229 --> 01:26:41,709
just taking half of what's left over or

1781
01:26:40,180 --> 01:26:43,450
like hasn't been touched each time

1782
01:26:41,710 --> 01:26:46,810
there's no such thing as what's left

1783
01:26:43,449 --> 01:26:49,779
over after you've split on year made

1784
01:26:46,810 --> 01:26:52,060
less than or greater than 1984 you made

1785
01:26:49,779 --> 01:26:53,710
still there right so later on you might

1786
01:26:52,060 --> 01:26:57,280
then spit on your maid left Center

1787
01:26:53,710 --> 01:26:59,829
greater than 1989 so so it's just each

1788
01:26:57,279 --> 01:27:01,539
time rather than checking every variable

1789
01:26:59,829 --> 01:27:04,149
to see where it's best fit is you just

1790
01:27:01,539 --> 01:27:05,529
check half of them and so the next time

1791
01:27:04,149 --> 01:27:07,449
you check a different hops the next time

1792
01:27:05,529 --> 01:27:10,449
you took a different path but I mean

1793
01:27:07,449 --> 01:27:14,710
like terms is as you got like further to

1794
01:27:10,449 --> 01:27:17,439
like the leaf you're gonna have less

1795
01:27:14,710 --> 01:27:19,060
options no you're not you'd never remove

1796
01:27:17,439 --> 01:27:20,949
the variables okay you can use them

1797
01:27:19,060 --> 01:27:23,719
again and again and again because you've

1798
01:27:20,949 --> 01:27:25,909
got lots of different spit points

1799
01:27:23,719 --> 01:27:27,579
so imagine for example that the

1800
01:27:25,909 --> 01:27:31,309
relationship was just entirely linear

1801
01:27:27,579 --> 01:27:33,829
between year made and price right then

1802
01:27:31,310 --> 01:27:36,500
in practice to actually model that you

1803
01:27:33,829 --> 01:27:41,300
know your real relationship is year made

1804
01:27:36,500 --> 01:27:42,948
versus price right but the best we could

1805
01:27:41,300 --> 01:27:46,310
do would be this kind of first of all

1806
01:27:42,948 --> 01:27:49,009
split here right and then just split

1807
01:27:46,310 --> 01:27:53,360
here and here right and like spitting

1808
01:27:49,010 --> 01:27:56,090
spitting split so yeah even if the

1809
01:27:53,359 --> 01:27:58,309
binary most random forest libraries

1810
01:27:56,090 --> 01:27:59,750
don't do anything special about that

1811
01:27:58,310 --> 01:28:01,940
they just kind of go okay we'll try this

1812
01:27:59,750 --> 01:28:04,399
variable oh it turns out there's only

1813
01:28:01,939 --> 01:28:06,500
one level left you know so yeah that

1814
01:28:04,399 --> 01:28:13,819
didn't they don't do any kind of clever

1815
01:28:06,500 --> 01:28:16,189
bookkeeping okay okay so if we add next

1816
01:28:13,819 --> 01:28:20,689
features equals 0.5 it goes up from 901

1817
01:28:16,189 --> 01:28:21,710
the 906 so that's better still and so as

1818
01:28:20,689 --> 01:28:23,089
we've been doing this we've also

1819
01:28:21,710 --> 01:28:25,369
hopefully have noticed that our root

1820
01:28:23,090 --> 01:28:28,489
mean squared error log price has been

1821
01:28:25,369 --> 01:28:33,859
dropping on a validation set as well and

1822
01:28:28,488 --> 01:28:36,979
so it's now down to 0.2 to 86 so how

1823
01:28:33,859 --> 01:28:38,630
good is that right so like our totally

1824
01:28:36,979 --> 01:28:40,879
untrue and random florists got us in

1825
01:28:38,630 --> 01:28:44,630
about the top 25 percent now remember

1826
01:28:40,880 --> 01:28:46,579
our validation set isn't identical to

1827
01:28:44,630 --> 01:28:48,529
the Kaggle test set right and this

1828
01:28:46,579 --> 01:28:51,649
competition unfortunately is old enough

1829
01:28:48,529 --> 01:28:54,439
that you can't even put in a kind of

1830
01:28:51,649 --> 01:28:55,849
after this after the time entry to find

1831
01:28:54,439 --> 01:28:57,859
out how you would have gone so we can

1832
01:28:55,850 --> 01:28:59,660
only approximate how we could have gone

1833
01:28:57,859 --> 01:29:01,539
but you know generally speaking is going

1834
01:28:59,659 --> 01:29:04,119
to be a pretty good approximation so

1835
01:29:01,539 --> 01:29:06,260
2:286

1836
01:29:04,119 --> 01:29:08,979
here is the competition here's the

1837
01:29:06,260 --> 01:29:08,980
public leaderboard

1838
01:29:10,329 --> 01:29:18,340
two two eight six here we go

1839
01:29:14,380 --> 01:29:19,119
14th or 15th place so you know roughly

1840
01:29:18,340 --> 01:29:20,920
speaking

1841
01:29:19,119 --> 01:29:24,340
looks like we would be about in the top

1842
01:29:20,920 --> 01:29:26,350
20 of this competition with basically

1843
01:29:24,340 --> 01:29:28,510
totally brainless random forest with

1844
01:29:26,350 --> 01:29:34,329
some totally brainless minor hyper

1845
01:29:28,510 --> 01:29:35,890
parameter tuning and so this is kind of

1846
01:29:34,329 --> 01:29:39,100
why the random forest is such an

1847
01:29:35,890 --> 01:29:41,020
important not just first step but often

1848
01:29:39,100 --> 01:29:43,110
only step from the Xin learning because

1849
01:29:41,020 --> 01:29:45,160
it's kind of hard to screw it up like

1850
01:29:43,109 --> 01:29:46,859
even when we didn't tune the hyper

1851
01:29:45,159 --> 01:29:48,970
parameters we still got a good result

1852
01:29:46,859 --> 01:29:50,259
and then a small amount of type of

1853
01:29:48,970 --> 01:29:54,159
parameter tuning got us a much better

1854
01:29:50,260 --> 01:29:56,020
result and so any kind of model so and

1855
01:29:54,159 --> 01:29:59,019
I'm particularly thinking of like linear

1856
01:29:56,020 --> 01:30:00,940
type models which have a whole bunch of

1857
01:29:59,020 --> 01:30:02,410
statistical assumptions and you have to

1858
01:30:00,939 --> 01:30:05,649
get a whole bunch of things right before

1859
01:30:02,409 --> 01:30:07,510
they start to work at all can really

1860
01:30:05,649 --> 01:30:09,519
throw you off track right because they

1861
01:30:07,510 --> 01:30:11,230
give you like totally wrong answers

1862
01:30:09,520 --> 01:30:14,470
about how accurate the predictions can

1863
01:30:11,229 --> 01:30:17,500
be but also the random forest you know

1864
01:30:14,470 --> 01:30:19,420
generally speaking they tend to work on

1865
01:30:17,500 --> 01:30:21,640
most data sets most of the time with

1866
01:30:19,420 --> 01:30:30,760
most sets of hyper parameters so for

1867
01:30:21,640 --> 01:30:32,380
example we did this thing with it with

1868
01:30:30,760 --> 01:30:39,960
our categorical variables in fact let's

1869
01:30:32,380 --> 01:30:41,590
take a look at our tree a single tree

1870
01:30:39,960 --> 01:30:46,810
look at this right

1871
01:30:41,590 --> 01:30:58,710
fi product class desc less than 7.5 what

1872
01:30:46,810 --> 01:31:02,289
does that mean so f I product plus desc

1873
01:30:58,710 --> 01:31:03,909
here's some examples of that column all

1874
01:31:02,289 --> 01:31:05,109
right so what does it mean to be less

1875
01:31:03,909 --> 01:31:08,680
than or equal to 7

1876
01:31:05,109 --> 01:31:12,759
well we'd have to look at dark cat dark

1877
01:31:08,680 --> 01:31:18,190
categories to find out okay and so it's

1878
01:31:12,760 --> 01:31:20,170
0 1 2 3 4 5 6 7 so what it's done is its

1879
01:31:18,189 --> 01:31:23,349
created a split where all of the backhoe

1880
01:31:20,170 --> 01:31:24,099
loaders and these three types of

1881
01:31:23,350 --> 01:31:26,440
hydraulics

1882
01:31:24,099 --> 01:31:29,500
enter in one group and everything else

1883
01:31:26,439 --> 01:31:32,979
is in the other group so like that's

1884
01:31:29,500 --> 01:31:35,229
like weird you know like like these

1885
01:31:32,979 --> 01:31:37,089
aren't even in order we could have made

1886
01:31:35,229 --> 01:31:39,638
them in order if we had you know bother

1887
01:31:37,090 --> 01:31:43,239
to say the categories have this order

1888
01:31:39,639 --> 01:31:46,300
but we hadn't right so how come this

1889
01:31:43,238 --> 01:31:56,589
even works like because when we turn it

1890
01:31:46,300 --> 01:32:00,449
into codes it's actually this is

1891
01:31:56,590 --> 01:32:04,060
actually what the random forest sees and

1892
01:32:00,448 --> 01:32:06,460
so imagine to think about this imagine

1893
01:32:04,060 --> 01:32:08,949
like the only thing that mattered was

1894
01:32:06,460 --> 01:32:10,868
whether as a hydraulic excavator at zero

1895
01:32:08,948 --> 01:32:12,819
to two metric tons and nothing else

1896
01:32:10,868 --> 01:32:16,059
mattered imagine that right so it has to

1897
01:32:12,819 --> 01:32:18,460
pick out this this single level well it

1898
01:32:16,060 --> 01:32:20,170
can do that because first of all it

1899
01:32:18,460 --> 01:32:22,239
could say okay let's pick out everything

1900
01:32:20,170 --> 01:32:25,090
less than seven versus greater than

1901
01:32:22,238 --> 01:32:27,968
seven to create you know this is one

1902
01:32:25,090 --> 01:32:30,130
group and this is another group right

1903
01:32:27,969 --> 01:32:32,529
and then within this group they could

1904
01:32:30,130 --> 01:32:34,389
then pick out everything less than six

1905
01:32:32,529 --> 01:32:36,609
versus greater than six we're just going

1906
01:32:34,389 --> 01:32:39,010
to pick out this one item right so with

1907
01:32:36,609 --> 01:32:42,639
two split points we can pull out a

1908
01:32:39,010 --> 01:32:45,070
single category so this is why it works

1909
01:32:42,639 --> 01:32:47,650
right is because the tree is like

1910
01:32:45,069 --> 01:32:49,899
infinitely flexible even with a

1911
01:32:47,649 --> 01:32:51,789
categorical variable if there's

1912
01:32:49,899 --> 01:32:54,179
particular categories which have

1913
01:32:51,789 --> 01:32:57,010
different levels of price it can like

1914
01:32:54,179 --> 01:33:00,099
gradually zoom in on those groups by

1915
01:32:57,010 --> 01:33:02,650
using multiple splits right now you can

1916
01:33:00,099 --> 01:33:04,599
help it by telling it the order of your

1917
01:33:02,649 --> 01:33:07,420
categorical variable but even if you

1918
01:33:04,599 --> 01:33:10,000
don't it's okay it's just going to take

1919
01:33:07,420 --> 01:33:12,368
a few more decisions to get there and so

1920
01:33:10,000 --> 01:33:14,590
you can see here it's actually using

1921
01:33:12,368 --> 01:33:18,670
this product class desk quite a few

1922
01:33:14,590 --> 01:33:21,610
times right and and as you go deeper

1923
01:33:18,670 --> 01:33:23,440
down the tree you'll see it used more

1924
01:33:21,609 --> 01:33:25,809
and more right where else in a linear

1925
01:33:23,439 --> 01:33:29,198
model or almost any kind of other model

1926
01:33:25,810 --> 01:33:32,199
certainly any any non tree model pretty

1927
01:33:29,198 --> 01:33:34,210
much encoding a categorical variable

1928
01:33:32,198 --> 01:33:36,549
like this won't work at all because

1929
01:33:34,210 --> 01:33:37,649
there's no linear relationship to train

1930
01:33:36,550 --> 01:33:41,710
totally arbitrary

1931
01:33:37,649 --> 01:33:43,119
identifiers and anything right so so

1932
01:33:41,710 --> 01:33:45,539
these are the kinds of things that make

1933
01:33:43,119 --> 01:33:49,659
Brennan forests very easy to use and and

1934
01:33:45,539 --> 01:33:51,429
very resilient and so by using that you

1935
01:33:49,659 --> 01:33:56,829
know we've gotten ourselves a model

1936
01:33:51,429 --> 01:33:58,480
which is clearly you know world class at

1937
01:33:56,829 --> 01:34:00,219
this point already it's like you know

1938
01:33:58,479 --> 01:34:02,229
probably will in the top 20 of this

1939
01:34:00,219 --> 01:34:07,050
cackle competition and then in our next

1940
01:34:02,229 --> 01:34:07,049
lesson we're going to learn about how to

1941
01:34:07,649 --> 01:34:13,809
analyze that model to learn more about

1942
01:34:10,060 --> 01:34:17,770
the data to make it even better right so

1943
01:34:13,810 --> 01:34:19,870
this week try and like really experiment

1944
01:34:17,770 --> 01:34:22,000
right have a look inside look try and

1945
01:34:19,869 --> 01:34:24,670
draw the trees try and plot the

1946
01:34:22,000 --> 01:34:26,939
different errors try maybe using

1947
01:34:24,670 --> 01:34:29,770
different data sets to see how they work

1948
01:34:26,939 --> 01:34:32,379
really experiment to try to get a sense

1949
01:34:29,770 --> 01:34:35,230
and maybe try to like replicate things

1950
01:34:32,380 --> 01:34:36,429
like write your own r-squared you know

1951
01:34:35,229 --> 01:34:39,428
write your own versions or some of these

1952
01:34:36,429 --> 01:34:41,050
functions see if ya see how much you can

1953
01:34:39,429 --> 01:34:43,420
really learn about your data set about

1954
01:34:41,050 --> 01:34:45,779
the random person great see you on

1955
01:34:43,420 --> 01:34:45,779
Thursday

1956
01:34:49,840 --> 01:34:52,449
[Music]

