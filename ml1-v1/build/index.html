<p><strong>Lesson 01</strong></p>
<ul>
<li>
<p><a href="https://youtu.be/CzdWqFTmn0Y?t=2m14s">00:02:14</a> AWS or Crestle Deep Learning</p>
</li>
<li>
<p><a href="https://youtu.be/CzdWqFTmn0Y?t=5m14s">00:05:14</a> lesson1-rf notebook Random Forests</p>
</li>
<li>
<p><a href="https://youtu.be/CzdWqFTmn0Y?t=10m14s">00:10:14<span class="badge badge-notification clicks" title="1 click">1</span></a> <code>?display</code> documentation, <code>??display</code> source code</p>
</li>
<li>
<p><a href="https://youtu.be/CzdWqFTmn0Y?t=12m14s">00:12:14</a> Blue Book for Bulldozers Kaggle competition: predict auction sale price,<br>
Download Kaggle data to AWS using a nice trick with FireFox javascript console, getting a full cURL link,<br>
Using Jupyter “New Terminal”</p>
</li>
<li>
<p><a href="https://youtu.be/CzdWqFTmn0Y?t=23m55s">00:23:55</a> using <code>!ls {PATH}</code> in Jupyter Notebook</p>
</li>
<li>
<p><a href="https://youtu.be/CzdWqFTmn0Y?t=26m14s">00:26:14</a> Structured Data vs data like Computer Vision, NLP, Audio,<br>
‘<span style=""><span id="MathJax-Element-1-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 126%;"><span id="MJXc-Node-1" class="mjx-math"><span id="MJXc-Node-2" class="mjx-mrow"><span id="MJXc-Node-3" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.23em; padding-bottom: 0.286em;">v</span></span><span id="MJXc-Node-4" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.456em; padding-bottom: 0.286em;">i</span></span><span id="MJXc-Node-5" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.23em; padding-bottom: 0.286em;">m</span></span><span id="MJXc-Node-6" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.456em; padding-bottom: 0.286em;">i</span></span><span id="MJXc-Node-7" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.23em; padding-bottom: 0.286em;">m</span></span><span id="MJXc-Node-8" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.23em; padding-bottom: 0.456em;">p</span></span><span id="MJXc-Node-9" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.23em; padding-bottom: 0.286em;">o</span></span><span id="MJXc-Node-10" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.23em; padding-bottom: 0.286em;">r</span></span><span id="MJXc-Node-11" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.4em; padding-bottom: 0.286em;">t</span></span><span id="MJXc-Node-12" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.23em; padding-bottom: 0.286em;">s</span></span><span id="MJXc-Node-13" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.167em; padding-bottom: 0.343em;">.</span></span><span id="MJXc-Node-14" class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.23em; padding-bottom: 0.456em;">p</span></span><span id="MJXc-Node-15" class="mjx-msup"><span class="mjx-base" style="margin-right: -0.006em;"><span id="MJXc-Node-16" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.23em; padding-bottom: 0.456em; padding-right: 0.006em;">y</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.082em; padding-right: 0.071em;"><span id="MJXc-Node-17" class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.286em; padding-bottom: 0.343em;">′</span></span></span></span><span id="MJXc-Node-18" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.456em; padding-bottom: 0.286em;">i</span></span><span id="MJXc-Node-19" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.23em; padding-bottom: 0.286em;">n</span></span><span id="MJXc-Node-20" class="mjx-texatom"><span id="MJXc-Node-21" class="mjx-mrow"><span id="MJXc-Node-22" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.456em; padding-bottom: 0.57em;">/</span></span></span></span><span id="MJXc-Node-23" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.456em; padding-bottom: 0.456em; padding-right: 0.06em;">f</span></span><span id="MJXc-Node-24" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.23em; padding-bottom: 0.286em;">a</span></span><span id="MJXc-Node-25" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.23em; padding-bottom: 0.286em;">s</span></span><span id="MJXc-Node-26" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.4em; padding-bottom: 0.286em;">t</span></span><span id="MJXc-Node-27" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.23em; padding-bottom: 0.286em;">a</span></span><span id="MJXc-Node-28" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.456em; padding-bottom: 0.286em;">i</span></span></span></span></span><script type="math/tex" id="MathJax-Element-1">/* Code removed by ScrapBook */</script></span>, ‘low_memory=False’, ‘parse_dates’,<br>
Python 3.6 format string f’{PATH}Train.csv’,<br>
‘display_all()’</p>
</li>
<li>
<p><a href="https://youtu.be/CzdWqFTmn0Y?t=33m14s">00:33:14</a> Why Jeremy’s doesn’t do a lot of EDA,<br>
Bulldozer RMSLE difference between the log of prices</p>
</li>
<li>
<p><a href="https://youtu.be/CzdWqFTmn0Y?t=36m14s">00:36:14</a> Intro to Random Forests, in general doesn’t overfit, no need to setup a validation set.<br>
The Silly Concepts of Cursive Dimensionality and No Free Lunch theorem,<br>
Brief history of ML and lots of theory vs practice in the 90’s.</p>
</li>
<li>
<p><a href="https://youtu.be/CzdWqFTmn0Y?t=43m14s">00:43:14</a> RandomForestRegressor, RandomForestClassifier<br>
Stack Trace: how to fix an error</p>
</li>
<li>
<p><a href="https://youtu.be/CzdWqFTmn0Y?t=48m14s">00:48:14</a> Continuous and categorical variables, <code>add_datepart()</code></p>
</li>
<li>
<p><a href="https://youtu.be/CzdWqFTmn0Y?t=57m14s">00:57:14</a> Dealing with strings in data (“low, medium, high” etc.), which must be converted into numeric coding, with train_cats() creating a mapping of integers to the strings.<br>
Warning: make sure to use the same mapping string-numbers in Training and Test sets,<br>
Use “apply_cats” for that,<br>
Change order of index of .cat.categories with .cat.set_categories.</p>
</li>
<li>
<p><a href="https://youtu.be/CzdWqFTmn0Y?t=1h7m14s">01:07:14<span class="badge badge-notification clicks" title="1 click">1</span></a> Pre-processing to replace categories with their numeric codes,<br>
Handle missing continuous values,<br>
And split the dependant variable into a separate variable.<br>
proc_df() and fix_missing()</p>
</li>
<li>
<p><a href="https://youtu.be/CzdWqFTmn0Y?t=1h14m1s">01:14:01<span class="badge badge-notification clicks" title="1 click">1</span></a> ‘split_vals()’<br>
<br>
</p>
</li>
</ul>
<p><strong>Lesson 02</strong></p>
<ul>
<li>
<p><a href="https://youtu.be/blyXCk4sgEg?t=3m30s">00:03:30</a> simlink sim link to fastai directory</p>
</li>
<li>
<p><a href="https://youtu.be/blyXCk4sgEg?t=6m15s">00:06:15</a> understand the RMSLE relation to RMSE, and why use np.log(‘SalePrice’) with RMSE as a result</p>
</li>
<li>
<p><a href="https://youtu.be/blyXCk4sgEg?t=9m1s">00:09:01</a> proc_df, numericalize</p>
</li>
<li>
<p><a href="https://youtu.be/blyXCk4sgEg?t=11m1s">00:11:01<span class="badge badge-notification clicks" title="1 click">1</span></a> rsquare root square of mean errors RMSE,<br>
What the formula rsquare (and others in general) does and understand it</p>
</li>
<li>
<p><a href="https://youtu.be/blyXCk4sgEg?t=17m30s">00:17:30</a> Creating a good validation set, ‘split_vals()’ explained<br>
“I don’t trust ML, we tried it, it looked great, we put it in production, it didn’t work” because the validation set was not representative !</p>
</li>
<li>
<p><a href="https://youtu.be/blyXCk4sgEg?t=21m1s">00:21:01</a> overfitting over-fitting underfitting ‘don’t look at test set !’,<br>
Example of failed methodology in sociology, psychology,<br>
Hyperparameters,<br>
Using PEP8 (or not) for ML prototyping models</p>
</li>
<li>
<p><a href="https://youtu.be/blyXCk4sgEg?t=29m1s">00:29:01</a> RMSE function and RandomForestRegressor,<br>
Speeding things up with a smaller dataset (subset = ),<br>
Use of ‘_’ underscore in Python</p>
</li>
<li>
<p><a href="https://youtu.be/blyXCk4sgEg?t=32m1s">00:32:01</a> Single Tree model and visualize it,<br>
max_depth=3,<br>
bootstrap=False</p>
</li>
<li>
<p><a href="https://youtu.be/blyXCk4sgEg?t=47m1s">00:47:01</a> Bagging of little Boostraps, ensembling</p>
</li>
<li>
<p><a href="https://youtu.be/blyXCk4sgEg?t=57m1s">00:57:01</a> scikit-learn ExtraTreeRegressor randomly tries variables</p>
</li>
<li>
<p><a href="https://youtu.be/blyXCk4sgEg?t=1h4m1s">01:04:01</a> m.estimators_,<br>
Using list comprehension</p>
</li>
<li>
<p><a href="https://youtu.be/blyXCk4sgEg?t=1h10m">01:10:00</a> Out-of-bag (OOB) score</p>
</li>
<li>
<p><a href="https://youtu.be/blyXCk4sgEg?t=1h13m45s">01:13:45</a> Automate hyperparameters hyper-parameters with grid-search gridsearch<br>
Randomly subsample the dataset to reduce overfitting with ‘set_rf_samples()’, code detail at 1h18m25s</p>
</li>
<li>
<p><a href="https://youtu.be/blyXCk4sgEg?t=1h17m20s">01:17:20</a> Tip for Favorita Grocery competition,<br>
‘set_rf_samples()’,<br>
‘reset_rf_samples()’,<br>
‘min_samples_leaf=’,<br>
‘max_features=’</p>
</li>
<li>
<p><a href="https://youtu.be/blyXCk4sgEg?t=1h30m20s">01:30:20</a> Looking at ‘fiProductClassDesc’ column with <code>.cat.categories</code> and <code>.cat.codes</code><br>
<br>
</p>
</li>
</ul>
<p><strong>Lesson 03</strong></p>
<ul>
<li>
<p><a href="https://youtu.be/YSFG_W8JxBo?t=2m44s">00:02:44</a> When to use or not Random Forests (unstructured data like CV or Sound works better with DL),<br>
Collaborative filtering for Favorita</p>
</li>
<li>
<p><a href="https://youtu.be/YSFG_W8JxBo?t=5m10s">00:05:10</a> dealing with missing values present in Test but not Train (or vice-versa) in ‘proc_df()’ with “nas” dictionary whose keys are names of columns with missing values, and the values are the medians.</p>
</li>
<li>
<p><a href="https://youtu.be/YSFG_W8JxBo?t=9m30s">00:09:30</a> Starting Favorita notebook,<br>
The ability to explain the goal of a Kaggle competition or a project,<br>
What are independent and dependant variables ?<br>
Star schema warehouse database, snowflake schema</p>
</li>
<li>
<p><a href="https://youtu.be/YSFG_W8JxBo?t=15m30s">00:15:30<span class="badge badge-notification clicks" title="1 click">1</span></a> Use dtypes to read data without ‘low_memory = False’</p>
</li>
<li>
<p><a href="https://youtu.be/YSFG_W8JxBo?t=20m30s">00:20:30</a> Use ‘<code>shuf</code>’ to read a sample of large dataset at start</p>
</li>
<li>
<p><a href="https://youtu.be/YSFG_W8JxBo?t=26m30s">00:26:30</a> Take the Log of the sales with ‘np.log1p()’,<br>
Apply ‘add_datepart)’,<br>
‘split_vals(a,n)’,</p>
</li>
<li>
<p><a href="https://youtu.be/YSFG_W8JxBo?t=28m30s">00:28:30</a> Models,<br>
‘set_rf_samples’,<br>
‘np.array(trn, dtype=np.float32’,<br>
Use ‘%prun’ to find lines of code that takes a long time to run</p>
</li>
<li>
<p><a href="https://youtu.be/YSFG_W8JxBo?t=33m30s">00:33:30</a> We only get reasonable results, but nothing great on the leaderboard: WHY  ?</p>
</li>
<li>
<p><a href="https://youtu.be/YSFG_W8JxBo?t=43m30s">00:43:30</a> Quick look at Rossmann grocery competition winners,<br>
Looking at the choice of validation set with Favorita Leaderboard by Terence Parr (his @ pseudo here ?)</p>
</li>
<li>
<p><a href="https://youtu.be/YSFG_W8JxBo?t=50m30s">00:50:30</a> Lesson2-rf interpretation,<br>
Why is ‘nas’ an input AND an output variable in ‘proc_df()’</p>
</li>
<li>
<p><a href="https://youtu.be/YSFG_W8JxBo?t=55m30s">00:55:30</a> How confident are we in our predictions (based on tree variance) ?<br>
Using ‘set_rf_samples()’ again.<br>
‘parallel_trees()’ for multithreads parallel processing,<br>
EROPS, OROPS, Enclosure</p>
</li>
<li>
<p><a href="https://youtu.be/YSFG_W8JxBo?t=1h7m15s">01:07:15</a> Feature importance with ‘rf_feat_importance()’</p>
</li>
<li>
<p><a href="https://youtu.be/YSFG_W8JxBo?t=1h12m15s">01:12:15</a> Data leakage example,<br>
Colinearity<br>
<br>
</p>
</li>
</ul>
<p><strong>Lesson 04</strong></p>
<ul>
<li>
<p><a href="https://youtu.be/0v93qHDqq_g?t=4s">00:00:04</a> How to deal with version control and notebooks ? Make a copy and rename it with “tmp-blablabla” so it’s hidden from Git Pull</p>
</li>
<li>
<p><a href="https://youtu.be/0v93qHDqq_g?t=1m50s">00:01:50</a> Summarize the relationship between hyperparameters in Random Forests, overfitting and colinearity.<br>
‘set_rf_samples()’, ‘oob_score = True’,<br>
‘min_samples_leaf=’ 8m45s,<br>
‘max_features=’ 12m15s</p>
</li>
<li>
<p><a href="https://youtu.be/0v93qHDqq_g?t=18m50s">00:18:50<span class="badge badge-notification clicks" title="1 click">1</span></a> Random Forest Interpretation lesson2-rf_interpretation,<br>
‘rf_feat_importance()’</p>
</li>
<li>
<p><a href="https://youtu.be/0v93qHDqq_g?t=26m50s">00:26:50</a> ‘to_keep = fi[fi.imp&gt;0.005]’ to remove less important features,<br>
high cardinality variables 29m45s,</p>
</li>
<li>
<p><a href="https://youtu.be/0v93qHDqq_g?t=32m15s">00:32:15</a> Two reasons why Validation Score is not good or getting worse: overfitting, and validation set is not a random sample (something peculiar in it, not in Train),<br>
The meaning of the five numbers results in ‘print_score(m)’, RMSE of Training &amp; Validation, R² of Train &amp; Valid &amp; OOB.<br>
We care about the RMSE of Validation set.</p>
</li>
<li>
<p><a href="https://youtu.be/0v93qHDqq_g?t=35m50s">00:35:50</a> How Feature Importance is normally done in Industry and Academics outside ML: they use Logistic Regression Coefficients, not Random Forests Feature/Variable Importance.</p>
</li>
<li>
<p><a href="https://youtu.be/0v93qHDqq_g?t=39m50s">00:39:50</a> Doing One-hot encoding for categorical variables,<br>
Why and how works ‘max_n_cat=7’ based on Cardinality 49m15s, ‘numericalize’</p>
</li>
<li>
<p><a href="https://youtu.be/0v93qHDqq_g?t=55m5s">00:55:05</a> Removing redundant features using a dendogram and '.spearmanr()'for rank correlation, ‘get_oob(df)’, ‘to_drop = []’ variables,  ‘reset_rf_samples()’</p>
</li>
<li>
<p><a href="https://youtu.be/0v93qHDqq_g?t=1h7m15s">01:07:15</a> Partial dependence: how important features relate to the dependent variable, ‘ggplot() + stat_smooth()’, ‘plot_pdp()’</p>
</li>
<li>
<p><a href="https://youtu.be/0v93qHDqq_g?t=1h21m50s">01:21:50</a> What is the purpose of interpretation, what to do with that information ?</p>
</li>
<li>
<p><a href="https://youtu.be/0v93qHDqq_g?t=1h30m15s">01:30:15</a> What is EROPS / OROPS ?</p>
</li>
<li>
<p><a href="https://youtu.be/0v93qHDqq_g?t=1h32m25s">01:32:25</a> Tree interpreter<br>
<br>
</p>
</li>
</ul>
<p><strong>Lesson 05</strong></p>
<ul>
<li>
<p><a href="https://youtu.be/3jl2h9hSRvc?t=4s">00:00:04</a> Review of Training, Test set and OOB score, intro to Cross-Validation (CV),<br>
In Machine Learning, we care about Generalization Accuracy/Error.</p>
</li>
<li>
<p><a href="https://youtu.be/3jl2h9hSRvc?t=11m35s">00:11:35</a> Kaggle Public and Private test sets for Leaderboard,<br>
the risk of using a totally random validation set, rerun the model including Validation set.</p>
</li>
<li>
<p><a href="https://youtu.be/3jl2h9hSRvc?t=22m15s">00:22:15<span class="badge badge-notification clicks" title="1 click">1</span></a> Is my Validation set truly representative of my Test set. Build 5 very different models and score them on Validation and on Test. Examples with Favorita Grocery.</p>
</li>
<li>
<p><a href="https://youtu.be/3jl2h9hSRvc?t=28m10s">00:28:10</a> Why building a representative Test set is crucial in the Real World machine learning (not in Kaggle),<br>
Sklearn make train/test split or cross-validation = bad in real life (for Time Series) !</p>
</li>
<li>
<p><a href="https://youtu.be/3jl2h9hSRvc?t=31m">00:31:04</a> What is Cross-Validation and why you shouldn’t use it most of the time (hint: random is bad)</p>
</li>
<li>
<p><a href="https://youtu.be/3jl2h9hSRvc?t=38m4s">00:38:04</a> Tree interpretation revisited, lesson2-rf_interpreter.ipynb, waterfall plot for increase and decrease in tree splits,<br>
‘ti.predict(m, row)’</p>
</li>
<li>
<p><a href="https://youtu.be/3jl2h9hSRvc?t=48m50s">00:48:50</a> Dealing with Extrapolation in Random Forests,<br>
RF can’t extrapolate like Linear Model, avoid Time variables as predictors if possible ?<br>
Trick: find the differences between Train and Valid sets, ie. any temporal predictor ? Build a RF to identify components present in Valid only and not in Train ‘x,y = proc_df(df_ext, ‘is_valid’)’,<br>
Use it in Kaggle by putting Train and Test sets together and add a column ‘is_test’, to check if Test is a random sample or not.</p>
</li>
<li>
<p><a href="https://youtu.be/3jl2h9hSRvc?t=59m15s">00:59:15</a> Our final model of Random Forests, almost as good as Kaggle <span class="hashtag">#1</span> (Leustagos &amp; Giba)</p>
</li>
<li>
<p><a href="https://youtu.be/3jl2h9hSRvc?t=1h3m">01:03:04</a> What to expect for the in-class exam</p>
</li>
<li>
<p><a href="https://youtu.be/3jl2h9hSRvc?t=1h5m">01:05:04</a> Lesson3-rf_foundations.ipynb, writing our own Random Forests code.<br>
Basic data structures code,  class ‘TreeEnsemble()’, np.random.seed(42)’ as pseudo random number generator<br>
How to make a prediction in Random Forests (theory) ?</p>
</li>
<li>
<p><a href="https://youtu.be/3jl2h9hSRvc?t=1h21m">01:21:04</a> class ‘DecisionTree()’,<br>
Bonus: Object-Oriented-Programming (OOP) overview, critical for PyTorch<br>
<br>
</p>
</li>
</ul>
<p><strong>Lesson 06</strong></p>
<p>Note: this lesson has a VERY practical discussion with USF students about the use of Machine Learning in business/corporation, Jeremy shares his experience as a business consultant (McKinsey) and entrepreneur in AI/ML. Deffo not PhD’s stuff, too real-life.</p>
<ul>
<li>
<p><a href="https://youtu.be/BFIYUvBRTpE?t=1s">00:00:04</a> Review of previous lessons: Random Forests interpretation techniques,<br>
Confidence based on tree variance,<br>
Feature importance,<br>
Removing redundant features,<br>
Partial dependence…<br>
And why do we do Machine Learning, what’s the point ?<br>
Looking at PowerPoint ‘intro.ppx’ in Fastai GitHub: ML applications (horizontal &amp; vertical) in real-life.<br>
Churn (which customer is going to leave) in Telecom: google “jeremy howard data products”,<br>
drive-train approach with ‘Defined Objective’ -&gt; ‘Company Levers’ -&gt; ‘Company Data’ -&gt; ‘Models’</p>
</li>
<li>
<p><a href="https://youtu.be/BFIYUvBRTpE?t=10m1s">00:10:01<span class="badge badge-notification clicks" title="1 click">1</span></a> "In practice, you’ll care more about the results of your simulation than your predictive model directly ",<br>
Example with Amazon 'not-that-smart’recommendations vs optimization model.<br>
More on Churn and Machine Learning Applications in Business</p>
</li>
<li>
<p><a href="https://youtu.be/BFIYUvBRTpE?t=20m30s">00:20:30</a> Why is it hard/key to define the problem to solve,<br>
ICYMI: read “Designing great data products” from Jeremy in March 28, 2012 ^!^<br>
Healthcare applications like ‘Readmission risk’. Retail applications examples.<br>
There’s a lot more than what you read about Facebook or Google applications in Tech media.<br>
Machine Learning in Social Sciences today: not much.</p>
</li>
<li>
<p><a href="https://youtu.be/BFIYUvBRTpE?t=37m15s">00:37:15</a> More on Random Forests interpretation techniques.<br>
Confidence based on tree variance</p>
</li>
<li>
<p><a href="https://youtu.be/BFIYUvBRTpE?t=42m30s">00:42:30</a> Feature importance, and Removing redundant features</p>
</li>
<li>
<p><a href="https://youtu.be/BFIYUvBRTpE?t=50m45s">00:50:45</a> Partial dependence (or dependance)</p>
</li>
<li>
<p><a href="https://youtu.be/BFIYUvBRTpE?t=1h2m45s">01:02:45</a> Tree interpreter (and a great example of effective technical communications by a student)<br>
Using Excel waterfall chart from Chris<br>
Using ‘<a href="http://hub.github.com/">hub.github.com</a>’, a command-line wrapper for git that makes you better at GitHub.</p>
</li>
<li>
<p><a href="https://youtu.be/BFIYUvBRTpE?t=1h16m15s">01:16:15</a> Extrapolation, with a 20 mins session of live coding by Jeremy<br>
<br>
</p>
</li>
</ul>
<p><strong>Lesson 07</strong></p>
<ul>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=1s">00:00:01</a> Review of Random Forest previous lessons,<br>
Lots of historical/theoritical techniques in ML that we don’t use anymore (like SVM)<br>
Use of ML in Industry vs Academia, Decision-Trees Ensemble</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=5m30s">00:05:30<span class="badge badge-notification clicks" title="1 click">1</span></a> How big the Validation Set needs to be ? How much the accuracy of your model matters ?<br>
Demo with Excel, T-distribution and n&gt;22 observations in every class<br>
Standard Deviation : n<em>p</em>(1-p), Standard Error (stdev mean): stdev/sqrt(n)</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=18m45s">00:18:45</a> Back to Random Forest from scratch.<br>
“Basic data structures” reviewed</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=32m45s">00:32:45<span class="badge badge-notification clicks" title="1 click">1</span></a> Single Branch<br>
Find the best split given variable with ‘find_better_split’, using Excel demo again</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=45m30s">00:45:30</a> Speeding things up</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=55m">00:55:00</a> Full single tree</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=1h1m30s">01:01:30</a> Predictions with ‘predict(self,x)’,<br>
and ‘predict_row(self, xi)’</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=1h9m5s">01:09:05</a> Putting it all together,<br>
Cython an optimising static compiler for Python and C</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=1h18m1s">01:18:01</a> “Your mission, for next class, is to implement”:<br>
Confidence based on tree variance,<br>
Feature importance,<br>
Partial dependence,<br>
Tree interpreter.</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=1h20m15s">01:20:15</a> Reminder: How to ask for Help on Fastai forums<br>
<a href="http://wiki.fast.ai/index.php/How_to_ask_for_Help">http://wiki.fast.ai/index.php/How_to_ask_for_Help</a><br>
Getting a screenshot, resizing it.<br>
For lines of code, create a “Gist”, using the extension ‘Gist-it’ for “Create/Edit Gist of Notebook” with ‘nbextensions_configurator’ on Jupyter Notebook, ‘Collapsible Headings’, ‘Chrome Clipboard’, ‘Hide Header’</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=1h23m15s">01:23:15</a> We’re done with Random Forests, now we move on to Neural Networks.<br>
Random Forests can’t extrapolate, it just averages data that it has already seen, Linear Regression can but only in very limited ways.<br>
Neural Networks give us the best of both worlds.<br>
Intro to SGD for MNIST, unstructured data.<br>
Quick comparison with Fastai/Jeremy’s Deep Learning Course.<br>
<br>
</p>
</li>
</ul>
<p><strong>Lesson 08</strong></p>
<ul>
<li>
<p><a href="https://youtu.be/DzE0eSdy5Hk?t=45s">00:00:45</a> Moving from Decision Trees Ensemble to Neural Nets with Mnist<br>
lesson4-mnist_sgd.ipynb notebook</p>
</li>
<li>
<p><a href="https://youtu.be/DzE0eSdy5Hk?t=8m20s">00:08:20</a> About Python ‘pickle()’ pros &amp; cons for Pandas, vs ‘feather()’,<br>
Flatten a tensor</p>
</li>
<li>
<p><a href="https://youtu.be/DzE0eSdy5Hk?t=13m45s">00:13:45<span class="badge badge-notification clicks" title="1 click">1</span></a> Reminder on the jargon: a vector in math is a 1d array in CS,<br>
a rank 1 tensor in deep learning.<br>
A matrix is a 2d array or a rank 2 tensor, rows are axis 0 and columns are axis 1</p>
</li>
<li>
<p><a href="https://youtu.be/DzE0eSdy5Hk?t=17m45s">00:17:45<span class="badge badge-notification clicks" title="1 click">1</span></a> Normalizing the data: subtracting off the mean and dividing by stddev<br>
Important: use the mean and stddev of Training data for the Validation data as well.<br>
Use the ‘np.reshape()’ function</p>
</li>
<li>
<p><a href="https://youtu.be/DzE0eSdy5Hk?t=34m25s">00:34:25<span class="badge badge-notification clicks" title="1 click">1</span></a> Slicing into a tensor, ‘plots()’ from Fastai lib.</p>
</li>
<li>
<p><a href="https://youtu.be/DzE0eSdy5Hk?t38m20s">00:38:20<span class="badge badge-notification clicks" title="1 click">1</span></a> Overview of a Neural Network<br>
Michael Nielsen universal approximation theorem: a visual proof that neural nets can compute any function<br>
Why you should blog (by Rachel Thomas)</p>
</li>
<li>
<p><a href="https://youtu.be/DzE0eSdy5Hk?t=47m15s">00:47:15<span class="badge badge-notification clicks" title="1 click">1</span></a> Intro to PyTorch &amp; Nvidia GPUs for Deep Learning<br>
Website to buy a laptop with a good GPU: <a href="http://xoticpc.com/">xoticpc.com</a><br>
Using cloud services like <a href="http://crestle.com/">Crestle.com<span class="badge badge-notification clicks" title="1 click">1</span></a> or AWS (and how to gain access EC2 w/ “Request limit increase”)</p>
</li>
<li>
<p><a href="https://youtu.be/DzE0eSdy5Hk?t=57m45s">00:57:45</a> Create a Neural Net for Logistic Regression in PyTorch<br>
‘net = nn.Sequential(nn.Linear(28*28, 10), nn.LogSoftmax()).cuda()’<br>
‘md = ImageClassifierData.from_arrays(path, (x,y), (x_valid, y_valid))’<br>
Loss function such as ‘nn.NLLLoss()’ or Negative Log Likelihood Loss or Cross-Entropy (binary or categorical)<br>
Looking at Loss with Excel</p>
</li>
<li>
<p><a href="https://youtu.be/DzE0eSdy5Hk?t=1h9m5s">01:09:05</a> Let’s fit the model then make predictions on Validation set.<br>
‘fit(net, md, epochs=1, crit=loss, opt=opt, metrics=metrics)’<br>
Note: PyTorch doesn’t use the word “loss” but the word “criterion”, thus ‘crit=loss’<br>
‘preds = predict(net, md.val_dl)’<br>
‘preds.shape’ -&gt; (10000, 10)<br>
‘preds.argmax(axis=1)[:5]’, argmax will return the index of the value which is the number itself.<br>
‘np.mean(preds == y_valid)’ to check how accurate the model is on Validation set.</p>
</li>
<li>
<p><a href="https://youtu.be/DzE0eSdy5Hk?t=1h16m5s">01:16:05</a> A second pass on “Michael Nielsen universal approximation theorem”<br>
A Neural Network can approximate any other function to close accuracy, as long as it’s large enough.</p>
</li>
<li>
<p><a href="https://youtu.be/DzE0eSdy5Hk?t=1h18m15s">01:18:15<span class="badge badge-notification clicks" title="1 click">1</span></a> Defining Logistic Regression ourselves, from scratch, not using PyTorch ‘nn.Sequential()’<br>
Demo explanation with drawings by Jeremy.<br>
Look at Excel ‘entropy_example.xlsx’ for Softmax and Sigmoid</p>
</li>
<li>
<p><a href="https://youtu.be/DzE0eSdy5Hk?t=1h31m5s">01:31:05</a> Assignements for the week, student question on ‘Forward(self, x)’<br>
<br>
</p>
</li>
</ul>
<p><strong>Lesson 09</strong></p>
<p>Jeremy starts with a selection of students’ posts.</p>
<ul>
<li>
<p><a href="https://youtu.be/PGC0UxakTvM?t=1s">00:00:01</a> Structuring the Unstructured: a visual demo of Bagging with Random Forests.<br>
<a href="http://structuringtheunstructured.blogspot.se/2017/11/coloring-with-random-forests.html">http://structuringtheunstructured.blogspot.se/2017/11/coloring-with-random-forests.html</a></p>
</li>
<li>
<p><a href="https://youtu.be/PGC0UxakTvM?t=4m1s">00:04:01<span class="badge badge-notification clicks" title="1 click">1</span></a> Parfit: a library for quick and powerful hyper-parameter optimization with visualizations.<br>
. How to make SGD Classifier perfomr as well as Logistic Regression using Parfit<br>
. Intuitive Interpretation of Random Forest<br>
. Statoil/C-Core Iceberg Classifier Challenge on Kaggle: a Keras Model for Beginners + EDA</p>
</li>
</ul>
<p>Back to the course.</p>
<ul>
<li>
<p><a href="https://youtu.be/PGC0UxakTvM?t=9m1s">00:09:01</a> Why write a post on your learning experience, for you and for newcomers.</p>
</li>
<li>
<p><a href="https://youtu.be/PGC0UxakTvM?t=9m50s">00:09:50</a> Using SGD on MNIST for digit recognition<br>
. lesson4-mnist_sgd.ipynb notebook</p>
</li>
<li>
<p><a href="https://youtu.be/PGC0UxakTvM?t=11m30s">00:11:30<span class="badge badge-notification clicks" title="1 click">1</span></a> Training the simplest Neural Network in PyTorch<br>
(long step-by-step demo, 30 mins approx)</p>
</li>
<li>
<p><a href="https://youtu.be/PGC0UxakTvM?t=46m55s">00:46:55</a> Intro to Broadcasting: “The MOST important programming concept in this course and in Machine Learning”<br>
. Performance comparison between C and Python<br>
. SIMD: “Single Instruction Multiple Data”<br>
. Multiple processors/cores and CUDA</p>
</li>
<li>
<p><a href="https://youtu.be/PGC0UxakTvM?t=52m10s">00:52:10</a> Broadcasting in details</p>
</li>
<li>
<p><a href="https://youtu.be/PGC0UxakTvM?t=1h5m50s">01:05:50</a> Broadcasting goes back to the days of APL (1950’s) and Jsoftware<br>
. More on Broadcasting</p>
</li>
<li>
<p><a href="https://youtu.be/PGC0UxakTvM?t=1h12m30s">01:12:30</a> Matrix Multiplication -and not-.<br>
. Writing our own training loop.</p>
</li>
</ul>


<p><strong>Lesson 10</strong></p>
<ul>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1s">00:00:01</a> Fast.ai is now available on PIP !<br>
And more USF students publications: class-wise Processing in NLP, Class-wise Regex Functions<br>
. Porto Seguro’s Safe Driver Prediction (Kaggle): 1st place solution with zero feature engineering !<br>
Dealing with semi-supervised-learning (ie. labeled and unlabeled data)<br>
Data augmentation to create new data examples by creating slightly different versions of data you already have.<br>
In this case, he used Data Augmentation by creating new rows with 15% randomly selected data.<br>
Also used “auto-encoder”: the independant variable is the same as the dependant variable, as in “try to predict your input” !</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=8m30s">00:08:30</a> Back to a simple Logistic Regression with MNIST summary<br>
‘lesson4-mnist_sgd.ipynb’ notebook</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=11m30s">00:11:30</a> PyTorch tutorial on Autograd</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=15m30s">00:15:30</a> “Stream Processing” and “Generator Python”<br>
. “l.backward()”<br>
. “net2 = LogReg().cuda()”</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=32m30s">00:32:30</a> Building a complete Neural Net, from scratch, for Logistic Regression in PyTorch, with “nn.Sequential()”</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=58m">00:58:00<span class="badge badge-notification clicks" title="1 click">1</span></a> Fitting the model in ‘lesson4-mnist_sgd.ipynb’ notebook<br>
The secret in modern ML (as covered in the Deep Learning course): massively over-paramaterized the solution to your problem, then use Regularization.</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1h2m10s">01:02:10</a> Starting NLP with IMDB dataset and the sentiment classification task<br>
NLP = Natural Language Processing</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1h3m10s">01:03:10</a> Tokenizing and ‘term-document matrix’ &amp; "Bag-of-Words’ creation<br>
“trn, trn_y = texts_from_folders(f’{PATH}train’, names)” from Fastai library to build arrays of reviews and labels<br>
Throwing the order of words with Bag-of-Words !</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1h8m50s">01:08:50</a> sklearn “CountVectorizer()”<br>
“fit_transform(trn)” to find the vocabulary in the training set and build a term-document matrix.<br>
“transform(val)” to apply the <strong>same</strong> transformation to the validation set.</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1h12m30s">01:12:30</a> What is a ‘sparse matrix’ to store only key info and save memory.<br>
More details in Rachel’s “Computational Algebra” course on Fastai</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1h16m40s">01:16:40<span class="badge badge-notification clicks" title="1 click">1</span></a> Using “Naive Bayes” for “Bag-of-Words” approaches.<br>
Transforming words into features, and dealing with the bias/risk of “zero probabilities” from the data.<br>
Some demo/discussion about calculating the probabilities of classes.</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1h25m">01:25:00</a> Why is it called “Naive Bayes”</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1h30m">01:30:00</a> The difference between theory and practice for “Naive Bayes”<br>
Using Logistic regression where the features are the unigrams</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1h35m40s">01:35:40</a> Using Bigram &amp; Trigram with Naive Bayes (NB) features</p>
</li>
</ul>


<p><strong>Lesson 11</strong></p>
<ul>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=1s">00:00:01</a> Review of optimizing multi-layer functions with SGD<br>
“d(h(g(f(x)))) / dw = 0,6”</p>
</li>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=9m45s">00:09:45</a> Review of Naive Bayes &amp; Logistic Regression for NLP with lesson5-nlp.ipynb notebook</p>
</li>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=16m30s">00:16:30</a> Cross-Entropy as a popular Loss Function for Classification (vs RMSE for Regression)</p>
</li>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=21m30s">00:21:30</a> Creating more NLP features with Ngrams (bigrams, trigrams)</p>
</li>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=23m1s">00:23:01</a> Going back to Naive Bayes and Logistic Regression,<br>
then ‘We do something weird but actually not that weird’ with “x_nb = x.multiply®”<br>
Note: watch the whole 15 mins segment for full understanding.</p>
</li>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=39m45s">00:39:45</a> ‘Baselines and Bigrams: Simple, Good Sentiment and Topic Classification’ paper by Sida Wang and Christopher Manning, Stanford U.</p>
</li>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=43m31s">00:43:31<span class="badge badge-notification clicks" title="1 click">1</span></a> Improving it with PyTorch and GPU, with Fastai Naive Bayes or ‘Fastai NBSVM++’ and “class DotProdNB(nn.Module):”<br>
Note: this long section includes lots of mathematical demonstration and explanation.</p>
</li>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=1h17m30s">01:17:30</a> Deep Learning: Structured and Time-Series data with Rossmann Kaggle competition, with the 3rd winning solution ‘Entity Embeddings of Categorical Variables’ by Guo/Berkhahn.</p>
</li>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=1h21m30s">01:21:30</a> Rossmann Kaggle: data cleaning &amp; feature engineering.<br>
Using Pandas to join tables with ‘Left join’</p>
</li>
</ul>


<p><strong>Lesson 12</strong></p>
<p><strong>Note: you may want to pay specific attention to the second part of this final lesson, where Jeremy brings up delicate issues on Data Science &amp; Ethics.</strong><br>
<strong>This goes beyond what most courses on DS cover.</strong></p>
<ul>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=2s">00:00:01</a> Final lesson program !</p>
</li>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=1m2s">00:01:01<span class="badge badge-notification clicks" title="1 click">1</span></a> Review of Rossmann Kaggle competition with ‘lesson3-rossman.ipynb’<br>
Using “df.apply(lambda x:…)” and “create_promo2since(x)”</p>
</li>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=4m30s">00:04:30</a> Durations function “get_elapsed(fld, pre):” using “zip()”<br>
Check the notebook for detailed explanations.</p>
</li>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=16m10s">00:16:10</a> Rolling function (or windowing function) for moving-average<br>
Hint: learn the Pandas API for Time-Series, it’s extremely diverse and powerful</p>
</li>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=21m40s">00:21:40</a> Create Features, assign to ‘cat_vars’ and ‘contin_vars’<br>
‘joined_samp’, ‘do_scale=True’, ‘mapper’,<br>
‘yl = np.log(y)’ for RMSPE (Root Mean Squared Percent Error)<br>
Selecting a most recent Validation set in Time-Series, if possible of the exact same length as Test set.<br>
Then dropping the Validation set with ‘val_idx = [0]’ for final training of the model.</p>
</li>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=32m30s">00:32:30</a> How to create our Deep Learning algorithm (or model), using ‘ColumnarModelData.from_data_frame()’<br>
Use the cardinality of each variable to decide how large to make its embeddings.<br>
Jeremy’s Golden Rule on difference between modern ML and old ML:<br>
“In old ML, we controlled complexity by reducing the number of parameters.<br>
In modern ML, we control it by regularization. We are not much concerned about Overfitting because we use increasing Dropout or Weight-Decay to avoid it”</p>
</li>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=39m20s">00:39:20<span class="badge badge-notification clicks" title="1 click">1</span></a> Checking our submission vs Kaggle Public Leaderboard (not great), then Private Leaderboard (great!).<br>
Why Kaggle Public LB (LeaderBoard) is NOT a good replacement to your own Validation set.<br>
What is the relation between Kaggle Public LB and Private LB ?</p>
</li>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=44m15s">00:44:15<span class="badge badge-notification clicks" title="1 click">1</span></a> Course review (lessons 1 to 12)<br>
Two ways to train a model: one by building a tree, one with SGD (Stochastic Gradient Descent)<br>
Reminder: Tree-building can be combined with Bagging (Random Forests) or Boosting (GBM)</p>
</li>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=46m15s">00:46:15</a> How to represent Categorical variables with Decision Trees<br>
One-hot encoding a vector and its relation with embedding</p>
</li>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=55m50s">00:55:50</a> Interpreting Decision Trees, Random Forests in particular, with Feature Importance.<br>
Use the same techniques to interpret Neural Networks, shuffling Features.</p>
</li>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=59m">00:59:00<span class="badge badge-notification clicks" title="1 click">1</span></a> Why Jeremy usually doesn’t care about ‘Statistical Significant’ in ML, due to Data volume, but more about ‘Practical Significance’.</p>
</li>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=1h3m10s">01:03:10<span class="badge badge-notification clicks" title="1 click">1</span></a> Jeremy talks about “The most important part in this course: Ethics and Data Science, it matters.”<br>
How does Machine Learning influence people’s behavior, and the responsibility that comes with it ?<br>
As a ML practicioner, you should care about the ethics and think about them BEFORE you are involved in one situation.<br>
BTW, you can end up in jail/prison as a techie doing “his job”.</p>
</li>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=1h8m15s">01:08:15</a> IBM and the “Death’s Calculator” used in gas chamber by the Nazis.<br>
Facebook data science algorithm and the ethnic cleansing in Myanmar’s Rohingya crisis: the Myth of Neutral Platforms.<br>
Facebook lets advertisers exclude users by race enabled advertisers to reach “Jew Haters”.<br>
Your algorithm/model could be exploited by trolls, harassers, authoritarian governements for surveillance, for propaganda or disinformation.</p>
</li>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=1h66m45s">01:16:45</a> Runaway feedback loops: when Recommendation Systems go bad.<br>
Social Network algorithms are distorting reality by boosting conspiracy theories.<br>
Runaway feedback loops in Predictive Policing: an algorithm biased by race and impacting Justice.</p>
</li>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=1h21m45s">01:21:45</a> Bias in Image Software (Computer Vision), an example with Faceapp or Google Photos. The first International Beauty Contest judged by A.I.</p>
</li>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=1h25m15s">01:25:15<span class="badge badge-notification clicks" title="1 click">1</span></a> Bias in Natural Language Processing (NLP)<br>
Another example with an A.I. built to help US Judicial system.<br>
Taser invests in A.I. and body-cameras to “anticipate criminal activity”.</p>
</li>
<li>
<p><a href="https://youtu.be/5_xFdhfUnvQ?t=1h34m30s">01:34:30<span class="badge badge-notification clicks" title="1 click">1</span></a> Questions you should ask yourself when you work on A.I.<br>
You have options !</p></li></ul>
