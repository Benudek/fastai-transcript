<p><strong>Lesson 07</strong></p>
<ul>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=1s">00:00:01</a> Review of Random Forest previous lessons,<br>
Lots of historical/theoritical techniques in ML that we don’t use anymore (like SVM)<br>
Use of ML in Industry vs Academia, Decision-Trees Ensemble</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=5m30s">00:05:30</a> How big the Validation Set needs to be ? How much the accuracy of your model matters ?<br>
Demo with Excel, T-distribution and n&gt;22 observations in every class<br>
Standard Deviation : n<em>p</em>(1-p), Standard Error (stdev mean): stdev/sqrt(n)</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=18m45s">00:18:45</a> Back to Random Forest from scratch.<br>
“Basic data structures” reviewed</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=32m45s">00:32:45</a> Single Branch<br>
Find the best split given variable with ‘find_better_split’, using Excel demo again</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=45m30s">00:45:30</a> Speeding things up</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=55m">00:55:00</a> Full single tree</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=1h1m30s">01:01:30</a> Predictions with ‘predict(self,x)’,<br>
and ‘predict_row(self, xi)’</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=1h9m5s">01:09:05</a> Putting it all together,<br>
Cython an optimising static compiler for Python and C</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=1h18m1s">01:18:01</a> “Your mission, for next class, is to implement”:<br>
Confidence based on tree variance,<br>
Feature importance,<br>
Partial dependence,<br>
Tree interpreter.</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=1h20m15s">01:20:15</a> Reminder: How to ask for Help on Fastai forums<br>
<a href="http://wiki.fast.ai/index.php/How_to_ask_for_Help">http://wiki.fast.ai/index.php/How_to_ask_for_Help</a><br>
Getting a screenshot, resizing it.<br>
For lines of code, create a “Gist”, using the extension ‘Gist-it’ for “Create/Edit Gist of Notebook” with ‘nbextensions_configurator’ on Jupyter Notebook, ‘Collapsible Headings’, ‘Chrome Clipboard’, ‘Hide Header’</p>
</li>
<li>
<p><a href="https://youtu.be/O5F9vR2CNYI?t=1h23m15s">01:23:15</a> We’re done with Random Forests, now we move on to Neural Networks.<br>
Random Forests can’t extrapolate, it just averages data that it has already seen, Linear Regression can but only in very limited ways.<br>
Neural Networks give us the best of both worlds.<br>
Intro to SGD for MNIST, unstructured data.<br>
Quick comparison with Fastai/Jeremy’s Deep Learning Course.<br>
<br>
</p>
</li>
</ul>


