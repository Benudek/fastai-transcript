<p><strong>Lesson 10</strong></p>
<ul>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1s">00:00:01</a> Fast.ai is now available on PIP !<br>
And more USF students publications: class-wise Processing in NLP, Class-wise Regex Functions<br>
. Porto Seguro’s Safe Driver Prediction (Kaggle): 1st place solution with zero feature engineering !<br>
Dealing with semi-supervised-learning (ie. labeled and unlabeled data)<br>
Data augmentation to create new data examples by creating slightly different versions of data you already have.<br>
In this case, he used Data Augmentation by creating new rows with 15% randomly selected data.<br>
Also used “auto-encoder”: the independant variable is the same as the dependant variable, as in “try to predict your input” !</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=8m30s">00:08:30</a> Back to a simple Logistic Regression with MNIST summary<br>
‘lesson4-mnist_sgd.ipynb’ notebook</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=11m30s">00:11:30</a> PyTorch tutorial on Autograd</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=15m30s">00:15:30</a> “Stream Processing” and “Generator Python”<br>
. “l.backward()”<br>
. “net2 = LogReg().cuda()”</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=32m30s">00:32:30</a> Building a complete Neural Net, from scratch, for Logistic Regression in PyTorch, with “nn.Sequential()”</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=58m">00:58:00</a> Fitting the model in ‘lesson4-mnist_sgd.ipynb’ notebook<br>
The secret in modern ML (as covered in the Deep Learning course): massively over-paramaterized the solution to your problem, then use Regularization.</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1h2m10s">01:02:10</a> Starting NLP with IMDB dataset and the sentiment classification task<br>
NLP = Natural Language Processing</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1h3m10s">01:03:10</a> Tokenizing and ‘term-document matrix’ &amp; "Bag-of-Words’ creation<br>
“trn, trn_y = texts_from_folders(f’{PATH}train’, names)” from Fastai library to build arrays of reviews and labels<br>
Throwing the order of words with Bag-of-Words !</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1h8m50s">01:08:50</a> sklearn “CountVectorizer()”<br>
“fit_transform(trn)” to find the vocabulary in the training set and build a term-document matrix.<br>
“transform(val)” to apply the <strong>same</strong> transformation to the validation set.</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1h12m30s">01:12:30</a> What is a ‘sparse matrix’ to store only key info and save memory.<br>
More details in Rachel’s “Computational Algebra” course on Fastai</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1h16m40s">01:16:40</a> Using “Naive Bayes” for “Bag-of-Words” approaches.<br>
Transforming words into features, and dealing with the bias/risk of “zero probabilities” from the data.<br>
Some demo/discussion about calculating the probabilities of classes.</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1h25m">01:25:00</a> Why is it called “Naive Bayes”</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1h30m">01:30:00</a> The difference between theory and practice for “Naive Bayes”<br>
Using Logistic regression where the features are the unigrams</p>
</li>
<li>
<p><a href="https://youtu.be/37sFIak42Sc?t=1h35m40s">01:35:40</a> Using Bigram &amp; Trigram with Naive Bayes (NB) features</p>
</li>
</ul>




