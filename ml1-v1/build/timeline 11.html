<p><strong>Lesson 11</strong></p>
<ul>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=1s">00:00:01</a> Review of optimizing multi-layer functions with SGD<br>
“d(h(g(f(x)))) / dw = 0,6”</p>
</li>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=9m45s">00:09:45</a> Review of Naive Bayes &amp; Logistic Regression for NLP with lesson5-nlp.ipynb notebook</p>
</li>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=16m30s">00:16:30</a> Cross-Entropy as a popular Loss Function for Classification (vs RMSE for Regression)</p>
</li>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=21m30s">00:21:30</a> Creating more NLP features with Ngrams (bigrams, trigrams)</p>
</li>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=23m1s">00:23:01</a> Going back to Naive Bayes and Logistic Regression,<br>
then ‘We do something weird but actually not that weird’ with “x_nb = x.multiply®”<br>
Note: watch the whole 15 mins segment for full understanding.</p>
</li>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=39m45s">00:39:45</a> ‘Baselines and Bigrams: Simple, Good Sentiment and Topic Classification’ paper by Sida Wang and Christopher Manning, Stanford U.</p>
</li>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=43m31s">00:43:31</a> Improving it with PyTorch and GPU, with Fastai Naive Bayes or ‘Fastai NBSVM++’ and “class DotProdNB(nn.Module):”<br>
Note: this long section includes lots of mathematical demonstration and explanation.</p>
</li>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=1h17m30s">01:17:30</a> Deep Learning: Structured and Time-Series data with Rossmann Kaggle competition, with the 3rd winning solution ‘Entity Embeddings of Categorical Variables’ by Guo/Berkhahn.</p>
</li>
<li>
<p><a href="https://youtu.be/XJ_waZlJU8g?t=1h21m30s">01:21:30</a> Rossmann Kaggle: data cleaning &amp; feature engineering.<br>
Using Pandas to join tables with ‘Left join’</p>
</li>
</ul>




