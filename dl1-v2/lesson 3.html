<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Lesson 3: Improving your image classifier</title>
    <meta charset="UTF-8">
  </head>
  <body>
  <p style="text-align: right"><a href="http://course.fast.ai/">fast.ai: Deep Learning Part 1 (v2) (2018)</a></p>
  <h1><a href="http://course.fast.ai/lessons/lesson3.html">Lesson 3: Improving your image classifier</a></h1>
  <h2>Outline</h2>
<p>We explain convolutional networks from several different angles: the theory, a video visualization, and an Excel demo. You’ll see how to use deep learning for structured/tabular data, such as time-series sales data.</p>

<p>We also teach a couple of key bits of math that you really need for deep learning: exponentiation and the logarithm. You will learn how to download data to your deep learning server, how to submit to a Kaggle competition, and key differences between PyTorch and Keras/TensorFlow.</p>

  <h2>Video Timelines and Transcript</h2>


<h3>1. <a href="https://youtu.be/9C06ZPF8Uuc?t=5s">00:00:05</a></h3>

<ul style="list-style-type: square;">

<li><b> Cool guides &amp; posts made by <a href="http://fast.ai/">Fast.ai</a> classmates</b></li>

<li><b>tmux, summary of lesson 2, learning rate finder, guide to Pytorch, learning rate vs batch size,</b></li>

<li><b>decoding ResNet architecture, beginner’s forum</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Welcome back everybody, , I'm sure you've noticed, but there's been a lot 
of cool activity on the forum this week, and one of the things that's been 
really great to see is that a lot of you have started creating really 
helpful materials both for your Classmates to better understand stuff, and 
also for you to better understand stuff by trying to teach what you've 
learned. I just wanted to highlight a few. I've actually posted to the wiki 
thread of a few of these, but there's there's lots more. Russian has posted 
a whole bunch of nice introductory tutorials, so, for example, if you're 
having any trouble getting connected with AWS she's got a whole 
step-by-step. How to go about logging in and getting everything working, 
which, i think is a really terrific thing, and so it's a kind of thing 
that, if you are writing some notes for yourself to remind you how to do 
it, you may as well post them for others To do it as well and by using a 
markdown file like this, and it's actually good practice if you haven't 
used github before, if you put it up on github, everybody can now use it 
or, of course you can just put it in the forum. So more advanced a thing 
that Reshma wrote up about is she noticed that I like using Tmax, which is 
a handy little thing which lets me lets me basically have a window I'll 
show you so as soon as I log into my computer.</p>

<p>If I run Tmax you'll see 
that all of my windows pop straight up basically - and I can like continue 
running stuff in the background - and I can like I've - got them over here 
and I can kind of zoom into it or I can move over to the Top, which is here 
so Jupiter, colonel running and so forth. So if that sounds interesting, 
Reshma has a tutorial here on how you can use two maps and it's actually 
got a whole bunch of stuff in her github. So that's that's really cool I 
built, among has written a very nice kind of summary, basically of our last 
lesson, which kind of covers. What are the key things we did, and why did 
we do them? So if you are a kind of wondering like how does it fit 
together, I think this is a really helpful summary like what, if those 
couple of hours look like, if we summarize it all into a page or two, I 
also really like Pavel has dad kind Of done a deep dive on the learning 
rate finder, which is a topic that a lot of you have been interested in 
learning more about, particularly those of you who have done deep learning 
before. I realized that this is like a solution to a problem that you've 
been having for a long time and haven't seen before, and so it's kind of 
something which hasn't really been blogged about before. So this is the 
first I've seen it's logged about. So when I put this on Twitter, a link to 
pebbles post, it's been shared, you know hundreds of times it's been really 
really popular and viewed many thousands of times.</p>

<p>So that's some great 
content. Radec has posted lots of cool stuff. I really like this 
practitioners guide to apply torch, which again this is more for more 
advanced students, but it's like digging into people who have never used 
pytorch before but know a bit about numerical programming in general. And 
it's a quick introduction to how high torch is different and then there's 
been some interesting little bits of research like what's the relationship 
between learning rate and batch sites. So one of the students actually 
asked me this before class, and I said oh well, one of the other students 
has written an analysis of exactly that. So what he's done is basically 
looked through and tried different batch sizes and different learning rates 
and tried to see how they seemed to relate together, and these are all like 
cool experiments, which you know you can try yourself. I predict again he's 
written something again, a kind of a research into this question. I made a 
claim that the the stochastic gradient descent with restarts finds more 
generalizable parts of the function surface, because they're kind of 
flatter and he's been trying to figure out. Is there a way to measure that 
more directly, not quite successful yet? But a really interesting piece of 
research got some introductions to convolutional neural networks and then 
something that we'll be learning about towards the end of this course.</p>

<p>But 
I'm sure you've noticed we're using something called ResNet and a nonce. 
Aha, actually posted a pretty impressive analysis of like watts arrest net, 
and why is it interesting and this one's actually being very already shared 
very widely around the internet? I've seen also so some more advanced 
students who are interested in jumping ahead can look at that, and uphill 
Tamang also has done something similar, so lots of yeah lots of stuff going 
on on the forums, I'm sure you've also noticed. We have a beginner forum 
now specifically, for you know, asking questions which you know. It is 
always the case that there are no dumb questions, but when there's lots of 
people around you talking about advanced topics, it might not feel that 
way. So hopefully, the beginners forum is just a less intimidating space 
and if there are more advanced student who can help answer those questions, 
please do but remember when you do answer those questions, try to answer in 
a way that's friendly to people that maybe you know have No more than a 
year of programming experience, you haven't done any machine learning 
before so you know, I hope, other </p>


<h3>2. <a href="https://youtu.be/9C06ZPF8Uuc?t=5m45s">00:05:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Where we go from here</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>People in the class feel like you can contribute as well and just remember 
all of the people we just looked at or many of them, I believe, have never 
hosted anything to the internet before right I mean you, don't have to be a 
particular kind of person To be allowed to block something you can just jot 
down your notes, throw it up there, and one handy thing is: if you just put 
it on the forum and you're, not quite sure of some of the details, then 
then you know you have an opportunity to Get feedback and say like: oh 
well, that's not quite how that works. You know. Actually it works this way 
instead or oh, that's a really interesting insight. Have you thought about 
taking this further and so forth? So what we've done so far is a kind of an 
injury, an introduction as a just as a practitioner to convolutional neural 
networks for images, and we haven't really talked much at all about the 
theory or why they work or the math of them. But on the other hand, what we 
have done is seen how to build a model which actually works exceptionally 
well. Compact world-class level models and we'll kind of review a little 
bit of that today and then also today, we're going to dig in a little quite 
a lot more actually into the underlying theory of like what is a. What is a 
CNN? What's a convolution, how does this work and then we're going to kind 
of go through this this cycle, where we're going to dig we're going to do a 
little intro into a whole bunch of application areas using neural nets for 
structured data, so kind of like logistics Or forecasting, or you know, 
financial data or that kind of thing, and then looking at language 
applications and LP applications using recurrent neural Nets and then 
collaborative filtering for recommendations and systems, and so these will 
all be like similar to what we've done for cnn's. For images.</p>

<p>Would be like 
here's how you can get a state-of-the-art result without digging into the 
theory, but but knowing how to actually make a work and then we're kind of 
going to go back through those almost in reverse order. So then, we're 
going to dig right into collaborative filtering in a lot of detail and see 
how how to write the code underneath and how the math works underneath and 
then we're going to do the same thing for the structured data analysis. 
We're going to do the same thing for comp nets, for images and, finally, an 
in depth deep dive into apparent neural networks. So that's </p>

<h3>3. <a href="https://youtu.be/9C06ZPF8Uuc?t=8m20s">00:08:20</a></h3>

<ul style="list-style-type: square;">

<li><b> How to complete last week assignement “Dog breeds detection”</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Kind of where we're okay! So, let's start by doing a little bit of a 
review, and I want to also provide a bit more detail on some on some steps 
that we only briefly slipped over. So I want to make sure that we're all 
able to complete kind of last week's assignment, which was that the dog 
breeze I mean to basically apply what you've learned with another data set, 
and I thought the easiest one to do with me. The dog breeds cattle 
competition, and so I want to make sure everybody has everything you need 
to do this right now so, and the </p>

<h3>4. <a href="https://youtu.be/9C06ZPF8Uuc?t=8m55s">00:08:55</a></h3>

<ul style="list-style-type: square;">

<li><b> How to download data from Kaggle (Kaggle CLI) or anywhere else</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>First thing is to make sure that you know how to download data, and so 
there's there's two main places at the moment. We're kind of downloading 
data from one is from cattle and the other is from like anywhere else and 
so I'll. First of all, do the the casual version so to download from cattle 
we use something called cattle CLI, which is gear and to install what I 
think it's already in the system will shake yeah, so it should already be 
in your environment. But to make sure one thing that happens is because 
this is downloading from the cattle website through experience rating every 
time cap will change us the website it breaks so anytime. You try to use it 
and if the cattles websites changed recent, when you'll need to make sure 
you get the most recent version, so you can always go to pip, install cable 
CL I upgrade, and so that'll just make sure that you've got the latest 
version of Of it and everything that it depends on okay, and so then, 
having done that, you can follow the instructions. Actually I think Reshma 
was kind enough to they go. There's a cable CLI. You know, like everything 
you need to know, can be under Reshma's github, so basically to do that at 
the next step you go kg download and then you provide your username with 
you. You provide your password with P and then see it.</p>

<p>The competition name 
and a lot of people in the forum is being confused about what to enter 
here, and so the key thing to note is that, when you're at a capital, 
competition after the /c, there's a specific name - planet, understanding, 
etcetera right, that's the name! You need okay, the other thing you'll need 
to make sure is that you've on your own computer have attempted to click 
download, at least once because when you do ask you to accept the rules, if 
you've forgotten, to do that, kg download will give you a hint It'll say: 
oh, it looks like you might have forgotten the rules. If you log into 
cattle with like a Google account like anything other than a username 
password, this won't work so you'll need to click forgot password on Kaggle 
and get them to send you a normal password. So that's the cattle version 
right, and so, when you do that, you end up with a whole folder created for 
you with all of that competition and data in it. So a couple of reasons you 
might want to not use that the first years that you're using a data set 
that's not on cattle. The second is that you don't want all of the data 
sets in a cattle competition, for example the planet, competition that 
we've been looking at a little bit. We'll look at again today has data in 
two formats TIFF and JPEG. The TIFF is 19 gigabytes and the JPEG is 600 
megabytes. </p>

<h3>5. <a href="https://youtu.be/9C06ZPF8Uuc?t=12m5s">00:12:05</a></h3>

<ul style="list-style-type: square;">

<li><b> Cool tip to download only the files you need: using CulrWget</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>So you probably don't want to download both so I'll. Show you a really cool 
kit which actually somebody on the forum taught me, I think, was one of the 
young MSN students here at USF. There's a Chrome extension cord curl wget 
so you can just search for a curl wget, and then you install it by just 
clicking on installed and having an extension before and then from now on. 
Every time you try to download something so I'll, try and download this 
file and I'll just go ahead and cancel it right, and now you see this 
little yellow button. That's added up here: there's a whole command here 
right, so I can copy that and paste it into my window and hit go and it's 
there cuz okay. So what that does is like all of your cookies and headers, 
and everything else needed to download that file is like say so. This is 
not just useful for downloading data. It's also useful. If you like, trying 
to download some. I don't know TV show or something anything where you're 
it's hidden behind a login or something you can. You can grab it, and 
actually that is very useful for data science, because quite often we want 
to analyze things like videos on our on our consoles. So this is a good 
trick. Alright, so there's two ways to get the data, so then, having 
</p>

<h3>6. <a href="https://youtu.be/9C06ZPF8Uuc?t=13m35s">00:13:35</a></h3>

<ul style="list-style-type: square;">

<li><b> Dogs vs Cats example</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Got the data you then need to build your model right, so what I tend to do 
like you'll notice that I tend to assume that the data is in a directory 
called data. That's a subdirectory of wherever your notebook is right. Now 
you don't necessarily actually want to put your data there. You might want 
to put it directly in your home directory or you might wan na put it on 
another drive or whatever. So what I do is, if you look inside my courses, 
do one folder you'll see that data is actually a symbolic link to a 
different drive. Alright, so you can put it anywhere you like, and then you 
can just add a symbolic link or you can just put it there directly. It's up 
to you if you haven't used symlinks before they're, like aliases or 
shortcuts on the mac or windows very handy and there's some threads on the 
forum about how to use them. If you want help with that, that, for example, 
is also how we actually have the fastai modules available from the same 
place as our notebooks, it's just a symlink to where they come from 
anytime. You want to see like where things actually point to in Linux. You 
can just use the L flag listing a directory and that'll show you where the 
symlinks exist still lost I'll, show you which scenes the directories so 
forth. Okay, so one thing which may be a little unclear based on what we've 
done so far, is like how little code you actually need to do this end to 
it. So what I've got here is is in a single window.</p>

<p>Is an entire end-to-end 
process to get a state-of-the-art result? Put cats versus dogs all right, 
I've only step. I've skipped is the bit where we've downloaded it from 
title and then, where we unzip it all right. So these are literally all the 
steps, and so we import our libraries and actually, if you import this one 
Kampf loner that basically imports everything else. So that's that we need 
to tell at the path of where things are the size that we want the batch 
size that we want right so then, and we're going to learn a lot more about 
what these do very shortly. But basically we say how do we want to 
transform our data, so we want to transform it in a way, that's suitable to 
this particular kind of model, and it assumes that the photos aside on 
photos and that we're going to zoom in up to ten percent. Each time we say 
that we want to get some data based on paths, and so remember this is this 
idea that there's a path called cats and the path called dogs and they're 
inside a path called train and a path called valid. Note that you can 
always overwrite these with other things. So if your things are in 
different folders, you could either rename them or you can see here, 
there's like a train name and a bowel name. You can always pick something 
else here. Also notice there's a test name, so if you want to submit some 
into cattle, you'll need to fill in the name, the name of the folder, where 
the test sentence and obviously those those won't be labeled.</p>

<p>So then we 
create a model from a pre-training model. It's from a resonant 50 model 
using this data, and then we call fit and remember by default that has all 
of the layers, but the last few frozen and again we'll learn a lot more 
</p>

<h3>7. <a href="https://youtu.be/9C06ZPF8Uuc?t=17m15s">00:17:15</a></h3>

<ul style="list-style-type: square;">

<li><b> What means “Precompute = True” and “learn.bn_freeze”</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>About what that means - and so that's that's what that does so that that 
took two and a half minutes notice. Here, I didn't say, pre-compute equals 
true again, there's been some confusion on the forums about like what that 
means. It's only a is only something that makes it a little faster for this 
first step right, so you can always skip it and if you're at all confused 
about it or it's causing you any problems, just leave it off right because 
it's just a it's just a Shortcut which caches some of the intermediate 
steps that don't have to be recalculating each time and remember that when 
we are using pre computed, activations data or augmentation doesn't work 
right. So, even if you ask for a data augmentation, if you've got pre 
compute equals true, it doesn't actually do any data augmentation, because 
it's using the cached non augmented activations. So in this case to keep 
this as simple as possible, I have no pre computed anything going on, so I 
do three cycles of length one and then I can then unfreeze. So it's now 
going to train the whole thing something we haven't seen before and we'll 
learn about in the second half is called B and freeze. For now. All you 
need to know is that, if you're using a model like a bigger, deeper model 
like ResNet, 50 or rez next 101 on a data set, that's very very similar to 
imagenet like these cat sandbox data set sort of words. It's like sidon 
photos of standard objects.</p>

<p>You know of a similar size to image turn and 
money somewhere between 200 and 500 pixels. You should probably add this 
line when you unfreeze for those of you that are more advanced. What it's 
doing is it's causing the batch normalization moving averages to not be 
updated, but in the second half of this course we're gon na learn all 
about. Why we do that. It's something that's not supported by any other 
library, but it turns out to be super important anyway. So we do one more 
epoch with training the whole network and then at the end we use test time 
augmentation to ensure that we get the best predictions we can, and that 
gives us ninety nine point, four. Five percent. So that's that's it right. 
So when you try a new data set they're, basically the minimum set of steps 
that you would need to follow. You'll notice. This is assuming I already 
know what learning wrote to use so you'd use the learning rate finder. For 
that. It's assuming that I know that the directory layout and so forth, so 
that's kind of a minimum set. Now one of the things that I wanted to make 
sure you had an understanding of how to do is how to use other libraries 
</p>

<h3>8. <a href="https://youtu.be/9C06ZPF8Uuc?t=20m10s">00:20:10</a></h3>

<ul style="list-style-type: square;">

<li><b> Intro &amp; comparison to Keras with TensorFlow</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Other than fastai - and so I feel like the best thing to to look at, is to 
look at care us because care us is a library just like fastai sits on top 
of pytorch care. Us sits on top of actually a whole variety of different 
backends. It fits mainly people nowadays use it with tensorflow there's, 
also an MX net version, there's also a Microsoft CNT K version. So what 
I've got? If you do a git pull you'll see that there's a something called 
care us less than one where I've attempted to replicate at least parts of 
lesson, one in care us just to give you a sense of how that works. I'm not 
going to talk more about batch two norm, freeze now other than to say, if 
you're, using something which has got a number larger than 34 at the end, 
so like ResNet, 50 or res next 101 and you're trading. A data set that has 
that is very similar to image net. So it's like normal photos of normal 
sizes. Where the thing of interest takes up most of the frame, then you 
probably should at the end fries true after unfreeze, if in doubt try 
trading it with and then try trading it without more advanced students will 
certainly talk about it on the forums this week and We will be talking 
about the details of it in the second half of the course when we come back 
to our CNN in death section in the second last lesson, so with care us 
again, we import a bunch of stuff and remember.</p>

<p>I mentioned that this idea 
that you've got a thing called train and a thing called valid and inside 
that you've got a thing called dogs and things called cats is a standard 
way of providing image, labelled images, so Karis does that too right? So 
it's going to tell it where the training set and the validation set are 
twice what batch size to used now you'll notice in Karis. We need much much 
much more code to do the same thing. More importantly, each part of that 
code has many many many more things you have to set and if you set them 
wrong, everything breaks right so I'll give you a summary of what they are 
so you're. Basically, rather than creating a single data object in chaos, 
we first of all have to define something called a data generator to say 
kind of generate the data, and so a data generator. We basically have to 
say what kind of data augmentation we want to do, and we also we actually 
have to say what kind of normalization do we want to do so we're else with 
fastai. We just say whatever ResNet 50 requires just do that. For me, 
please, we actually have to kind of know a little bit about. What's 
expected of us. Generally speaking, copying and pasting cos code from the 
internet is a good way to make sure you've got the right, the right stuff 
to make that work and again it doesn't have a kind of a standard set of 
like here. The best data augmentation parameters to use for photos, so you 
know I've copied and pasted all of this from the Kaos documentation.</p>

<p>So I 
don't know if it's I don't think it's the best set to use it all, but it's 
the set that they're using in their docks. So having said this is how I 
want to generate data so horizontally fit. Sometimes you know zoom, 
sometimes sheer. Sometimes we then create a generator from that by taking 
that data generator and saying I want to generate images by looking from a 
directory, we pass in the directory, which is of the same directory 
structure that fastai users and you'll, see there's some overlaps with kind 
Of how fastai works here, you tell it what size images you want to create. 
You tell at what batch size you want in your mini batches and then there's 
something you not to worry about too much, but basically, if you're just 
got two possible outcomes, you would generally say binary here. If you've 
got multiple possible outcomes would say categorical, yeah. So we've only 
got cats or dogs, so it's binary. So an example of like where things get a 
little more complex. Is you have to do the same thing for the validation 
set? So it's up to you to create a data generator that doesn't have data 
augmentation because, obviously for the validation set unless you're using 
t/ta. That's going to start things up you also when you train you randomly 
reorder the images so that they're always shown in different orders to make 
it more random, but with a validation.</p>

<p>It's vital that you don't do that, 
because if you shuffle the validation set, you then can't track how well 
you're doing it's in a different order for the labels. That's a basically, 
these are the kind of steps you have to do every time with care us. So 
again, the reason I was using ResNet 50 before is chaos doesn't have ResNet 
34. Unfortunately, so I just wanted to compare like with Mike so we're 
going to use resident 50 here there isn't the same idea with chaos of 
saying, like constructor model that is suitable for this data set for me, 
so you have to do it by hand right. So the way you do it is to basically 
say this is my base model, and then you have to construct on top of that 
manually. The layers that you want to add, and so by the end of this course 
you'll understand a way. It is that these particular three layers, other 
layers, that we add so having done that in chaos, you basically say: okay, 
this is my model and then again there isn't like a concept to it like 
automatically freezing things or an API for that. So you just have to allow 
look through the layers that you want to freeze and call dot. Trainable 
equals false on them. In Karis, there's a concept we don't have in fastai 
or play a torch of compiling a model. So, basically, once your model is 
ready to use, you have to compile it passing in what kind of optimizer to 
use what kind of loss to look for about metric so again with fastai.</p>

<p>You 
don't have to pass this in because we know what loss is the write loss to 
use. You can always override it, but for a particular model we give you 
good defaults, okay, so having done all that rather than calling fit, you 
call generator passing in those two generators that you saw earlier: the 
Train generator in the validation generator for reasons. I don't quite 
understand. Chaos expects you to also tell it how many batches there are 
per epoch, so the number of batches is a quarter the size of the generator 
divided by the batch size. You can tell it how many epochs, just like in 
fastai. You can say how many processes or how many workers to use for 
pre-processing , unlike fastai, the default in chaos, is basically not to 
use any so to get good speed. You're going to make sure you include this, 
and so that's basically enough to start fine tuning the last layers. So, as 
you can see, I got to a validation, accuracy of 95 %, but, as you can also 
see something really weird happened. We're after one it was like 49 and 
then it was 69 and then 95. I don't know why these are so low, that's not 
normal. I may have there may be a bug and chaos. They may be a bug. In my 
code. I reached out on Twitter to see if anybody could figure it out, but 
they couldn't. I guess this is one of the challenges with using something 
like this is one of the reasons I wanted to use fast.</p>

<p>Ai, for this course 
is it's much harder to screw things up, so I don't know if I screwed 
something up or somebody else did yes, you know this is you've, seen the 
chance to float back end yeah yeah, and if you want to run this to try It 
out yourself, you just can just go: pip install tensorflow, GPU, Kerris, 
okay, because it's not part of the faster I environment by default. But 
that should be all you need to do to get that working. So then there isn't 
a concept of like layer, groups or differential learning rates or partial 
unfreezing or whatever. So you have to decide like I had to print out all 
of the layers and decide manually how many I wanted to fine-tune. So I 
decided to fine-tune everything from a layer 140 onwards. So that's why I 
just looked through like this after you change that you have to recompile 
the model and then after that I then ran another step. And again I don't 
know what happened here. The accuracy of the training set stayed about the 
same, but the validation set totally fill in the hole, and I mean the main 
thing to notice. Even if we put aside the validation set, we're getting, I 
mean, I guess the main thing is there's a hell of a lot more code here 
which is kind of annoying, but also the performance is very different. So 
where else is here even on the training set? We're getting like 97 % after 
four epochs that took a total of about eight minutes. You know over here we 
had 99.5 % on the validation set and it ran a lot faster.</p>

<p>So I was like 
four or five minutes right so, depending on what you do, particularly if 
you end up wanting to deploy stuff to </p>

<h3>9. <a href="https://youtu.be/9C06ZPF8Uuc?t=30m10s">00:30:10</a></h3>

<ul style="list-style-type: square;">

<li><b> Porting PyTorch <a href="http://fast.ai/">fast.ai</a> library to Keras+TensorFlow project</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Mobile devices at the moment, the kind of pytorch on mobile situation is 
very early, so you may find yourself wanting to use tensorflow or you may 
work for a company, that's kind of settled on tensorflow. So if you need to 
convert something like redo, something you've learnt here intensive flow, 
you probably want to do it with care us, but just recognize. You know it's 
got to take a bit more work to get there and by default it's much harder to 
get. I mean I to get the same state of the out results you get the faster I 
you'd have to like replicate all of the state-of-the-art algorithms that 
are in first a nice. So it's hard to get the same level of results, but you 
can see the basic ideas are similar. Okay and it's certainly it's certainly 
possible. You know like there's nothing I'm doing in fastai that, like 
would be impossible, but, like you'd, have to implement stochastic gradient 
percent with restarts, you would have to implement differential learning 
rates. You would have to implement batch norm, freezing which you probably 
don't want to do. I know and well that's not quite true, I think somewhat 
one person, at least on the forum is attempting to create a chaos, 
compatible version of or a tensorflow compatible version of fastai, which I 
think I hope will get there. I actually spoke to Google about this a few 
weeks ago and they're very interested in getting faster. I ported to 
tensorflow, so maybe by the time you're looking at this on the mooc.</p>

<p>Maybe 
that will exist. I certainly hope so. We will see hey wait. So Karis is 
Karis. Intensive flow was certainly not you know that difficult to handle, 
and so I don't think you should worry. If you're told you have to learn 
them after this course. For some reason, even let me take you a couple of 
days, I'm sure. So that's kind of most of the stuff. You would need to kind 
of complete this. This kind of assignment from last week, which was like 
try to do everything you've seen already but on the dog of reinstated, said 
just to remind you that kind of last few minutes of last week's lesson. I 
show you how to do much of that, including like </p>

<h3>10. <a href="https://youtu.be/9C06ZPF8Uuc?t=32m30s">00:32:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Create a submission to Kaggle</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>How I actually explored the data to find out like what the classes were and 
how big the images were and stuff like that right. So if you've forgotten 
that or didn't quite follow at all last week, check out the video from last 
week to see one thing that we didn't talk about is how do you actually 
submit to Carol? So how do you actually get predictions, so I just wanted 
to show you that last piece as well and on the wiki thread this week. I've 
already put a little image of this to show you these days. But if you go to 
the kaggle website for every competition, there's a section called 
evaluation and they tell you what it's a bit, and so I just copied and 
pasted these two lines from from there. And so it says we're expected to 
submit a file where the first line contains the the work, the word ID and 
then a comma separated list of all of the possible dog breeds. And then 
every line after that will contain the idea itself, followed by all the 
probabilities of all the different dog breeds. So how do you create that so 
recognize that inside our data object, there's a dot classes which has got 
in alphabetical order? All of the four other classes, and then so it's got 
all of the different classes and then inside data dot test data set just 
yes, you can also see there's all the file names so and just to remind you, 
dogs and cats.</p>

<p>Sorry, cats dog breeds was not provided in the kind of care, 
our style format where the dogs and cats from different folders, but 
instead it was provided as a CSV file of labels right. So when you get a 
CSV file of labels, you use image classifier data from CSV, rather than 
image classifier data from parts there isn't an equivalent in care us so 
you'll, see like on the cattle forums. People share scripts for how to 
convert it to a care. Our style folders, but in our case we don't have to 
we just go image: classifier data from CSV passing in that CSV file, and so 
the CSV file will, you know, has automatically told the data. You know what 
the masses are and then also we can see from the folder of test images, 
what the file names of those are. So, with those two pieces of information 
we're ready to go. So I always think it's a good idea to use TTA, as you 
saw with that dogs and cats example. Just now. It can really improve 
things, particularly when your model is less good, so I can say, learn dot, 
t ta and if you pass in yeah, if you pass in is test equals true, then it's 
going to give you predictions on the test set rather than the validation 
Set okay and now, obviously, we can't nowget an accuracy or anything 
because by definition we don't know the labels for the test set right. So 
by default, most high-touch models.</p>

<p>Give you back the log of the 
predictions. So then we just have to go X. Of that to get back out 
probabilities, so in this case the test set had ten thousand three hundred 
and fifty seven images in it and there are 120 possible breeds all right. 
So we get back a matrix of of that size, and so we now need to turn that 
into something that looks like this, and so the easiest way to do that is 
with pandas. If you're not familiar with pandas. There's lots of 
information online about it or check out the machine learning course intro 
to machine learning that we have where we do lots of stuff with pandas. But 
basically we can describe PD data frame and pass in that matrix and then we 
can say the names of the columns are equal to data duck classes and then 
finally, we can insert a new column at position. 0 called ID that contains 
the file names, but you'll notice that the file names contain five letters 
at the end. I start we don't want and four letters at the end we don't 
want, so I just subset in like so right. So at that point I've got a data 
frame that looks like this, which is what we want, so you can now call a 
data frame data, so social cues dated DF not des. Let's fix it now, data 
frame. Okay, so you can now call data frame to CSV and quite often you'll 
find these files actually get quite big. So it's a good idea to say 
compression equals gzip and that'll, zip it up on the server for you and 
that's going to create a zipped up.</p>

<p>Csv file on the server on wherever 
you're running is Jupiter notebook. So you need apps that you now need to 
get that back to your computer, so you can upload it or you can use carol 
CLA, so you can type kgs submit and do it that way. I generally download it 
to my computer so like how often back to this lab double check. It all 
looks, ok, so to do that. There's a cool little theme called file link and 
if you run file link with a path on your server, it gives you back a URL 
which you can click on and it will download that file from the server onto 
your computer. So if I click on that now I can go ahead and save it, and 
then I can see in my downloads there it is here's my submission file, they 
want to open their yeah and, as you can see, it's exactly what I asked for 
there's my ID And 120 different dog breeds and then here's my first row 
containing the file name and the 120 different probabilities. Okay, so then 
you can go ahead and submit that to cattle through their through their 
regular form, and so this is also a good way. You can see. We've now got a 
good way of both grabbing any file off the internet and getting a to our 
AWS instance or paper space or whatever, by using the cool little extension 
in chrome and we've also got a way of grabbing stuff off our server easily. 
Those of you that are more command line oriented.</p>

<p>You can also use SCP, of 
course, but I kind of like doing everything through the notebook all 
</p>

<h3>11. <a href="https://youtu.be/9C06ZPF8Uuc?t=39m30s">00:39:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Making an individual prediction on a single file</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Right one other question I had during the week was like what, if I want to 
just get a single, a single file that I want to, you know, get a prediction 
for so, for example, you know. Maybe I want to get this first file from my 
validation set, so there's its name, so you can always look at a file just 
by calling image, dot open that just uses regular imaging library, and so 
what you can do is there's actually I'll show you the Shortest version, you 
can just call learn, predict array passing in your your image. Okay, now 
the image needs to have been transformed, so you've seen, transform Trent 
transforms from model before normally we just put put it all in one 
variable, but actually behind the scenes it was returning to things it was 
returning. Training transforms and validation transforms, so I can actually 
split them apart, and so here you can see, I'm actually applying example. 
My training transforms or probably more likely that would apply, validation 
transforms. That gives me back an array containing the image. The 
transformed image, which I can then past everything that gets passed to or 
returned from bottles is generally assumed to be a mini batch right. It's 
generally assumed to be a bunch of images. So we'll talk more about some 
numpy tricks later, but basically, in this case we only have one image, so 
we have to turn that into a mini batch of images.</p>

<p>So, in other words, we 
need to create a tensor that basically is not just rows by columns by 
channels, but it's number of image by rows by columns by channels and as 
one image. So it's basically becomes a 4 dimensional tensor. So, there's a 
cool little trick in numpy that if you index into an array with none that 
basically adds additional unit access to the start. So it turns it from an 
image into a mini batch of one images. And so that's why we had to do that. 
So if you basically find you're trying to do things with a single image 
with any kind of pytorch or fastai thing, this is just something you might. 
You might find. It says, like expecting. Four dimensions only got three; it 
probably means that or if you get back a return value from something that 
has like some weird first access, that's probably why it's probably giving 
you like back a mini batch. Okay, and so we'll learn a lot more about this. 
But it's just something: </p>

<h3>12. <a href="https://youtu.be/9C06ZPF8Uuc?t=42m15s">00:42:15</a></h3>

<ul style="list-style-type: square;">

<li><b> The theory behind Convolutional Networks, and Otavio Good demo (Word Lens)</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>To be aware of okay, so that's kind of everything you need to do in 
practice. So now we're going to kind of get into a little bit of theory, 
what's actually going on behind the scenes with these convolutional neural 
networks, and you might remember it back in Lesson one. We actually saw our 
first little bit of theory, which we stole from this fantastic websites, a 
toaster dot, IO, either explained visually, and we learnt that a that. A 
convolution is something where we basically have a little matrix in deep 
learning, nearly always three by three. A little matrix that we basically 
multiply every element of that matrix by every element of a three by three 
section of an image add them all together to get the result of that 
convolution at one point all right now, let's see how that all gets turned 
together To create these, these various layers that we saw in the the 
Zeiler and burgers paper and to do that again, I'm going to steal off 
somebody who's, much smarter than I am we're going to steal from a guy 
called Ottavia good Ottavia. Oh good was the guy who created Word Lens, 
which nowadays is part of Google Translate. If I'm Google Translate you've 
ever like done that thing, where you point your camera at something at 
something which has any kind of foreign language on it and in real-time it 
overlays it with the translation. That was a views company that built that, 
and so Tokyo was kind enough to share this fantastic video.</p>

<p>He created he's 
at Google now and I want to kind of step you through works. I think it 
explains really really well what's going on and then after we look at the 
video we're going to see how to implement the whole a whole sequence of 
kintyre set of layers of convolution on your network in Microsoft, Excel so 
with you're, a visual learner Or a spreadsheet learner, hopefully you'll be 
able to understand all this okay, so we're going to start with an image and 
something that we're going to do later in the course is we're going to 
learn to nice digits. So we'll do it like end to end we'll. Do the whole 
thing, so this is pretty similar, so we're going to try and recognize in 
this case letters so here's an A which, obviously it's actually a grid of 
numbers right and so there's the creative numbers, and so what we do is we 
take our first Convolutional filter, so we're assuming this is all this is 
assuming that these are already learned right and you can see this point. 
It's got wiped down the right-hand side right and black down the left, so 
it's like zero, zero, zero, maybe negative. 1 negative 1 negative. 1. 0, 0, 
0, 1, 1, 1, and so we're taking each 3x3 part of the image and multiplying 
it by that 3x3 matrix, not as a matrix product that an element-wise 
product. And so you can see what happens is everywhere, where the the white 
edge is matching the edge of the a and the black edge.</p>

<p>Isn't we're getting 
green we're getting a positive and everywhere, where it's the opposite, 
we're getting a negative we're getting a red right, and so that's the first 
filter creating the first that the result of the first kernel right and so 
here's a new kernel. This one is: it's, got a white stripe along the top 
right, so we literally scan through every 3x3 part of the matrix 
multiplying those 3 bits of the a the neighbors of the a by the 9 bits as a 
filter to find out whether it's red or Green and how red or green it is ok, 
and so this is assuming we had two filters. One was a bottom edge, one was 
a left edge and you can see here the top edge - not surprisingly it's red 
here, so a bottom edge was red here and green here, the right edge right 
here in green here and then in the next step. We add a non-linearity, ok, 
the rectified linear unit, which literally means strongly the negatives. So 
here the Reds all gone: okay, so here's layer 1, the input, here's layup to 
the result of 2 convolutional filters, here's layer 3, which is which is 
throw away all of the red stuff and that's called a rectified linear unit 
and then layer 4 is something Called a max pull on a layer 4, we replace 
every 2 by 2 part of this grid and we replace it with its maximum mat. So 
it basically makes it half the size. It's basically the same thing but half 
the size, and then we can go through and do exactly the same thing.</p>

<p>We can 
have some new filter three by three filter that we put through each of the 
two results of the previous layer. Okay and again, we can throw away the 
red bits right so get rid of all the negatives, so we just keep the 
positives. That's called applying a rectified linear unit and that gets us 
to our next layer of this convolutional neural network. So you can see that 
by you know at this layer back here it was kind of very interpretive. All 
it's like we've either got bottom edges or left edges, but then the next 
layer was combining the results of convolutions. So it's starting to become 
a lot less clear, like intuitively, what's happening, but it's doing the 
same thing and then we do another max pull right. So we replace every 2x2 
or 3x3 section with a single digit. So here this 2x2, it's all black. So we 
replaced it with a black right and then we go and we take that and we we 
compare it to basically a kind of a template of what we would expect to see 
if it was an a it was a B, but the see it was D, give it an E and we see 
how closely it matches and we can do it in exactly the same way. We can 
multiply every one of the values in this four by eight matrix with every 
one of the four by eight in this one, and this one and this one - and we 
add - we just add them together to say like how often does it match versus 
how Often does it not match and then that could be converted to give us a 
percentage probability that this is a no.</p>

<p>So in this case this particular 
template matched well with a so notice, we're not doing an each training 
here right. This is how it would work if we have a pre trained model all 
right, so when we download a pre trained imagenet model off the internet 
and isn't on an image without any changing to it. This is what's happening 
or if we take a model that you've trained and you're, applying it to some 
test set or for some new image. This is what it's doing all right as it's 
basically taking it through it, buying a convolution to each layer to each 
multiple convolutional filters to each layer and then doing the rectified 
linear unit, so throw away the negatives and then do the max pull and then 
repeat That a bunch of times - and so then we can do it with a new letter, 
A or letter B or whatever, and keep going through that process right. So, 
as you can see, that's far nice, the visualization thing and I could have 
created because I'm not at a vo. So thanks to him for sharing this with us, 
because it's totally awesome he actually. This is not done by hand. He 
actually wrote a piece of computer software to actually do these 
</p>

<h3>13. <a href="https://youtu.be/9C06ZPF8Uuc?t=49m45s">00:49:45</a></h3>

<ul style="list-style-type: square;">

<li><b> ConvNet demo with Excel,</b></li>

<li><b>filter, Hidden layer, Maxpool, Dense weights, Fully-Connected layer</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Convolutions, this is actually being actually being done, dynamically, 
which is pretty cool, so I'm more of a spreadsheet guy. Personally, I'm a 
simple person. So here is the same thing now in spreadsheet. All right and 
so you'll find this in the github repo. So you can either get clone the 
repo to your own computer, open up the spreadsheet or you can just go to 
github.com, slash, /, ji and click on this. It's it's inside. If you go to 
our repo and just go to courses as usual, go to deal 1 as usual, you'll 
see, there's an Excel section there, okay, and so he lay all that, so you 
can just download them by clicking them or you can clone the whole repo And 
we're looking at cognitive example, convolution example all right, so you 
can see. I have here an input right. So in this case the input is the 
number 7. So I grab this from a dataset called m-must, MN ist, which we'll 
be looking at in a lot of detail, and I just took one of those digits at 
random and I put it into Excel, and so you can see every Hextall is 
actually just a number Between 9 1, okay, very often actually it'll be a 
bite between Norton 255 or sometimes it might be a float between naught and 
1. It doesn't really matter by the time it gets to pytorch, we're generally 
dealing with floats. So we, if one of the steps we often will take, will be 
to convert it to a number between naught 1. So then, you can see I've just 
use conditional formatting in Excel to kind of make the higher numbers more 
red. So you can clearly see that this is a red.</p>

<p>This is a 7, but but it's 
just a bunch of numbers that have been imported into Excel. Okay, so here's 
our input so remember what at a via did was he then applied two filters 
right with different shapes, so here I've created a filter which is 
designed to detect top edges. So this is a 3 by 3 filter. Okay and I've got 
ones along the top zeroes in the middle minus ones at the bottom right. So 
let's take a look at an example. That's here right, and so, if I hit that 
you can see here highlighted this is the 3 by 3 part of the input that this 
particular thing is calculating right. So here you can see, it's got. 1 1 1 
are all being multiplied by 1 and point 1. 0 0 are all being multiplied by 
negative 1. Okay, so in other words, all the positive bits are getting a 
lot of positive. The negative bits are getting nearly nothing at all. So we 
end up with a high number okay, where else on the other side of this bit of 
the 7 right, you can see how you know this is basically zeros here or 
perhaps more interestingly, on the top of it. Okay, here we've got high 
numbers at the top, but we've also got high numbers at the bottom, which 
are negating it ok, so you can see that the only place that we end up 
activating is where we're actually at an edge. So, in this case this here 
this number 3, this is called an activation.</p>

<p>Ok, so when I say an 
activation, I mean ah at number, a number that is calculated and it is 
calculated by taking some numbers from the input and applying some kind of 
linear operation, in this case, a convolutional kernel to calculate an 
output right, you'll notice, that Other than going inputs multiplied by 
kernel and summing it together right. So here's my some and here's my x 
then take that and I go max of 0 comma that and so that's my rectified 
linear unit. So it sounds very fancy rectified linear unit, but what they 
actually mean is open up, Excel and type equals max 0, comma C. Ok, that's 
all about! Then you'll see people in the biz so to say value a so ral. You 
means rectified. Linear unit means max 0 comma thing and I'm not like 
simplifying it. I really mean it like when I say like, if I'm simplifying, 
I always say so, I'm simplifying, but if I'm not saying I'm simplifying, 
that's the entirety, okay, so a rectified linear unit in its entirety is 
this and a convolution in its entirety is? Is this okay? So a single layer 
of a convolutional neural network is being implemented in its entirety here 
in Excel okay, and so you can see what it's done is it's deleted pretty 
much the vertical edges and highlighted the horizontal edges. So again, 
this is assuming that our network is trained and that, at the end of 
training it a created a convolutional filter with these specific line 
numbers in - and so here is a second convolutional filter.</p>

<p>It's just a 
different line numbers now: pytorch doesn't store them as two separate nine 
digit arrays. It stores it as a tensor right. Remember, a tensor just means 
an array with more dimensions. Okay, you can use the word array as well. 
It's the same thing, but in pytorch they always use the word tensor, so I'm 
going to say cancer, okay. So it's just a tensor with an additional axis 
which allows us to stack each of these filters together. Right, filter and 
kernel. Pretty much mean the same thing: yeah right. It refers to one of 
these three by three matrices or one of these three by three slices of a 
three dimensional tensor. So if I take this one - and here I've literally 
just copied the formulas in Excel from above. Okay - and so you can see 
this, one is now finding a vertebra which, as we would expect okay, so 
we've now created one layer right. This here is a layer them specifically 
we'd, say it's a hidden layer which is it's not an input layer and it's not 
an output layer. So everything else is a hidden layer. Okay, and this 
particular hidden layer has is a size two on this dimension right because 
it has two filters right, two kernels, so what happens next? Well, let's do 
another one! Okay, so, as we kind of go along, things can multiply a little 
bit in complexity right because my next filter is going to have to contain 
two of these three by threes, because I'm gon na have to say how do I want 
to bring Adam? I want to write these three things and at the same time, how 
do I want to wait the corresponding three things down here right because in 
pytorch, this is going to be.</p>

<p>This whole thing here is going to be stored 
as a multi-dimensional tensor right. So you shouldn't really think of this 
now as two 3x3 kernels, but one two by three by three eternal okay, so to 
calculate this value here, I've got the sum product of all of that plus the 
sum product of scroll down all of that. Okay, and so the top ones are being 
multiplied by this part of the kernel, and the bottom ones have been 
multiplied by this part of the kernel and so over time. You want to start 
to get very comfortable with the idea of these, like higher dimensional 
linear combinations right like it's it's harder to draw it on the screen, 
like I had to put one above the other, but conceptually just stuck it in 
your mind like this. That's really how you want to think right and actually 
Geoffrey Hinton in his original 2012 neural Nets. Coursera class has a tip, 
which is how all computer scientists deal with like very high dimensional 
spaces, which is that they basically just visualize the two-dimensional 
space and then say, like twelve dimensions, really fast, and they had lots 
of tires. So that's it right. We can see two dimensions on the screen and 
then you're just going to try to trust that you can have more dimensions 
like the Const. It's just you know, there's there's nothing different about 
them, and so you can see in Excel, you know Excel, doesn't have the ability 
to handle three-dimensional tensors. So I had to like say: okay, take this 
two-dimensional dot product.</p>

<p>Add on this two-dimensional dot product right, 
but if there was some kind of 3d Excel, I could have to stand that in a 
single line, all right and then again apply max 0, comma, otherwise known 
as rectified linear unit, otherwise known as value okay. So here is my 
second layer, and so when people create different architectures write, an 
architecture means like how big is your kernel at layer 1? How many filters 
are in your kernel at layer 1, so here I've got a 3 by 3 where's number 1 
and a 3 by 3 there's number 2. So like this architecture, I've created 
starts off with 2 3 by 3 convolutional kernels and then my second layer has 
another two kernels of size: 2 by 3 by 3. So there's the first one and then 
down here, here's a second 2 by 3 by 3 kernel. Okay, and so remember, one 
of these specific any one of these numbers is an activation okay. So this 
activation is being calculated from these three things here and other 3 
things up there and we're using these this 2 by 3 by 3 kernel, okay, and so 
what tends to happen? Is people generally give names to their layers? So I 
say: okay, let's call this layer here, con 1 and this layer here and this 
and this layer here con all right. So that's you know, but generally you'll 
just see that, like when you print out a summary of a network, every layer 
will have some kind of name. Okay, and so then what happens next well part 
of the architecture is like.</p>

<p>Do you have some max pooling where bounces up 
Matt spalling happen, so in this architecture, we're inventing we're going 
to next step is do max fully. Okay, Matt spooling is a little hard to kind 
of show in Excel, but we've got it so max pooling. If I do a two by two max 
pooling it's going to have the resolution both height and width, so you can 
see here that I've replaced these four numbers with the maximum of those 
four numbers right. And so because I'm having the resolution, it only makes 
sense to actually have something: every two cells - okay, so you can see 
here the way I've got kind of the same looking shape as I had back here: 
okay, but it's now half the resolution so for placed Every two by two with 
its max and you'll notice, like it's not every possible two by two I skip 
over from here. So this is like starting at beat Hugh and then the next one 
starts at BS right, so they're like non-overlapping. That's why it's 
decreasing the resolution? Okay, so anybody who's comfortable with 
spreadsheets. You know you can open this and have a look, and so after our 
max pooling, there's a number of different things we could do next and I'm 
going to show you a kind of classic old style approach nowadays, in fact, 
what generally happens nowadays is we do A max pool where we kind of like 
max across the entire size right but on older architectures and also on all 
the structured data stuff. We do.</p>

<p>We actually do something called a fully 
connected layer and so here's a fully connected layer. I'm going to take 
every single one of these activations and I've got to give every single one 
of them or weight right, and so then, I'm going to take over here here is 
the sum product of every one of the activations by every one of the 
weights. For both of the two levels of my three-dimensional tensor right - 
and so this is called a fully connected layer notice, it's different to a 
convolution - I'm not going through a few at a time right, but I'm creating 
a really big weight matrix right so rather than having A couple of little 
3x3 kernels, my weight matrix, is now as big as the entire input, and so, 
as you can imagine, architectures that make heavy use of fully 
convolutional layers can have a lot of weights, which means they can have 
trouble with overfitting, and they can Also be slow and so you're going to 
see a lot, an architecture called vgg because it was the first kind of 
successful, deeper architecture. It has up to 19 layers and vgg actually 
contains a fully connected layer with 4096 weights connected to a hidden 
layer with 4,000. Sorry, 4096 activations connected to a hidden layer with 
4096 activations, so you've got like 4096 by 4096 x, remember or apply it 
by the number of kind of kernels that we've calculated so in vgg.</p>

<p>There's 
this, I think it's like 300 million weights, of which something like 250 
million of them, are in these fully connected layers, so we'll learn later 
on in the course about how we can kind of avoid using these big, fully 
connected layers and behind the scenes. All the stuff that you've seen us 
using like ResNet and res next, none of them use very large, fully 
connected layers. You know you had a question. Sorry yeah come on um. So 
could you tell us more about, for example, if we had like three channels 
for the input, what would be the shape yeah these filters right? So that's 
a great question. So if we have 3 channels of input, it would look exactly 
like conv one right cons. One kind of has two channels right, and so you 
can see with cons one. We had two channels so therefore our filters had to 
have like two channels per filter, and so you could like imagine that this 
input didn't exist. You know - and actually this was the airport alright. 
So when you have a multi-channel input, it just means that your filters 
look like this and so images often full color. They have three red, green 
and blue. Sometimes they also have an alpha Channel. So, however, many you 
have that's how many inputs you need - and so something which I know 
Jeanette was playing with recently, was like using a full color image net 
model in medical imaging for something called bone age calculations which 
has a single channel and so what she Did was basically take the the input, 
the the single channel input and make three copies of it.</p>

<p>So you end up 
with basically like one two three versions of the same thing, which is like 
it's kind of a small idea like it's kind of redundant information that we 
don't quite want. But it does mean that then, if you had a something that 
expected a three channel, convolutional filter, you can use it right and so 
at the moment, there's a cable competition for iceberg detection using a 
some funky satellite specific data format that has two channels. So here's 
how you could do that you could either copy one of those two channels into 
the third channel or I think what people in Carroll are doing is to take 
the average of the two again. It's not ideal, but it's a way that you can 
use. Pre-Trained networks - yeah I've done a lot of fiddling around like 
that. You can also actually I've actually done things where I wanted to use 
a three channel image net Network on four channel data. I had a satellite 
data where the fourth channel was near-infrared, and so basically, I added 
an extra kind of level to my convolutional kernels that were all zeros and 
so basically like started off by ignoring the near-infrared band, and so 
what happens? It basically and you'll see this next week is that, rather 
than having these like carefully trained filters, when you're actually 
training something from scratch, we're actually going to start with random 
numbers. That's actually what we do.</p>

<p>We actually start with random numbers, 
and then we use this thing called stochastic gradient descent, which we've 
kind of seen conceptually to slightly improve those random numbers. 
</p>

<h3>14. <a href="https://youtu.be/9C06ZPF8Uuc?t=1h8m30s">01:08:30</a></h3>

<ul style="list-style-type: square;">

<li><b> ConvNet demo with Excel (continued)</b></li>

<li><b>output, probabilities adding to 1, activation function, Softmax</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>To make them less random and we basically do that again and again and 
again: okay, great, let's take a seven minute break and we'll come back at 
7:50, all right! So what happens next, so we've got as far as doing a fully 
connected layer right. So we had our the results of our max pooling layer 
got fed to a fully connected layer and he might notice those of you that 
remember your linear algebra, the fully connected layer is actually doing a 
classic traditional matrix product. Okay. So it's basically just going 
through each pair in turn, multiplying them together and then adding them 
up to do a matrix product now in practice. If we want to calculate which 
one of the ten digits we're looking at their single number, we've 
calculated isn't enough, we would actually calculate ten numbers, so what 
we will have is, rather than just having one set of fully connected weights 
like this and I say, set Because remember, there's like a whole 3d kind of 
tensor of them, we would actually need ten of those right, so you can see 
that these tensors start to get a little bit high, dimensional right, and 
so this is where my patients we're doing it. Next cell ran out, but imagine 
that I had done this ten times. I could now have ten different numbers or 
being calculated yeah using exactly the same process. Right we'll just be 
ten of these fully connected to by m-by-n erased. Basically, and so then we 
would have ten numbers being spat out.</p>

<p>So what happens next so next up we 
can open up a different Excel worksheet entropy example: dot XLS that's got 
two different worksheets. One of them is called soft mass and what happens 
here? Sorry. I've changed domains rather than predicting whether it's the 
number from one not to nine, I'm going to predict whether something is a 
cat, a dog, a plane of Fisher Building. Okay, so out of our that fully 
connected layer, we've got this case. We'd have five numbers and notice at 
this point. There's no rail, you! Okay! In the last layer, there's no rail, 
you, okay, so I can have negatives. So I want to turn these five numbers H 
into a probability. I want to turn it into a probability from naught to one 
that it's a cat, that's a dog, there's a plane that it's a fish that it's a 
building, and I want those probabilities to have a couple of 
characteristics. First is that each of them should be between zero and one, 
and the second is that this state together should add up to one right. It's 
definitely one of these five things. Okay, so to do that, we use a 
different kind of activation function. What's an activation function, an 
activation function is a function that is applied to activations so, for 
example, max 0 comma. Something is a function that I applied to an 
activation, so an activation function always takes in one number and spits 
out one number so max of 0. Comma X takes in a number X and spits out some 
different number value of s.</p>

<p>That's all an activation function is, and if 
you remember back to that PowerPoint we saw in Lesson one. Each of our 
layers was just a linear function and then, after every layer we said we 
needed some non-linearity act as if you stack a bunch of linear layers 
together right then all you end up with is a linear layer. Okay, so 
somebody's talking can. Can you not a slow just acting? Thank you if you 
stack a number of linear functions together, you just end up with a linear 
function and nobody does any cool, deep learning with displaying your 
functions right, but remember. We also learnt that by stacking linear 
functions with between each one, a non-linearity we could create like 
arbitrarily complex shapes, and so the non-linearity that we're using after 
every hidden layer is a rally rectified linear unit. A non-linearity is an 
activation function. An activation function is a non-linearity in with in 
deep way, obviously there's lots of other nonlinearities and in the world, 
but in deep learning. This is what we mean, so an activation function is 
any function that takes some activation in as a single number and spits 
out. Some new activation like max of 0 comma, so I'm now going to tell you 
about a different activation function. It's slightly more complicated than 
value, but not too much. It's called soft max soft max only ever occurs in 
the final layer at the very end and the reason.</p>

<p>Why is that soft max always 
spits out numbers as an activation function that always spits out a number 
between Norton 1 and it always spits out a bunch of numbers that add to 1. 
So a soft max gives us what we want right in theory, this isn't strictly 
necessary right, like we could ask our neural net to learn a set of kernels 
which have you know which which give probabilities that line up as closely 
as possible with what we want, But in general, with deep learning, if you 
can construct your architecture so that the desired characteristics are as 
easy to express as possible, you'll end up with better models like they'll, 
learn more quickly with less parameters. So in this case, we know that our 
probabilities should end up being between 9 1. We know that they should end 
up adding to 1. So if we construct an activation function, which always has 
those features, then we're going to make our neural network do a better 
job, it's gon na make it easier for it. It doesn't have to learn to do 
those things because it all happen automatically. Okay, </p>

<h3>15. <a href="https://youtu.be/9C06ZPF8Uuc?t=1h15m30s">01:15:30</a></h3>

<ul style="list-style-type: square;">

<li><b> The mathematics you really need to understand for Deep Learning</b></li>

<li><b>Exponentiation &amp; Logarithm</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>So, in order to make this work, we first of all have to get rid of all of 
the negatives right like we can't have negative probabilities so to make 
things not being negative. One way we could do. It is just go into the pair 
of right. So here you can see. My first step is to go X of the previous one 
right, and I think I've mentioned this before, but of all the math that you 
just need to be super familiar with to do deep learning. The one you really 
need is logarithms and asks write, all of deep learning and all of machine 
learning. They appear all the time right. So, for example, you absolutely 
need to know that log of x times y equals log of X, plus log of Y, all 
right and like not just know that that's a formula that exists but have a 
sense of like what does that mean? Why is that interesting? Oh, I can turn 
multiplications into additions that could be really handy right and 
therefore log of x over y equals log of X minus log of Y. Again, that's 
going to come in pretty handy, you know, rather than dividing. I can just 
subtract things right and also remember that if I've got log of x equals y, 
then that means a to the y equals x, in other words, log log and E to the 
for the inverse of each other. Okay, again, you just you need to really 
really understand these things and like so, if you, if you haven't spent 
much time with logs and X, for a while, try plotting them in Excel or a 
notebook, have a sense of what shape they are, how they combine Together 
just make sure you're really comfortable with them, so we're using it here 
right, we're using it here.</p>

<p>So one of the things that we know is a to the 
power of something is positive. Okay, so that's great the other thing, 
you'll notice, about e to the power of something is because it's a power 
numbers that are slightly bigger than other numbers, like four, is a little 
bit bigger than 2.8. When you go e to the power of it really accentuates 
that difference, okay, so we're going to take advantage of both of these 
features for the purpose of deep learning. Okay, so we take our the results 
of this fully connected layer. We go e to the power of for each of them and 
then we're gon na yeah and then we're going to add them up. Okay, so here 
is the sum of e to the power of so then here we're going to take e to the 
power of divided by the sum of e to the power of so, if you take all of 
these things divided by their sum, then by definition, All of those things 
must add up to 1 and furthermore, since we're dividing by their sum, they 
must always vary between 0 and 1 because they were always positive. Alright 
and that's it. So that's what softmax is ok, so I've got this kind of doing 
random numbers. Each time right - and so you can see like as I as I look 
through my softmax generally - has quite a few things that are so close to 
zero that they round down to zero. And you know, maybe one thing: that's 
nearly 1 right and the reason for that is what we just talked about, that 
is, with the X just having one number a bit bigger than the others tends to 
like push it out further right.</p>

<p>So, even though my inputs, here around on 
numbers between negative 5 and 5 right, my outputs from the softmax, don't 
really look that random at all, in the sense that they tend to have one big 
number and a bunch of small numbers. And now that's what we want right. We 
want to say like in terms of like. Is this a cat, a dog, a plane, a fish or 
a building? We really want it to say like it's, it's that you know it's. 
It's a dog or it's a plane not like I don't know. Okay, so softmax has lots 
of these cool properties right. It's going to return a probability that 
adds up to 1 and it's going to tend to want to pick one thing particularly 
strongly. Okay. So that's soft mess your net. Could you pass actually bust 
me up? We? How would we do something that, as let's say, you have any 
</p>

<h3>16. <a href="https://youtu.be/9C06ZPF8Uuc?t=1h20m30s">01:20:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Multi-label classification with Amazon Satellite competition</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Imaging you want to count in categorize, I was like cat and the dog or like 
has multiple things, but what kind of function will we try to use so 
happens. We're going to do that right now, so so hope you think about why 
we might want to do that and so runways, where you might want to do, that 
is to do multi-label classification, so we're looking now at listen to 
image models and specifically we're going to Take a look at the planet: 
competition, satellite, imaging competition. Now the satellite imaging 
competition has some similarities to stuff we've seen before right. So 
before we've seen cat versus dog and these images are a cat or a dog 
they're - not Maya, they're, not both right, but the satellite imaging 
competition has stayed as images that look like this and in fact, every 
single one of the images is classified by whether There's four kinds of 
weather, one of which is haze and another of which is clear. In addition to 
which there is a list of features that may be present, including 
agriculture, which is like some some cleared area used for agriculture, 
primary, which means primary rainforest and water, which means a river or a 
creek, so here is a clear day satellite image, showing Some agriculture, 
some primary rainforest and some water features and here's one which is in 
haze and is entirely primary rainforest.</p>

<p>So in this case, we're going to 
want to be able to show we're going to predict multiple things, and so 
softmax wouldn't be good, because softmax doesn't like predicting multiple 
things and like. I would definitely recommend anthropomorphizing your 
activation functions right. They have personalities, okay and the 
personality of the softmax. Is it wants to pick a thing and people forget 
this all the time I've seen many people even well-regarded researchers in 
famous academic papers, using like soft maps for multi-label 
classification. It happens all the time right and it's kind of ridiculous, 
because they're not understanding the personality of their activation 
function. So for multi classification, where each sample can belong to one 
or more classes. We have to change a few things, but here's the good news 
in fastai. We don't have to change anything right. So fastai will look at 
the labels in the CSV and if there is more than one label ever for any 
item, it will automatically switch into like multi-label mode. So I'm going 
to show you how it works behind the scenes, but the good news is you: don't 
actually have to care, it happens anywhere. So if you have multi label 
images, multi label objects. You obviously can't use the classic Kerris 
style approach where things are in folders, because something can't 
conveniently be in multiple folders at the same time. Right, so that's why 
we, you basically have to use the from CSV approach right.</p>

<p>So if we look at 
 an example actually I'll show you, I tend to take you through it right, so 
we can say: okay, this is the CSV file containing our labels. This looks 
exactly the same as I did before, but rather than side on its top down 
alright and top down I've mentioned before that can do our vertical flips. 
It actually does more than that. There's actually eight possible symmetries 
for a square, which is, it can be rotated through 90, 180, 270 or 0 
degrees, and for each of those it can be flipped. And if you think about 
it, for awhile you'll realize that that's a complete enumeration of 
everything that you can do in terms of symmetries to a square, so they're 
called it's called the dihedral group of eight. So if you see in the code 
there's actually a transform or dihedral, that's why it's called that, so 
this transforms will basically do the full set of eight symmetric, 
dihedral, rotations and flips, plus everything which we can do to dogs and 
cats. You know small clinical rotations. A little bit of zooming a little 
bit of contrast and brightness adjustment, so these images are a size 256 
by 256. So I just create a little function here to let me quickly grab, you 
know: data loader of any size, so here's a 256 by 256 once you've got a 
data object inside it. We've already seen that there's things called Val D 
s test D s, train D s: there are things that you can just index into and 
grab a particular image.</p>

<p>So you just use square brackets. 0. You'll also 
see that all of those things have a DL. That's a data loader so des is data 
set. Dl is data motor. These are concepts from PI watch. So if you, Google, 
pytorch data set or pipe watch data loader, you can basically see what it 
means, but the basic idea is a data set. Gives you a single image or a 
single object back a data loader gives you back a mini batch and 
specifically, it gives you back a transformed mini. So that's why, when we 
create our data object, we can pass in num workers and transforms it's like 
how many processes do you want to use? What transforms do you want, and so, 
with with a data loader, you can't ask for an individual image. You can 
only get back at a mini batch and you can't get back a particular mini 
batch. You can only get back the next mini, so something reverses look 
through grabbing a mini batch at a time and so in Python. The thing that 
does that is called a generator right or an iterator. This slightly 
different versions are the same thing so to turn a data loader into an 
iterator. You use the standard Python function, cordetta, that's a Python 
function, just a regular part of the Python basic language that returns you 
an iterator and an iterator is something that takes. You can pass the 
static, give pass it to the standard, Python function or statement next, 
and that just says give me another batch from this iterator so we're. 
Basically, this is one of the things I really like about pytorch.</p>

<p>Is it 
really leverages modern, pythons kind of stuff you know in in tensorflow 
they invent their whole new world earth ways of doing things, and so it's 
kind of more in a sense, it's more like cross-platform, but in another 
sense like it's, not a good fit to Any platform, so it's nice, if you, if 
you know Python well, pytorch comes very naturally. If you don't know 
Python well, pytorches are good reason to learn Python. Well, a pytorch, 
your module neural network module is a standard Python bus, for example. So 
any work you put into learning Python better will pay off with paid watch. 
So here I am using standard Python, iterators and next to grab my next mini 
batch from the validation sets data loader and that's going to return two 
things. It's going to return the images in the mini batch and the labels of 
the mini, so standard Python approach. I can pull them apart like so, and 
so here is one mini batch of labels, and so not surprisingly, since I said 
that my batch size, let's go ahead and find it Oh actually, it's the batch 
size by default is 64, so I didn't pass in a Batch size, and so just 
remember, shift tab to see like what are the things you can pass and what 
are the defaults so by default, my batch size is 64, so I've got that 
something of size 64 by 17. So there are 17 of the possible classes. Right 
so, let's take a look at the zeroth set of labels, so the zeroth images 
labels. So I can zip again standard Python things.</p>

<p>It takes two lists and 
combines it. So you get the zero theme from the first list as you're asking 
for the second list and the first thing for the first first, this first 
thing from the second list and so forth. So I can zip them together and 
that way I can find out for the zeroth image and the validation set is 
agriculture. It's clear, its primary rainforest, its slash-and-burn, its 
water? Okay. So, as you can see here, this is a MOLLE label. You see here's 
a way to do multi-label classification, so, by the same token, right, if we 
go back to our single label classification, it's a cat dog playing official 
building behind the scenes we haven't actually looked at it, but behind the 
scenes fastai imply torch are Turning our labels into something called one 
hot encoded labels, and so if it was actually a dog than the actual values 
would be like that right. So these are like the actuals okay. So do you 
remember at the very end of at AV, o --'s video? He showed how, like the 
template, had to match to one of the like five ABCDE templates, and so what 
it's actually doing is it's. Comparing when I said it's basically doing a 
dot product, it's actually a fully connected layer at the end right that 
calculates an output activation that goes through a soft Max and then the 
soft max is compared to the one hot encoded label right.</p>

<p>So if it was a 
dog, there would be a one here and then we take the difference between the 
actuals and the softmax activation is to say and add those add up those 
differences to say how much error is there essentially we're skipping over 
something called a Loss function that we'll learn about next week, but 
essentially we're basically doing that now, if it's one hot encoded like 
there's only one thing which have a 1 in it, then actually storing it as 0 
1 0 0 0 is terribly inefficient. Right like we could basically say what are 
the index of each of these things right, so we can say it's like 0, 1, 2, 
3, 4, like so right, and so, rather than storing it is 0. 1. 0. 0. 0. We 
actually just store the index value right. So if you look at the the Y 
values for the cats and dogs, competition or the dog breeds competition, 
you won't actually see a big lists of ones and zeros like this you'll see a 
single integer right, which is like what what class index is it right And 
internally inside pipe arch, it will actually turn that into a one hot 
encoded vector but, like you will literally never see it. Okay and and 
pytorch has different loss functions where you basically say this thing's 
won. This thing is one hot encoder door. This thing is not, and it uses 
different bus functions, that's all hidden by the faster I library right so 
like you, don't have to worry about it, but is.</p>

<p>But the the cool thing to 
realize is that this approach for multi-label encoding with these ones and 
zeros behind the scenes, the exact same thing happens for single level 
classification. Does it make sense to change the beginners of the sigmoid 
of the softmax function by changing the base? No because when you change 
the more math log base, a of B equals log B over log a so changing the base 
is just a linear scaling and linear scaling is something which the neural 
net can with very easily </p>

<h3>17. <a href="https://youtu.be/9C06ZPF8Uuc?t=1h33m35s">01:33:35</a></h3>

<ul style="list-style-type: square;">

<li><b> Example of improving a “washed-out” image</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Good question: okay, so here is that image right here is the image with 
slash-and-burn water, etc, etc. One of the things to notice here is like 
when I first displayed this image, it was so washed out. I really couldn't 
see it right, but remember images now. You know we know, images are just 
matrices of numbers, and so you can see here, I just said times 1.4 just to 
make it more visible right so, like now that you kind of it's, the kind of 
thing I want you to get familiar with is the Idea that this stuff you're 
dealing with they're just matrices of numbers and you can fiddle around 
with them, so if you're looking at something like guys a bit washed out, 
you can just multiply it by something to brighten it up a bit okay. So here 
we can see. I guess this is the slash-and-burn. Here's, the river, that's 
the water, here's, the primary rainforest, maybe that's the agriculture so 
forth. Okay! So so you know with all that background, how do we actually 
use this exactly the same way as everything we've done before right? So you 
know size and - and the interesting thing about playing around with this 
planet competition - is that these images are not at all like image there, 
and I would guess that the vast majority is of stuff that the vast majority 
of you do, involving convolutional neural Nets. Won't actually be anything 
like image net, you know, it'll be it'll, be medical, imaging it'll, be 
like classifying different kinds of steel, tube or figuring out whether a 
world you know is going to break or not or or looking at satellite images, 
or you know whatever right.</p>

<p>So it's it's good to experiment with stuff like 
this planet, competition to get a sense of kind of what you want to do, and 
so you'll see here I start out by resizing my data to 64 by 64. It starts 
out at 256 by 256 right now. I wouldn't want to do this for the cats and 
dogs competition because it cats end on competition. We start with a pre 
trained imagenet Network. It's it's nearly isn't. It starts off nearly 
perfect right. So if we resized everything to 64 by 64 and then retrained 
the whole set regular, it we'd basically destroy the weights that are 
already pre trained to be very good. Remember, imagenet, most imagenet 
models are trained at either 224 by 224 or $ 2.99 by 299. All right, so, if 
we like retrain them at 64 by 64, we're going to we're going to kill it. On 
the other hand, there's nothing in image net. That looks anything like 
this. You know, there's no satellite images, so the only useful bits of the 
image net Network for us are kind of layers like this one. You know finding 
edges and gradients, and this one you know finding kind of textures and 
repeating patterns, and maybe these ones are kind of finding more complex 
textures, but that's probably about it right. So so, in other words, you 
know, starting out by training. Very small images works pretty well when 
you're using stuff like satellites. So in this case I started right back at 
64 by 64 grab.</p>

<p>Some data built my model found out what learning rate to use 
and, interestingly, it turned out to be quite high. It seems that, because, 
like it's so unlike imagenet, I needed to do quite a bit more fitting with 
just that last layer before it started to flatten out </p>

<h3>18. <a href="https://youtu.be/9C06ZPF8Uuc?t=1h37m30s">01:37:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Seting different learning rates for different layers</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Then I unfreeze dit and again, this is the difference to image net like 
datasets is my learning rate in the initial layer. I set 2/9 the middle 
layers, I said 2/3, where else for stuff like it's like image net, I had a 
multiple of 10. Each of those you know again the idea being that that 
earlier layers, probably and not as close to what they need to be compared 
to the like dances again unfreeze train for a while, and you can kind of 
see here. You know there's cycle one there's cycle, there's cycle three and 
then I kind of increased double the size with my images fit for a while and 
freeze fit for a while double the size of the images again fit for a while. 
I'm freeze for a while and then add TTA, and so, as I mentioned last time, 
we looked at this. This process ends up. You knowgetting us about 30th 
place in this competition, which is really cool because people, you know a 
lot of very, very smart people. Just a few months ago worked very very hard 
on this competition. A couple of </p>

<h3>19. <a href="https://youtu.be/9C06ZPF8Uuc?t=1h38m45s">01:38:45</a></h3>

<ul style="list-style-type: square;">

<li><b> ‘data.resize()’ for speed-up, and ‘metrics=[f2]’ or ‘fbeta_score’ metric</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Things people have asked about, one is: what is this data dot resize do so 
a couple of different pieces here. The first is that when we say back here, 
what transforms do we apply and here's our transforms? We actually pass in 
a size right. So one of the things that that one of the things that data 
loaded does is to resize the images like on-demand every time it sees them. 
It's got nothing to do with that dot, resize method right. So this is this 
is the thing that happens at the end like whatever's passed in before it 
hits out that before our data loader spits it out, it's going to resize it 
to this size. If the initial input is like a thousand by a thousand 
reading, that JPEG and resizing it to 64 by 64 turns out to actually take 
more time than training the content, that's for each batch all right! So 
basically all resize does, is it says: hey I'm not going to be using any 
images bigger than size times 1.3, so just grow through once and create new 
JPEGs of this size right and they're rectangular right so new JPEGs, where 
the smallest edges of this size And again, it's like you never have to do 
this, there's no reason to ever use it. If you don't want to it's just a 
speed-up okay, but if you've got really big images coming in it saves you a 
lot of time and you'll often see on like Carol kernels or forum posts or 
whatever people will have like bash script.</p>

<p>Stuff, like that, like loop 
through and resize images to save time, you never have to do that right. 
Just you can just say: dot, resize and it'll just create you know, once-off 
it'll go through and create that if it's already there and it'll use the 
criticized ones for you, okay, so it's just it's just a speed up. 
Convenience function, no more! Okay! So for those of you that are kind of 
past dog breeds, I would be looking at planet next. You know like try it 
like play around with with trying to get a sense of like how can you get 
this as an accurate model? One thing to mention, and I'm not really going 
to go into it in details - there's nothing to do with deep learning, 
particularly, is that I'm using a different metric. I didn't use metrics 
equals accuracy, but I said metrics equals f2. Remember from last week that 
confusion matrix that, like two by two, you know correct incorrect for each 
of dogs and cats. There's a lot of different ways. You could turn that 
confusion matrix into a score. You know: do you care more about false 
negatives, or do you care more about false positives and how do you weight 
them and how do you combine them together, right, there's, a basic there's! 
Basically, a function called F beta where the beta says: how much do you 
weight, false negatives versus false positives and so f. 2 is f beta with 
beta equals 2, and it's basically, as particular way of weighting, false 
negatives and false positives and the reason we use it is because cattle 
told us that planet who are running this competition wanted to use this 
particular F beta metric.</p>

<p>The important thing for you to know is that you 
can create custom metrics. So in this case you can see here it says from 
Planet import, f2 and really I've got this here so that you can see how to 
do it right. So if you look inside courses, DL 1, you can see there's 
something called planet py right. And so, if I look at planet, py you'll 
see there's a function. There called f2 right and so f2 simply calls F beta 
score from psychic or side PI and patent. Where it came from and does a 
couple little tweets that are particularly important, but the important 
thing is like you can write any metric. You like right as long as it takes 
in set of predictions and a set of targets and they're both going to be 
numpy. Arrays one dimensional non pyros and then you return back a number 
okay, and so as long as you put a function that takes two vectors and 
returns at number, you can call it as a metric. And so then, when we said 
see here, learn, metrics equals and then past in that array, which just 
contains a single function. F2, then it's just going to be printed out 
after every for you, okay, so in general, like the the faster I library, 
everything is customizable. So kind of the idea is that everything is 
everything is kind of gives you what you might want by default, but also 
everything can be changed as well. Yes, you know, um.</p>

<p>We have a little 
confusion about the difference between multi-label and a single label. 
Uh-Huh the vanish as an example in which compared like similarly the 
example they just show us ah activation function, yeah. So so I'm so sorry, 
I said I'd do that. Then I didn't so. The activation the output activation 
function for a single label. Classification is softmax, but all the reasons 
that we talked today, but if we were trying to predict something that was 
like 0 0, 1, 1 0, then softmax would be a terrible choice because it's very 
hard to come up with something where both of these are high. In fact, it's 
impossible </p>

<h3>20. <a href="https://youtu.be/9C06ZPF8Uuc?t=1h45m10s">01:45:10</a></h3>

<ul style="list-style-type: square;">

<li><b> ‘sigmoid’ activation for multi-label</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Because they have to add up to 1, so the closest they could be would be 
point 5. So for multi-label classification, our activation function is 
called sigmoid, ok and again, the faster library does this automatically 
for you. If it notices, you have a multi label problem, and it does that by 
checking your data tip to see if anything has more than one label applied 
to it, and so sigmoid is a function which is equal to it's basically, the 
same thing, except rather than we Never add up all of these X, but instead 
we just take this X when we say it's just equal to it, divided by one plus 
it, and so the nice thing about that is that now, like multiple things can 
be high at once, right and so generally, Then, if something is less than 
zero, its sigmoid is going to be less than 0.5. If it's greater than zero 
is signal, it's going to be greater than 0.5, and so the important thing to 
know about a sigmoid function is that its shape is something which 
asymptotes at the top to one and asymptotes drew asymptotes at the bottom 
to zero. And so, therefore, it's a good thing to model a probability with 
anybody who has done any logistic regression will be familiar with. This is 
what we do in logistic regression, so it kind of appears everywhere in 
machine learning and you'll see that kind of a sigmoid and a softmax 
they're very close to each other conceptually.</p>

<p>But this is what we want is 
our activation function for multi-label, and this is what we want: the 
single label and again, fastai. Does it all for you? There was a question 
over here. Yes, I </p>

<h3>21. <a href="https://youtu.be/9C06ZPF8Uuc?t=1h47m30s">01:47:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Question on “Training only the last layers, not the initial freeze/frozen ones from ImageNet models”</b></li>

<li><b>‘learn.unfreeze()’ advanced discussion</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Have a question about the initial training that you do? If I understand 
correctly, you have we have frozen the the premium model and you only need 
initially try to train the latest playwright right, but from the other hand 
we said that only the initial layer. So, let's last probably, the first 
layer is like important to us and the other two are more like features that 
are, you must not related, and we then apply in this case. What that they, 
the lie, is a very important, but the pre-trained weights in them aren't so 
it's the later layers that we really want to train the most so earlier 
layers likely to be like already closer to what we want. Okay, so you 
started with the latest one, and then you go right. So if you go back to 
our quick dogs and cats right when we create a model from pre train from a 
pre train model, it returns something where all of the convolutional layers 
are frozen and some randomly set fully connected layers. We add to the end, 
our unfrozen, and so when we go fit at first it just trains, the randomly 
set, a randomly initialized, fully connected letters right and if something 
is like really close to imagenet, that's often all we need, but because the 
early early layers are Already good at finding edges, gradients repeating 
patterns for ears and dogs heads you know, so then, when we unfreeze, we 
set the learning rates for the early layers to be really low, because we 
don't want to change the mesh for us the later ones we set them To be 
higher, where else for satellite data right, this is no longer true.</p>

<p>You 
know the early layers are still like better than the later layers, but we 
still probably need to change them quite a bit. So that's right. This 
learning rate is nine times smaller than the final learning rate, rather 
than a thousand times smaller. The final loan rate, okay, you play with 
with the weights of the layers yeah normally most of the stuff. You see 
online. If they talk about this at all, they'll talk about unfreezing, 
different subsets of layers, and indeed we do unfreeze our randomly 
generated runs. But what I found is, although the first layer library you 
can type learn dot, freeze too and just freeze a subset of layers. This 
approach of using differential learning rates seems to be like more 
flexible to the point that I never find myself. I'm freezing subsets of 
layers that I would expect you to start with that with a different cell, 
the different learning rates, rather than trying to learn the last layer. 
So the reason okay. So you could skip this training just the last layers 
and just go straight to differential learning rates, but you probably don't 
want to, and the reason you probably don't want to is that there's a 
difference. The convolutional layers all contain pre-trained weights, so 
they're like they're, not random, for things that are close to imagenet 
they're, actually really good for things that are not close to imagenet 
they're better than that. All of our fully connected layers, however, are 
totally random.</p>

<p>So therefore, you would always want to make the fully 
connected weights better than random by training them a bit first, because 
otherwise, if you go straight to unfreeze, then you're actually going to be 
like fiddling around of those early early can early layer weights when the 
later Ones are still random. That's probably not what you want. I think 
that's another question here any possible. So when we unfreeze what are the 
things we're trying to change there? Will it change the Colonel's 
themselves that that's always what SGD does yeah? So the only thing? What 
training means is setting these numbers right and these numbers and these 
numbers the weights, so the weights are the weights of the fully connected 
layers and the weights in those kernels and the convolutions. So that's 
what training means it's and we'll learn about how to do it with SGD, but 
training literally is setting those numbers. These numbers, on the other 
hand, are activations, they're, calculated they're, calculated from the 
weights and the previous layers activations or amounts of questions. So you 
can lift it up higher and speak badly. So in your example of a cheerleader 
set of that English example. So you start with very small size existed for 
yeah, so does it literally mean you know? The model takes a small area from 
the entire image that is 64 bytes.</p>

<p>So how do we get that 64 by 64 depends 
on the transforms by default, our transform takes the smallest edge and 
recites the whole thing out samples it. So the smallest edge is societics. 
T4 and then it takes a Center crop of that. Okay, although when we're using 
data augmentation, it actually takes a randomly chosen prop ie the case 
where the image ties to multiple objects. Don't in this case like would it 
be possible that you would just lose the other things that they try to 
predict yeah, which is why data augmentation is important so by by and 
particularly their test time. Augmentation is going to be particularly 
important because you would you wouldn't want to. You know that there may 
be a artisanal mine out in the corner which, if you take a center crop, you 
you don't see so data augmentation becomes very important yeah. So when we 
talk on their tributaries, are he receiver up to that's, not really what a 
model choice? Delton, that's a great point. That's not the loss function, 
yeah right. The loss function is something we'll be learning about next 
week and it uses cross, entropy or otherwise known as like negative log 
likelihood. The metric is just this thing, that's printed, so we can see 
what's going on just next to that. So, in the context of my deep pass, 
modeling cannot change data.</p>

<p>Does it trading? It also have to be 
multiplied. So can I train on just like images of pure cats and dogs and 
expect it at prediction time to predict? If I give it a picture of both 
having cat eye on it over I've never tried that and I've never seen an 
example of something that needed it. I guess conceptually there's no reason 
it wouldn't work, but it's kind of out there and you still use a sigmoid. 
You would have to make sure you're using a sigmoid loss function. So, in 
this case, faster eyes default would not work because by default first day 
I would say your training data knitter has both a cat and the dog, so you 
would have to override the loss function when you use the differential 
learning rates. Those three learning rates: do they just kind of spread 
evenly across the layers. Yeah, we'll talk more about this later in the 
course, but I mean the faster I library there's a concept of layer groups 
so in something like a resonant 50. You know there's hundreds of layers and 
I think it you don't want to write down hundreds of learning rates, so I've 
basically decided for you how to split them and the the last one always 
refers just to the fully connected layers that we've randomly initialized 
and edit To the end, and then these ones are split generally about halfway 
through. Basically, I've tried to make it so that these you know these ones 
are kind of the ones which you hardly want to change at all, and these are 
the ones you might want to change.</p>

<p>A little bit - and I don't think we're 
covered in the course. But if you're interested we can </p>

<h3>22. <a href="https://youtu.be/9C06ZPF8Uuc?t=1h56m30s">01:56:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Visualize your model with ‘learn.summary()’, shows ‘OrderedDict()’</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Talk about in the forum, there are ways you can override this behavior to 
define your own layer groups. If you want to, and is there any way to 
visualize the model easily or like don't dump the layers of the model yeah, 
absolutely you can, let's make sure we've got one here. Okay, so if you 
just type learn, it doesn't tell you much at all, but what you can do is go 
learn summary and that spits out, basically everything, there's all the 
letters, and so you can see in this case. These are the names I mentioned. 
How they look up names right, so the first layer is called con 2 d 1, and 
it's going to take as input. This is useful to actually look at it's taking 
64 by 64 images, which is what we told it we're going to transform things 
to. This is three channels. High torch, like most things, have channels at 
the end, would say 64 by 64 by 3, ply torch music to the front, so it's 3 
by 64 by 64. That's because it turns out that some of the GPU computations 
run faster when it in that order. Okay, but that happens, all 
behind-the-scenes automatic plays a part of that transformation. Stuff, 
that's kind of all done automatically is to do that. Minus one means, 
however, however big the batch size is in care us, they use the number. 
They use a special number, none in pile types that used minus one. So this 
is a four dimensional mini batch. The number of elements in the amount of 
images in the mini batch is dynamic. You can change that.</p>

<p>The number of 
channels is three number, which is a 64 by 64, okay, and so then you can 
basically see that this particular convolutional kernel apparently has 64 
kernels in it, and it's also having we haven't talked about this, but 
convolutions can have something called a stride That is like Matt pullin: 
it changes the size, so it's returning a 32 by 32, 564 kernel, tenza and so 
on and so forth. So that's summary and we'll learn all about that's doing 
in detail on in the second half of the course one where I clicked in my own 
data set and I try to use the in as a really small data set. These 
currencies from Google Images and I tried to do a learning rate, find and 
then the plot and it just it, gave me some numbers which I didn't 
understand and the learning rate font yeah and then the plot was empty. So 
yeah I mean let's, let's talk about that on the forum, but basically the 
learning rate finder is going to go through a mini batch at a time if 
you've got a tiny data set, there's just not enough mini batches, so the 
trick is to make your Mini bit make your batch size really small like try 
making it like four okay. They were great questions. It's not nothing 
online to add. You know they were great questions. We've got a little bit. 
</p>

<h3>23. <a href="https://youtu.be/9C06ZPF8Uuc?t=1h59m45s">01:59:45</a></h3>

<ul style="list-style-type: square;">

<li><b> Working with Structured Data “Corporacion Favorita Grocery Sales Forecasting”</b></li>

<li><b>Based on the Rossman Stores competitition</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Past, where I hope to, but let's let's quickly talk about structured data, 
so we can start thinking about it for next week. So this is really weird 
right to me. There's basically, two types of data set. We use in machine 
learning, there's a type of data like audio images, natural language text, 
where all of the all of the things inside an object like all of the pixels 
inside an image are all the same kind of thing. They're, all pixels or 
they're. All apertures of a waveform or they're all words. I call this kind 
of data unstructured and then there's data sets like a profit and loss 
statement or the information about a Facebook user where each column is 
like structurally quite different. You know. One thing is representing like 
how many page views last month: another one is their sex. Another one is 
what zip code they're in, and I call this structure there. That particular 
terminology is not unusual, like lots of people use that terminology, but 
lots of people, don't there's. No particularly agreed-upon terminology, so 
when I say structured data, I'm referring to kind of columnar data, as you 
might find in a database or a spreadsheet, where different columns 
represent different kinds of things, and each row represents an 
observation, and so structured data is probably what most Of you are 
analyzing most of the time.</p>

<p>Funnily enough, you know, academics in the deep 
learning world don't really give a about structured data, because it's 
pretty hard to get published in fancy conference proceedings if you're like 
if you've got a better logistics model. You know it's the thing that makes 
the world goes round, it's a thing that makes everybody you know and 
efficiency and make stuff work, but it's largely ignored. Sadly, so we're 
not going to ignore it because we're a practical, deep learning and cackled 
doesn't ignore it either because people put prize money up on Kaggle to 
solve real-world problems. So there are some great capable competitions. We 
can look at there's one running right now, which is the grocery sales 
forecasting competition for Ecuador's largest chain. It's always a little. 
I've got to be a little careful about how much I show you about currently 
running competitions, because I don't want to you know, help you cheat, but 
it so happens. There was a competition a year or two ago for one of 
Germany's magistrate chains, which is almost identical, so I'm going to 
show you how to do that, so that was called the Rossman stores data, and so 
I would suggest you know first of all, try practicing What we're learning 
on Russman right, but then see if you can get it working on on grocery, 
because currently on the leaderboard, no one seems to basically know what 
they're doing in the groceries competition.</p>

<p>If you look at the leaderboard, 
the let's see here, yeah these ones around 5 to 9 v 3o are people that are 
literally finding like group averages and submitting those. I know because 
they're kernels that they're using so you know the basically the people 
around 20th place are not actually doing any machine learning so yeah, 
let's see if we can improve things so you'll see, there's a less than 3 
rossmann notebook sure you get pull. Ok, in fact, you know just reminder: 
you know before you start working, get pull in you're, faster, a repo and 
from time to time, Condor and update for you guys during the in-person 
course the Condor and update you should do it more often, because we're 
kind of Changing things a little bit, um folks in the MOOC you know more 
like once a month should be fine. So anyway, I just just changed this a 
little bit so make sure you get Paul to get lesson 3 Rossman and there's a 
couple of new libraries here. One is fast: AI dot structure, faster, guided 
structured, contain stuff, which is actually not at all high torch 
specific, and we actually use that in the machine learning course as well 
for doing random forests with no tie torch at all. I mentioned that because 
you can use that particular library without any of the other parts of 
fastai, so that can be handy and then we're also going to use faster column 
data, which is basically some stuff that allows us to do fast, a type a 
torch Stuff with columnar structured data for structured data, we need to 
use pandas a lot.</p>

<p>Anybody who's used. Our data frames will be very familiar 
with pandas. Pandas is basically an attempt to kind of replicate data 
friends in Python. You know, and a bit more, if you're not entirely 
familiar with pandas there's a great book, , which I think I might have 
mentioned before for data analysis by Wes McKinney, there's a new addition 
that just came out a couple of weeks ago, obviously being By the pandas 
author, its </p>

<h3>24. <a href="https://youtu.be/9C06ZPF8Uuc?t=2h5m30s">02:05:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Book: Python for Data Analysis, by Wes McKinney</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Coverage of pandas is excellent, but it also covers numpy, scipy, 
matplotlib, scikit-learn and Jupiter, really well, okay, and so I'm kind of 
going to assume that you know your way around these libraries to some 
extent also there was the workshop we did before they started and there's A 
video of that online, where we kind of have a brief mention of all of those 
tools. Structured data is generally shared as CSV files. It was no 
different in this competition as you'll see, there's a hyperlink to the 
rustman data set here right now. If you look at the bottom of my screen, 
you'll see this goes to file start faster day.i, because this doesn't 
require any login or anything to grab this data set it's as simple as 
right. Clicking copy link address head over to wherever you want it and 
just type wget and the URL okay. So that's because you know it's it's not 
behind a login or anything, so you can grab the grab it from there and you 
can always read a CSV file with just pandas. Don't read CSV now, in this 
particular case, there's a lot of pre-processing that we do and what I've 
actually done here is I've. I've actually stolen the entire pipeline from 
the third place, winner, roster, okay, so they made all their data they're 
really great. You know they better get hub available with everything that 
we need and I've ported it all across and simplified it and tried to make 
it pretty easy to understand.</p>

<p>This course is about deep learning, not about 
data processing, so I'm not going to go through it, but we will be going 
through it in the machine learning course in some detail because feature 
engineering is really important. So if you're interested, you know check 
out the machine learning course for that. I will, however, show you kind of 
what it looks like so once we read the CSV Xin, you can see basically 
what's there, so the key one is for a particular store. We have the we have 
the date and we have the sales for that particular store. We know whether 
that thing is on promo or not. We know the number of customers at that 
particular store had we know whether that date was a school holiday. We 
also know what kind of store it is, so this is pretty common right. You'll 
often get datasets where there's some column with like just some kind of 
code. We don't really know what the code means and most of the time I find 
it doesn't matter what it means like. Normally you get given a data 
dictionary when you start on a project and obviously, if you're working on 
an internal project, you can ask the people at your company. What does this 
column mean? I kind of stay away from learning too much about it. I prefer 
to like to see what the data says. First, there's something about what kind 
of product are we selling in this particular row and then there's 
information about like how far away is the nearest competitor? How long 
have they been open for? How long is the promo being on for for each store? 
We can find out what state it's in for each state we can find at the name 
of the state.</p>

<p>This is in Germany and, interestingly, they were allowed to 
download any data external data they wanted in this competition, just very 
common as long as you share it with everybody else, and so some folks tried 
downloading data from Google Trends. I'm not sure exactly what it was that 
they would check in the trend off, but we have this information from Google 
Trends. Somebody downloaded the weather for every day in Germany, every 
state and yeah. That's about it right, so you can get a data frame summary 
with pandas, which kind of lets you see how many observations and means and 
standard deviations again. I don't do a hell of a lot with that early on, 
but it's nice to note there. So what we do you know this is called a 
relational data set. A relational data set is one where there's quite a few 
tables. We have to join together. It's very easy to do that in pandas. 
There's a thing called merge, so great little function to do that, and so I 
just started joining everything together: joining the weather or the Google 
Trends stores. Yeah. That's about everything I guess you'll see, there's 
one thing that I'm using from the FASTA, a library which is called add date 
part. We talked about this a lot in the machine learning course, but 
basically this is going to take a date and pull out of a bunch of columns 
day of week is at the start of a quarter month of year, so on and so forth 
and add them All in to the dataset okay, so this is all standard 
pre-processing all right, so we join everything together we fiddle around 
with some of the dates a little bit.</p>

<p>Some of them are in month in year 
format. We turn it into date, format. We spend a lot of time trying to take 
information about, for example, holidays and add a column for like how long 
until the next holiday. How long has it been since the last holiday ditto 
for promos so on and so forth? Okay, so we do all that and at the very end 
we </p>

<h3>25. <a href="https://youtu.be/9C06ZPF8Uuc?t=2h11m50s">02:11:50</a></h3>

<ul style="list-style-type: square;">

<li><b> We save the dataframe with ‘Joined.to_feather()’ from Pandas, use ‘df = pd.read_feather()’ to load.</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Basically save a big structured data file that contains all that stuff, 
something that those of you that use pandas may not be aware of is that 
there's a very cool new format called feather, which you can save a pandas 
data frame into this feather format. It's kind of pretty much takes it as 
it sits in RAM and dumps it to the disk, and so it's like really really 
really fast. The reason that you need to know this is because the 
ecuadorian grocery competition is on now has 350 million records, so you 
will care about how long things take a talk. I believe about six seconds 
for me to save three hundred and fifty million records to feather format. 
So that's pretty cool so at the end of all that I'd save it as a feather 
format and for the rest of this discussion. I'm just going to take it as 
given that we've got this nicely. Pre-Processed feature engineered file and 
I can just go read better okay, but for you to play along at home, you will 
have to run those previous cells. Oh, except the see these ones have 
commented out, you don't have to run those because the file that you 
download from files, doc bastard AI, has already done that for you, okay, 
all right, so we basically have all these columns, so it basically is going 
to tell Us you know how many of this thing was sold on this date at this 
store, and so the goal of this competition is to find out how many things 
will be sold for each store for </p>

<h3>26. <a href="https://youtu.be/9C06ZPF8Uuc?t=2h5m30s">02:13:30</a></h3>

<ul style="list-style-type: square;">

<li><b> Split Rossman columns in two types: categorical vs continuous</b></li>

</ul>

<p style="color: #aaaaaa; text-align: center">(autogenerated subtitles follow, may contain gibberish/bad format - <a href="https://github.com/stas00/fastai-transcript/">please proofread to improve</a> - remove this note once proofread)</p>


<p>Each type of thing in the future: okay, and so that's, basically, what 
we're going to be trying to do, and so here's an example of what some of 
the data looks like and so next week we're going to see how to go through 
these steps. But basically, what we're going to learn is we're going to 
learn to split the columns into two types. Some columns were going to treat 
as categorical, which is to say, store ID one and store ID, I'm not 
numerically related to each other they're categories, right we're going to 
treat day of week like that, Monday and Tuesday day, zero and day, one not 
numerically related to Each other, where else distance in kilometers to the 
nearest competitor, that's a number that we're going to treat numerically 
right. So, in other words, the categorical variables we basically are going 
to one how to encode them. You can think of it as one hot encoding on where 
the continuous variables we're going to be feeding into fully connected 
layers, just as is okay, so what we'll be doing is we'll be basically 
creating a validation set and you'll. See like a lot of these are start to 
look familiar. This is the same function. We used on planet and dog breeds 
to create a validation set, there's some stuff that you haven't seen 
before, where we're going to basically, rather than saying image, data dot 
from CSV, we're going to say, columnar data from data frame right.</p>

<p>So you 
can see like the basic API concepts, will be the same but they're a little 
different right, but just like before we're going to get a learner and 
we're going to go, lr find to find our best learning rate and then we're 
going to go dot. Fit with a metric with a cycle length, okay, so the basic 
sequence, who's going to end up, looking, hopefully very familiar, okay, so 
we're out of time. So what I suggest you do this week is like try to enter 
as many capital image competitions as possible like like try to really get 
this feel for, like cycling learning rates plotting things. You know that 
that post, I showed you at the start of class today that kind of took you 
through lesson, one like really go through that on as many image datasets 
as you can, you just feel really comfortable with it right because you want 
to get to The point where next week, when we start talking about structured 
data, that this idea of like how learners kind of work and data works and 
data loaders and data sets and looking at pictures should be really, you 
know intuitive all right. Good luck see you next week, [ Applause, ] 
</p>






  </body>
</html>
