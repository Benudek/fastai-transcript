1
00:00:00,050 --> 00:00:09,980
okay so last class of part one

2
00:00:05,009 --> 00:00:12,599
I guess the theme of part one is

3
00:00:09,980 --> 00:00:15,089
classification and regression with deep

4
00:00:12,599 --> 00:00:17,759
wading and specifically it's about

5
00:00:15,089 --> 00:00:19,439
identifying and learning the best

6
00:00:17,760 --> 00:00:23,580
practices for classification and

7
00:00:19,439 --> 00:00:25,500
regression and we started out with the

8
00:00:23,579 --> 00:00:28,649
kind of here are three lines of code to

9
00:00:25,500 --> 00:00:31,649
do image classification and gradually

10
00:00:28,649 --> 00:00:34,920
we've been for the first four lessons

11
00:00:31,649 --> 00:00:36,420
within kind of going through NLP

12
00:00:34,920 --> 00:00:38,039
structured data collaborative filtering

13
00:00:36,420 --> 00:00:39,929
and kind of understanding some of the

14
00:00:38,039 --> 00:00:42,628
key pieces and most importantly

15
00:00:39,929 --> 00:00:44,450
understanding you know how to actually

16
00:00:42,628 --> 00:00:47,878
make these things work well in practice

17
00:00:44,450 --> 00:00:49,350
and then the last three lessons are then

18
00:00:47,878 --> 00:00:52,019
kind of going back over all of those

19
00:00:49,350 --> 00:00:53,550
topics in kind of reverse order to

20
00:00:52,020 --> 00:00:56,250
understand more detail about what was

21
00:00:53,549 --> 00:00:57,780
going on and understanding what the code

22
00:00:56,250 --> 00:01:02,429
looks like behind the scenes and wanting

23
00:00:57,780 --> 00:01:07,680
to kind of write them from scratch part

24
00:01:02,429 --> 00:01:09,118
two of the course we'll move from a

25
00:01:07,680 --> 00:01:12,540
focus on classification and regression

26
00:01:09,118 --> 00:01:15,599
which is kind of predicting a thing like

27
00:01:12,540 --> 00:01:17,479
a number or or at most a small number of

28
00:01:15,599 --> 00:01:20,250
things like a small number of labels and

29
00:01:17,478 --> 00:01:23,209
we'll focus more on generative modeling

30
00:01:20,250 --> 00:01:25,259
generative modeling means predicting

31
00:01:23,209 --> 00:01:28,438
kind of lots of things

32
00:01:25,259 --> 00:01:31,049
for example creating a sentence such as

33
00:01:28,438 --> 00:01:33,750
in Ural translation or image captioning

34
00:01:31,049 --> 00:01:38,368
or question-answering while creating an

35
00:01:33,750 --> 00:01:41,938
image such as in style transfer

36
00:01:38,368 --> 00:01:46,670
super-resolution segmentation and so

37
00:01:41,938 --> 00:01:49,648
forth and then in part two it'll move

38
00:01:46,670 --> 00:01:52,170
away from being just here are some best

39
00:01:49,649 --> 00:01:54,390
practices you know established best

40
00:01:52,170 --> 00:01:56,340
practices either through people that are

41
00:01:54,390 --> 00:01:58,530
written papers or through research that

42
00:01:56,340 --> 00:01:59,850
last day is done and it kind of got

43
00:01:58,530 --> 00:02:02,670
convinced that these are best practices

44
00:01:59,849 --> 00:02:04,890
to some stuff which would be a little

45
00:02:02,670 --> 00:02:07,368
bit more speculative you know some stuff

46
00:02:04,890 --> 00:02:11,370
which is maybe recent papers that

47
00:02:07,368 --> 00:02:12,989
haven't been fully tested yet and

48
00:02:11,370 --> 00:02:14,519
sometimes in part two like pickles will

49
00:02:12,989 --> 00:02:16,560
come out in the middle of the course and

50
00:02:14,519 --> 00:02:18,180
will change direction with the course

51
00:02:16,560 --> 00:02:20,340
and study that paper because it's just

52
00:02:18,180 --> 00:02:23,489
you know interesting and so if you're

53
00:02:20,340 --> 00:02:26,430
interested in kind of learning a bit

54
00:02:23,489 --> 00:02:28,408
more about how to read a paper and how

55
00:02:26,430 --> 00:02:30,239
to implement it from scratch and so

56
00:02:28,408 --> 00:02:34,158
forth then that's another good reason to

57
00:02:30,239 --> 00:02:37,439
do part two it still doesn't assume any

58
00:02:34,158 --> 00:02:39,509
particular math background but it does

59
00:02:37,439 --> 00:02:41,310
beyond kind of high school but but it

60
00:02:39,509 --> 00:02:44,669
does assume that you're prepared to

61
00:02:41,310 --> 00:02:46,859
spend time like you know digging through

62
00:02:44,669 --> 00:02:50,548
the notation and understanding it and

63
00:02:46,859 --> 00:02:52,980
converting it to code and so forth all

64
00:02:50,549 --> 00:02:55,439
right so we're we're up to is is our

65
00:02:52,979 --> 00:02:58,378
intent at the moment and I think one of

66
00:02:55,439 --> 00:03:00,539
the issues I find most with teaching

67
00:02:58,378 --> 00:03:03,268
iron ends is trying to ensure that

68
00:03:00,539 --> 00:03:05,548
people understand they're not in any way

69
00:03:03,269 --> 00:03:07,739
different or unusual or magical they're

70
00:03:05,549 --> 00:03:11,969
they're just a standard fully connected

71
00:03:07,739 --> 00:03:13,620
Network and so let's go back to the

72
00:03:11,969 --> 00:03:15,750
standard fully connected Network which

73
00:03:13,620 --> 00:03:19,650
looks like this right so to remind you

74
00:03:15,750 --> 00:03:22,818
the arrows represent one or more layer

75
00:03:19,650 --> 00:03:26,069
operations generally speaking a linear

76
00:03:22,818 --> 00:03:29,250
followed by a nonlinear function in this

77
00:03:26,068 --> 00:03:34,259
case their matrix modifications followed

78
00:03:29,250 --> 00:03:37,128
by real new raw or fan and the arrows of

79
00:03:34,259 --> 00:03:39,840
the same color represent the same

80
00:03:37,128 --> 00:03:43,078
exactly the same weight matrix being

81
00:03:39,840 --> 00:03:45,359
used and so one thing which was just

82
00:03:43,079 --> 00:03:47,760
slightly different from previous fully

83
00:03:45,359 --> 00:03:51,959
connected networks we've seen is that we

84
00:03:47,759 --> 00:03:53,068
have an input coming in at the not just

85
00:03:51,959 --> 00:03:54,389
at the first layer but also for the

86
00:03:53,068 --> 00:03:56,399
second layer and also at the third layer

87
00:03:54,389 --> 00:03:58,650
and we tried a couple of approaches one

88
00:03:56,400 --> 00:04:00,930
was concatenating the inputs and one was

89
00:03:58,650 --> 00:04:03,810
adding the airport's okay but there was

90
00:04:00,930 --> 00:04:09,510
nothing at all conceptually different

91
00:04:03,810 --> 00:04:15,030
about this so that code looked like this

92
00:04:09,509 --> 00:04:18,298
we had a model where we basically

93
00:04:15,030 --> 00:04:19,908
defined the the three arrows colors we

94
00:04:18,298 --> 00:04:25,079
had as three different weight matrices

95
00:04:19,908 --> 00:04:26,819
okay and by using the linear

96
00:04:25,079 --> 00:04:29,848
we got actually both the weight matrix

97
00:04:26,819 --> 00:04:33,030
and the bias vector wrapped up for free

98
00:04:29,848 --> 00:04:35,878
for us and then we went through and we

99
00:04:33,029 --> 00:04:39,750
did each of our embeddings put it

100
00:04:35,879 --> 00:04:42,060
through our first linear layer and then

101
00:04:39,750 --> 00:04:48,870
we did each of our we call them hiddens

102
00:04:42,060 --> 00:04:51,500
being the orange orange areas and in

103
00:04:48,870 --> 00:04:54,209
order to avoid the fact that there's no

104
00:04:51,500 --> 00:04:57,269
orange arrow coming into the first one

105
00:04:54,209 --> 00:04:59,609
we decided to kind of invent an empty

106
00:04:57,269 --> 00:05:02,008
matrix and that way every one of these

107
00:04:59,610 --> 00:05:06,650
rows about the same right and so then we

108
00:05:02,009 --> 00:05:06,650
did exactly the same thing except we

109
00:05:08,269 --> 00:05:13,589
used to loop just to refactor the cutter

110
00:05:11,579 --> 00:05:16,620
okay so it's just it was just a code

111
00:05:13,589 --> 00:05:19,288
refactoring there was no change of

112
00:05:16,620 --> 00:05:21,870
anything conceptually and since we were

113
00:05:19,288 --> 00:05:23,399
doing a refactoring we took advantage of

114
00:05:21,870 --> 00:05:25,110
that to increase the number of

115
00:05:23,399 --> 00:05:27,598
characters to eight because I was too

116
00:05:25,110 --> 00:05:30,180
lazy to type 8 when the alias but I'm

117
00:05:27,598 --> 00:05:30,750
quite happy to change the loop in that

118
00:05:30,180 --> 00:05:34,468
stage

119
00:05:30,750 --> 00:05:36,750
yeah so this now looked through this

120
00:05:34,468 --> 00:05:43,918
exact same thing but we had eight of

121
00:05:36,750 --> 00:05:46,110
these rather than three so then we

122
00:05:43,918 --> 00:05:49,079
refactored that again by taking

123
00:05:46,110 --> 00:05:51,750
advantage of an end errand in which

124
00:05:49,079 --> 00:05:57,810
basically puts that loop together for us

125
00:05:51,750 --> 00:06:01,228
and keeps track of the this.h as it goes

126
00:05:57,810 --> 00:06:04,379
along for us and so by using that we

127
00:06:01,228 --> 00:06:07,468
were able to replace the loop with a

128
00:06:04,379 --> 00:06:12,560
single call and so again that's just a

129
00:06:07,468 --> 00:06:12,560
refactoring doing exactly the same thing

130
00:06:14,000 --> 00:06:21,209
okay so then we looked at something

131
00:06:17,180 --> 00:06:27,750
which was mainly designed to save some

132
00:06:21,209 --> 00:06:33,418
training time which was previously we

133
00:06:27,750 --> 00:06:36,939
had if we had a big piece of text right

134
00:06:33,418 --> 00:06:39,250
so we've got like a movie review

135
00:06:36,939 --> 00:06:43,600
but we were basically splitting it up

136
00:06:39,250 --> 00:06:46,689
into eight character segments and we'd

137
00:06:43,600 --> 00:06:50,650
grab like segment number one and use

138
00:06:46,689 --> 00:06:52,930
that to predict the next character right

139
00:06:50,649 --> 00:06:54,849
but in order to make sure that we kind

140
00:06:52,930 --> 00:06:57,459
of used all of the data we didn't just

141
00:06:54,850 --> 00:07:01,540
put it up like that we actually said

142
00:06:57,459 --> 00:07:03,609
like okay here's our whole thing let's

143
00:07:01,540 --> 00:07:06,220
grab the first will be to grab this

144
00:07:03,610 --> 00:07:08,889
section the second will be to grab that

145
00:07:06,220 --> 00:07:10,630
section in that section then that

146
00:07:08,889 --> 00:07:13,419
section and each time would predict

147
00:07:10,629 --> 00:07:17,560
predicting the next one character a lot

148
00:07:13,420 --> 00:07:19,300
okay and so you know I was bit concerned

149
00:07:17,560 --> 00:07:22,689
that that seems pretty wasteful because

150
00:07:19,300 --> 00:07:24,670
like as we calculate this section nearly

151
00:07:22,689 --> 00:07:28,719
all of it overlaps with the previous

152
00:07:24,670 --> 00:07:30,720
section okay so instead what we did was

153
00:07:28,720 --> 00:07:32,820
we said all right well what if we

154
00:07:30,720 --> 00:07:37,510
actually did split it into

155
00:07:32,819 --> 00:07:41,500
non-overlapping pieces right and we said

156
00:07:37,509 --> 00:07:46,810
all right let's grab this section here

157
00:07:41,500 --> 00:07:49,389
and use it to predict every one of the

158
00:07:46,810 --> 00:07:51,910
characters one along right and then

159
00:07:49,389 --> 00:07:53,439
let's grab this section here and use it

160
00:07:51,910 --> 00:07:55,510
to predict every one of the characters

161
00:07:53,439 --> 00:07:57,730
went along so after we look at the first

162
00:07:55,509 --> 00:07:59,740
character in we try to predict the

163
00:07:57,730 --> 00:08:01,390
second character and then now if we look

164
00:07:59,740 --> 00:08:03,819
at the second character we try to

165
00:08:01,389 --> 00:08:05,289
predict the third character and so okay

166
00:08:03,819 --> 00:08:08,949
and so that's where you've got to and

167
00:08:05,290 --> 00:08:10,689
then what if you perceptive folks asked

168
00:08:08,949 --> 00:08:13,750
a really interesting question or

169
00:08:10,689 --> 00:08:16,329
expressed their concern which was hey

170
00:08:13,750 --> 00:08:22,449
after we got through the first the first

171
00:08:16,329 --> 00:08:25,629
point here after we got through the

172
00:08:22,449 --> 00:08:30,069
first point here we kind of withdrew

173
00:08:25,629 --> 00:08:32,439
away our H activations and started a new

174
00:08:30,069 --> 00:08:36,009
one which meant that when it was trying

175
00:08:32,440 --> 00:08:39,300
to use character one to predict

176
00:08:36,009 --> 00:08:41,559
character - it's got nothing to go on

177
00:08:39,299 --> 00:08:44,529
you know it hasn't built it's only built

178
00:08:41,559 --> 00:08:47,169
it's only done one linear layer and so

179
00:08:44,529 --> 00:08:49,019
that seems like a problem which indeed

180
00:08:47,169 --> 00:08:50,569
it is okay

181
00:08:49,019 --> 00:08:54,509
so we're going to do the obvious thing

182
00:08:50,570 --> 00:08:58,740
which is let's not throw away H okay so

183
00:08:54,509 --> 00:08:59,069
let's not throw away that that matrix at

184
00:08:58,740 --> 00:09:04,589
all

185
00:08:59,070 --> 00:09:06,899
so in code the big problem is here but

186
00:09:04,589 --> 00:09:08,520
every time we call forward so in other

187
00:09:06,899 --> 00:09:12,589
words every time we do a new mini-batch

188
00:09:08,519 --> 00:09:16,079
we're creating our our hidden state

189
00:09:12,589 --> 00:09:18,870
right which remember is the orange

190
00:09:16,080 --> 00:09:21,180
circles okay we're resetting it back to

191
00:09:18,870 --> 00:09:23,519
a bunch of zeros and so as we go to the

192
00:09:21,179 --> 00:09:25,079
next non-overlapping section we're

193
00:09:23,519 --> 00:09:28,009
saying forget everything that's come

194
00:09:25,080 --> 00:09:30,240
before but in fact the whole point is we

195
00:09:28,009 --> 00:09:31,679
know exactly where we are we're at the

196
00:09:30,240 --> 00:09:33,839
end of the previous section and about to

197
00:09:31,679 --> 00:09:34,799
start the new next contiguous section so

198
00:09:33,839 --> 00:09:37,980
let's not throw it away

199
00:09:34,799 --> 00:09:45,870
so instead the idea would be to cut this

200
00:09:37,980 --> 00:09:49,379
out right move it up to here okay store

201
00:09:45,870 --> 00:09:51,959
it away in self and then kind of keep

202
00:09:49,379 --> 00:09:54,320
updating it right now so we're going to

203
00:09:51,958 --> 00:09:58,769
do that and there's going to be some

204
00:09:54,320 --> 00:10:02,190
minor details to get right so let's

205
00:09:58,769 --> 00:10:07,189
start by looking at the model so here's

206
00:10:02,190 --> 00:10:07,190
the model it's it's nearly identical and

207
00:10:09,019 --> 00:10:12,028
okay

208
00:10:10,169 --> 00:10:15,000
here's the model it's nearly identical

209
00:10:12,028 --> 00:10:17,730
but I've got as expected one more line

210
00:10:15,000 --> 00:10:20,250
in my constructor where I call something

211
00:10:17,730 --> 00:10:26,070
called init hidden and as expected in it

212
00:10:20,250 --> 00:10:30,809
hidden sets self dot H to be a bunch of

213
00:10:26,070 --> 00:10:33,839
zeros okay so that's entirely

214
00:10:30,809 --> 00:10:39,569
unsurprising and then as you can see our

215
00:10:33,839 --> 00:10:42,810
R and n now takes in self garage and it

216
00:10:39,570 --> 00:10:45,540
as before spits out our new hidden

217
00:10:42,809 --> 00:10:49,708
activations and so now the trick is to

218
00:10:45,539 --> 00:10:53,909
now store that away inside self dot H

219
00:10:49,708 --> 00:10:56,939
and so here's wrinkle number one if you

220
00:10:53,909 --> 00:11:02,549
think about it if I was to simply do it

221
00:10:56,940 --> 00:11:02,850
like like that right and now I train

222
00:11:02,549 --> 00:11:04,769
this

223
00:11:02,850 --> 00:11:08,389
on a document that's I don't know a

224
00:11:04,769 --> 00:11:12,149
million words million characters long

225
00:11:08,389 --> 00:11:18,180
then the size of this unrolled are a 10

226
00:11:12,149 --> 00:11:21,089
is has a million circles here and so

227
00:11:18,179 --> 00:11:22,559
that's fine going forwards right there

228
00:11:21,089 --> 00:11:24,000
when I finally get to the end and I say

229
00:11:22,559 --> 00:11:26,119
here's my character and actually

230
00:11:24,000 --> 00:11:30,028
remember we're doing multi output now so

231
00:11:26,120 --> 00:11:32,009
multi output looks like this right or if

232
00:11:30,028 --> 00:11:34,860
we were to draw the unrolled version of

233
00:11:32,009 --> 00:11:39,600
multi output we would have a triangle

234
00:11:34,860 --> 00:11:42,360
coming off at every point okay so the

235
00:11:39,600 --> 00:11:44,850
problem is that then when we do back

236
00:11:42,360 --> 00:11:49,039
propagation we're calculating you know

237
00:11:44,850 --> 00:11:52,259
how much does the error at character one

238
00:11:49,039 --> 00:11:54,360
impact the final answer how much does

239
00:11:52,259 --> 00:11:56,818
the error character to impact the final

240
00:11:54,360 --> 00:12:00,000
answer and so forth and so we need to go

241
00:11:56,818 --> 00:12:02,129
back through and say like how do we have

242
00:12:00,000 --> 00:12:05,549
to update our weights based on all of

243
00:12:02,129 --> 00:12:09,179
those you know errors and so if there

244
00:12:05,549 --> 00:12:12,208
are our million characters my unrolled R

245
00:12:09,179 --> 00:12:15,528
and N is a million layers long I have a

246
00:12:12,208 --> 00:12:18,119
1 million layer fully connected Network

247
00:12:15,528 --> 00:12:19,679
all right and like I didn't have to

248
00:12:18,120 --> 00:12:21,269
write the million layers because I have

249
00:12:19,679 --> 00:12:24,028
for loop and the for loops hidden away

250
00:12:21,269 --> 00:12:28,799
behind that you know the self dot R and

251
00:12:24,028 --> 00:12:31,289
n but it's still there right we so so

252
00:12:28,799 --> 00:12:33,269
this is actually a 1 million layer fully

253
00:12:31,289 --> 00:12:34,828
connected Network and so the problem

254
00:12:33,269 --> 00:12:37,438
with that is it's going to be very

255
00:12:34,828 --> 00:12:39,629
memory intensive because in order to do

256
00:12:37,438 --> 00:12:42,169
the chain rule I have to be able to

257
00:12:39,629 --> 00:12:46,828
multiply at every step like you know

258
00:12:42,169 --> 00:12:48,120
after a few times she acts right and so

259
00:12:46,828 --> 00:12:50,789
like I've got that means I have to

260
00:12:48,120 --> 00:12:53,339
remember that those values you the value

261
00:12:50,789 --> 00:12:54,870
of every set of layers so I'm gonna have

262
00:12:53,339 --> 00:12:56,009
to remember all those million layers and

263
00:12:54,870 --> 00:12:57,990
I'm going to do have to have to do a

264
00:12:56,009 --> 00:13:01,680
million multiplications and I'm going to

265
00:12:57,990 --> 00:13:05,399
have to do that every batch okay so that

266
00:13:01,679 --> 00:13:07,649
would be bad so to avoid that we

267
00:13:05,399 --> 00:13:10,860
basically say all right well from time

268
00:13:07,649 --> 00:13:13,500
to time I want you to forget your

269
00:13:10,860 --> 00:13:15,750
history okay so we can still remember

270
00:13:13,500 --> 00:13:16,559
the state right which is to remember

271
00:13:15,750 --> 00:13:19,769
like what's the

272
00:13:16,559 --> 00:13:21,568
values in our hidden matrix right but we

273
00:13:19,769 --> 00:13:23,519
can remember the state without

274
00:13:21,568 --> 00:13:23,969
remembering everything about how we got

275
00:13:23,519 --> 00:13:27,299
there

276
00:13:23,970 --> 00:13:35,360
so there's a little function called

277
00:13:27,299 --> 00:13:41,188
repackage variable which literally is

278
00:13:35,360 --> 00:13:42,990
just this right it just simply says grab

279
00:13:41,188 --> 00:13:44,938
the tensor out of it

280
00:13:42,990 --> 00:13:46,889
right because remember the tensor itself

281
00:13:44,938 --> 00:13:49,498
doesn't have any concept of history

282
00:13:46,889 --> 00:13:51,419
right and create a new variable out of

283
00:13:49,499 --> 00:13:55,110
that and so this variables going to have

284
00:13:51,419 --> 00:13:57,419
the same value but no no history of

285
00:13:55,110 --> 00:13:59,818
operations and therefore when it tries

286
00:13:57,419 --> 00:14:00,269
to back propagate it all it'll stop

287
00:13:59,818 --> 00:14:02,578
there

288
00:14:00,269 --> 00:14:05,278
so basically what we're going to do then

289
00:14:02,578 --> 00:14:07,528
is we're going to call this in our

290
00:14:05,278 --> 00:14:09,899
forward so that means it's going to do

291
00:14:07,528 --> 00:14:13,740
add characters it's going to back

292
00:14:09,899 --> 00:14:15,778
propagate through eight layers it's

293
00:14:13,740 --> 00:14:18,329
going to keep track of the actual values

294
00:14:15,778 --> 00:14:20,028
in our hidden state but it's going to

295
00:14:18,328 --> 00:14:24,328
throw away at the end of those eight

296
00:14:20,028 --> 00:14:27,720
it's its history of operations so this

297
00:14:24,328 --> 00:14:30,870
is this approach it's called back prop

298
00:14:27,720 --> 00:14:32,490
through time and you know when you read

299
00:14:30,870 --> 00:14:36,089
about it online people make it sound

300
00:14:32,490 --> 00:14:38,399
like like a different algorithm or some

301
00:14:36,089 --> 00:14:40,379
big insight or something but it's it's

302
00:14:38,399 --> 00:14:43,889
not at all right it's just saying hey

303
00:14:40,379 --> 00:14:46,409
after our for loop you know just throw

304
00:14:43,889 --> 00:14:48,600
away your your history operations and

305
00:14:46,409 --> 00:14:50,879
start afresh so we're keeping our hidden

306
00:14:48,600 --> 00:14:56,699
state but we're not keeping our hidden

307
00:14:50,879 --> 00:14:58,110
States history okay so that's that's

308
00:14:56,698 --> 00:15:00,599
wrinkle number one that's what this

309
00:14:58,110 --> 00:15:01,110
repackage bar is doing and so what do

310
00:15:00,600 --> 00:15:05,339
you see

311
00:15:01,110 --> 00:15:07,438
BP BP TT that's referring to that crop

312
00:15:05,339 --> 00:15:10,769
through time and you might remember we

313
00:15:07,438 --> 00:15:14,129
saw that in our original errand in

314
00:15:10,769 --> 00:15:16,589
Lesson we had a variable called BP t t

315
00:15:14,129 --> 00:15:18,990
equals 70 and so when we set that

316
00:15:16,589 --> 00:15:22,019
they're actually saying how many layers

317
00:15:18,990 --> 00:15:23,698
backprop through another good reason not

318
00:15:22,019 --> 00:15:26,149
to back crop through too many layers is

319
00:15:23,698 --> 00:15:28,649
if you have any kind of gradient

320
00:15:26,149 --> 00:15:29,379
instability like gradient explosion or

321
00:15:28,649 --> 00:15:31,659
gradients

322
00:15:29,379 --> 00:15:33,970
banishing you know too many more of the

323
00:15:31,659 --> 00:15:37,028
more layers you have the harder the

324
00:15:33,970 --> 00:15:40,569
network s to Train so smaller and less

325
00:15:37,028 --> 00:15:43,479
resilient on the other hand and longer

326
00:15:40,568 --> 00:15:48,219
value for VP TT means that you're able

327
00:15:43,479 --> 00:15:53,109
to explicitly capture a longer kind of

328
00:15:48,220 --> 00:15:55,379
memory more state okay so that's a

329
00:15:53,109 --> 00:15:59,699
that's something that you get to tune

330
00:15:55,379 --> 00:15:59,699
when you create your area

331
00:16:00,389 --> 00:16:09,788
all right wrinkle number two is how are

332
00:16:06,339 --> 00:16:12,489
we going to put the data into this right

333
00:16:09,788 --> 00:16:16,948
like it's all very well the way I

334
00:16:12,489 --> 00:16:16,949
described it just now where we said you

335
00:16:19,499 --> 00:16:27,489
know we could do this and we can first

336
00:16:24,308 --> 00:16:30,338
of all look at this section then this

337
00:16:27,489 --> 00:16:33,069
section in this section but we're going

338
00:16:30,339 --> 00:16:35,970
to do a mini batch at a time right we

339
00:16:33,068 --> 00:16:43,328
want to do a bunch at a time

340
00:16:35,970 --> 00:16:45,989
so in other words we want to say let's

341
00:16:43,328 --> 00:16:45,988
do it like this

342
00:16:50,259 --> 00:16:56,600
so mini-batch number one would say let's

343
00:16:53,269 --> 00:16:59,259
look at this section and predict that

344
00:16:56,600 --> 00:17:01,370
section and at the same time in parallel

345
00:16:59,259 --> 00:17:04,430
let's look at this totally different

346
00:17:01,370 --> 00:17:06,289
section and predict this and at the same

347
00:17:04,430 --> 00:17:08,509
time in parallel let's look at this

348
00:17:06,289 --> 00:17:12,170
totally different section and predict

349
00:17:08,509 --> 00:17:16,338
this right and so then because remember

350
00:17:12,170 --> 00:17:18,589
in our in our hidden state we have a

351
00:17:16,338 --> 00:17:20,088
vector of hidden state for everything in

352
00:17:18,588 --> 00:17:22,099
our mini batch right so it's going to

353
00:17:20,088 --> 00:17:23,779
keep track of at the end of this is

354
00:17:22,099 --> 00:17:25,849
going to be a you know a vector here a

355
00:17:23,779 --> 00:17:28,519
vector here a vector here and then we

356
00:17:25,849 --> 00:17:31,309
can move across to the next one and say

357
00:17:28,519 --> 00:17:34,279
okay so this part of the mini batch use

358
00:17:31,309 --> 00:17:36,769
this to predict that and use this to

359
00:17:34,279 --> 00:17:38,930
predict that and use this to predict

360
00:17:36,769 --> 00:17:40,730
that right so you can see that we're

361
00:17:38,930 --> 00:17:43,730
moving that we've got like a number of

362
00:17:40,730 --> 00:17:46,509
totally separate bits of our text that

363
00:17:43,730 --> 00:17:48,920
we're moving through in parallel right

364
00:17:46,509 --> 00:17:53,109
so hopefully this is going to ring a few

365
00:17:48,920 --> 00:17:56,450
bells for you because what happened was

366
00:17:53,109 --> 00:17:58,819
was back when we started looking at

367
00:17:56,450 --> 00:18:00,590
torch texture the first time we started

368
00:17:58,819 --> 00:18:03,679
talking about how it creates these mini

369
00:18:00,589 --> 00:18:07,929
batches and I said what happened was we

370
00:18:03,680 --> 00:18:10,670
took our whole big long document

371
00:18:07,930 --> 00:18:13,130
consisting of like you know the entire

372
00:18:10,670 --> 00:18:15,200
works of nature or all of the IMDB

373
00:18:13,130 --> 00:18:17,750
reviews concatenated together or

374
00:18:15,200 --> 00:18:19,009
whatever and a lot of a lot of you not

375
00:18:17,750 --> 00:18:20,750
surprisingly because this really said

376
00:18:19,009 --> 00:18:22,160
this is really weird at first a lot of

377
00:18:20,750 --> 00:18:24,980
you didn't quite hear what I said

378
00:18:22,160 --> 00:18:29,390
correctly what I said was we split this

379
00:18:24,980 --> 00:18:31,940
into 64 equal sized chunks and a lot of

380
00:18:29,390 --> 00:18:35,030
your brains when Jeremy just said we

381
00:18:31,940 --> 00:18:37,130
split this into chunks of size 64 but

382
00:18:35,029 --> 00:18:40,160
that's not what Theresa Jeremy said we

383
00:18:37,130 --> 00:18:42,950
split it into 64 equal sized chunks

384
00:18:40,160 --> 00:18:45,680
right so if this whole thing was length

385
00:18:42,950 --> 00:18:49,190
64 million right which would be a

386
00:18:45,680 --> 00:18:51,950
reasonable sized corpus not an unusual

387
00:18:49,190 --> 00:18:55,240
size corpus then each of our 64 chunks

388
00:18:51,950 --> 00:18:58,519
would have been of length 1 million

389
00:18:55,240 --> 00:19:01,069
right and so then what we did was we

390
00:18:58,519 --> 00:19:03,019
talked the first chunk of 1 million and

391
00:19:01,069 --> 00:19:04,789
we put it here

392
00:19:03,019 --> 00:19:06,289
and then we took the second chunk of 1

393
00:19:04,789 --> 00:19:08,509
million and we put it here

394
00:19:06,289 --> 00:19:13,639
the third chunk of 1 million we put it

395
00:19:08,509 --> 00:19:18,410
here and so forth to create 64 chunks

396
00:19:13,640 --> 00:19:22,150
and then H mini-batch consisted of us

397
00:19:18,410 --> 00:19:27,950
going let's split this down here and

398
00:19:22,150 --> 00:19:34,340
here and here and each of these is of

399
00:19:27,950 --> 00:19:37,100
size BP te T which I think we had

400
00:19:34,339 --> 00:19:39,678
something like 70 right and so what

401
00:19:37,099 --> 00:19:42,849
happened was we said alright let's look

402
00:19:39,679 --> 00:19:45,890
at our first mini batch is all of these

403
00:19:42,849 --> 00:19:49,839
right so we do all of those at once and

404
00:19:45,890 --> 00:19:53,690
predict everything accrue off set by one

405
00:19:49,839 --> 00:19:57,409
and then at the end of that first mini

406
00:19:53,690 --> 00:19:58,880
batch we went to the second chunk right

407
00:19:57,410 --> 00:20:03,200
and used each one of these to predict

408
00:19:58,880 --> 00:20:06,380
the next one offset by one ok so that's

409
00:20:03,200 --> 00:20:08,210
that's why we did that slightly weird

410
00:20:06,380 --> 00:20:10,340
thing right is that we wanted to have a

411
00:20:08,210 --> 00:20:13,490
bunch of things we can look through in

412
00:20:10,339 --> 00:20:15,740
parallel each of which like hopefully a

413
00:20:13,490 --> 00:20:17,480
far enough away from each other you know

414
00:20:15,740 --> 00:20:19,039
that we don't have to worry about the

415
00:20:17,480 --> 00:20:21,289
fact that you know the truth is this

416
00:20:19,039 --> 00:20:23,119
starting the start of this million

417
00:20:21,289 --> 00:20:26,450
characters was actually in the middle of

418
00:20:23,119 --> 00:20:28,129
a sentence but you know who cares right

419
00:20:26,450 --> 00:20:33,880
because it's you know that only happens

420
00:20:28,130 --> 00:20:36,170
once every million characters honey I

421
00:20:33,880 --> 00:20:38,690
was wondering if you could talk a little

422
00:20:36,170 --> 00:20:41,420
bit more about augmentation for this

423
00:20:38,690 --> 00:20:42,890
kind of data set and how to data

424
00:20:41,420 --> 00:20:46,490
augmentation of this kind of data said

425
00:20:42,890 --> 00:20:49,220
yeah no I can't because I don't I really

426
00:20:46,490 --> 00:20:52,039
know a good way it's one of the things

427
00:20:49,220 --> 00:20:55,910
I'm going to be studying between now and

428
00:20:52,039 --> 00:20:58,879
part two there have been some recent

429
00:20:55,910 --> 00:21:00,410
developments particularly something we

430
00:20:58,880 --> 00:21:01,790
talked about the machine learning course

431
00:21:00,410 --> 00:21:04,820
and I think we've refinished in here

432
00:21:01,789 --> 00:21:06,829
which was somebody for a recent careful

433
00:21:04,819 --> 00:21:12,619
competition won it by doing data

434
00:21:06,829 --> 00:21:16,069
augmentation by randomly inserting parts

435
00:21:12,619 --> 00:21:18,288
of different rows basic

436
00:21:16,069 --> 00:21:20,000
something like that may be useful here

437
00:21:18,288 --> 00:21:23,388
and I've seen it I've seen some papers

438
00:21:20,000 --> 00:21:26,240
that do something like that but yeah I

439
00:21:23,388 --> 00:21:28,128
haven't seen any kind of recent ish

440
00:21:26,240 --> 00:21:32,480
state-of-the-art new

441
00:21:28,128 --> 00:21:33,949
NLP papers that that are doing this kind

442
00:21:32,480 --> 00:21:39,259
of data orientation so it's something

443
00:21:33,950 --> 00:21:47,058
we're planning to work on so it's

444
00:21:39,259 --> 00:21:48,048
generally how the issues be PTT so

445
00:21:47,058 --> 00:21:49,908
there's a couple of things to think

446
00:21:48,048 --> 00:21:52,009
about when you pick your be PTT the

447
00:21:49,909 --> 00:21:57,220
first is that you'll note that the the

448
00:21:52,009 --> 00:22:08,120
matrix size for a mini batch has a B PTT

449
00:21:57,220 --> 00:22:11,500
the the TT by batch size so one issue is

450
00:22:08,119 --> 00:22:14,898
your GPU Ram needs to be able to fit

451
00:22:11,500 --> 00:22:17,089
that by your embedding matrix racks

452
00:22:14,898 --> 00:22:20,329
every one of these is going to have B of

453
00:22:17,089 --> 00:22:23,388
length embedding length plus all of the

454
00:22:20,329 --> 00:22:25,369
hidden state so one thing is to you know

455
00:22:23,388 --> 00:22:29,658
if you get a cruder out of memory error

456
00:22:25,369 --> 00:22:33,138
you need to reduce one of those if

457
00:22:29,659 --> 00:22:35,269
you're finding your training is very

458
00:22:33,138 --> 00:22:38,628
unstable like your loss is shooting off

459
00:22:35,269 --> 00:22:40,579
to LAN suddenly then you could try to

460
00:22:38,628 --> 00:22:43,778
decreasing your B PTT because you've got

461
00:22:40,579 --> 00:22:47,089
less layers to gradient explode through

462
00:22:43,778 --> 00:22:49,460
it's too slow you could try decreasing

463
00:22:47,089 --> 00:22:51,168
your B PTT because it's going to kind of

464
00:22:49,460 --> 00:22:57,889
do one of those steps at a time like

465
00:22:51,169 --> 00:23:00,769
that for loop can't be paralyzed well I

466
00:22:57,888 --> 00:23:03,138
say that there's a recent thing called

467
00:23:00,769 --> 00:23:04,548
QR an N which is will hopefully talk

468
00:23:03,138 --> 00:23:05,959
about in part two which kind of does

469
00:23:04,548 --> 00:23:08,418
paralyze it but the versions we're

470
00:23:05,960 --> 00:23:09,740
looking at don't paralyze it so there

471
00:23:08,419 --> 00:23:12,169
would be the main issues I think before

472
00:23:09,740 --> 00:23:14,929
look at performance look at memory and

473
00:23:12,169 --> 00:23:17,149
look at stability and try and find a

474
00:23:14,929 --> 00:23:19,490
number that's you know as high as you

475
00:23:17,148 --> 00:23:21,819
can make it but all of those things work

476
00:23:19,490 --> 00:23:21,819
for you

477
00:23:23,099 --> 00:23:31,298
okay so trying to get although that

478
00:23:29,220 --> 00:23:33,009
chunking and lining up and anything to

479
00:23:31,298 --> 00:23:35,499
work is more code than I want to write

480
00:23:33,009 --> 00:23:37,690
so for this section we're going to go

481
00:23:35,499 --> 00:23:44,769
back and use torched s together okay

482
00:23:37,690 --> 00:23:46,808
so when you're using AP is like fast AI

483
00:23:44,769 --> 00:23:48,940
and torch text which in these case these

484
00:23:46,808 --> 00:23:50,288
two API is a desire to or at least from

485
00:23:48,940 --> 00:23:52,749
the first day I site designed to work

486
00:23:50,288 --> 00:23:55,960
together you often have a choice which

487
00:23:52,749 --> 00:23:58,298
is like okay this API has a number of

488
00:23:55,960 --> 00:24:01,028
methods that expect the data in this

489
00:23:58,298 --> 00:24:03,548
kind of format and you can either change

490
00:24:01,028 --> 00:24:07,480
your data to fit that format or you can

491
00:24:03,548 --> 00:24:08,918
write your own data set subclass to

492
00:24:07,480 --> 00:24:12,808
handle the format that your data is

493
00:24:08,919 --> 00:24:14,919
already in I've noticed on the forum a

494
00:24:12,808 --> 00:24:17,769
lot of you are spending a lot of time

495
00:24:14,919 --> 00:24:20,679
writing your own dataset classes whereas

496
00:24:17,769 --> 00:24:23,079
I am way lazier than you and I spend my

497
00:24:20,679 --> 00:24:27,580
time instead changing my data to fit the

498
00:24:23,079 --> 00:24:31,418
data set classes I have like I that's

499
00:24:27,579 --> 00:24:34,538
fine and if you realize like oh there's

500
00:24:31,419 --> 00:24:36,220
a kind of a format of data that me and

501
00:24:34,538 --> 00:24:37,839
other people are likely to be seen quite

502
00:24:36,220 --> 00:24:40,210
often and it's not in the first day our

503
00:24:37,839 --> 00:24:42,609
library then by all means write the data

504
00:24:40,210 --> 00:24:45,610
set subclass it submitted as a PR and

505
00:24:42,609 --> 00:24:48,548
then everybody can benefit you know but

506
00:24:45,609 --> 00:24:52,479
in this case I just kind of thought I

507
00:24:48,548 --> 00:24:55,868
want to have some Nietzsche data fed

508
00:24:52,480 --> 00:24:57,610
into torch text I'm just going to put it

509
00:24:55,868 --> 00:25:00,249
in the format that watch text kind of

510
00:24:57,609 --> 00:25:02,439
already support so torch text already

511
00:25:00,249 --> 00:25:04,089
has or at least the first day I wrap her

512
00:25:02,440 --> 00:25:05,528
around Twitter text already has

513
00:25:04,089 --> 00:25:09,220
something where you can have a training

514
00:25:05,528 --> 00:25:11,259
path and a validation path and you know

515
00:25:09,220 --> 00:25:12,999
one or more text files in each path

516
00:25:11,259 --> 00:25:14,889
containing a bunch of stuff that's

517
00:25:12,999 --> 00:25:17,740
concatenated together for your language

518
00:25:14,888 --> 00:25:21,248
model so in this case all I did was I

519
00:25:17,740 --> 00:25:23,499
made a copy of my nature file copied it

520
00:25:21,249 --> 00:25:26,019
into training made another copy stuck it

521
00:25:23,499 --> 00:25:29,048
into the validation and then in one of

522
00:25:26,019 --> 00:25:32,259
the you know in the training set I did I

523
00:25:29,048 --> 00:25:34,480
deleted the last twenty percent of rows

524
00:25:32,259 --> 00:25:35,440
and in the validation set I deleted all

525
00:25:34,480 --> 00:25:39,190
except for the last one

526
00:25:35,440 --> 00:25:40,840
Center for us and I was done right so I

527
00:25:39,190 --> 00:25:43,000
found this that in this case I found

528
00:25:40,839 --> 00:25:45,490
that easier than writing a custom

529
00:25:43,000 --> 00:25:47,919
dataset class the other benefit of doing

530
00:25:45,490 --> 00:25:50,200
it that way was that I felt like it was

531
00:25:47,919 --> 00:25:53,230
more realistic to have a validation set

532
00:25:50,200 --> 00:25:56,500
that wasn't a random shuffled set of

533
00:25:53,230 --> 00:25:59,378
rows of text that was like a totally

534
00:25:56,500 --> 00:26:01,179
separate part of the corpus because I

535
00:25:59,378 --> 00:26:02,859
feel like in practice you're very often

536
00:26:01,179 --> 00:26:05,798
going to be saying like oh I've got I

537
00:26:02,859 --> 00:26:07,209
don't know these books or these authors

538
00:26:05,798 --> 00:26:08,679
I'm learning from and then I want to

539
00:26:07,210 --> 00:26:10,210
apply it to these different books and

540
00:26:08,679 --> 00:26:13,360
these different authors you know so I

541
00:26:10,210 --> 00:26:15,730
felt like for getting a more realistic

542
00:26:13,359 --> 00:26:18,519
validation of my nietzsche model I

543
00:26:15,730 --> 00:26:20,710
should use like a whole separate piece

544
00:26:18,519 --> 00:26:23,829
of the text so in this case it's the

545
00:26:20,710 --> 00:26:28,450
last you know 20% of the rows if the

546
00:26:23,829 --> 00:26:31,240
corpus so I haven't created this for you

547
00:26:28,450 --> 00:26:32,528
right intentionally because you know

548
00:26:31,240 --> 00:26:35,048
this is the kind of stuff I want to do

549
00:26:32,528 --> 00:26:36,548
practicing is making sure that you're

550
00:26:35,048 --> 00:26:38,048
familiar enough comfortable enough with

551
00:26:36,548 --> 00:26:40,058
with batch or whatever that you can

552
00:26:38,048 --> 00:26:42,628
create these and that you understand

553
00:26:40,058 --> 00:26:48,220
what they need to look like and so forth

554
00:26:42,628 --> 00:26:51,579
so in this case you can see I've now got

555
00:26:48,220 --> 00:26:57,159
you know a train and a validation here

556
00:26:51,579 --> 00:26:59,168
and then I could yeah okay so you can

557
00:26:57,159 --> 00:27:00,909
see I've literally just got one file in

558
00:26:59,169 --> 00:27:02,860
it because it's a fire when you're doing

559
00:27:00,909 --> 00:27:04,750
a language model ie predicting the next

560
00:27:02,859 --> 00:27:07,479
character or predicting the next word

561
00:27:04,750 --> 00:27:09,490
you don't really need separate files

562
00:27:07,480 --> 00:27:11,110
it's fine if you do have separate files

563
00:27:09,490 --> 00:27:15,419
but they just get capped native together

564
00:27:11,109 --> 00:27:19,058
anyway alright so that's my source data

565
00:27:15,419 --> 00:27:20,919
and so here is you know the same lines

566
00:27:19,058 --> 00:27:22,240
of code that we've seen before and let's

567
00:27:20,919 --> 00:27:25,600
go over them again so it's a couple of

568
00:27:22,240 --> 00:27:28,269
lessons ago right so in torch text we

569
00:27:25,599 --> 00:27:31,888
create this thing called a field and the

570
00:27:28,269 --> 00:27:36,128
field initially is just a description of

571
00:27:31,888 --> 00:27:38,859
how to go about pre-processing the test

572
00:27:36,128 --> 00:27:41,469
okay now you in this case I'm gonna say

573
00:27:38,859 --> 00:27:44,079
hey lowercase it you know cuz I don't

574
00:27:41,470 --> 00:27:45,069
mean now I think about it there's no

575
00:27:44,079 --> 00:27:46,628
particular reason to have done this

576
00:27:45,069 --> 00:27:49,220
lower case upper case would work fine

577
00:27:46,628 --> 00:27:50,658
too and then how do I talk

578
00:27:49,220 --> 00:27:53,480
maser and so you might remember last

579
00:27:50,659 --> 00:27:56,179
time we used a tokenization function

580
00:27:53,480 --> 00:27:57,769
which kind of largely spit on white

581
00:27:56,179 --> 00:27:59,780
space and try to do some flavor things

582
00:27:57,769 --> 00:28:02,179
with punctuation right and that gave us

583
00:27:59,779 --> 00:28:04,700
a word model in this case I want to

584
00:28:02,179 --> 00:28:07,730
character model so I actually want every

585
00:28:04,700 --> 00:28:10,250
character put into a separate token so I

586
00:28:07,730 --> 00:28:16,839
could just use the function list in

587
00:28:10,250 --> 00:28:19,548
Python because list in Python does that

588
00:28:16,839 --> 00:28:23,689
okay so this is where you can kind of

589
00:28:19,548 --> 00:28:25,788
see like understanding how libraries

590
00:28:23,690 --> 00:28:27,620
like torch text and fast ar e are

591
00:28:25,788 --> 00:28:30,200
designed to be extended can make your

592
00:28:27,619 --> 00:28:33,439
life a lot easier right so when you

593
00:28:30,200 --> 00:28:35,870
realize that very often both of these

594
00:28:33,440 --> 00:28:38,808
libraries kind of expect you to pass a

595
00:28:35,869 --> 00:28:40,939
function that does something and then

596
00:28:38,808 --> 00:28:44,960
you realize like oh I can write any

597
00:28:40,940 --> 00:28:47,509
function I like all right okay so this

598
00:28:44,960 --> 00:28:49,569
is now going to mean that each mini

599
00:28:47,509 --> 00:28:52,519
batch is going to contain a list of

600
00:28:49,569 --> 00:28:55,700
characters and so here's where we get to

601
00:28:52,519 --> 00:28:58,389
define all our different parameters and

602
00:28:55,700 --> 00:29:00,529
so to make it the same as previous

603
00:28:58,390 --> 00:29:03,350
sections of this notebook I'm going to

604
00:29:00,529 --> 00:29:04,548
use the same batch size the same number

605
00:29:03,349 --> 00:29:06,649
of characters then they're going to

606
00:29:04,548 --> 00:29:08,379
rename it to their PT t since we know

607
00:29:06,650 --> 00:29:12,250
what that means

608
00:29:08,380 --> 00:29:14,960
the number of the size of the embedding

609
00:29:12,250 --> 00:29:16,400
and the size of our hidden state okay

610
00:29:14,960 --> 00:29:21,110
remembering that size of our hidden

611
00:29:16,400 --> 00:29:25,159
state simply means going all the way

612
00:29:21,109 --> 00:29:28,009
back to the start right and hidden

613
00:29:25,159 --> 00:29:29,870
simply means the size of the state

614
00:29:28,009 --> 00:29:31,640
that's created by each of those orange

615
00:29:29,869 --> 00:29:37,869
arrows so it's the size of each of those

616
00:29:31,640 --> 00:29:37,870
circles yeah okay

617
00:29:38,289 --> 00:29:42,950
so having done that we can then create a

618
00:29:40,910 --> 00:29:45,259
little dictionary saying what's our

619
00:29:42,950 --> 00:29:46,910
training validation and test set in this

620
00:29:45,259 --> 00:29:50,240
case I don't have a separate test set so

621
00:29:46,910 --> 00:29:51,890
I just use the same thing and then I can

622
00:29:50,240 --> 00:29:55,220
say all right I want a language model

623
00:29:51,890 --> 00:29:58,250
data subclass with model data I'm going

624
00:29:55,220 --> 00:30:03,230
to grab it from text files and this is

625
00:29:58,250 --> 00:30:06,220
my path and this is my field which I

626
00:30:03,230 --> 00:30:11,450
defined earlier and these are my files

627
00:30:06,220 --> 00:30:12,740
and these are my hyper parameters min

628
00:30:11,450 --> 00:30:14,779
fracks not going to do anything actually

629
00:30:12,740 --> 00:30:15,799
in this case because there's not I don't

630
00:30:14,779 --> 00:30:17,769
think there's going to be any character

631
00:30:15,799 --> 00:30:22,549
that appears less than three times

632
00:30:17,769 --> 00:30:24,609
that's probably redundant okay so at the

633
00:30:22,549 --> 00:30:29,210
end of that it says there's going to be

634
00:30:24,609 --> 00:30:31,179
963 batches to go through and so if you

635
00:30:29,210 --> 00:30:35,029
think about it that should be equal to

636
00:30:31,180 --> 00:30:38,810
the number of tokens divided by the

637
00:30:35,029 --> 00:30:40,629
batch size divided by B PTT because

638
00:30:38,809 --> 00:30:43,629
that's like the size of each of those

639
00:30:40,630 --> 00:30:43,630
rectangles

640
00:30:45,130 --> 00:30:49,850
you'll find that in practice it's not

641
00:30:47,690 --> 00:30:52,400
exactly that and the reason it's not

642
00:30:49,849 --> 00:30:56,000
exactly that is that the authors of

643
00:30:52,400 --> 00:30:57,950
torch text did something pretty smart

644
00:30:56,000 --> 00:30:59,839
which I think we've briefly mentioned

645
00:30:57,950 --> 00:31:02,120
this before they said okay we can't

646
00:30:59,839 --> 00:31:03,649
shuffle the data like with images we'd

647
00:31:02,119 --> 00:31:04,759
like to shuffle the order so every time

648
00:31:03,650 --> 00:31:06,530
we see them in a different order so

649
00:31:04,759 --> 00:31:08,329
there's a bit more random listed we

650
00:31:06,529 --> 00:31:11,210
can't shuffle because we need to be

651
00:31:08,329 --> 00:31:13,789
contiguous but what we could do is

652
00:31:11,210 --> 00:31:16,279
randomize the length of you know

653
00:31:13,789 --> 00:31:19,819
basically randomize be PTT a little bit

654
00:31:16,279 --> 00:31:21,819
each time and so that's what PI torch

655
00:31:19,819 --> 00:31:26,720
does it's not always going to give us

656
00:31:21,819 --> 00:31:30,139
exactly 8 characters long 5% of the time

657
00:31:26,720 --> 00:31:32,660
it'll actually cut it enough and then

658
00:31:30,140 --> 00:31:35,030
it's going to add on a small little

659
00:31:32,660 --> 00:31:36,560
standard deviation you know to make it

660
00:31:35,029 --> 00:31:38,539
slightly bigger or smaller than for

661
00:31:36,559 --> 00:31:44,829
white okay so it's going to be slightly

662
00:31:38,539 --> 00:31:44,829
different to eight on average

663
00:31:45,130 --> 00:31:53,950
yes just to make sure is it going to be

664
00:31:50,788 --> 00:31:57,038
constant per Vinny watch yeah yeah

665
00:31:53,950 --> 00:32:01,480
exactly that's right so a mini batch you

666
00:31:57,038 --> 00:32:06,640
know has to kind of it needs to do a

667
00:32:01,480 --> 00:32:09,278
matrix multiplication and the mini batch

668
00:32:06,640 --> 00:32:14,288
size has to remain constant because

669
00:32:09,278 --> 00:32:16,569
we've got this age weight matrix that

670
00:32:14,288 --> 00:32:20,679
has to you know has to line up in size

671
00:32:16,569 --> 00:32:23,009
with the size of the mini batch yeah but

672
00:32:20,679 --> 00:32:33,370
the number you know the sequence length

673
00:32:23,009 --> 00:32:36,369
can can change their problem okay so

674
00:32:33,369 --> 00:32:37,989
that's why we have 963 that's so the

675
00:32:36,369 --> 00:32:39,849
length of a data loader is how many mini

676
00:32:37,990 --> 00:32:42,759
batches in this case it's so do it

677
00:32:39,849 --> 00:32:45,129
approximate okay number of tokens is how

678
00:32:42,759 --> 00:32:51,210
many unique things are in the vocabulary

679
00:32:45,130 --> 00:32:54,220
and remember after we run this line text

680
00:32:51,210 --> 00:32:56,409
now does not just contain in a

681
00:32:54,220 --> 00:33:01,710
description of what we want but it also

682
00:32:56,409 --> 00:33:06,610
contains an extra attribute called vocab

683
00:33:01,710 --> 00:33:10,090
right which contains stuff like a list

684
00:33:06,609 --> 00:33:14,859
of all of the unique items in the

685
00:33:10,089 --> 00:33:19,329
vocabulary and a reverse mapping from

686
00:33:14,859 --> 00:33:23,139
each item to its number okay so that

687
00:33:19,329 --> 00:33:31,599
text object is now an important thing to

688
00:33:23,140 --> 00:33:34,179
keep out all right so let's now try this

689
00:33:31,599 --> 00:33:36,639
so we do we started out by looking at

690
00:33:34,179 --> 00:33:39,130
the class so the class is exactly the

691
00:33:36,640 --> 00:33:41,140
same as the class we've had before the

692
00:33:39,130 --> 00:33:44,860
only key difference is to call in at

693
00:33:41,140 --> 00:33:46,630
hidden which calls sets out so H is not

694
00:33:44,859 --> 00:33:50,199
a variable anymore it's now an attribute

695
00:33:46,630 --> 00:33:54,778
itself that H is a variable containing a

696
00:33:50,200 --> 00:33:58,400
bunch of zeros now I've been shown that

697
00:33:54,778 --> 00:34:00,319
that size remains constant H time

698
00:33:58,400 --> 00:34:03,830
but unfortunately when I said that I

699
00:34:00,319 --> 00:34:07,539
lied to you and the way that I lied to

700
00:34:03,829 --> 00:34:10,969
you is that the very last mini batch

701
00:34:07,539 --> 00:34:12,529
will be shorter okay the very last mini

702
00:34:10,969 --> 00:34:14,209
batch is actually going to have less

703
00:34:12,530 --> 00:34:16,519
than 60 well it might be exactly the

704
00:34:14,210 --> 00:34:19,250
right size if it so happens that this

705
00:34:16,519 --> 00:34:21,500
data set is exactly divisible by B PTT

706
00:34:19,250 --> 00:34:23,750
times patch size but it probably isn't

707
00:34:21,500 --> 00:34:27,168
so the last batch will probably has a

708
00:34:23,750 --> 00:34:29,418
little bit less okay and so that's why I

709
00:34:27,168 --> 00:34:31,849
do a little check here that says let's

710
00:34:29,418 --> 00:34:35,239
check that the batch size inside self

711
00:34:31,849 --> 00:34:40,929
dot H right and so self dot H is going

712
00:34:35,239 --> 00:34:44,239
to be the height sorry the height is

713
00:34:40,929 --> 00:34:46,039
going to be the number of activations

714
00:34:44,239 --> 00:34:49,069
and the width is going to be the mini

715
00:34:46,039 --> 00:34:54,230
batch size okay check that that's equal

716
00:34:49,070 --> 00:34:56,360
to the actual sequence length sorry the

717
00:34:54,230 --> 00:34:58,849
actual batch size length that we've

718
00:34:56,360 --> 00:35:02,660
received okay and if they're not the

719
00:34:58,849 --> 00:35:05,210
same then set it back to 0 's again okay

720
00:35:02,659 --> 00:35:07,089
so this is just a minor little wrinkle

721
00:35:05,210 --> 00:35:10,849
that basically at the end of each epoch

722
00:35:07,090 --> 00:35:13,579
it's going to do like a little mini mini

723
00:35:10,849 --> 00:35:15,980
batch right and so then as soon as it

724
00:35:13,579 --> 00:35:17,329
starts the next epoch it's going to see

725
00:35:15,980 --> 00:35:19,190
that they're not the same again and

726
00:35:17,329 --> 00:35:21,949
it'll reinitialize it to the correct

727
00:35:19,190 --> 00:35:23,300
full batch size okay so that's why you

728
00:35:21,949 --> 00:35:25,460
know if you're wondering there's an

729
00:35:23,300 --> 00:35:28,580
inert hidden not just in the constructor

730
00:35:25,460 --> 00:35:31,880
but also inside forward it's to handle

731
00:35:28,579 --> 00:35:34,849
this kind of end of each epoch start of

732
00:35:31,880 --> 00:35:36,490
each epoch difference okay not an

733
00:35:34,849 --> 00:35:39,159
important point by any means but

734
00:35:36,489 --> 00:35:47,989
potentially confusing when you see it

735
00:35:39,159 --> 00:35:51,739
okay so the last wrinkle the last

736
00:35:47,989 --> 00:35:53,599
wrinkle is something which i think is

737
00:35:51,739 --> 00:35:55,309
something that slightly sucks about pi

738
00:35:53,599 --> 00:35:57,079
torch and maybe somebody can be nice

739
00:35:55,309 --> 00:35:59,650
enough to try and fix it with a PR if

740
00:35:57,079 --> 00:36:04,779
anybody feels like it which is that the

741
00:35:59,650 --> 00:36:04,780
loss functions such as softmax

742
00:36:04,960 --> 00:36:11,599
I'm not happy receiving a rank 3 tensor

743
00:36:09,099 --> 00:36:12,259
remember a rank 3 tensor is just another

744
00:36:11,599 --> 00:36:17,239
way of saying

745
00:36:12,260 --> 00:36:18,799
dimension three right okay there's no

746
00:36:17,239 --> 00:36:21,199
particular reason they ought to not be

747
00:36:18,798 --> 00:36:22,639
happy receiving a rank 3 tensor you know

748
00:36:21,199 --> 00:36:24,558
like somebody could write some code to

749
00:36:22,639 --> 00:36:27,199
say hey a wreck three tensor is probably

750
00:36:24,559 --> 00:36:31,579
you know a sequence length by batch size

751
00:36:27,199 --> 00:36:35,139
by you know results thing and so you

752
00:36:31,579 --> 00:36:39,710
should just do it for each of the two

753
00:36:35,139 --> 00:36:43,539
initial axis but no one's done that and

754
00:36:39,710 --> 00:36:45,798
so it expects it to be a rank two tensor

755
00:36:43,539 --> 00:36:48,710
funnily enough it can handle write to or

756
00:36:45,798 --> 00:37:00,619
rec for they're not right through yeah

757
00:36:48,710 --> 00:37:04,909
so we've got so we've got a rank two

758
00:37:00,619 --> 00:37:08,358
tensor containing you know for each time

759
00:37:04,909 --> 00:37:11,058
period I can't remember which way around

760
00:37:08,358 --> 00:37:17,420
the the y axes are but whatever for each

761
00:37:11,059 --> 00:37:21,190
time period for each batch we've got our

762
00:37:17,420 --> 00:37:27,170
predictions okay and then we've got our

763
00:37:21,190 --> 00:37:31,309
our actuals for each time period for

764
00:37:27,170 --> 00:37:34,039
each batch we've got our predictions and

765
00:37:31,309 --> 00:37:35,599
we've got our actuals okay and so we

766
00:37:34,039 --> 00:37:37,759
just want to check whether they're the

767
00:37:35,599 --> 00:37:39,920
same and so at an ideal world our lost

768
00:37:37,760 --> 00:37:42,319
function a loss function would check you

769
00:37:39,920 --> 00:37:44,298
know item 1 1 then item 1 2 and then

770
00:37:42,318 --> 00:37:46,849
item 1 3 but since that hasn't been

771
00:37:44,298 --> 00:37:48,980
written we just have to flatten them

772
00:37:46,849 --> 00:37:53,390
both out okay and we can literally just

773
00:37:48,980 --> 00:37:57,940
flatten them out put rose - rose and so

774
00:37:53,389 --> 00:38:04,039
that's why here I have to use dot view

775
00:37:57,940 --> 00:38:06,019
okay and so dot view says the number of

776
00:38:04,039 --> 00:38:07,219
columns will be equal to the size of the

777
00:38:06,019 --> 00:38:08,659
vocab because remember we're going to

778
00:38:07,219 --> 00:38:11,449
end up with a prediction you know a

779
00:38:08,659 --> 00:38:14,358
probability for each letter and then the

780
00:38:11,449 --> 00:38:15,980
number of rows is however big is

781
00:38:14,358 --> 00:38:24,259
necessary which will be equal to batch

782
00:38:15,980 --> 00:38:26,059
size times B PTT okay and then you may

783
00:38:24,260 --> 00:38:27,230
be wondering where I do that

784
00:38:26,059 --> 00:38:28,670
that's so that's where the predictions

785
00:38:27,230 --> 00:38:31,039
you may be wondering where I do that for

786
00:38:28,670 --> 00:38:32,480
the target and the answer is torch text

787
00:38:31,039 --> 00:38:33,050
knows that the target needs to look like

788
00:38:32,480 --> 00:38:34,699
that

789
00:38:33,050 --> 00:38:36,920
so torch text has already done that for

790
00:38:34,699 --> 00:38:39,099
us okay so torch text automatically

791
00:38:36,920 --> 00:38:41,659
changes the target to be flattened out

792
00:38:39,099 --> 00:38:45,139
as you might actually remember if you go

793
00:38:41,659 --> 00:38:48,019
back to lesson 4 when we actually looked

794
00:38:45,139 --> 00:38:50,539
at a mini batch that's bad out of torch

795
00:38:48,019 --> 00:38:52,730
text we did we noticed actually that it

796
00:38:50,539 --> 00:38:55,579
was flattened and I said we'll learn

797
00:38:52,730 --> 00:39:00,010
about why later and so later is now all

798
00:38:55,579 --> 00:39:06,920
right okay so they're the three wrinkles

799
00:39:00,010 --> 00:39:11,180
get rid of the history ooh I guess for

800
00:39:06,920 --> 00:39:16,970
wrinkles recreate the hidden state if

801
00:39:11,179 --> 00:39:19,789
the batch size changes flatten out and

802
00:39:16,969 --> 00:39:23,179
then you just torch text to create mini

803
00:39:19,789 --> 00:39:26,690
batches that line up nicely so once we

804
00:39:23,179 --> 00:39:30,469
do those things we can then create our

805
00:39:26,690 --> 00:39:38,900
model create our optimizer with that

806
00:39:30,469 --> 00:39:48,649
models parameters and fit it one thing

807
00:39:38,900 --> 00:39:55,700
to be careful of here is that softmax

808
00:39:48,650 --> 00:39:59,030
now as of hi torch 0.3 requires that we

809
00:39:55,699 --> 00:40:02,079
pass in a number here saying which

810
00:39:59,030 --> 00:40:05,180
access do we want to do the softmax over

811
00:40:02,079 --> 00:40:08,420
so at this point this is a

812
00:40:05,179 --> 00:40:10,819
three-dimensional tensor right and so we

813
00:40:08,420 --> 00:40:13,700
want to do the softmax over the final

814
00:40:10,820 --> 00:40:15,830
axis right so when i say which axis do

815
00:40:13,699 --> 00:40:18,469
we do the softmax over remember we

816
00:40:15,829 --> 00:40:21,409
divide by there we go e to the X I

817
00:40:18,469 --> 00:40:24,439
divided by the sum of e to the X I so

818
00:40:21,409 --> 00:40:26,119
it's saying which axis do we sum over so

819
00:40:24,440 --> 00:40:29,119
which access we want to sum to one and

820
00:40:26,119 --> 00:40:31,279
so in this case clearly we want to do it

821
00:40:29,119 --> 00:40:33,019
over the last axis because the last axis

822
00:40:31,280 --> 00:40:36,140
is the one that contains the probability

823
00:40:33,019 --> 00:40:37,759
per letter of the alphabet and we want

824
00:40:36,139 --> 00:40:40,069
all of those probabilities to sum to one

825
00:40:37,760 --> 00:40:45,590
okay

826
00:40:40,070 --> 00:40:48,500
so therefore to run this notebook you're

827
00:40:45,590 --> 00:40:50,450
going to need PI torch 0.3 which just

828
00:40:48,500 --> 00:40:51,920
came out this week ok so if you're doing

829
00:40:50,449 --> 00:40:54,710
this on the milk you're fine I'm sure

830
00:40:51,920 --> 00:40:56,840
you've got at least a 0.3 or later ok

831
00:40:54,710 --> 00:40:58,820
where else the students here if you just

832
00:40:56,840 --> 00:41:02,809
go Conda and update it will

833
00:40:58,820 --> 00:41:06,769
automatically update you to 0.3 the

834
00:41:02,809 --> 00:41:08,630
really great news is that 0.3 although

835
00:41:06,769 --> 00:41:11,110
it does not yet officially support

836
00:41:08,630 --> 00:41:14,030
windows it does in practice I

837
00:41:11,110 --> 00:41:16,130
successfully installed 0.3 from Condor

838
00:41:14,030 --> 00:41:19,519
yesterday by typing Condor install torch

839
00:41:16,130 --> 00:41:21,500
PI torch in Windows are then attempted

840
00:41:19,519 --> 00:41:24,139
to use the entirety of lesson 1 and

841
00:41:21,500 --> 00:41:27,949
every single part worked so I actually

842
00:41:24,139 --> 00:41:30,920
ran it on this very laptop so for those

843
00:41:27,949 --> 00:41:32,509
who are interested in doing deep burning

844
00:41:30,920 --> 00:41:36,650
on their laptop can definitely recommend

845
00:41:32,510 --> 00:41:42,290
the new surface book the new surface

846
00:41:36,650 --> 00:41:45,349
book 15-inch has a gtx 1066 gig GPU in

847
00:41:42,289 --> 00:41:53,230
it and i was getting and it was running

848
00:41:45,349 --> 00:41:55,940
about 3 times slower than my 1080 TI

849
00:41:53,230 --> 00:42:00,679
which i think means it's about the same

850
00:41:55,940 --> 00:42:02,539
speed as an AW sp2 the instance and as

851
00:42:00,679 --> 00:42:04,699
you can see it's also a nice convertible

852
00:42:02,539 --> 00:42:07,130
tablet that you can write on and it's

853
00:42:04,699 --> 00:42:09,619
thin and light and so it's like I've

854
00:42:07,130 --> 00:42:13,160
never seen a such a good deep learning

855
00:42:09,619 --> 00:42:16,190
boss also I successfully installed Linux

856
00:42:13,159 --> 00:42:18,980
on eart and all of the faster I staff

857
00:42:16,190 --> 00:42:22,309
worked on Linux as well so really good

858
00:42:18,980 --> 00:42:25,460
option if you're interested in a laptop

859
00:42:22,309 --> 00:42:26,559
that can run deep learning stuff

860
00:42:25,460 --> 00:42:29,150
[Music]

861
00:42:26,559 --> 00:42:30,980
alright so that's that's going to be

862
00:42:29,150 --> 00:42:33,230
aware of with this team equals minus 1

863
00:42:30,980 --> 00:42:36,949
so then we can go ahead and construct

864
00:42:33,230 --> 00:42:39,619
this and we can call fit and yeah we're

865
00:42:36,949 --> 00:42:43,359
basically going to get pretty similar

866
00:42:39,619 --> 00:42:49,369
results to what we got the ball

867
00:42:43,360 --> 00:42:52,309
alright so then we can go a bit further

868
00:42:49,369 --> 00:42:53,109
without RNN by just kind of unpacking it

869
00:42:52,309 --> 00:42:55,929
a bit more

870
00:42:53,110 --> 00:42:57,460
and so this is now again exactly the

871
00:42:55,929 --> 00:43:02,079
same thing gives exactly the same

872
00:42:57,460 --> 00:43:05,500
answers but I have removed the cold air

873
00:43:02,079 --> 00:43:09,130
in it so I've got rid of this self to

874
00:43:05,500 --> 00:43:10,690
iron in okay and so this is just

875
00:43:09,130 --> 00:43:11,680
something I won't spend time on it but

876
00:43:10,690 --> 00:43:14,889
you can check it out

877
00:43:11,679 --> 00:43:17,710
so instead I've now to find iron in as R

878
00:43:14,889 --> 00:43:19,809
and n cell and I've copied and pasted

879
00:43:17,710 --> 00:43:22,449
the code above don't run it this is just

880
00:43:19,809 --> 00:43:24,099
for your reference from PI torch this is

881
00:43:22,449 --> 00:43:26,500
this is the color the definition of

882
00:43:24,099 --> 00:43:29,559
eridan so in PI torch and I want you to

883
00:43:26,500 --> 00:43:32,320
see that you can now read PI torch

884
00:43:29,559 --> 00:43:33,549
source code and understand it not only

885
00:43:32,320 --> 00:43:35,200
that you'll recognize it as being

886
00:43:33,550 --> 00:43:38,560
something we've done before it's a

887
00:43:35,199 --> 00:43:42,250
matrix modification of the weights by

888
00:43:38,559 --> 00:43:45,309
the inputs plus biases so f dot linear

889
00:43:42,250 --> 00:43:47,320
simply does a matrix product followed by

890
00:43:45,309 --> 00:43:51,329
an addition right and interestingly

891
00:43:47,320 --> 00:43:55,180
you'll see they do not concatenate the

892
00:43:51,329 --> 00:43:58,150
the input bit and the hidden bit they

893
00:43:55,179 --> 00:44:00,250
sum them together which is our first

894
00:43:58,150 --> 00:44:02,500
approach and I'm as I said you can do

895
00:44:00,250 --> 00:44:03,909
either neither one's right or wrong but

896
00:44:02,500 --> 00:44:05,230
it's interesting to see this is the

897
00:44:03,909 --> 00:44:10,059
definition yeah

898
00:44:05,230 --> 00:44:12,900
yes you know can give some insight about

899
00:44:10,059 --> 00:44:18,840
what are they using that particular

900
00:44:12,900 --> 00:44:20,889
activation function firm yeah yeah I

901
00:44:18,840 --> 00:44:22,360
think you might have briefly covered

902
00:44:20,889 --> 00:44:28,690
this last week but very happy to do it

903
00:44:22,360 --> 00:44:31,829
again if I did basically fan that's

904
00:44:28,690 --> 00:44:31,829
positive 1 and negative 1

905
00:44:36,659 --> 00:44:42,670
then looks like that so in other words

906
00:44:39,550 --> 00:44:45,640
it's a sigmoid function double the

907
00:44:42,670 --> 00:44:49,530
height minus one naturally they're

908
00:44:45,639 --> 00:44:53,559
they're equal so it's it's it's a nice

909
00:44:49,530 --> 00:44:55,810
function in that it's forcing it to be

910
00:44:53,559 --> 00:44:57,699
you know no smaller than minus one no

911
00:44:55,809 --> 00:44:59,860
bigger than plus one and since we're

912
00:44:57,699 --> 00:45:03,309
multiplying by this white matrix again

913
00:44:59,860 --> 00:45:05,970
and again and again and again we might

914
00:45:03,309 --> 00:45:08,920
worry that our Lu because it's unbounded

915
00:45:05,969 --> 00:45:11,829
might have more of a gradient explosion

916
00:45:08,920 --> 00:45:19,269
problem that's basically the theory

917
00:45:11,829 --> 00:45:24,429
having said that you can actually ask pi

918
00:45:19,269 --> 00:45:26,320
torch for an RNN cell which uses a

919
00:45:24,429 --> 00:45:28,089
different non-linearity so you can see

920
00:45:26,320 --> 00:45:31,450
by default it uses then you can ask for

921
00:45:28,090 --> 00:45:32,829
a value as well but yeah most people

922
00:45:31,449 --> 00:45:36,359
seem to pretty much everybody still

923
00:45:32,829 --> 00:45:38,769
seems to use them as far as I can tell

924
00:45:36,360 --> 00:45:40,030
so you can basically see here this is

925
00:45:38,769 --> 00:45:41,949
all the same except now I've got an

926
00:45:40,030 --> 00:45:44,590
errand in cell which means now I need to

927
00:45:41,949 --> 00:45:48,179
put my for loop back alright and you can

928
00:45:44,590 --> 00:45:51,460
see every time I call my my little

929
00:45:48,179 --> 00:45:54,940
linear function I just obtained the

930
00:45:51,460 --> 00:45:57,490
result onto my list okay and at the end

931
00:45:54,940 --> 00:46:00,220
the result is that all stacked up

932
00:45:57,489 --> 00:46:04,299
together okay so like just trying to

933
00:46:00,219 --> 00:46:06,489
show you how nothing inside PI torches

934
00:46:04,300 --> 00:46:08,530
is mysterious right you should find you

935
00:46:06,489 --> 00:46:11,259
get basically the fact you I found I got

936
00:46:08,530 --> 00:46:14,320
exactly the same answer from this as the

937
00:46:11,260 --> 00:46:15,970
previous one okay in practice you would

938
00:46:14,320 --> 00:46:17,920
never write it like this but what you

939
00:46:15,969 --> 00:46:19,509
may well find in practice is that

940
00:46:17,920 --> 00:46:22,119
somebody will come up with like a new

941
00:46:19,510 --> 00:46:24,190
kind of eridan cell or a different way

942
00:46:22,119 --> 00:46:25,690
of kind of keeping track of things over

943
00:46:24,190 --> 00:46:30,789
time or a different way of doing

944
00:46:25,690 --> 00:46:34,019
regularization and so inside posterize

945
00:46:30,789 --> 00:46:37,179
code you will find that we we do this

946
00:46:34,019 --> 00:46:39,309
exactly this basically we have this by

947
00:46:37,179 --> 00:46:41,079
hand because we use some regularization

948
00:46:39,309 --> 00:46:43,349
approaches that are supported by pi

949
00:46:41,079 --> 00:46:43,349
torch

950
00:46:43,789 --> 00:46:47,779
all right so then another thing I'm not

951
00:46:46,369 --> 00:46:50,960
going to spend much time on but I'll

952
00:46:47,780 --> 00:46:54,500
mention briefly is that nobody really

953
00:46:50,960 --> 00:46:56,240
uses this hour an insult in practice and

954
00:46:54,500 --> 00:46:58,940
the reason we don't use that eridan so

955
00:46:56,239 --> 00:47:03,589
in practice is even though the fan is

956
00:46:58,940 --> 00:47:05,780
here you do tend to find gradient

957
00:47:03,590 --> 00:47:08,059
explosions are still a problem and so we

958
00:47:05,780 --> 00:47:10,250
have to use pretty low learning rates to

959
00:47:08,059 --> 00:47:15,710
get these to train and pretty small

960
00:47:10,250 --> 00:47:17,719
values for B PTT to get them to Train so

961
00:47:15,710 --> 00:47:20,659
what we do instead is we replace the

962
00:47:17,719 --> 00:47:26,889
eridan cell with something like this

963
00:47:20,659 --> 00:47:26,889
this is called a GI u cell and a GI u so

964
00:47:29,590 --> 00:47:35,769
here it is has a picture of it and

965
00:47:35,800 --> 00:47:41,150
there's the equations for it so

966
00:47:38,389 --> 00:47:42,619
basically I'll show you both quickly but

967
00:47:41,150 --> 00:47:47,570
we'll talk about it much more in part

968
00:47:42,619 --> 00:47:51,739
two we've got our input okay

969
00:47:47,570 --> 00:47:56,660
and our input normally goes straight in

970
00:47:51,739 --> 00:48:00,799
gets multiplied by a weight matrix to

971
00:47:56,659 --> 00:48:02,019
create our new activations that's not

972
00:48:00,800 --> 00:48:04,640
what happens

973
00:48:02,019 --> 00:48:06,980
and then we've cost we also we add it to

974
00:48:04,639 --> 00:48:10,489
the existing activations that's not what

975
00:48:06,980 --> 00:48:14,510
happens here in this case our input goes

976
00:48:10,489 --> 00:48:16,189
into this H tilde temporary thing and it

977
00:48:14,510 --> 00:48:18,170
doesn't just get added to our

978
00:48:16,190 --> 00:48:20,059
activations their previous activations

979
00:48:18,170 --> 00:48:24,740
that our previous activations get

980
00:48:20,059 --> 00:48:29,449
multiplied by this value R and R stands

981
00:48:24,739 --> 00:48:31,609
for reset it's a reset gate okay and how

982
00:48:29,449 --> 00:48:33,619
do we calculate that this this value

983
00:48:31,610 --> 00:48:36,890
goes between Norton one right in our

984
00:48:33,619 --> 00:48:40,099
reset gate well the answer is it's

985
00:48:36,889 --> 00:48:43,819
simply equal to a matrix product between

986
00:48:40,099 --> 00:48:46,460
some weight matrix and the concatenation

987
00:48:43,820 --> 00:48:49,000
of our previous hidden state and our new

988
00:48:46,460 --> 00:48:53,030
input in other words this is a little

989
00:48:49,000 --> 00:48:54,199
one hidden layer neural net and in

990
00:48:53,030 --> 00:48:56,240
particular it's a one hidden layer

991
00:48:54,199 --> 00:48:57,649
neural net because we're then put it

992
00:48:56,239 --> 00:48:59,209
through the sigmoid function

993
00:48:57,650 --> 00:49:00,889
when you seasick but one of the things I

994
00:48:59,210 --> 00:49:03,230
hate about mathematical notation is

995
00:49:00,889 --> 00:49:05,088
symbols are overloaded a lot right

996
00:49:03,230 --> 00:49:07,490
sometimes when you see Sigma that means

997
00:49:05,088 --> 00:49:09,528
standard deviation when you see it next

998
00:49:07,489 --> 00:49:14,018
to a parenthesis like this it means the

999
00:49:09,528 --> 00:49:14,018
sigmoid function okay so in other words

1000
00:49:16,838 --> 00:49:28,130
that okay which looks like that okay so

1001
00:49:26,179 --> 00:49:29,598
this is like a little mini neural net

1002
00:49:28,130 --> 00:49:30,920
with no hidden layers or to think of it

1003
00:49:29,599 --> 00:49:34,099
another way it's like a little logistic

1004
00:49:30,920 --> 00:49:35,420
regression okay and this is I mentioned

1005
00:49:34,099 --> 00:49:37,640
this briefly because it's going to come

1006
00:49:35,420 --> 00:49:39,499
up a lot in part two and so it's a good

1007
00:49:37,639 --> 00:49:43,098
thing to like start learning about it's

1008
00:49:39,498 --> 00:49:45,889
this idea that like in the very learning

1009
00:49:43,099 --> 00:49:47,749
itself you can have like little mini

1010
00:49:45,889 --> 00:49:50,328
neural nets inside your neural nets

1011
00:49:47,748 --> 00:49:53,389
right and so this little mini neural net

1012
00:49:50,329 --> 00:49:56,028
is going to be used to decide how much

1013
00:49:53,389 --> 00:49:58,278
of my hidden state am I going to

1014
00:49:56,028 --> 00:50:00,579
remember right and so it might learn

1015
00:49:58,278 --> 00:50:03,079
that Oh in this particular situation

1016
00:50:00,579 --> 00:50:05,900
forget everything you know for example

1017
00:50:03,079 --> 00:50:07,489
oh there's a full-stop you know hey when

1018
00:50:05,900 --> 00:50:09,139
you see a full-stop you should throw

1019
00:50:07,489 --> 00:50:11,059
away nearly all of your hidden state

1020
00:50:09,139 --> 00:50:12,858
that is probably something it would

1021
00:50:11,059 --> 00:50:14,839
learn and that that's very easy for it

1022
00:50:12,858 --> 00:50:17,119
to learn using this little mini neuron

1023
00:50:14,838 --> 00:50:20,328
there okay and so that goes through to

1024
00:50:17,119 --> 00:50:22,730
create my new hidden state along with

1025
00:50:20,329 --> 00:50:24,410
the input and then there's a second

1026
00:50:22,730 --> 00:50:28,309
thing that happens which is there's this

1027
00:50:24,409 --> 00:50:31,759
gate here called Z and what Z says is

1028
00:50:28,309 --> 00:50:34,369
all right you've got your some amount of

1029
00:50:31,759 --> 00:50:36,559
your previous hidden state plus your new

1030
00:50:34,369 --> 00:50:39,259
input right and it's going to go through

1031
00:50:36,559 --> 00:50:41,749
to create your new state and I'm going

1032
00:50:39,259 --> 00:50:45,798
to let you decide to what degree do you

1033
00:50:41,748 --> 00:50:47,719
use this new input version of your

1034
00:50:45,798 --> 00:50:49,130
hidden state and to what degree where

1035
00:50:47,719 --> 00:50:50,988
you just leave the hidden state the same

1036
00:50:49,130 --> 00:50:53,809
as before so this thing here is called

1037
00:50:50,989 --> 00:50:55,519
the update gate right and so it's got

1038
00:50:53,809 --> 00:50:57,920
two choices it can make the first-years

1039
00:50:55,518 --> 00:50:59,719
to throw away some hidden state when

1040
00:50:57,920 --> 00:51:02,900
deciding how much to incorporate that

1041
00:50:59,719 --> 00:51:05,328
versus my new input and how much to

1042
00:51:02,900 --> 00:51:08,150
update my hidden state versus just leave

1043
00:51:05,329 --> 00:51:10,130
it exactly the same and the equation

1044
00:51:08,150 --> 00:51:11,329
hopefully is going to look pretty

1045
00:51:10,130 --> 00:51:14,200
similar

1046
00:51:11,329 --> 00:51:16,519
to you which is check this out here

1047
00:51:14,199 --> 00:51:19,730
remember how I said you want to start to

1048
00:51:16,519 --> 00:51:20,539
recognize some some common ways of

1049
00:51:19,730 --> 00:51:25,519
looking at things

1050
00:51:20,539 --> 00:51:30,259
well here I have a 1 - something by a

1051
00:51:25,519 --> 00:51:33,139
thing and a something without the 1 - by

1052
00:51:30,260 --> 00:51:35,660
a thing which remember is a linear

1053
00:51:33,139 --> 00:51:39,650
interpolation right so in other words

1054
00:51:35,659 --> 00:51:43,129
the value of Z is going to decide to

1055
00:51:39,650 --> 00:51:46,490
what degree do I have keep the previous

1056
00:51:43,130 --> 00:51:49,880
hidden state and to what degree do I use

1057
00:51:46,489 --> 00:51:52,849
the new hidden state right so that's why

1058
00:51:49,880 --> 00:51:55,490
they draw it here as this kind of like

1059
00:51:52,849 --> 00:51:57,230
it's not actually a switch but like you

1060
00:51:55,489 --> 00:51:59,149
can put it in any position you can be

1061
00:51:57,230 --> 00:52:03,880
like oh it's here or it's here or it's

1062
00:51:59,150 --> 00:52:06,200
here to decide how much that'll do ok so

1063
00:52:03,880 --> 00:52:07,820
so they're basically the equations it's

1064
00:52:06,199 --> 00:52:09,500
a it's a little mini neural net with its

1065
00:52:07,820 --> 00:52:11,510
own weight matrix to decide how much to

1066
00:52:09,500 --> 00:52:13,159
update little mini neural net with its

1067
00:52:11,510 --> 00:52:15,440
own weight matrix to decide how much to

1068
00:52:13,159 --> 00:52:17,420
reset and then that's used to do an

1069
00:52:15,440 --> 00:52:21,590
interpolation between the two hidden

1070
00:52:17,420 --> 00:52:24,769
states so that's called giu

1071
00:52:21,590 --> 00:52:27,519
gated recurrent Network there's the

1072
00:52:24,769 --> 00:52:31,009
definition from the PI torch source code

1073
00:52:27,519 --> 00:52:32,329
they have some slight optimizations here

1074
00:52:31,010 --> 00:52:35,540
that if you're interested in we can talk

1075
00:52:32,329 --> 00:52:39,079
about them on the forum but it's exactly

1076
00:52:35,539 --> 00:52:43,880
the same for we just saw and so if you

1077
00:52:39,079 --> 00:52:47,119
go and NDI you that it uses this same

1078
00:52:43,880 --> 00:52:51,619
code but it replaces the iron in so with

1079
00:52:47,119 --> 00:52:53,409
this cell okay and as a result rather

1080
00:52:51,619 --> 00:52:58,699
than having something where we needed

1081
00:52:53,409 --> 00:53:02,059
where we were getting a 1.54 we're now

1082
00:52:58,699 --> 00:53:03,649
getting down to one point 400 and we can

1083
00:53:02,059 --> 00:53:05,840
keep training even more get right down

1084
00:53:03,650 --> 00:53:08,599
to one point three six okay so in

1085
00:53:05,840 --> 00:53:09,980
practice a giu or very nearly

1086
00:53:08,599 --> 00:53:12,949
equivalently we'll see in a moment in

1087
00:53:09,980 --> 00:53:18,889
lsdm in practice what pretty much

1088
00:53:12,949 --> 00:53:22,099
everybody always uses so the art he and

1089
00:53:18,889 --> 00:53:24,650
HT are ultimately scalars after they go

1090
00:53:22,099 --> 00:53:28,009
through the sigmoid but there are

1091
00:53:24,650 --> 00:53:29,349
Clyde element-wise is that correct -

1092
00:53:28,010 --> 00:53:31,819
mm-hmm

1093
00:53:29,349 --> 00:53:36,859
yeah although of course one for each

1094
00:53:31,818 --> 00:53:44,929
mini batch but yes the scaler yeah okay

1095
00:53:36,858 --> 00:53:47,119
great thanks and on the excellent colas

1096
00:53:44,929 --> 00:53:50,389
blog Chris Ellis blog there's an

1097
00:53:47,119 --> 00:53:52,849
understanding LS TM networks post which

1098
00:53:50,389 --> 00:53:55,190
you can read all about this in much more

1099
00:53:52,849 --> 00:53:57,200
detail if you're interested and also the

1100
00:53:55,190 --> 00:53:59,480
other one I was dealing from here is a

1101
00:53:57,199 --> 00:54:01,129
wild ml also have a good blog post on

1102
00:53:59,480 --> 00:54:02,599
this there's somebody wants to be

1103
00:54:01,130 --> 00:54:06,039
helpful feel free to put them in the

1104
00:54:02,599 --> 00:54:06,039
lesson wiki

1105
00:54:08,318 --> 00:54:15,650
okay so then putting it all together I'm

1106
00:54:13,068 --> 00:54:16,818
now going to replace my GI you with

1107
00:54:15,650 --> 00:54:18,380
Nellis TM I'm not going to bother

1108
00:54:16,818 --> 00:54:21,829
showing you the soul for this it's very

1109
00:54:18,380 --> 00:54:24,890
similar to GI u but the LS TM has one

1110
00:54:21,829 --> 00:54:27,109
more piece of state in it called the

1111
00:54:24,889 --> 00:54:29,750
sell state not just the hidden state so

1112
00:54:27,108 --> 00:54:31,788
if you do use an LS TM you're now inside

1113
00:54:29,750 --> 00:54:34,608
you're in it hidden have to return a

1114
00:54:31,789 --> 00:54:37,339
couple of matrices they're exactly the

1115
00:54:34,608 --> 00:54:40,130
same size as the hidden state but you

1116
00:54:37,338 --> 00:54:43,009
just have to return the tupple okay the

1117
00:54:40,130 --> 00:54:44,150
details don't matter too much but we can

1118
00:54:43,010 --> 00:54:48,650
talk about it during the week if you're

1119
00:54:44,150 --> 00:54:50,869
interested you know when you pass in you

1120
00:54:48,650 --> 00:54:53,088
still pass in self dot H still returns a

1121
00:54:50,869 --> 00:54:53,750
new value of H is to repackage it in the

1122
00:54:53,088 --> 00:54:55,429
usual way

1123
00:54:53,750 --> 00:54:58,219
so this code is identical to the code

1124
00:54:55,429 --> 00:55:02,058
before one thing I've done though is

1125
00:54:58,219 --> 00:55:05,659
I've had a drop out inside my air and in

1126
00:55:02,059 --> 00:55:07,130
which you can do with the height watch R

1127
00:55:05,659 --> 00:55:09,828
and n function so that's going to do

1128
00:55:07,130 --> 00:55:11,180
drop out after a time step and I've

1129
00:55:09,829 --> 00:55:13,250
doubled the size of my hidden layer

1130
00:55:11,179 --> 00:55:15,409
since I've now added point five drop out

1131
00:55:13,250 --> 00:55:19,489
and so my hope was that this would make

1132
00:55:15,409 --> 00:55:24,980
it be able to learn more but be more

1133
00:55:19,489 --> 00:55:29,358
resilient as it does so so then I wanted

1134
00:55:24,980 --> 00:55:32,858
to show you how to take advantage of a

1135
00:55:29,358 --> 00:55:35,838
little bit more fast AI magic without

1136
00:55:32,858 --> 00:55:39,819
using the layer class and so I'm going

1137
00:55:35,838 --> 00:55:44,049
to show you how to use Cobra

1138
00:55:39,820 --> 00:55:46,460
and specifically we're going to do s GDR

1139
00:55:44,050 --> 00:55:49,220
without without using the learner class

1140
00:55:46,460 --> 00:55:52,130
ok so to do that we create our model

1141
00:55:49,219 --> 00:55:54,769
again just a standard PI torch model ok

1142
00:55:52,130 --> 00:55:56,750
and this time rather than going

1143
00:55:54,769 --> 00:56:00,289
remember the usual pipe torch approach

1144
00:55:56,750 --> 00:56:01,639
is opt equals up to M naught atom and

1145
00:56:00,289 --> 00:56:03,650
you pass in the parameters and a

1146
00:56:01,639 --> 00:56:06,679
learning rate I'm not going to do that

1147
00:56:03,650 --> 00:56:12,309
I'm going to use the fast AI layer

1148
00:56:06,679 --> 00:56:16,608
optimizer class which takes my opt-in

1149
00:56:12,309 --> 00:56:20,679
class constructor from PI torch it takes

1150
00:56:16,608 --> 00:56:25,039
my model it takes my learning rate and

1151
00:56:20,679 --> 00:56:28,309
optionally takes weight decay ok and so

1152
00:56:25,039 --> 00:56:31,099
this class is tiny it doesn't do very

1153
00:56:28,309 --> 00:56:33,650
much at all the key reason it exists is

1154
00:56:31,099 --> 00:56:36,109
to do differential learning rates and

1155
00:56:33,650 --> 00:56:39,108
differential weight decay right but the

1156
00:56:36,108 --> 00:56:41,480
reason we need to use it is that all of

1157
00:56:39,108 --> 00:56:43,489
the mechanics inside fast AI assumes

1158
00:56:41,480 --> 00:56:46,250
that you have one of these right so if

1159
00:56:43,489 --> 00:56:49,549
you want to use like callbacks or SPDR

1160
00:56:46,250 --> 00:56:52,400
or whatever in code where you're not

1161
00:56:49,550 --> 00:56:54,859
using the learner class then you need to

1162
00:56:52,400 --> 00:56:56,930
use rather than saying you know opt

1163
00:56:54,858 --> 00:57:00,469
equals off to m dot atom and here's my

1164
00:56:56,929 --> 00:57:02,500
parameters you instead say lay optimizer

1165
00:57:00,469 --> 00:57:06,829
okay

1166
00:57:02,500 --> 00:57:08,719
so that gives us a layer optimizer

1167
00:57:06,829 --> 00:57:12,139
object and if you're interested

1168
00:57:08,719 --> 00:57:18,709
basically behind the scenes you can now

1169
00:57:12,139 --> 00:57:20,389
grab a dot opt property which actually

1170
00:57:18,710 --> 00:57:21,920
gives you the optimizer but you don't

1171
00:57:20,389 --> 00:57:23,269
have to worry about that itself but

1172
00:57:21,920 --> 00:57:25,809
that's basically what happens behind the

1173
00:57:23,269 --> 00:57:29,989
scenes the key thing we can now do is

1174
00:57:25,809 --> 00:57:34,039
that we can now when we call fit we can

1175
00:57:29,989 --> 00:57:36,829
pass in that optimizer and we can also

1176
00:57:34,039 --> 00:57:40,190
pass in some callbacks and specifically

1177
00:57:36,829 --> 00:57:42,679
we're going to use the cosine annealing

1178
00:57:40,190 --> 00:57:45,380
callback okay and so the cosine

1179
00:57:42,679 --> 00:57:48,169
annealing callback requires a layer

1180
00:57:45,380 --> 00:57:50,059
optimizer object right and so what this

1181
00:57:48,170 --> 00:57:51,920
is going to do is it's going to do

1182
00:57:50,059 --> 00:57:52,369
cosine annealing by changing the

1183
00:57:51,920 --> 00:57:57,650
learning

1184
00:57:52,369 --> 00:57:59,690
right inside this object okay so the

1185
00:57:57,650 --> 00:58:01,190
details aren't terribly important we can

1186
00:57:59,690 --> 00:58:02,900
talk about them on the forum it's really

1187
00:58:01,190 --> 00:58:05,028
the concept I wanted to get across here

1188
00:58:02,900 --> 00:58:06,079
right which is that now that we've done

1189
00:58:05,028 --> 00:58:08,329
this we can say all right

1190
00:58:06,079 --> 00:58:10,460
create a cosine and kneeling callback

1191
00:58:08,329 --> 00:58:14,930
which is going to update the learning

1192
00:58:10,460 --> 00:58:18,079
rates in this layer optimizer the length

1193
00:58:14,929 --> 00:58:19,548
of an epoch is equal to this here right

1194
00:58:18,079 --> 00:58:22,039
how many mini batches are there in an

1195
00:58:19,548 --> 00:58:24,318
epoch well it's whatever the length of

1196
00:58:22,039 --> 00:58:25,759
this data loader is okay so because it's

1197
00:58:24,318 --> 00:58:27,768
going to be it's going to be doing the

1198
00:58:25,759 --> 00:58:31,880
cosine annealing it needs to know how

1199
00:58:27,768 --> 00:58:33,588
often to reset okay and then you can

1200
00:58:31,880 --> 00:58:37,999
pass in the cycle more in the usual way

1201
00:58:33,588 --> 00:58:39,828
and then we can even save our model

1202
00:58:37,998 --> 00:58:42,129
automatically like you remember how

1203
00:58:39,829 --> 00:58:44,329
there was that cycle saved name

1204
00:58:42,130 --> 00:58:45,920
parameter that we can pass to learn not

1205
00:58:44,329 --> 00:58:48,499
fit this is what it does behind the

1206
00:58:45,920 --> 00:58:51,499
scenes behind the scenes it sets an on

1207
00:58:48,498 --> 00:58:53,480
cycle end callback and so here I have to

1208
00:58:51,498 --> 00:58:57,409
find that callback as being something

1209
00:58:53,480 --> 00:59:00,048
that saves my model okay so there's

1210
00:58:57,409 --> 00:59:02,598
quite a lot of cool stuff that you can

1211
00:59:00,048 --> 00:59:04,338
do with callbacks callbacks are

1212
00:59:02,599 --> 00:59:06,410
basically things where you can define

1213
00:59:04,338 --> 00:59:07,818
like at the start of training or at the

1214
00:59:06,409 --> 00:59:09,710
start of an epoch or at the side of a

1215
00:59:07,818 --> 00:59:10,969
batch or at the end of training or at

1216
00:59:09,710 --> 00:59:13,818
the end of an epoch or at the end of a

1217
00:59:10,969 --> 00:59:17,419
batch please call this club okay and so

1218
00:59:13,818 --> 00:59:20,409
we've written some for you including SGD

1219
00:59:17,420 --> 00:59:23,239
R which is the cosine annealing callback

1220
00:59:20,409 --> 00:59:26,088
and then so how I recently wrote a new

1221
00:59:23,239 --> 00:59:28,970
callback to implement the new approach

1222
00:59:26,088 --> 00:59:31,578
to decoupled weight decay we use

1223
00:59:28,969 --> 00:59:34,939
callbacks to draw those little graphs of

1224
00:59:31,579 --> 00:59:36,650
the loss over time so there's lots of

1225
00:59:34,940 --> 00:59:39,139
cool stuff you can do with callbacks so

1226
00:59:36,650 --> 00:59:43,400
in this case by passing in that callback

1227
00:59:39,139 --> 00:59:48,379
we're getting as JDR and that's able to

1228
00:59:43,400 --> 00:59:49,910
get us down to one point three one here

1229
00:59:48,380 --> 00:59:54,108
and then we can train a little bit more

1230
00:59:49,909 --> 00:59:58,808
and eventually get down to one point two

1231
00:59:54,108 --> 01:00:01,818
five and so we can now test that out and

1232
00:59:58,809 --> 01:00:05,090
so if we passed in a few characters of

1233
01:00:01,818 --> 01:00:08,949
text we get not surprisingly and a

1234
01:00:05,090 --> 01:00:12,769
after four thus let's do then 400 and

1235
01:00:08,949 --> 01:00:14,329
now we have our own Nietzsche so

1236
01:00:12,769 --> 01:00:17,539
Nietzsche tends to start his sections

1237
01:00:14,329 --> 01:00:20,420
with a number and a dot so 2 9 3 perhaps

1238
01:00:17,539 --> 01:00:22,159
that every life the values of blood have

1239
01:00:20,420 --> 01:00:24,050
intercourse when it senses there is

1240
01:00:22,159 --> 01:00:27,139
unscrupulous his very rights and still

1241
01:00:24,050 --> 01:00:29,630
impulse love ok so I mean it's slightly

1242
01:00:27,139 --> 01:00:33,440
less clear than Nietzsche normally but

1243
01:00:29,630 --> 01:00:36,410
it gets the tone right ok and it's

1244
01:00:33,440 --> 01:00:38,570
actually quite interesting like if to

1245
01:00:36,409 --> 01:00:40,579
play around with training these

1246
01:00:38,570 --> 01:00:44,120
character based language models to like

1247
01:00:40,579 --> 01:00:45,469
run this at different levels of loss to

1248
01:00:44,119 --> 01:00:49,159
get a sense of like what does it look

1249
01:00:45,469 --> 01:00:53,209
like like you really notice that this is

1250
01:00:49,159 --> 01:00:57,139
like 1.25 and like that's slightly worse

1251
01:00:53,210 --> 01:00:59,210
like 1.3 this looks like total junk you

1252
01:00:57,139 --> 01:01:01,940
know there's like punctuation in random

1253
01:00:59,210 --> 01:01:03,740
places and you know nothing makes sense

1254
01:01:01,940 --> 01:01:06,829
and like you start to realize that the

1255
01:01:03,739 --> 01:01:09,919
difference between you know Nietzsche

1256
01:01:06,829 --> 01:01:12,469
and random junk is not that far in kind

1257
01:01:09,920 --> 01:01:13,430
of language model terms and so if you

1258
01:01:12,469 --> 01:01:15,859
train this for a little bit longer

1259
01:01:13,429 --> 01:01:18,519
you'll suddenly find like oh it's it's

1260
01:01:15,860 --> 01:01:21,460
it's making more and more sense okay so

1261
01:01:18,519 --> 01:01:23,630
if you are playing around with NLP stuff

1262
01:01:21,460 --> 01:01:27,409
particularly generative stuff like this

1263
01:01:23,630 --> 01:01:29,780
and you're like there is also like kind

1264
01:01:27,409 --> 01:01:31,099
of okay but not great don't be

1265
01:01:29,780 --> 01:01:33,260
disheartened because that means you're

1266
01:01:31,099 --> 01:01:34,730
actually very very nearly there you know

1267
01:01:33,260 --> 01:01:36,800
the the difference between like

1268
01:01:34,730 --> 01:01:38,480
something which is starting to create

1269
01:01:36,800 --> 01:01:40,700
something which almost vaguely looks

1270
01:01:38,480 --> 01:01:42,320
English if you squint and something

1271
01:01:40,699 --> 01:01:45,500
that's actually a very good generation

1272
01:01:42,320 --> 01:01:49,519
it's it's not it's not far in most

1273
01:01:45,500 --> 01:01:50,900
motion tests okay great so let's take a

1274
01:01:49,519 --> 01:01:52,849
five-minute break we'll come back at

1275
01:01:50,900 --> 01:01:55,240
7:45 and we're going to go back to

1276
01:01:52,849 --> 01:01:55,239
computer vision

1277
01:01:57,610 --> 01:02:10,970
okay so now become full circle back to

1278
01:02:04,489 --> 01:02:18,319
vision so now we're looking at less than

1279
01:02:10,969 --> 01:02:20,689
seven so far ten that book you might

1280
01:02:18,320 --> 01:02:24,860
have heard of so far ten it's a really

1281
01:02:20,690 --> 01:02:26,420
well-known data set in academia and it's

1282
01:02:24,860 --> 01:02:30,200
partly it's well known it's actually

1283
01:02:26,420 --> 01:02:33,230
pretty old by you know computer vision

1284
01:02:30,199 --> 01:02:35,719
standards well before image net was

1285
01:02:33,230 --> 01:02:37,490
around there was sci-fi 10 you might

1286
01:02:35,719 --> 01:02:39,019
wonder why we're going to be looking at

1287
01:02:37,489 --> 01:02:44,059
such an old data set and actually I

1288
01:02:39,019 --> 01:02:47,059
think small data sets are much more

1289
01:02:44,059 --> 01:02:48,590
interesting than image net because like

1290
01:02:47,059 --> 01:02:51,289
most of the time you're likely to be

1291
01:02:48,590 --> 01:02:54,620
working with stuff with a small number

1292
01:02:51,289 --> 01:02:56,300
of thousands of images rather than one

1293
01:02:54,619 --> 01:02:57,259
and a half million images some of you

1294
01:02:56,300 --> 01:02:59,570
will work at one and a half million

1295
01:02:57,260 --> 01:03:01,460
images but most of you won't right so

1296
01:02:59,570 --> 01:03:02,870
learning how to use these kind of data

1297
01:03:01,460 --> 01:03:05,059
sets I think is much more interesting

1298
01:03:02,869 --> 01:03:07,039
often also a lot of the stuff we're

1299
01:03:05,059 --> 01:03:09,320
looking at like in medical imaging we're

1300
01:03:07,039 --> 01:03:11,300
looking at like the specific area where

1301
01:03:09,320 --> 01:03:14,809
there's a lung nodule you're probably

1302
01:03:11,300 --> 01:03:16,550
looking at like 32 by 32 pixels at most

1303
01:03:14,809 --> 01:03:19,639
as being the area where that lung nodule

1304
01:03:16,550 --> 01:03:20,900
actually exists right and so sci-fi 10

1305
01:03:19,639 --> 01:03:23,029
is small both in terms of it doesn't

1306
01:03:20,900 --> 01:03:25,250
have many images and the images are very

1307
01:03:23,030 --> 01:03:27,200
small and so therefore I think this is

1308
01:03:25,250 --> 01:03:30,110
like it's been in a lot of ways is much

1309
01:03:27,199 --> 01:03:31,549
more challenging then something like

1310
01:03:30,110 --> 01:03:34,640
image net and in some ways it's much

1311
01:03:31,550 --> 01:03:36,860
more interesting right and also most

1312
01:03:34,639 --> 01:03:38,719
importantly you can run stuff much more

1313
01:03:36,860 --> 01:03:41,240
quickly on earth so it's much better to

1314
01:03:38,719 --> 01:03:43,309
test out your algorithms with something

1315
01:03:41,239 --> 01:03:45,829
you can run quickly and they're still

1316
01:03:43,309 --> 01:03:47,920
challenging and so I hear a lot of

1317
01:03:45,829 --> 01:03:51,259
researchers complain about like how they

1318
01:03:47,920 --> 01:03:52,670
can't afford to study all the different

1319
01:03:51,260 --> 01:03:55,220
versions that their algorithm properly

1320
01:03:52,670 --> 01:03:57,139
because it's too expensive and they're

1321
01:03:55,219 --> 01:03:59,599
doing that on imagenet so like it's

1322
01:03:57,139 --> 01:04:02,929
literally a week of you know expensive

1323
01:03:59,599 --> 01:04:04,610
CP GPU work for every study they do and

1324
01:04:02,929 --> 01:04:06,289
like I don't understand why you would do

1325
01:04:04,610 --> 01:04:07,130
that kind of study on imagenet doesn't

1326
01:04:06,289 --> 01:04:10,199
make sense

1327
01:04:07,130 --> 01:04:13,890
yeah and so

1328
01:04:10,199 --> 01:04:15,059
this has been a particularly you know

1329
01:04:13,889 --> 01:04:17,069
there's been a particular a lot of kind

1330
01:04:15,059 --> 01:04:19,019
of debate about this this week because

1331
01:04:17,070 --> 01:04:22,260
I'm really interesting researcher named

1332
01:04:19,019 --> 01:04:24,750
Ali raha me nips this week gave a talk a

1333
01:04:22,260 --> 01:04:28,200
really great talk about kind of the need

1334
01:04:24,750 --> 01:04:30,059
for rigor in experiments in deep

1335
01:04:28,199 --> 01:04:31,980
learning and you know he felt like

1336
01:04:30,059 --> 01:04:34,019
there's a lack of rigor and I've talked

1337
01:04:31,980 --> 01:04:38,789
to him about it quite a bit since that

1338
01:04:34,019 --> 01:04:40,289
time and I'm not sure we yet quite

1339
01:04:38,789 --> 01:04:42,719
understand each other as to where we're

1340
01:04:40,289 --> 01:04:44,730
coming from but but we have very similar

1341
01:04:42,719 --> 01:04:48,000
kinds of concerns which is basically

1342
01:04:44,730 --> 01:04:50,309
people aren't doing carefully tuned

1343
01:04:48,000 --> 01:04:52,739
carefully thought about experiments but

1344
01:04:50,309 --> 01:04:54,630
instead they kind of throw lots of GPUs

1345
01:04:52,739 --> 01:04:57,199
or lots of data and consider that a day

1346
01:04:54,630 --> 01:04:59,670
and so this idea of like saying like

1347
01:04:57,199 --> 01:05:01,289
well you know it's my data statement

1348
01:04:59,670 --> 01:05:04,829
it's my algorithm meant to be good

1349
01:05:01,289 --> 01:05:07,529
that's moral imagers at small data sets

1350
01:05:04,829 --> 01:05:09,599
well if so let's study on sci-fi 10

1351
01:05:07,530 --> 01:05:12,150
revin studying it on imagenet and then

1352
01:05:09,599 --> 01:05:13,230
do more studies of different versions of

1353
01:05:12,150 --> 01:05:15,269
the algorithm and learning different

1354
01:05:13,230 --> 01:05:18,559
bits on and off understand which parts

1355
01:05:15,269 --> 01:05:20,969
are actually important and so forth

1356
01:05:18,559 --> 01:05:23,549
people also complain a lot about amnesty

1357
01:05:20,969 --> 01:05:25,019
which we've talked about looked at

1358
01:05:23,550 --> 01:05:26,670
before and I would say the same thing

1359
01:05:25,019 --> 01:05:27,869
about em this right which is like if

1360
01:05:26,670 --> 01:05:29,430
you're actually trying to understand

1361
01:05:27,869 --> 01:05:32,190
rich parts of your algorithm make a

1362
01:05:29,429 --> 01:05:34,259
difference and why using m mr that kind

1363
01:05:32,190 --> 01:05:36,420
of study is a very good idea and all

1364
01:05:34,260 --> 01:05:38,520
these people who complain about em NIST

1365
01:05:36,420 --> 01:05:40,110
I think they're just showing off they're

1366
01:05:38,519 --> 01:05:42,480
saying like oh I work at Google and I

1367
01:05:40,110 --> 01:05:44,760
have you know a part of TP use and I

1368
01:05:42,480 --> 01:05:48,300
have $100,000 a week of time just being

1369
01:05:44,760 --> 01:05:49,800
on it no worries but I don't know I

1370
01:05:48,300 --> 01:05:51,810
think that's all it is you know it's

1371
01:05:49,800 --> 01:05:55,680
just signalling rather than actually

1372
01:05:51,809 --> 01:05:58,079
academically rigorous okay so I'm so far

1373
01:05:55,679 --> 01:05:59,579
ten you can download from here and this

1374
01:05:58,079 --> 01:06:04,860
person is very kindly made it available

1375
01:05:59,579 --> 01:06:06,630
in image form if you google for size 510

1376
01:06:04,860 --> 01:06:09,329
you'll find us a much less convenient

1377
01:06:06,630 --> 01:06:10,500
form so please use this one it's already

1378
01:06:09,329 --> 01:06:13,139
in the exact form you need once you

1379
01:06:10,500 --> 01:06:19,199
download it you can use it in the usual

1380
01:06:13,139 --> 01:06:22,409
way so here's a list of the classes that

1381
01:06:19,199 --> 01:06:23,609
are there now you'll see here I've

1382
01:06:22,409 --> 01:06:26,429
created this thing called

1383
01:06:23,610 --> 01:06:31,849
that's normally when we've been using

1384
01:06:26,429 --> 01:06:34,710
pre trained models we have been seeing

1385
01:06:31,849 --> 01:06:37,069
transforms from model and that's

1386
01:06:34,710 --> 01:06:41,070
actually created the necessary

1387
01:06:37,070 --> 01:06:43,559
transforms to convert our data set into

1388
01:06:41,070 --> 01:06:46,200
a normalized data set based on the means

1389
01:06:43,559 --> 01:06:48,299
and standard deviations of each channel

1390
01:06:46,199 --> 01:06:50,669
in the original model that was trained

1391
01:06:48,300 --> 01:06:53,039
in our case though this time we got a

1392
01:06:50,670 --> 01:06:55,889
trainer model from scratch so we have no

1393
01:06:53,039 --> 01:06:59,070
such thing so we actually need to tell

1394
01:06:55,889 --> 01:07:02,400
it the mean and standard deviation of

1395
01:06:59,070 --> 01:07:04,230
our data to normalize it okay and so in

1396
01:07:02,400 --> 01:07:06,210
this case I haven't included the code

1397
01:07:04,230 --> 01:07:08,159
here to do it you should try and try

1398
01:07:06,210 --> 01:07:09,329
this yourself to confirm that you can do

1399
01:07:08,159 --> 01:07:11,969
this and understand where it comes from

1400
01:07:09,329 --> 01:07:14,429
but this is just the mean channel and

1401
01:07:11,969 --> 01:07:20,909
the standard deviation per channel of

1402
01:07:14,429 --> 01:07:24,169
all of the images alright so we're going

1403
01:07:20,909 --> 01:07:26,989
to try and create a model from scratch

1404
01:07:24,170 --> 01:07:30,480
and so the first thing we need is some

1405
01:07:26,989 --> 01:07:34,349
transformations so for so far 10 people

1406
01:07:30,480 --> 01:07:38,820
generally do data augmentation of simply

1407
01:07:34,349 --> 01:07:41,659
flipping randomly horizontally so here's

1408
01:07:38,820 --> 01:07:44,910
how we can create a specific list of

1409
01:07:41,659 --> 01:07:47,250
augmentations to use and then they also

1410
01:07:44,909 --> 01:07:49,619
tend to add a little bit of padding

1411
01:07:47,250 --> 01:07:53,190
black padding around the edge and then

1412
01:07:49,619 --> 01:07:55,319
randomly pick a 32 by 32 spot from

1413
01:07:53,190 --> 01:07:58,710
within that pattern image so if you add

1414
01:07:55,320 --> 01:08:01,680
the pad parameter to any of the fast a a

1415
01:07:58,710 --> 01:08:05,250
transform creators it'll it'll do that

1416
01:08:01,679 --> 01:08:09,269
for you okay and so in this case I'm

1417
01:08:05,250 --> 01:08:12,900
just going to add 4 pixels around each

1418
01:08:09,269 --> 01:08:14,639
size and so now that I've got my

1419
01:08:12,900 --> 01:08:18,659
transforms I can go ahead and create my

1420
01:08:14,639 --> 01:08:22,109
image classifier data from paths in the

1421
01:08:18,659 --> 01:08:24,630
usual way okay I'm going to use a batch

1422
01:08:22,109 --> 01:08:26,130
size of 256 because these are pretty

1423
01:08:24,630 --> 01:08:28,650
small so it's going to let me do a

1424
01:08:26,130 --> 01:08:31,560
little bit more at a time so here's what

1425
01:08:28,649 --> 01:08:33,779
the data looks like so for example

1426
01:08:31,560 --> 01:08:37,420
here's a boat and just to show you how

1427
01:08:33,779 --> 01:08:42,548
tough this is what's that

1428
01:08:37,420 --> 01:08:45,579
okay it is it's a not chicken prog-rock

1429
01:08:42,548 --> 01:08:47,109
so I guess it's this big thing whatever

1430
01:08:45,579 --> 01:08:51,009
the thing is called there's your frog

1431
01:08:47,109 --> 01:08:54,789
okay so so these are the kinds of things

1432
01:08:51,009 --> 01:08:57,969
that we want to look at so I'm going to

1433
01:08:54,789 --> 01:09:00,039
start out so our student Karen we saw

1434
01:08:57,969 --> 01:09:03,779
one of his posts earlier in this course

1435
01:09:00,039 --> 01:09:07,569
he he made this really cool log book

1436
01:09:03,779 --> 01:09:12,040
which shows how different optimizers

1437
01:09:07,569 --> 01:09:14,109
works there we go so Karen made this

1438
01:09:12,039 --> 01:09:16,929
really cool notebook I think it was

1439
01:09:14,109 --> 01:09:19,060
maybe last week in which he showed how

1440
01:09:16,929 --> 01:09:20,739
to create various different optimizers

1441
01:09:19,060 --> 01:09:22,900
from scratch so this is kind of like the

1442
01:09:20,738 --> 01:09:25,178
Excel thing I had but this is the Python

1443
01:09:22,899 --> 01:09:27,278
version of momentum and Adam and

1444
01:09:25,179 --> 01:09:29,649
Nesterov and Atta grad all written from

1445
01:09:27,279 --> 01:09:32,710
scratch it is very cool one of the nice

1446
01:09:29,649 --> 01:09:35,500
things he did was he showed a tiny

1447
01:09:32,710 --> 01:09:37,480
little general-purpose fully connected

1448
01:09:35,500 --> 01:09:39,399
Network generator so we're going to

1449
01:09:37,479 --> 01:09:42,250
start with his so so he called that

1450
01:09:39,399 --> 01:09:47,349
simple net so are we so here's a simple

1451
01:09:42,250 --> 01:09:49,600
class which has a list of fully

1452
01:09:47,350 --> 01:09:52,298
connected layers okay

1453
01:09:49,600 --> 01:09:54,789
whenever you create a list of layers in

1454
01:09:52,298 --> 01:09:57,039
pi torch you have to wrap it in an end

1455
01:09:54,789 --> 01:10:01,988
module list just to tailpipe torch to

1456
01:09:57,039 --> 01:10:04,238
like register these as attributes and so

1457
01:10:01,988 --> 01:10:05,619
then we just go ahead and flatten the

1458
01:10:04,238 --> 01:10:07,178
data that comes in because it's fully

1459
01:10:05,619 --> 01:10:12,550
connected layers and then go through

1460
01:10:07,179 --> 01:10:15,250
each layer and call that linear layer do

1461
01:10:12,550 --> 01:10:17,469
the value to it and at the end do a soft

1462
01:10:15,250 --> 01:10:21,819
mess okay so there's a really simple

1463
01:10:17,469 --> 01:10:23,829
approach and so we can now take that

1464
01:10:21,819 --> 01:10:26,198
model and now I'm going to show you how

1465
01:10:23,829 --> 01:10:28,359
to step up one level of the API higher

1466
01:10:26,198 --> 01:10:30,428
rather than calling the fit function

1467
01:10:28,359 --> 01:10:31,929
we're going to create a learn object but

1468
01:10:30,429 --> 01:10:35,260
we're going to create a learn object

1469
01:10:31,929 --> 01:10:37,149
from a custom model and so we can do

1470
01:10:35,260 --> 01:10:39,280
that by say we want a convolutional

1471
01:10:37,149 --> 01:10:42,639
learner we want to create it from a

1472
01:10:39,279 --> 01:10:45,819
model and from some data and the model

1473
01:10:42,640 --> 01:10:49,300
is this one so this is just a general

1474
01:10:45,819 --> 01:10:51,189
height watch model and this is a model

1475
01:10:49,300 --> 01:10:53,529
data object of the usual kind

1476
01:10:51,189 --> 01:10:55,779
and that will return a loaner so this is

1477
01:10:53,529 --> 01:10:57,159
a bit easier than what we just saw with

1478
01:10:55,779 --> 01:10:59,889
the RNN we don't have to fiddle around

1479
01:10:57,159 --> 01:11:01,630
with lair optimizers and cosine and

1480
01:10:59,890 --> 01:11:03,369
kneeling and callbacks and whatever this

1481
01:11:01,630 --> 01:11:06,310
is now a loaner that we can do all the

1482
01:11:03,369 --> 01:11:11,890
usual stuff with that we can do it with

1483
01:11:06,310 --> 01:11:14,440
any model that we created okay so if we

1484
01:11:11,890 --> 01:11:16,090
just go learn that'll go ahead and print

1485
01:11:14,439 --> 01:11:17,739
it out okay so you can see we've got

1486
01:11:16,090 --> 01:11:19,539
three thousand and seventy two features

1487
01:11:17,739 --> 01:11:22,630
coming in because you've got 32 by 32

1488
01:11:19,539 --> 01:11:24,640
pixels by three channels okay and then

1489
01:11:22,630 --> 01:11:26,980
we've got 40 features coming out of the

1490
01:11:24,640 --> 01:11:28,660
first layer that's going to go into the

1491
01:11:26,979 --> 01:11:31,389
second layer ten features coming out

1492
01:11:28,659 --> 01:11:35,859
because we've got the ten so far ten

1493
01:11:31,390 --> 01:11:38,590
categories okay you can call dot summary

1494
01:11:35,859 --> 01:11:42,460
to see that a little bit more detail we

1495
01:11:38,590 --> 01:11:44,949
can do LR find we can plot that and we

1496
01:11:42,460 --> 01:11:48,630
can then go fetch and we can use cycle

1497
01:11:44,949 --> 01:11:50,099
length and so forth okay so with a

1498
01:11:48,630 --> 01:11:52,750
simple

1499
01:11:50,100 --> 01:11:54,880
how many hidden layers do we have one

1500
01:11:52,750 --> 01:11:58,720
hidden layer right one getting layer one

1501
01:11:54,880 --> 01:12:00,940
output layer one hidden layer model with

1502
01:11:58,720 --> 01:12:05,820
and here we can see the number of

1503
01:12:00,939 --> 01:12:13,210
parameters we have is that over 120,000

1504
01:12:05,819 --> 01:12:15,969
okay we get a 47 percent accuracy so not

1505
01:12:13,210 --> 01:12:18,189
great all right so let's kind of try and

1506
01:12:15,970 --> 01:12:19,600
improve it right and so the goal here is

1507
01:12:18,189 --> 01:12:24,039
we're going to try and eventually

1508
01:12:19,600 --> 01:12:26,320
replicate the basic kind of architecture

1509
01:12:24,039 --> 01:12:27,519
of a resonator okay so that's where

1510
01:12:26,319 --> 01:12:30,759
we're going to try and get to hear it

1511
01:12:27,520 --> 01:12:32,920
specially built up to a resonant so the

1512
01:12:30,760 --> 01:12:34,750
first step is to replace our fully

1513
01:12:32,920 --> 01:12:44,409
connected model with a convolutional

1514
01:12:34,750 --> 01:12:47,529
model okay so to remind you so to remind

1515
01:12:44,409 --> 01:12:50,229
you a fully connected layer is simply

1516
01:12:47,529 --> 01:12:54,460
doing a dot product right so if we had

1517
01:12:50,229 --> 01:12:59,469
like all of these data points and all of

1518
01:12:54,460 --> 01:13:02,149
these weights right then we basically

1519
01:12:59,470 --> 01:13:03,800
do a sum product of all of those

1520
01:13:02,149 --> 01:13:06,129
together right in other words it's a

1521
01:13:03,800 --> 01:13:10,489
matrix multiply right then that's a

1522
01:13:06,130 --> 01:13:12,890
fully connected layer okay and so we

1523
01:13:10,489 --> 01:13:15,559
need the white matrix is going to take

1524
01:13:12,890 --> 01:13:17,240
contain an item for every every element

1525
01:13:15,560 --> 01:13:19,730
of the input for every element of the

1526
01:13:17,239 --> 01:13:26,420
output okay so that's why we have here a

1527
01:13:19,729 --> 01:13:28,129
pretty big weight matrix and so that's

1528
01:13:26,420 --> 01:13:31,520
why we had despite the fact that we have

1529
01:13:28,130 --> 01:13:33,829
such a crappy accuracy we have a lot of

1530
01:13:31,520 --> 01:13:34,490
parameters because in this very first

1531
01:13:33,829 --> 01:13:39,470
layer

1532
01:13:34,489 --> 01:13:41,569
we've got 3072 coming in and for T

1533
01:13:39,470 --> 01:13:44,600
coming out so that gives us three

1534
01:13:41,569 --> 01:13:46,609
thousand times forty parameters and so

1535
01:13:44,600 --> 01:13:48,170
we end up not using them very

1536
01:13:46,609 --> 01:13:50,029
efficiently because we're basically

1537
01:13:48,170 --> 01:13:52,039
saying every single pixel in the input

1538
01:13:50,029 --> 01:13:53,599
has a different weight and of course

1539
01:13:52,039 --> 01:13:56,269
what we really want to do is kind of

1540
01:13:53,600 --> 01:13:58,100
find groups of three by three pixels

1541
01:13:56,270 --> 01:14:00,350
that have particular patterns to them

1542
01:13:58,100 --> 01:14:10,820
okay and remember we call that a

1543
01:14:00,350 --> 01:14:14,780
convolution okay so a convolution looks

1544
01:14:10,819 --> 01:14:20,090
like so we have like little 3x3 section

1545
01:14:14,779 --> 01:14:22,609
of our image and a corresponding 3x3 set

1546
01:14:20,090 --> 01:14:25,130
of filters right or our filter with a

1547
01:14:22,609 --> 01:14:28,009
three by three kernel and we just do a

1548
01:14:25,130 --> 01:14:31,279
sum product of just that three by three

1549
01:14:28,010 --> 01:14:35,300
by that three by three okay and then we

1550
01:14:31,279 --> 01:14:37,219
do that for every single part of our

1551
01:14:35,300 --> 01:14:38,960
image right and so when we do that

1552
01:14:37,220 --> 01:14:42,470
across the whole image that's called a

1553
01:14:38,960 --> 01:14:45,649
convolution and remember in this case we

1554
01:14:42,470 --> 01:14:47,420
actually had multiple filters right so

1555
01:14:45,649 --> 01:14:50,000
the result of that convolution actually

1556
01:14:47,420 --> 01:14:53,029
had multiple it was a tensor with an

1557
01:14:50,000 --> 01:14:58,220
additional third dimension to it

1558
01:14:53,029 --> 01:15:01,009
effectively so let's take exactly the

1559
01:14:58,220 --> 01:15:04,010
same code that we had before but we're

1560
01:15:01,010 --> 01:15:09,020
going to replace n n dot linear with NN

1561
01:15:04,010 --> 01:15:11,360
com2 D okay now what I want to do in

1562
01:15:09,020 --> 01:15:13,120
this case though is each time I have a

1563
01:15:11,359 --> 01:15:17,019
layer I want to

1564
01:15:13,119 --> 01:15:20,579
the next layer smaller and so the way I

1565
01:15:17,020 --> 01:15:24,580
did that in my Excel example was I used

1566
01:15:20,579 --> 01:15:27,970
max Pauling right so max Pauling took

1567
01:15:24,579 --> 01:15:31,899
every 2x2 section and replaced it with

1568
01:15:27,970 --> 01:15:34,150
this maximum value right nowadays we

1569
01:15:31,899 --> 01:15:37,179
don't use that kind of max bowling much

1570
01:15:34,149 --> 01:15:39,460
at all instead nowadays what we tend to

1571
01:15:37,180 --> 01:15:40,360
do is do what's called a stride to

1572
01:15:39,460 --> 01:15:43,420
convolution

1573
01:15:40,359 --> 01:15:49,469
let's drag to convolution rather than

1574
01:15:43,420 --> 01:15:54,210
saying let's go through every single 3x3

1575
01:15:49,470 --> 01:15:57,100
it says let's go through every second

1576
01:15:54,210 --> 01:15:58,449
3x3 so rather than moving this three by

1577
01:15:57,100 --> 01:16:01,210
three one to the right

1578
01:15:58,449 --> 01:16:02,979
we move it two to the right and then

1579
01:16:01,210 --> 01:16:05,739
when we get to the end of the row rather

1580
01:16:02,979 --> 01:16:08,109
than moving one row down we move two

1581
01:16:05,739 --> 01:16:10,420
rows down okay so that's called a stride

1582
01:16:08,109 --> 01:16:13,389
to convolution and so it's tried to

1583
01:16:10,420 --> 01:16:15,640
convolution has the same kind of effect

1584
01:16:13,390 --> 01:16:18,480
as a max pooling which is you end up

1585
01:16:15,640 --> 01:16:21,579
having the resolution in each dimension

1586
01:16:18,479 --> 01:16:24,669
so we can ask for that by saying stroud

1587
01:16:21,579 --> 01:16:26,229
equals to okay we can say we wanted to

1588
01:16:24,670 --> 01:16:28,690
be three by three by saying kernel size

1589
01:16:26,229 --> 01:16:30,189
and then the first term parameters are

1590
01:16:28,689 --> 01:16:31,960
exactly the same as nn but linear

1591
01:16:30,189 --> 01:16:34,169
they're the number of features coming in

1592
01:16:31,960 --> 01:16:37,869
and the number of features coming out

1593
01:16:34,170 --> 01:16:42,279
okay so we create a multiple list of

1594
01:16:37,869 --> 01:16:44,319
those layers and then at the very end of

1595
01:16:42,279 --> 01:16:46,599
that so in this case I'm going to say

1596
01:16:44,319 --> 01:16:48,699
okay I've got three channels coming in

1597
01:16:46,600 --> 01:16:52,510
the first one layer will come out with

1598
01:16:48,699 --> 01:16:53,829
20 then a at 40 and then 80 so if we

1599
01:16:52,510 --> 01:16:56,949
look at the summary we're going to start

1600
01:16:53,829 --> 01:17:01,239
with a 32 by 32 we're going to spit out

1601
01:16:56,949 --> 01:17:06,399
of 15 by 15 and then a 7 by 7 and then a

1602
01:17:01,239 --> 01:17:08,920
3 by 3 right and so what do we do now to

1603
01:17:06,399 --> 01:17:12,339
get that down to a prediction of one of

1604
01:17:08,920 --> 01:17:15,220
10 classes what we do is we do something

1605
01:17:12,340 --> 01:17:17,260
called adaptive max pooling and this is

1606
01:17:15,220 --> 01:17:19,480
what is pretty standard now for

1607
01:17:17,260 --> 01:17:24,730
state-of-the-art algorithms is that the

1608
01:17:19,479 --> 01:17:26,409
very last layer we do a max pool but

1609
01:17:24,729 --> 01:17:30,129
rather than doing like a 2

1610
01:17:26,409 --> 01:17:31,899
go to next Paul we say like it doesn't

1611
01:17:30,130 --> 01:17:33,609
have you to bow to could have been 3x3

1612
01:17:31,899 --> 01:17:35,439
which is like replace every three by

1613
01:17:33,609 --> 01:17:38,619
three pixels with its maximum could have

1614
01:17:35,439 --> 01:17:41,199
been four by four adaptive backs Paul is

1615
01:17:38,619 --> 01:17:44,649
where you say I'm not going to tell you

1616
01:17:41,199 --> 01:17:46,869
how big an area to pull but instead I'm

1617
01:17:44,649 --> 01:17:51,210
going to tell you how big a resolution

1618
01:17:46,869 --> 01:17:54,539
to create right so if I said for example

1619
01:17:51,210 --> 01:17:59,109
I think my input here is like 28 by 28

1620
01:17:54,539 --> 01:18:01,539
right if I said do a 14 by 14 adaptive

1621
01:17:59,109 --> 01:18:03,639
max Paul that would be the same as a 2

1622
01:18:01,539 --> 01:18:06,060
by 2 max Paul because in other that's

1623
01:18:03,640 --> 01:18:10,390
saying please create a 14 by 14 output

1624
01:18:06,060 --> 01:18:12,760
if I said do a 2 by 2 adaptive max Paul

1625
01:18:10,390 --> 01:18:17,440
right then that would be the same as

1626
01:18:12,760 --> 01:18:19,210
saying do a 14 by 14 max Paul and so

1627
01:18:17,439 --> 01:18:22,419
what we pretty much always do in modern

1628
01:18:19,210 --> 01:18:28,750
cnn's is we make our per northmet layer

1629
01:18:22,420 --> 01:18:33,579
a 1 by 1 adaptive Max Paul so in other

1630
01:18:28,750 --> 01:18:39,100
words find the single largest cell and

1631
01:18:33,579 --> 01:18:43,059
use that as our new activation right and

1632
01:18:39,100 --> 01:18:47,500
so once we've got that we've now got a 1

1633
01:18:43,060 --> 01:18:50,350
by 1 tensor right we're actually 1 by 1

1634
01:18:47,500 --> 01:18:54,909
by number of features tensor so we can

1635
01:18:50,350 --> 01:18:57,970
then on top of that go view X dot view X

1636
01:18:54,909 --> 01:19:01,420
dot size comma minus 1 and actually

1637
01:18:57,970 --> 01:19:03,579
there are no other dimensions to this

1638
01:19:01,420 --> 01:19:07,359
basically right so this is going to

1639
01:19:03,579 --> 01:19:10,510
return a matrix of mini-batch by number

1640
01:19:07,359 --> 01:19:15,699
of features and so then we can feed that

1641
01:19:10,510 --> 01:19:18,130
into a linear layer with however many

1642
01:19:15,699 --> 01:19:20,409
classes we need right so you can see

1643
01:19:18,130 --> 01:19:22,119
here the last thing I pass in is how

1644
01:19:20,409 --> 01:19:23,859
many classes am I trying to predict and

1645
01:19:22,119 --> 01:19:26,170
that's what's going to be used to create

1646
01:19:23,859 --> 01:19:28,779
that last layer so it goes through every

1647
01:19:26,170 --> 01:19:33,690
convolutional layer does a convolution

1648
01:19:28,779 --> 01:19:36,550
does arel you does an adaptive max pool

1649
01:19:33,689 --> 01:19:39,729
this dot view just gets rid of those

1650
01:19:36,550 --> 01:19:42,190
trailing unit backsies

1651
01:19:39,729 --> 01:19:45,159
one comma one axis which is not

1652
01:19:42,189 --> 01:19:47,738
necessary that allows us to fit that

1653
01:19:45,159 --> 01:19:51,699
into our final linear layer that spits

1654
01:19:47,738 --> 01:19:54,729
out something of size C which here is

1655
01:19:51,699 --> 01:19:59,979
ten so you can now see how it works

1656
01:19:54,729 --> 01:20:04,439
it goes 32 to 15 to 7 by 7 to 3 by 3 the

1657
01:19:59,979 --> 01:20:07,119
adaptive next pull makes it 80 by 1 by 1

1658
01:20:04,439 --> 01:20:10,029
right and then our dot view makes it

1659
01:20:07,119 --> 01:20:12,579
just a mini batch size by 80 and then

1660
01:20:10,029 --> 01:20:15,090
finally a linear layer which makes it

1661
01:20:12,579 --> 01:20:19,059
from 80 to 10 which is what we wanted

1662
01:20:15,090 --> 01:20:21,310
okay so that's our like most basic you

1663
01:20:19,060 --> 01:20:23,620
would call this a fully convolutional

1664
01:20:21,310 --> 01:20:25,630
network so a fully convolutional network

1665
01:20:23,619 --> 01:20:30,359
is something where every layer is

1666
01:20:25,630 --> 01:20:30,359
convolutional except for the very last

1667
01:20:31,350 --> 01:20:39,489
so again we can now go Li dot find and

1668
01:20:35,770 --> 01:20:41,110
now in this case when I did ll find it

1669
01:20:39,488 --> 01:20:43,750
went through the entire data set and

1670
01:20:41,109 --> 01:20:46,659
we're still getting better and so in

1671
01:20:43,750 --> 01:20:48,460
other words even a the default final

1672
01:20:46,659 --> 01:20:50,649
learning rate rises 10 and even at that

1673
01:20:48,460 --> 01:20:52,149
point it was still like pretty much

1674
01:20:50,649 --> 01:20:54,429
getting better so you can always

1675
01:20:52,149 --> 01:20:57,009
override the final learning rate by

1676
01:20:54,430 --> 01:20:58,390
saying n del R equals that and that will

1677
01:20:57,010 --> 01:21:01,360
get it just to get it to try morphine's

1678
01:20:58,390 --> 01:21:06,930
okay and so here is the learning rate

1679
01:21:01,359 --> 01:21:09,670
finder and so I picked 10 to the minus 1

1680
01:21:06,930 --> 01:21:10,510
trained that for a while and that's

1681
01:21:09,670 --> 01:21:12,310
looking pretty good

1682
01:21:10,510 --> 01:21:14,260
so I try to put the cycle length of 1

1683
01:21:12,310 --> 01:21:18,150
and it's starting to flatten out at

1684
01:21:14,260 --> 01:21:20,770
about 60% right so you can see here the

1685
01:21:18,149 --> 01:21:24,639
number of elements the number of

1686
01:21:20,770 --> 01:21:28,510
parameters I have here are 500 7000

1687
01:21:24,640 --> 01:21:30,340
28,000 about 30,000 right so I have

1688
01:21:28,510 --> 01:21:34,289
about a quarter of the number of routers

1689
01:21:30,340 --> 01:21:38,860
that my accuracy has gone up from 47% to

1690
01:21:34,289 --> 01:21:43,180
60% right and the time per epoch here is

1691
01:21:38,859 --> 01:21:44,529
under 30 seconds and here also so the

1692
01:21:43,180 --> 01:21:46,239
time period box about the same and

1693
01:21:44,529 --> 01:21:49,059
that's not surprising because when you

1694
01:21:46,238 --> 01:21:51,339
use small simple architectures most of

1695
01:21:49,060 --> 01:21:53,140
the time is the memory transfer the

1696
01:21:51,340 --> 01:21:58,720
actual time during the compute is

1697
01:21:53,140 --> 01:22:01,630
is trivial okay so I'm going to refactor

1698
01:21:58,720 --> 01:22:05,440
this slightly because I want to try and

1699
01:22:01,630 --> 01:22:07,779
put less stuff inside my forward and so

1700
01:22:05,439 --> 01:22:09,429
calling RAL you every time you know it

1701
01:22:07,779 --> 01:22:13,569
doesn't seem ideal so I'm going to

1702
01:22:09,430 --> 01:22:15,850
create a new class called conf lair okay

1703
01:22:13,569 --> 01:22:18,279
and the conf lair class is going to

1704
01:22:15,850 --> 01:22:21,160
contain a convolution with a kernel size

1705
01:22:18,279 --> 01:22:22,239
of three and a stride of two one thing

1706
01:22:21,159 --> 01:22:22,960
I'm going to do now is I'm going to add

1707
01:22:22,239 --> 01:22:25,869
padding

1708
01:22:22,960 --> 01:22:30,460
did you notice here the first layer went

1709
01:22:25,869 --> 01:22:33,340
from 32 by 32 to 15 by 15 not 16 by 16

1710
01:22:30,460 --> 01:22:40,170
and the reason for that is that at the

1711
01:22:33,340 --> 01:22:40,170
very edge of your convolution right here

1712
01:22:42,060 --> 01:22:46,990
see how this first convolution like

1713
01:22:45,189 --> 01:22:49,899
there isn't a convolution where the

1714
01:22:46,989 --> 01:22:52,119
middle is the top left point right

1715
01:22:49,899 --> 01:22:55,420
because there's like nothing outside it

1716
01:22:52,119 --> 01:22:57,849
where else if we had put a row of zeros

1717
01:22:55,420 --> 01:23:01,029
at the top and a row of zeros at the

1718
01:22:57,850 --> 01:23:03,150
edge of each column we now could go all

1719
01:23:01,029 --> 01:23:09,069
the way to the edge alright so pad

1720
01:23:03,149 --> 01:23:10,839
equals 1 adds that little layer of zeros

1721
01:23:09,069 --> 01:23:12,789
around the edge for us ok

1722
01:23:10,840 --> 01:23:16,360
and so this way we're going to make sure

1723
01:23:12,789 --> 01:23:18,760
that we go 32 by 32 to 16 by 16 to 8 by

1724
01:23:16,359 --> 01:23:20,199
8 it doesn't matter too much when you've

1725
01:23:18,760 --> 01:23:23,680
got these bigger layers but by the time

1726
01:23:20,199 --> 01:23:25,420
you get down to like say 4 by 4 you

1727
01:23:23,680 --> 01:23:27,700
really don't want to throw away a whole

1728
01:23:25,420 --> 01:23:32,079
piece right so padding becomes important

1729
01:23:27,699 --> 01:23:33,939
so by refactoring it to put this with

1730
01:23:32,079 --> 01:23:35,859
its defaults here and then in the

1731
01:23:33,939 --> 01:23:39,039
forward I put the value in here as well

1732
01:23:35,859 --> 01:23:41,139
it makes by confident you know a little

1733
01:23:39,039 --> 01:23:42,880
bit smaller and you know more to the

1734
01:23:41,140 --> 01:23:44,530
point it's going to be easier for me to

1735
01:23:42,880 --> 01:23:46,180
make sure that everything is correct in

1736
01:23:44,529 --> 01:23:47,380
the future by always using this common

1737
01:23:46,180 --> 01:23:50,079
player class ok

1738
01:23:47,380 --> 01:23:52,720
so now you know not only how to create

1739
01:23:50,079 --> 01:23:55,479
your own neural network model but how to

1740
01:23:52,720 --> 01:23:58,750
create your own neural network layer so

1741
01:23:55,479 --> 01:24:00,549
here now I can use conf layer right and

1742
01:23:58,750 --> 01:24:03,430
this is such a cool thing about pi torch

1743
01:24:00,550 --> 01:24:04,960
is a layer definition and a neural

1744
01:24:03,430 --> 01:24:06,850
network definition are literally

1745
01:24:04,960 --> 01:24:09,699
identical okay they both

1746
01:24:06,850 --> 01:24:12,010
have a constructor and a forward and so

1747
01:24:09,699 --> 01:24:13,359
anytime you've got the lair you can use

1748
01:24:12,010 --> 01:24:15,239
it as a neural net anytime you have a

1749
01:24:13,359 --> 01:24:18,250
neural net you can use it as a lair

1750
01:24:15,239 --> 01:24:21,130
okay so this is now the exact same thing

1751
01:24:18,250 --> 01:24:23,979
as we had before one difference is I now

1752
01:24:21,130 --> 01:24:24,970
have padding okay and another thing just

1753
01:24:23,979 --> 01:24:29,349
to show you you can do things

1754
01:24:24,970 --> 01:24:33,100
differently back here my max pool I did

1755
01:24:29,350 --> 01:24:35,920
as as an object likely I use the class n

1756
01:24:33,100 --> 01:24:37,530
n dot adaptive max pool and I stuck it

1757
01:24:35,920 --> 01:24:40,029
in this attribute and then I called it

1758
01:24:37,529 --> 01:24:44,050
but this actually doesn't have any state

1759
01:24:40,029 --> 01:24:45,309
there's no weights inside max pooling so

1760
01:24:44,050 --> 01:24:47,760
I can actually do it with a little bit

1761
01:24:45,310 --> 01:24:50,440
less code by calling it as a function

1762
01:24:47,760 --> 01:24:52,329
right so everything that you can do as a

1763
01:24:50,439 --> 01:24:54,699
class you can also do as a function it's

1764
01:24:52,329 --> 01:24:59,079
inside this capital F which is n n dot

1765
01:24:54,699 --> 01:25:03,569
functional okay so this should be a tiny

1766
01:24:59,079 --> 01:25:06,130
bit better because this time I've got

1767
01:25:03,569 --> 01:25:09,460
the padding I didn't trade it for as

1768
01:25:06,130 --> 01:25:14,949
long to actually check so let's skip

1769
01:25:09,460 --> 01:25:19,659
over that all right so one issue here is

1770
01:25:14,949 --> 01:25:21,149
that in the end this is having I when I

1771
01:25:19,659 --> 01:25:26,019
tried to add more layers

1772
01:25:21,149 --> 01:25:27,489
I had travel training it okay and the

1773
01:25:26,020 --> 01:25:30,100
reason I was having trouble training it

1774
01:25:27,489 --> 01:25:31,750
is it was you know if I used larger

1775
01:25:30,100 --> 01:25:34,000
learning rates it would go off to NI N

1776
01:25:31,750 --> 01:25:35,470
and if I use smaller learning rates that

1777
01:25:34,000 --> 01:25:37,319
are kind of takes forever and doesn't

1778
01:25:35,470 --> 01:25:41,199
really have a chance to explore properly

1779
01:25:37,319 --> 01:25:43,449
so it wasn't resilient so to make my

1780
01:25:41,199 --> 01:25:44,909
model more resilient I'm going to use

1781
01:25:43,449 --> 01:25:47,559
something called batch normalization

1782
01:25:44,909 --> 01:25:51,699
which literally everybody calls bachelor

1783
01:25:47,560 --> 01:25:54,940
and bachelorettes a couple of years old

1784
01:25:51,699 --> 01:25:57,039
now and it's been pretty transformative

1785
01:25:54,939 --> 01:26:00,219
since it came along because it suddenly

1786
01:25:57,039 --> 01:26:02,470
makes it really easy to train deeper

1787
01:26:00,220 --> 01:26:04,630
networks alright so the network I'm

1788
01:26:02,470 --> 01:26:07,000
going to create is going to have more

1789
01:26:04,630 --> 01:26:09,190
layers right I've got one two three four

1790
01:26:07,000 --> 01:26:11,560
five convolutional layers plus a fully

1791
01:26:09,189 --> 01:26:13,149
connected layer right so like back in

1792
01:26:11,560 --> 01:26:15,460
the old days that would be considered a

1793
01:26:13,149 --> 01:26:17,739
pretty deep network and we considered

1794
01:26:15,460 --> 01:26:20,529
pretty hard to train nowadays super

1795
01:26:17,739 --> 01:26:23,409
simple thanks to vaginal

1796
01:26:20,529 --> 01:26:25,840
now to use batch norm you can just write

1797
01:26:23,409 --> 01:26:28,300
in end on that to learn about it we're

1798
01:26:25,840 --> 01:26:32,500
going to write it from scratch okay so

1799
01:26:28,300 --> 01:26:35,980
the basic idea of batch norm is that

1800
01:26:32,500 --> 01:26:37,989
we've got some vector of activations

1801
01:26:35,979 --> 01:26:39,519
anytime I draw a vector of activations

1802
01:26:37,989 --> 01:26:41,260
obviously I mean you can repeat it for

1803
01:26:39,520 --> 01:26:43,000
the mini batch so I pretend it's a mini

1804
01:26:41,260 --> 01:26:46,409
batch with one so we've got some veteran

1805
01:26:43,000 --> 01:26:49,560
activations and it's coming into some

1806
01:26:46,409 --> 01:26:52,359
layer right so so probably some

1807
01:26:49,560 --> 01:26:55,680
convolutional matrix multiplication and

1808
01:26:52,359 --> 01:26:58,630
then something comes out the other side

1809
01:26:55,680 --> 01:27:01,990
so imagine this this is just a matrix

1810
01:26:58,630 --> 01:27:11,199
multiply which was like I don't know say

1811
01:27:01,989 --> 01:27:12,670
it was a identity matrix right then

1812
01:27:11,199 --> 01:27:14,710
every time I'd multiply it by that

1813
01:27:12,670 --> 01:27:16,060
across lots and lots of layers my

1814
01:27:14,710 --> 01:27:17,619
activations are not getting bigger

1815
01:27:16,060 --> 01:27:20,380
they're not getting smaller they're not

1816
01:27:17,619 --> 01:27:22,390
changing at all okay that's all fine

1817
01:27:20,380 --> 01:27:28,000
right but imagine if it was actually

1818
01:27:22,390 --> 01:27:29,680
like 2 2 2 right and so if every one of

1819
01:27:28,000 --> 01:27:32,739
my weight matrices or filters was like

1820
01:27:29,680 --> 01:27:35,860
that then my activations are doubling

1821
01:27:32,739 --> 01:27:40,359
each time right and so suddenly I've got

1822
01:27:35,859 --> 01:27:41,710
as exponential growth and that in deep

1823
01:27:40,359 --> 01:27:44,619
models that's going to be a disaster

1824
01:27:41,710 --> 01:27:47,890
right because my gradients are exploding

1825
01:27:44,619 --> 01:27:50,670
at an exponential rate and so the

1826
01:27:47,890 --> 01:27:54,310
challenge you have is that it's it's

1827
01:27:50,670 --> 01:27:57,609
very unlikely unless you try carefully

1828
01:27:54,310 --> 01:28:00,190
to deal with it that your matrices your

1829
01:27:57,609 --> 01:28:03,369
weight matrices on average are not going

1830
01:28:00,189 --> 01:28:04,929
to cause your activations to keep

1831
01:28:03,369 --> 01:28:06,460
getting smaller and smaller or keep

1832
01:28:04,930 --> 01:28:08,890
getting bigger and bigger right you have

1833
01:28:06,460 --> 01:28:11,439
to kind of carefully control things to

1834
01:28:08,890 --> 01:28:13,000
make sure that they stay you know at a

1835
01:28:11,439 --> 01:28:16,960
reasonable size you want to you know

1836
01:28:13,000 --> 01:28:20,350
keep them at a reasonable scale so we

1837
01:28:16,960 --> 01:28:23,670
start things off with zero mean standard

1838
01:28:20,350 --> 01:28:27,340
deviation one by normalizing the inputs

1839
01:28:23,670 --> 01:28:30,810
really like to do is to normalize every

1840
01:28:27,340 --> 01:28:34,409
layer not just the inputs all right and

1841
01:28:30,810 --> 01:28:37,469
so okay fine

1842
01:28:34,408 --> 01:28:39,988
let's do that right so here I've created

1843
01:28:37,469 --> 01:28:42,179
a BN layer which is exactly like my

1844
01:28:39,988 --> 01:28:45,658
Kampf layer it's got my common 2d with

1845
01:28:42,179 --> 01:28:48,750
my stride my padding right I do my

1846
01:28:45,658 --> 01:28:53,429
condom I value right and then I

1847
01:28:48,750 --> 01:28:56,609
calculate the mean of each channel or of

1848
01:28:53,429 --> 01:28:58,859
each filter and the standard deviation

1849
01:28:56,609 --> 01:29:02,099
of each channel or each filter and then

1850
01:28:58,859 --> 01:29:06,869
I subtract the means and divide by the

1851
01:29:02,100 --> 01:29:09,150
standard deviations right so now I don't

1852
01:29:06,869 --> 01:29:11,158
actually need to normalize my input at

1853
01:29:09,149 --> 01:29:11,908
all because it's actually going to do it

1854
01:29:11,158 --> 01:29:15,569
automatically

1855
01:29:11,908 --> 01:29:17,729
right it's normalizing it per channel or

1856
01:29:15,569 --> 01:29:22,738
and for later layers its normalizing it

1857
01:29:17,729 --> 01:29:24,269
per filter so it turns out that's not

1858
01:29:22,738 --> 01:29:29,759
enough right

1859
01:29:24,270 --> 01:29:33,150
because SGD is bloody-minded right and

1860
01:29:29,760 --> 01:29:35,880
so if sgt decided that it what's the

1861
01:29:33,149 --> 01:29:38,158
weight matrix to be you know like so

1862
01:29:35,880 --> 01:29:41,090
where that matrix is something which is

1863
01:29:38,158 --> 01:29:45,359
going to you know increase the values

1864
01:29:41,090 --> 01:29:47,250
overall repeatedly then trying to divide

1865
01:29:45,359 --> 01:29:49,589
it by the subtract domains and divide by

1866
01:29:47,250 --> 01:29:51,149
the standard deviations just means the

1867
01:29:49,590 --> 01:29:52,409
next mini-batch it's going to try and do

1868
01:29:51,149 --> 01:29:54,539
it again and they were try and do it

1869
01:29:52,408 --> 01:29:57,388
again it'll try and do it again so it

1870
01:29:54,539 --> 01:29:58,649
turns out that this actually doesn't

1871
01:29:57,389 --> 01:30:01,529
help like it literally does nothing

1872
01:29:58,649 --> 01:30:03,029
because SGD is just going to go ahead

1873
01:30:01,529 --> 01:30:08,550
and undo it

1874
01:30:03,029 --> 01:30:13,948
the next mini batch so what we do is we

1875
01:30:08,550 --> 01:30:19,289
create a new multiplier for each channel

1876
01:30:13,948 --> 01:30:21,658
and a new added value for each Channel

1877
01:30:19,289 --> 01:30:23,760
literally just and we just start them

1878
01:30:21,658 --> 01:30:25,948
out as the addition and addition is just

1879
01:30:23,760 --> 01:30:28,560
a bunch of zeros so for the first layer

1880
01:30:25,948 --> 01:30:31,408
three zeros and the multiplier for the

1881
01:30:28,560 --> 01:30:33,510
first layer is just three ones okay so

1882
01:30:31,408 --> 01:30:36,479
number of filters for the first layer is

1883
01:30:33,510 --> 01:30:39,840
just three and so we then like basically

1884
01:30:36,479 --> 01:30:42,419
undo exactly what we just did or

1885
01:30:39,840 --> 01:30:44,789
potentially we undo them right so by

1886
01:30:42,420 --> 01:30:47,289
saying this is an addenda parameter that

1887
01:30:44,789 --> 01:30:50,439
tells PI torch you're allowed

1888
01:30:47,289 --> 01:30:52,180
to learn these as weights right so

1889
01:30:50,439 --> 01:30:54,239
initially it says okay so check the

1890
01:30:52,180 --> 01:30:59,079
means divided by the standard deviations

1891
01:30:54,239 --> 01:31:01,239
multiplied by one add on zero

1892
01:30:59,079 --> 01:31:05,500
okay that's fine nothing much happened

1893
01:31:01,239 --> 01:31:08,439
there but what it turns out is that now

1894
01:31:05,500 --> 01:31:11,500
rather than like if it wants to kind of

1895
01:31:08,439 --> 01:31:13,629
scale the layer up it doesn't have to

1896
01:31:11,500 --> 01:31:17,399
scale up every single value in the

1897
01:31:13,630 --> 01:31:21,579
matrix it can just scale up this single

1898
01:31:17,399 --> 01:31:23,949
trio of numbers self dot M if it wants

1899
01:31:21,579 --> 01:31:26,019
to shift it all Apple down a bit doesn't

1900
01:31:23,949 --> 01:31:28,779
have to shift the entire weight matrix

1901
01:31:26,020 --> 01:31:34,660
they can just shift this trio of numbers

1902
01:31:28,779 --> 01:31:36,399
self dot a so I will say this I'm at

1903
01:31:34,659 --> 01:31:38,559
this talk I mentioned at nips Alley

1904
01:31:36,399 --> 01:31:40,599
Rahim ease talk about rigor he actually

1905
01:31:38,560 --> 01:31:44,100
pointed to this paper this batch norm

1906
01:31:40,600 --> 01:31:48,960
paper as being a particularly useful

1907
01:31:44,100 --> 01:31:52,590
particularly interesting paper where a

1908
01:31:48,960 --> 01:31:56,649
lot of people don't necessarily we quite

1909
01:31:52,590 --> 01:31:58,779
quite know why it was right and so if

1910
01:31:56,649 --> 01:32:00,789
you're thinking like okay subtracting

1911
01:31:58,779 --> 01:32:05,409
out the means and then adding some

1912
01:32:00,789 --> 01:32:09,569
learned weights of exactly the same rank

1913
01:32:05,409 --> 01:32:11,800
and size sounds like a weird thing to do

1914
01:32:09,569 --> 01:32:15,159
there are a lot of people that feel the

1915
01:32:11,800 --> 01:32:17,380
same way right so at the moment I think

1916
01:32:15,159 --> 01:32:21,789
the best is I can say like intuitively

1917
01:32:17,380 --> 01:32:24,489
is what's going on here is that we're

1918
01:32:21,789 --> 01:32:28,000
normalizing the data and then we're

1919
01:32:24,489 --> 01:32:31,779
saying you can then shift it and scale

1920
01:32:28,000 --> 01:32:34,270
it using far fewer parameters than would

1921
01:32:31,779 --> 01:32:36,219
have been necessary if I was asking you

1922
01:32:34,270 --> 01:32:39,070
to actually shift and scale the entire

1923
01:32:36,220 --> 01:32:42,039
set of convolutional filters right

1924
01:32:39,069 --> 01:32:46,289
that's the kind of basic intuition more

1925
01:32:42,039 --> 01:32:50,739
importantly in practice what this does

1926
01:32:46,289 --> 01:32:52,750
is it adds is it basically allows us to

1927
01:32:50,739 --> 01:32:54,670
increase our learning rates and it

1928
01:32:52,750 --> 01:32:58,239
increases the resilience of training and

1929
01:32:54,670 --> 01:33:00,139
allows us to add more layers so once I

1930
01:32:58,239 --> 01:33:05,539
added

1931
01:33:00,139 --> 01:33:08,690
a PN layer rather than a common flower I

1932
01:33:05,539 --> 01:33:10,609
found I was able to add more layers to

1933
01:33:08,689 --> 01:33:17,599
my model and it still trained

1934
01:33:10,609 --> 01:33:20,329
effectively generally are we worried

1935
01:33:17,600 --> 01:33:24,619
about anything that maybe we are divided

1936
01:33:20,329 --> 01:33:28,130
by something very small or anything like

1937
01:33:24,618 --> 01:33:29,349
that once we do this probably I think in

1938
01:33:28,130 --> 01:33:33,079
the pie chart

1939
01:33:29,350 --> 01:33:35,989
version it would probably be divided by

1940
01:33:33,079 --> 01:33:42,260
itself dudes plus Epsilon or something

1941
01:33:35,988 --> 01:33:43,849
yeah this worked fine for me but yeah

1942
01:33:42,260 --> 01:33:45,440
that is definitely something to think

1943
01:33:43,850 --> 01:33:52,789
about if you were trying to make this

1944
01:33:45,439 --> 01:33:55,638
more reliable I mentioned poplar so the

1945
01:33:52,789 --> 01:33:57,800
self dot m and self dot a getting it's

1946
01:33:55,639 --> 01:34:00,529
getting updated through back propagation

1947
01:33:57,800 --> 01:34:02,600
as well yeah so by putting like saying

1948
01:34:00,529 --> 01:34:05,149
it's an N n dot parameter that's how we

1949
01:34:02,600 --> 01:34:10,130
flag to pi torch to learn it through

1950
01:34:05,149 --> 01:34:11,809
that probe exactly right the other

1951
01:34:10,130 --> 01:34:16,219
interesting thing it turns out the batch

1952
01:34:11,810 --> 01:34:18,530
norm does is it regularizes in other

1953
01:34:16,219 --> 01:34:21,170
words you can often decrease or remove

1954
01:34:18,529 --> 01:34:23,868
drop out or decrease or remove weight

1955
01:34:21,170 --> 01:34:27,819
okay when you use batch normal and the

1956
01:34:23,868 --> 01:34:30,349
reason why is if you think about it each

1957
01:34:27,819 --> 01:34:32,420
mini batch is going to have a different

1958
01:34:30,350 --> 01:34:34,760
mean and a different standard deviation

1959
01:34:32,420 --> 01:34:37,310
to the previous mini batch so these

1960
01:34:34,760 --> 01:34:39,860
things keep changing and because they

1961
01:34:37,310 --> 01:34:41,900
keep changing it's kind of changing the

1962
01:34:39,859 --> 01:34:44,000
meaning of the filters in this subtle

1963
01:34:41,899 --> 01:34:46,399
way and so it's adding a regularization

1964
01:34:44,000 --> 01:34:50,000
effect because it's noise that when you

1965
01:34:46,399 --> 01:34:52,819
add noise of any kind it regularizes

1966
01:34:50,000 --> 01:34:55,939
your model all right I'm actually

1967
01:34:52,819 --> 01:34:59,049
cheating a little bit here in the real

1968
01:34:55,939 --> 01:35:01,569
version of batch norm you don't just use

1969
01:34:59,050 --> 01:35:04,400
this batches mean and standard deviation

1970
01:35:01,569 --> 01:35:07,279
but instead you take an exponentially

1971
01:35:04,399 --> 01:35:08,469
weighted moving average standard

1972
01:35:07,279 --> 01:35:10,750
deviation and

1973
01:35:08,469 --> 01:35:12,279
and so if you wanted to exercise to try

1974
01:35:10,750 --> 01:35:15,100
a during the week that would be a good

1975
01:35:12,279 --> 01:35:16,599
thing to try but I will point out

1976
01:35:15,100 --> 01:35:23,079
something very important here which is

1977
01:35:16,600 --> 01:35:25,949
if self-training when we are doing our

1978
01:35:23,079 --> 01:35:28,300
training loop this will be true when

1979
01:35:25,948 --> 01:35:30,549
it's being applied to the training set

1980
01:35:28,300 --> 01:35:33,340
and it will be false when it's being

1981
01:35:30,550 --> 01:35:35,050
applied to the validation set and this

1982
01:35:33,340 --> 01:35:36,789
is really important because when you're

1983
01:35:35,050 --> 01:35:38,380
going through the validation set you do

1984
01:35:36,789 --> 01:35:42,010
not want to be changing the meaning of

1985
01:35:38,380 --> 01:35:45,010
the model okay so this is this really

1986
01:35:42,010 --> 01:35:48,310
important idea is that there are some

1987
01:35:45,010 --> 01:35:53,409
types of layer that are actually

1988
01:35:48,310 --> 01:35:55,300
sensitive to what the mode of the of the

1989
01:35:53,409 --> 01:35:58,059
network is whether it's in training mode

1990
01:35:55,300 --> 01:36:00,550
or as plight which calls it evaluation

1991
01:35:58,060 --> 01:36:03,550
mode or we might say a test mode right

1992
01:36:00,550 --> 01:36:06,250
and actually we actually had a bug a

1993
01:36:03,550 --> 01:36:08,440
couple of weeks ago when we did our mini

1994
01:36:06,250 --> 01:36:11,319
net for movie lens the collaborative

1995
01:36:08,439 --> 01:36:14,469
filtering we actually had F dot dropout

1996
01:36:11,319 --> 01:36:18,849
in our forward pass without protecting

1997
01:36:14,469 --> 01:36:20,619
it with a F self training F dot dropout

1998
01:36:18,850 --> 01:36:23,620
as a result of which we were actually

1999
01:36:20,619 --> 01:36:25,029
doing dropout in the validation piece as

2000
01:36:23,619 --> 01:36:26,979
well as the training piece which

2001
01:36:25,029 --> 01:36:28,960
obviously isn't what you want okay

2002
01:36:26,979 --> 01:36:32,619
so I've actually gone back and fixed

2003
01:36:28,960 --> 01:36:36,189
this by changing it to using n n dot

2004
01:36:32,619 --> 01:36:38,198
dropout and n n dot dropout has already

2005
01:36:36,189 --> 01:36:40,539
been written for us to check whether

2006
01:36:38,198 --> 01:36:43,210
it's being used in training mode or not

2007
01:36:40,539 --> 01:36:47,649
that or alternatively I could have added

2008
01:36:43,210 --> 01:36:50,948
a if self dot training before I use the

2009
01:36:47,649 --> 01:36:53,049
dropout yeah okay so it's important to

2010
01:36:50,948 --> 01:36:54,609
think about that you know any and the

2011
01:36:53,050 --> 01:36:57,640
main the main true or pretty much the

2012
01:36:54,609 --> 01:37:02,409
only two built-in two pi torch where

2013
01:36:57,640 --> 01:37:05,770
this happens is dropout and and so

2014
01:37:02,409 --> 01:37:07,630
interestingly this is also a key

2015
01:37:05,770 --> 01:37:12,880
difference in fast AI which no other

2016
01:37:07,630 --> 01:37:16,510
library does is that these means and

2017
01:37:12,880 --> 01:37:20,380
standard deviations get updated in

2018
01:37:16,510 --> 01:37:22,300
training mode in every other library as

2019
01:37:20,380 --> 01:37:24,130
soon as you basically say I'm

2020
01:37:22,300 --> 01:37:27,400
training regardless even of whether that

2021
01:37:24,130 --> 01:37:28,900
layer is set to trainable or not and it

2022
01:37:27,399 --> 01:37:30,939
turns out that with a pre trained

2023
01:37:28,899 --> 01:37:33,399
network that's a terrible idea

2024
01:37:30,939 --> 01:37:35,138
if you have a pre trained network for

2025
01:37:33,399 --> 01:37:37,299
specific values of those means and

2026
01:37:35,139 --> 01:37:39,340
standard deviations in batch norm if you

2027
01:37:37,300 --> 01:37:42,010
change them it changes the meaning of

2028
01:37:39,340 --> 01:37:45,520
those pre trained layers right and so in

2029
01:37:42,010 --> 01:37:47,710
fast AI always by default it won't touch

2030
01:37:45,520 --> 01:37:51,099
those means and standard deviations if

2031
01:37:47,710 --> 01:37:54,489
your layer is frozen okay as soon as you

2032
01:37:51,099 --> 01:37:59,679
I'm freezing it'll start updating them

2033
01:37:54,488 --> 01:38:02,558
unless you've set won't be and freeze

2034
01:37:59,679 --> 01:38:05,170
true if you set learned up being freeze

2035
01:38:02,559 --> 01:38:07,150
true it says never touch these met means

2036
01:38:05,170 --> 01:38:12,099
and standard deviations and you know

2037
01:38:07,149 --> 01:38:13,899
I've found in practice that that often

2038
01:38:12,099 --> 01:38:16,328
seems to work a lot better for

2039
01:38:13,899 --> 01:38:17,769
pre-trained models particularly if

2040
01:38:16,328 --> 01:38:19,689
you're working with data that's quite

2041
01:38:17,770 --> 01:38:33,760
similar to what the pre trained model

2042
01:38:19,689 --> 01:38:35,439
was trained with you know as you look

2043
01:38:33,760 --> 01:38:38,889
like I did a lot of work did you say

2044
01:38:35,439 --> 01:38:40,629
sorry like quite a lot of code here well

2045
01:38:38,889 --> 01:38:42,010
you're doing more work than you would

2046
01:38:40,630 --> 01:38:44,260
normally do essentially you're

2047
01:38:42,010 --> 01:38:46,559
calculating all these aggregates as you

2048
01:38:44,260 --> 01:38:49,480
go through each each each layer yes

2049
01:38:46,559 --> 01:38:52,719
wouldn't this mean you're training like

2050
01:38:49,479 --> 01:38:54,519
your epoch time like now this is like

2051
01:38:52,719 --> 01:38:58,389
super fast like if you think about what

2052
01:38:54,520 --> 01:39:01,540
a cone has to do a cones has to go

2053
01:38:58,389 --> 01:39:03,699
through every 3x3 you know with a stride

2054
01:39:01,539 --> 01:39:06,609
and do this multiplication and then

2055
01:39:03,698 --> 01:39:10,149
addition like that is a lot more work

2056
01:39:06,609 --> 01:39:12,308
than simply calculating the per channel

2057
01:39:10,149 --> 01:39:14,859
mean so this is so and that's a little

2058
01:39:12,309 --> 01:39:21,400
bit of time but it's it's less time

2059
01:39:14,859 --> 01:39:25,319
intensive than the convolution would it

2060
01:39:21,399 --> 01:39:26,589
be like right after like decomposition

2061
01:39:25,319 --> 01:39:28,929
yeah

2062
01:39:26,590 --> 01:39:32,078
we'll talk about that in a moment so at

2063
01:39:28,929 --> 01:39:34,840
the moment we have it after the rally

2064
01:39:32,078 --> 01:39:35,208
and in the original batch norm paper I

2065
01:39:34,840 --> 01:39:41,748
will

2066
01:39:35,208 --> 01:39:43,938
that's where they put it so this this

2067
01:39:41,748 --> 01:39:48,800
idea of something called an ablation

2068
01:39:43,939 --> 01:39:52,010
study and an ablation study is something

2069
01:39:48,800 --> 01:39:55,729
where you basically try kind of turning

2070
01:39:52,010 --> 01:39:57,800
on and off different pieces of your

2071
01:39:55,729 --> 01:39:59,840
model to see like which bits make which

2072
01:39:57,800 --> 01:40:01,880
impacts and one of the things that

2073
01:39:59,840 --> 01:40:04,239
wasn't done in the original batch norm

2074
01:40:01,880 --> 01:40:06,979
paper was any kind of really effective

2075
01:40:04,238 --> 01:40:08,208
ablation study and one of the things

2076
01:40:06,979 --> 01:40:10,039
therefore that was missing was this

2077
01:40:08,208 --> 01:40:12,380
question which you just asked which is

2078
01:40:10,038 --> 01:40:13,488
like where do you put the vaginal before

2079
01:40:12,380 --> 01:40:16,849
the early year after the earlier

2080
01:40:13,488 --> 01:40:18,978
whatever and so since that time you know

2081
01:40:16,849 --> 01:40:20,510
that oversight has caused a lot of

2082
01:40:18,979 --> 01:40:22,789
problems because it turned out the

2083
01:40:20,510 --> 01:40:25,849
original paper didn't actually put it in

2084
01:40:22,788 --> 01:40:27,198
the best spot and so then other people

2085
01:40:25,849 --> 01:40:29,150
since then have now figured that out

2086
01:40:27,198 --> 01:40:31,098
and they're like every time I show

2087
01:40:29,149 --> 01:40:33,049
people code where it's actually in the

2088
01:40:31,099 --> 01:40:35,538
spot that turns out to be better people

2089
01:40:33,050 --> 01:40:37,369
always say your bedrooms in the wrong

2090
01:40:35,538 --> 01:40:38,538
spot and I have to go back and say no I

2091
01:40:37,368 --> 01:40:39,889
know that's what the paper said what

2092
01:40:38,538 --> 01:40:42,708
they're doing now that's what I thought

2093
01:40:39,889 --> 01:40:44,449
and so it's kind of causes confusion so

2094
01:40:42,708 --> 01:40:46,688
there's there's been a lot of question

2095
01:40:44,448 --> 01:40:46,688
about that

2096
01:40:46,988 --> 01:40:53,299
so a little bit of a higher-level

2097
01:40:49,488 --> 01:40:59,419
question so we started out with cipher

2098
01:40:53,300 --> 01:41:02,748
data yes it's the basic reasoning that

2099
01:40:59,420 --> 01:41:07,279
you use a smaller data set to quickly

2100
01:41:02,748 --> 01:41:11,599
train a new model and then you take it

2101
01:41:07,279 --> 01:41:14,448
the same model and you're using much

2102
01:41:11,599 --> 01:41:17,529
much much bigger data set to get a

2103
01:41:14,448 --> 01:41:21,078
higher accuracy level is that the basic

2104
01:41:17,529 --> 01:41:23,569
maybe so if you want to you know if you

2105
01:41:21,078 --> 01:41:27,708
had a large data set or if you were like

2106
01:41:23,569 --> 01:41:29,538
interested in the question of like how

2107
01:41:27,708 --> 01:41:31,969
good is this technique on a large data

2108
01:41:29,538 --> 01:41:33,198
set then yes what you just said would be

2109
01:41:31,969 --> 01:41:36,498
what I would do I would do lots of

2110
01:41:33,198 --> 01:41:39,529
testing on a small data set which I had

2111
01:41:36,498 --> 01:41:41,538
already discovered had the same kinds of

2112
01:41:39,529 --> 01:41:43,219
properties as my larger data set and

2113
01:41:41,538 --> 01:41:45,228
therefore my conclusions would likely

2114
01:41:43,219 --> 01:41:45,559
carry forward and then our test them at

2115
01:41:45,229 --> 01:41:48,980
the end

2116
01:41:45,559 --> 01:41:53,239
having said that personally I matched

2117
01:41:48,979 --> 01:41:55,569
be more interested in actually studying

2118
01:41:53,238 --> 01:41:59,029
small datasets for their own sake

2119
01:41:55,569 --> 01:42:01,488
because I find most people I speak to in

2120
01:41:59,029 --> 01:42:03,738
the real world don't have a million

2121
01:42:01,488 --> 01:42:05,179
images they have you know somewhere

2122
01:42:03,738 --> 01:42:06,979
between about two thousand and twenty

2123
01:42:05,180 --> 01:42:10,989
thousand images seems to be much more

2124
01:42:06,979 --> 01:42:14,689
common so I'm very you know very

2125
01:42:10,988 --> 01:42:16,929
interested in having fewer rows because

2126
01:42:14,689 --> 01:42:19,219
I think it's more valuable in factors

2127
01:42:16,930 --> 01:42:21,560
I'm also pretty interested in small

2128
01:42:19,220 --> 01:42:24,230
images not just for the rest you

2129
01:42:21,560 --> 01:42:26,570
mentioned which is it allows me to test

2130
01:42:24,229 --> 01:42:29,179
things out more quickly but also as I

2131
01:42:26,569 --> 01:42:30,979
mentioned before often a small part of

2132
01:42:29,180 --> 01:42:32,600
an image actually turns out to be what

2133
01:42:30,979 --> 01:42:38,149
you're interested in that's certainly

2134
01:42:32,600 --> 01:42:41,270
true in in medicine I have two questions

2135
01:42:38,149 --> 01:42:43,549
the first is on what you mentioned in

2136
01:42:41,270 --> 01:42:44,900
terms of small datasets particular

2137
01:42:43,550 --> 01:42:47,029
middle medical imaging you've you've

2138
01:42:44,899 --> 01:42:48,439
heard of I guess is it vicarious to

2139
01:42:47,029 --> 01:42:50,238
start up in the specialization and

2140
01:42:48,439 --> 01:42:53,238
one-shot learning so your opinions on

2141
01:42:50,238 --> 01:42:57,349
that and then this second being this is

2142
01:42:53,238 --> 01:42:58,879
related to I guess Ali's talk at nips so

2143
01:42:57,350 --> 01:43:01,220
it was I don't say its controversial but

2144
01:42:58,880 --> 01:43:01,760
like young laocoön there was like a

2145
01:43:01,220 --> 01:43:04,039
really

2146
01:43:01,760 --> 01:43:05,630
I guess controversial thread attacking

2147
01:43:04,039 --> 01:43:08,390
you in terms of what you're talking

2148
01:43:05,630 --> 01:43:11,960
about as a baseline of theory just not

2149
01:43:08,390 --> 01:43:13,369
keeping up with practice and so I mean I

2150
01:43:11,960 --> 01:43:15,470
guess I was siding with the on where's

2151
01:43:13,369 --> 01:43:17,800
all he actually he tweeted at me quite a

2152
01:43:15,470 --> 01:43:22,280
bit trying to defend like he wasn't

2153
01:43:17,800 --> 01:43:24,739
attacking yawn at all but in fact he was

2154
01:43:22,279 --> 01:43:26,359
you know trying to support him but I

2155
01:43:24,738 --> 01:43:29,839
just kind of feel like a lot of theory

2156
01:43:26,359 --> 01:43:31,069
as as you go is just sort of at it they

2157
01:43:29,840 --> 01:43:33,560
even it's hard to keep up whether then

2158
01:43:31,069 --> 01:43:35,630
you know no archive from on Draco party

2159
01:43:33,560 --> 01:43:37,160
to keep up but if the theory isn't

2160
01:43:35,630 --> 01:43:38,420
keeping up but industry is the one

2161
01:43:37,159 --> 01:43:40,760
that's actually sitting in the standard

2162
01:43:38,420 --> 01:43:43,220
then doesn't that mean that you know

2163
01:43:40,760 --> 01:43:44,539
people who are actual practitioners are

2164
01:43:43,220 --> 01:43:45,890
the ones like young lacunae are

2165
01:43:44,539 --> 01:43:47,840
publishing the theory that are keeping

2166
01:43:45,890 --> 01:43:49,520
up to date or is like academic research

2167
01:43:47,840 --> 01:43:50,989
institutions are actually behind so I

2168
01:43:49,520 --> 01:43:52,670
don't have any comments on the vicarious

2169
01:43:50,988 --> 01:43:54,919
papers because I haven't read them I'm

2170
01:43:52,670 --> 01:43:59,060
not aware of any of them have as

2171
01:43:54,920 --> 01:44:01,279
actually showing you know better results

2172
01:43:59,060 --> 01:44:02,539
than the papers but I think they've come

2173
01:44:01,279 --> 01:44:05,809
a long way in the last twelve

2174
01:44:02,539 --> 01:44:06,979
so that might be wrong yeah yeah I

2175
01:44:05,810 --> 01:44:08,270
viewed the discussion between yarn

2176
01:44:06,979 --> 01:44:09,739
lacunae and a lyric Jimmy is very

2177
01:44:08,270 --> 01:44:11,210
interesting because they're both smart

2178
01:44:09,739 --> 01:44:16,969
people who have interesting things to

2179
01:44:11,210 --> 01:44:19,489
say unfortunately a lot of people talk

2180
01:44:16,970 --> 01:44:21,560
Ally's talk as meaning something which

2181
01:44:19,489 --> 01:44:23,750
he says it didn't mean and when I

2182
01:44:21,560 --> 01:44:25,370
listened to his talk I'm not sure he

2183
01:44:23,750 --> 01:44:27,079
didn't actually made it at the time but

2184
01:44:25,369 --> 01:44:29,630
he clearly doesn't mean it now which is

2185
01:44:27,079 --> 01:44:31,970
he's he's now said many times he didn't

2186
01:44:29,630 --> 01:44:33,760
he was not talking about theory he was

2187
01:44:31,970 --> 01:44:35,720
not saying we need more theory at all

2188
01:44:33,760 --> 01:44:39,800
actually he thinks we need more

2189
01:44:35,720 --> 01:44:41,539
experiments and so specifically he's

2190
01:44:39,800 --> 01:44:43,550
he's also now saying he wished he hadn't

2191
01:44:41,539 --> 01:44:46,250
used the word rigour which I also wish

2192
01:44:43,550 --> 01:44:48,020
because rigour is it's kind of

2193
01:44:46,250 --> 01:44:50,779
meaningless and everybody can kind of

2194
01:44:48,020 --> 01:44:55,790
say when he says rigor he means the

2195
01:44:50,779 --> 01:44:57,229
specific thing I study you know so a

2196
01:44:55,789 --> 01:44:59,090
lots of people have kind of taken his

2197
01:44:57,229 --> 01:45:01,519
talk as being like oh yes this proves

2198
01:44:59,090 --> 01:45:04,369
that nobody else should work in neural

2199
01:45:01,520 --> 01:45:08,510
networks unless they are experts at the

2200
01:45:04,369 --> 01:45:09,890
one thing I'm an expert in so yeah so

2201
01:45:08,510 --> 01:45:11,300
I'm going to catch up with him and talk

2202
01:45:09,890 --> 01:45:12,410
about more about this in January and

2203
01:45:11,300 --> 01:45:16,820
hopefully we'll pick up some more stuff

2204
01:45:12,409 --> 01:45:18,619
out together but basically what would we

2205
01:45:16,819 --> 01:45:22,460
can clearly agree on and I think you and

2206
01:45:18,619 --> 01:45:25,579
also agrees on is careful experiments

2207
01:45:22,460 --> 01:45:27,710
are important just doing things on

2208
01:45:25,579 --> 01:45:30,289
massive amounts of data using massive

2209
01:45:27,710 --> 01:45:32,659
amounts of TP use or GP users not

2210
01:45:30,289 --> 01:45:35,300
interesting of itself and we should

2211
01:45:32,659 --> 01:45:36,649
instead try to design experiments that

2212
01:45:35,300 --> 01:45:42,820
give us the maximum amount of insight

2213
01:45:36,649 --> 01:45:46,479
into what's going on so Jeremy is it a

2214
01:45:42,819 --> 01:45:51,649
good statement to say something like so

2215
01:45:46,479 --> 01:45:54,319
drop out and bash norm are very

2216
01:45:51,649 --> 01:45:57,649
different things drop out is the

2217
01:45:54,319 --> 01:45:59,689
realization technique bash norm has

2218
01:45:57,649 --> 01:46:02,539
maybe some realization effect but it's

2219
01:45:59,689 --> 01:46:06,739
actually just about convergence of the

2220
01:46:02,539 --> 01:46:10,100
optimization method yeah yeah and and I

2221
01:46:06,739 --> 01:46:12,880
would further say like I can't see any

2222
01:46:10,100 --> 01:46:16,219
reason not to use pattern or

2223
01:46:12,880 --> 01:46:18,199
there are versions of batch norm that in

2224
01:46:16,219 --> 01:46:22,279
certain situations turned out not to

2225
01:46:18,198 --> 01:46:24,529
work so well that people have figured

2226
01:46:22,279 --> 01:46:26,539
out ways around that for nearly every

2227
01:46:24,529 --> 01:46:29,269
one of those situations now so I would

2228
01:46:26,539 --> 01:46:32,988
always seek to find a way to use batch

2229
01:46:29,270 --> 01:46:33,650
norm it may be a little harder in our

2230
01:46:32,988 --> 01:46:37,339
own ends

2231
01:46:33,649 --> 01:46:39,289
at least but even there there are ways

2232
01:46:37,340 --> 01:46:41,779
of doing batch norm in our attenders as

2233
01:46:39,289 --> 01:46:44,448
well so you know try try and always use

2234
01:46:41,779 --> 01:46:47,539
batch norm on every layer if you can and

2235
01:46:44,448 --> 01:46:51,439
the question that somebody asked is does

2236
01:46:47,539 --> 01:46:57,039
it mean I have to I can stop normalize

2237
01:46:51,439 --> 01:46:57,039
in my data yeah yeah it does

2238
01:46:57,670 --> 01:47:06,199
although do it anyway because it's not

2239
01:47:03,679 --> 01:47:09,529
at all hard to do it and at least that

2240
01:47:06,198 --> 01:47:11,000
way the people using your data I don't

2241
01:47:09,529 --> 01:47:15,559
know they kind of know how you've

2242
01:47:11,000 --> 01:47:17,469
normalized it and particularly with

2243
01:47:15,560 --> 01:47:19,850
these issues around a lot of libraries

2244
01:47:17,469 --> 01:47:23,090
in my opinion at least warm not my

2245
01:47:19,850 --> 01:47:25,010
opinion my experiments don't deal with

2246
01:47:23,090 --> 01:47:27,920
batch norm correctly for pre-trained

2247
01:47:25,010 --> 01:47:31,489
models just remember that when somebody

2248
01:47:27,920 --> 01:47:32,869
starts retraining those averages and

2249
01:47:31,488 --> 01:47:35,269
stuff are going to change for your data

2250
01:47:32,869 --> 01:47:36,920
set and so if your new data set has very

2251
01:47:35,270 --> 01:47:40,730
different input averages it could really

2252
01:47:36,920 --> 01:47:43,520
cause a lot of problems so so yeah I

2253
01:47:40,729 --> 01:47:46,158
went through a period where I actually

2254
01:47:43,520 --> 01:47:49,219
stopped normalizing my data and you know

2255
01:47:46,158 --> 01:47:55,158
things kind of worked but it's probably

2256
01:47:49,219 --> 01:47:58,670
not worth it okay so so the rest of this

2257
01:47:55,158 --> 01:48:02,269
is identical right all I've done is I've

2258
01:47:58,670 --> 01:48:04,460
changed conf layer to BN layer but I've

2259
01:48:02,270 --> 01:48:06,080
done one more thing which is I'm kind of

2260
01:48:04,460 --> 01:48:08,119
trying to get closer and closer to

2261
01:48:06,079 --> 01:48:11,050
modern approaches which I've added a

2262
01:48:08,119 --> 01:48:14,840
single convolutional layer at the start

2263
01:48:11,050 --> 01:48:19,310
with a bigger kernel size and a stride

2264
01:48:14,840 --> 01:48:23,360
of one why have I done that so the basic

2265
01:48:19,310 --> 01:48:26,090
idea is that I want my first layer to

2266
01:48:23,359 --> 01:48:28,309
kind of have a richer input right so

2267
01:48:26,090 --> 01:48:29,630
before my first layer had an input of

2268
01:48:28,310 --> 01:48:32,510
just three because there's just three

2269
01:48:29,630 --> 01:48:41,810
channels right but if I start with my

2270
01:48:32,510 --> 01:48:46,159
image right and and I kind of take a

2271
01:48:41,810 --> 01:48:49,520
bigger area few different color I kind

2272
01:48:46,158 --> 01:48:51,829
of take a bigger area right and I do a

2273
01:48:49,520 --> 01:48:57,590
convolution using that bigger area in

2274
01:48:51,829 --> 01:49:00,859
this case I'm doing five by five right

2275
01:48:57,590 --> 01:49:04,730
then that kind of allows me to try and

2276
01:49:00,859 --> 01:49:08,259
find more interesting richer features in

2277
01:49:04,729 --> 01:49:11,178
that 5x5 area and so then I spin out a

2278
01:49:08,260 --> 01:49:14,389
bigger output this case I spit out a

2279
01:49:11,179 --> 01:49:17,480
filter size good about ten five by five

2280
01:49:14,389 --> 01:49:19,429
filters and so the idea is like pretty

2281
01:49:17,479 --> 01:49:21,529
much every state of the art

2282
01:49:19,429 --> 01:49:25,219
convolutional architecture now starts

2283
01:49:21,529 --> 01:49:27,559
out with a single con flare with like a

2284
01:49:25,219 --> 01:49:31,399
five by five or seven by seven or

2285
01:49:27,560 --> 01:49:36,199
sometimes even like 11 by 11 convolution

2286
01:49:31,399 --> 01:49:39,379
with like quite a few filters you know

2287
01:49:36,198 --> 01:49:42,259
something like you know thirty two

2288
01:49:39,380 --> 01:49:44,599
filters coming out and it's just just a

2289
01:49:42,260 --> 01:49:46,760
way of kind of trying to and like

2290
01:49:44,599 --> 01:49:50,119
because I use the straight of one and

2291
01:49:46,760 --> 01:49:50,690
the padding of kernel size minus one

2292
01:49:50,118 --> 01:49:52,549
over two

2293
01:49:50,689 --> 01:49:55,098
it means that my outputs going to be

2294
01:49:52,550 --> 01:49:57,980
exactly the same size as my input but

2295
01:49:55,099 --> 01:50:00,380
just got more filters now this is just a

2296
01:49:57,979 --> 01:50:03,500
good way of trying to create a richer

2297
01:50:00,380 --> 01:50:04,550
starting point for my sequence of

2298
01:50:03,500 --> 01:50:08,270
convolutional layers

2299
01:50:04,550 --> 01:50:10,550
okay so that's the basic theory of why

2300
01:50:08,270 --> 01:50:12,770
I've added this single convolution which

2301
01:50:10,550 --> 01:50:14,690
I just do once at the start and then I

2302
01:50:12,770 --> 01:50:17,179
just go through all my layers and then I

2303
01:50:14,689 --> 01:50:20,089
do my adaptive max pooling and my final

2304
01:50:17,179 --> 01:50:24,079
classifier okay so it's a minor tweak

2305
01:50:20,090 --> 01:50:28,069
but it helps right and so you'll see now

2306
01:50:24,079 --> 01:50:34,039
I kind of can go for a Moodle I have 60%

2307
01:50:28,069 --> 01:50:36,649
and after a couple is 45% now after a

2308
01:50:34,039 --> 01:50:38,510
couple that's 57% and after a few more

2309
01:50:36,649 --> 01:50:40,189
I'm out for 68% okay

2310
01:50:38,510 --> 01:50:42,680
so you can see it's you know the the

2311
01:50:40,189 --> 01:50:45,169
batch norm and you know tiny bit the

2312
01:50:42,680 --> 01:50:46,310
conveyor at the start it's helping now

2313
01:50:45,170 --> 01:50:49,430
what's more you can see this is still

2314
01:50:46,310 --> 01:50:52,880
increasing right so that's looking

2315
01:50:49,430 --> 01:50:54,860
pretty encouraging okay so given that

2316
01:50:52,880 --> 01:51:01,430
this is looking pretty good an obvious

2317
01:50:54,859 --> 01:51:03,289
thing to try might be to see is to try

2318
01:51:01,430 --> 01:51:07,220
increasing the depth of the model and

2319
01:51:03,289 --> 01:51:10,579
now I can't just add more of most dried

2320
01:51:07,220 --> 01:51:13,070
to layers because remember how it half

2321
01:51:10,579 --> 01:51:14,960
the size of the image each time I'm

2322
01:51:13,069 --> 01:51:18,380
basically down to two by two at the end

2323
01:51:14,960 --> 01:51:21,649
right so I can't add much more so what I

2324
01:51:18,380 --> 01:51:23,390
did instead was I said okay here's my

2325
01:51:21,649 --> 01:51:26,089
original layers so you must trade two

2326
01:51:23,390 --> 01:51:29,180
layers for everyone also create a

2327
01:51:26,090 --> 01:51:31,579
straight one layer so astrayed one layer

2328
01:51:29,180 --> 01:51:36,500
doesn't change the size and so now I'm

2329
01:51:31,579 --> 01:51:38,960
saying zip my stride two layers and my

2330
01:51:36,500 --> 01:51:41,029
stride one layers together and so first

2331
01:51:38,960 --> 01:51:43,340
of all do the straight too and then do

2332
01:51:41,029 --> 01:51:50,988
the straight one so this is now actually

2333
01:51:43,340 --> 01:51:53,960
twice as deep okay so this is so this is

2334
01:51:50,988 --> 01:51:56,809
now twice as deep but I end up with the

2335
01:51:53,960 --> 01:52:00,890
exact same you know two by two that I

2336
01:51:56,810 --> 01:52:03,860
had before and so if I try this you know

2337
01:52:00,890 --> 01:52:06,050
here after one two three four epochs is

2338
01:52:03,859 --> 01:52:08,839
at sixty-five percent after one two

2339
01:52:06,050 --> 01:52:13,460
three epochs I'm still at 65% it hasn't

2340
01:52:08,840 --> 01:52:17,140
helped right and so the reason it hasn't

2341
01:52:13,460 --> 01:52:20,270
helped is I'm now too deep

2342
01:52:17,140 --> 01:52:23,150
even for batch norm two handlers now my

2343
01:52:20,270 --> 01:52:30,860
depth is now one two three four five

2344
01:52:23,149 --> 01:52:34,519
times two is ten eleven kampf 112 okay

2345
01:52:30,859 --> 01:52:36,799
so 12 layers deep it's possible to train

2346
01:52:34,520 --> 01:52:38,120
a standard confident 12 layers deep but

2347
01:52:36,800 --> 01:52:40,100
it starts to get difficult to do it

2348
01:52:38,119 --> 01:52:42,500
properly right and it certainly doesn't

2349
01:52:40,100 --> 01:52:45,410
seem to be really helping much if at all

2350
01:52:42,500 --> 01:52:49,189
so that's where I'm instead going to

2351
01:52:45,409 --> 01:52:51,920
replace this with a ResNet all right so

2352
01:52:49,189 --> 01:52:55,159
a rest net is our final stage

2353
01:52:51,920 --> 01:52:58,130
what a resin it does is I'm going to

2354
01:52:55,159 --> 01:53:00,710
replace our BN layer right I'm going to

2355
01:52:58,130 --> 01:53:04,699
inherit from BN layer and replace our

2356
01:53:00,710 --> 01:53:06,909
forward with that and that's it

2357
01:53:04,698 --> 01:53:09,829
everything else is going to be identical

2358
01:53:06,909 --> 01:53:11,119
except now I'm going to do like way lots

2359
01:53:09,829 --> 01:53:13,399
of layers I'm going to make it four

2360
01:53:11,119 --> 01:53:17,738
times deeper right and it's going to

2361
01:53:13,399 --> 01:53:22,460
Train beautifully just because of that

2362
01:53:17,738 --> 01:53:25,069
so why does that help so much so this is

2363
01:53:22,460 --> 01:53:30,698
called a ResNet block and as you can see

2364
01:53:25,069 --> 01:53:30,698
I'm saying that's not what I meant to do

2365
01:53:31,479 --> 01:53:40,099
I'm saying my predictions equals my

2366
01:53:36,079 --> 01:53:42,429
input plus some function you know in

2367
01:53:40,100 --> 01:53:44,690
this case a convolution of my input

2368
01:53:42,429 --> 01:53:48,380
alright that's that's that's what I've

2369
01:53:44,689 --> 01:53:53,269
written here and so I'm now going to

2370
01:53:48,380 --> 01:54:00,880
shuffle that around a little bit and I'm

2371
01:53:53,270 --> 01:54:06,199
going to say I'm going to say f of X

2372
01:54:00,880 --> 01:54:09,650
equals y minus X ok so there's the same

2373
01:54:06,198 --> 01:54:11,738
thing shuffled around right that's my

2374
01:54:09,649 --> 01:54:16,399
prediction within the previous layer

2375
01:54:11,738 --> 01:54:19,759
right and so what this is then doing is

2376
01:54:16,399 --> 01:54:22,250
it's trying to fit a function to the

2377
01:54:19,760 --> 01:54:28,550
difference between these two right and

2378
01:54:22,250 --> 01:54:31,119
so the difference is actually the

2379
01:54:28,550 --> 01:54:31,119
residual

2380
01:54:35,909 --> 01:54:44,889
so if this is what I'm trying to

2381
01:54:41,649 --> 01:54:46,539
calculate my actual Y value and this is

2382
01:54:44,890 --> 01:54:48,700
the thing that I've most recently

2383
01:54:46,539 --> 01:54:51,430
calculated then the difference between

2384
01:54:48,699 --> 01:54:54,159
the two is basically the error in terms

2385
01:54:51,430 --> 01:54:56,230
of what I've calculated so far and so

2386
01:54:54,159 --> 01:54:59,670
this is therefore saying that okay try

2387
01:54:56,229 --> 01:55:04,329
to find a set of convolutional weights

2388
01:54:59,670 --> 01:55:08,760
that attempts to fill in the the amount

2389
01:55:04,329 --> 01:55:13,689
I was off by so in other words if we

2390
01:55:08,760 --> 01:55:16,900
let's clear this out if we have some

2391
01:55:13,689 --> 01:55:18,909
inputs coming in right and then we have

2392
01:55:16,899 --> 01:55:21,939
this function which is basically trying

2393
01:55:18,909 --> 01:55:25,180
to predict the error it's like how much

2394
01:55:21,939 --> 01:55:27,159
are we off by right and then we add that

2395
01:55:25,180 --> 01:55:29,440
on so we basically add on this

2396
01:55:27,159 --> 01:55:31,659
additional like prediction of how much

2397
01:55:29,439 --> 01:55:33,789
will be wrong by and then we add on

2398
01:55:31,659 --> 01:55:35,710
another prediction of how much were we

2399
01:55:33,789 --> 01:55:37,750
wrong by that time and add on another

2400
01:55:35,710 --> 01:55:40,899
prediction of how much we wrong by that

2401
01:55:37,750 --> 01:55:44,319
time then that each time we're kind of

2402
01:55:40,899 --> 01:55:46,509
zooming in getting closer and closer to

2403
01:55:44,319 --> 01:55:48,609
our correct answer and each time we're

2404
01:55:46,510 --> 01:55:51,760
saying like okay we've got to a certain

2405
01:55:48,609 --> 01:55:53,859
point but we still got an error you've

2406
01:55:51,760 --> 01:55:56,470
still got a residual so let's try and

2407
01:55:53,859 --> 01:55:58,569
create a model that just predicts that

2408
01:55:56,470 --> 01:56:00,430
residual and add that on to our previous

2409
01:55:58,569 --> 01:56:02,649
model and then let's build another model

2410
01:56:00,430 --> 01:56:04,450
that predicts the residual and add that

2411
01:56:02,649 --> 01:56:06,639
on to our previous model and if we keep

2412
01:56:04,449 --> 01:56:10,840
doing that again and again we should get

2413
01:56:06,640 --> 01:56:14,440
closer and closer to our answer and this

2414
01:56:10,840 --> 01:56:16,420
is based on a theory called boosting

2415
01:56:14,439 --> 01:56:17,829
which people that have done some machine

2416
01:56:16,420 --> 01:56:20,579
learning will certainly come across

2417
01:56:17,829 --> 01:56:30,039
right and so basically the trick here is

2418
01:56:20,579 --> 01:56:32,909
that by specifying that as being the

2419
01:56:30,039 --> 01:56:32,909
thing that we're trying to calculate

2420
01:56:36,560 --> 01:56:41,900
then we kind of get boosting for free

2421
01:56:39,380 --> 01:56:44,130
right it's like because we couldn't just

2422
01:56:41,899 --> 01:56:47,039
juggle that around to show that actually

2423
01:56:44,130 --> 01:56:53,390
it's just calculating a model on the

2424
01:56:47,039 --> 01:56:53,390
wrist Jill so that's kind of amazing and

2425
01:56:53,510 --> 01:56:59,909
you know it totally works as you can see

2426
01:56:56,369 --> 01:57:02,369
here I've now got my standard batch norm

2427
01:56:59,908 --> 01:57:05,158
layer okay which is something which is

2428
01:57:02,369 --> 01:57:07,170
going to reduce my size by two because

2429
01:57:05,158 --> 01:57:08,908
it's got the stride too and then I've

2430
01:57:07,170 --> 01:57:11,789
got a resident layers dried one and

2431
01:57:08,908 --> 01:57:13,738
another resident layer astride one right

2432
01:57:11,789 --> 01:57:15,600
and sorry I think I said that was four

2433
01:57:13,738 --> 01:57:17,669
of these is actually three of these so

2434
01:57:15,600 --> 01:57:19,860
this is now three times deeper I zipped

2435
01:57:17,670 --> 01:57:22,859
through all of those and so I've now got

2436
01:57:19,859 --> 01:57:26,670
a function of a function of a function

2437
01:57:22,859 --> 01:57:29,488
so three layers per group and then my

2438
01:57:26,670 --> 01:57:32,310
con at the start and my linear at the

2439
01:57:29,488 --> 01:57:37,559
end so this is now three times bigger

2440
01:57:32,310 --> 01:57:39,780
than my original and if I fit it you can

2441
01:57:37,560 --> 01:57:41,699
see it's just keeps going up and up and

2442
01:57:39,779 --> 01:57:45,359
up and up I keep fitting it more he's

2443
01:57:41,698 --> 01:57:49,198
going up and up and it's still going up

2444
01:57:45,359 --> 01:57:54,049
when I kind of got bored okay so the

2445
01:57:49,198 --> 01:57:58,529
rest net has been a really important

2446
01:57:54,050 --> 01:58:03,510
development and it's allowed us to

2447
01:57:58,529 --> 01:58:06,238
create these really deep networks right

2448
01:58:03,510 --> 01:58:09,539
now the full risk net does not quite

2449
01:58:06,238 --> 01:58:11,988
look the way I've described it here the

2450
01:58:09,539 --> 01:58:14,760
full res net doesn't just have one

2451
01:58:11,988 --> 01:58:16,769
convolution right but it actually has

2452
01:58:14,760 --> 01:58:19,380
two convolutions right so the way people

2453
01:58:16,770 --> 01:58:21,420
normally draw resident blocks is they

2454
01:58:19,380 --> 01:58:26,520
normally say you've got some input

2455
01:58:21,420 --> 01:58:31,289
coming in to the layer it goes through

2456
01:58:26,520 --> 01:58:35,550
one convolution to convolutions and then

2457
01:58:31,289 --> 01:58:37,948
gets added back to the original input

2458
01:58:35,550 --> 01:58:40,039
right that's the full version of a

2459
01:58:37,948 --> 01:58:43,408
ResNet block in my case I've just done

2460
01:58:40,039 --> 01:58:47,958
one convolution okay and then you'll see

2461
01:58:43,408 --> 01:58:52,728
also in every block

2462
01:58:47,958 --> 01:58:56,899
right one of them it's actually the

2463
01:58:52,729 --> 01:58:59,208
first one does he it's actually the

2464
01:58:56,899 --> 01:59:04,248
first one here is not a resident block

2465
01:58:59,208 --> 01:59:07,099
but a standard convolution with a stride

2466
01:59:04,248 --> 01:59:09,859
of two right this is called a bottleneck

2467
01:59:07,099 --> 01:59:12,769
layer right and the idea is this is not

2468
01:59:09,859 --> 01:59:14,839
a ResNet block so from time to time we

2469
01:59:12,769 --> 01:59:17,900
actually change the geometry right we're

2470
01:59:14,840 --> 01:59:19,099
doing this trade to in resident we don't

2471
01:59:17,899 --> 01:59:21,319
actually use just a standard

2472
01:59:19,099 --> 01:59:23,958
convolutional layer there's actually a

2473
01:59:21,319 --> 01:59:25,158
different form of bottleneck block that

2474
01:59:23,958 --> 01:59:27,260
I'm not going to picture in this course

2475
01:59:25,158 --> 01:59:29,748
I'm going to teach you in part two okay

2476
01:59:27,260 --> 01:59:31,820
but as you can see even this somewhat

2477
01:59:29,748 --> 01:59:35,630
simplified version of a resin it still

2478
01:59:31,819 --> 01:59:38,569
works pretty well and so we can make it

2479
01:59:35,630 --> 01:59:41,229
a little bit bigger all right and so

2480
01:59:38,569 --> 01:59:45,170
here I've just increased all of my sizes

2481
01:59:41,229 --> 01:59:47,570
I have still got my three and also I've

2482
01:59:45,170 --> 01:59:49,788
had it drop out right so at this point

2483
01:59:47,569 --> 01:59:52,549
I'm gonna say this is other than the

2484
01:59:49,788 --> 01:59:54,978
minor simplification of ResNet you know

2485
01:59:52,550 --> 01:59:56,979
a reasonable approximation of a good

2486
01:59:54,979 --> 02:00:00,110
starting point for a modern architecture

2487
01:59:56,979 --> 02:00:02,479
okay and so now I've added in my point

2488
02:00:00,109 --> 02:00:06,288
to drop out I've increased the size here

2489
02:00:02,479 --> 02:00:08,090
and if I train this you know I can treat

2490
02:00:06,288 --> 02:00:09,800
it for a while it's going pretty well I

2491
02:00:08,090 --> 02:00:13,189
can then add in gta at the end

2492
02:00:09,800 --> 02:00:16,248
eventually I get 85% and you know this

2493
02:00:13,189 --> 02:00:17,900
is at a point now where like literally I

2494
02:00:16,248 --> 02:00:19,368
wrote this whole notebook in like three

2495
02:00:17,899 --> 02:00:22,518
hours right we can like create this

2496
02:00:19,368 --> 02:00:26,389
thing in three hours and this is like an

2497
02:00:22,519 --> 02:00:29,239
accuracy that in kind of 2012-2013 was

2498
02:00:26,389 --> 02:00:32,420
considered pretty much data the art for

2499
02:00:29,238 --> 02:00:35,658
say pocket right so this is actually no

2500
02:00:32,420 --> 02:00:37,849
this is actually pretty damn good to get

2501
02:00:35,658 --> 02:00:40,788
you know nowadays the most recent

2502
02:00:37,849 --> 02:00:42,380
results are like 97% you know there are

2503
02:00:40,788 --> 02:00:44,779
there's plenty of room we can still

2504
02:00:42,380 --> 02:00:46,309
improve but they're all based on these

2505
02:00:44,779 --> 02:00:49,998
techniques like there isn't really

2506
02:00:46,309 --> 02:00:52,489
anything you know when we start looking

2507
02:00:49,998 --> 02:00:53,958
in in part to it like how to get this

2508
02:00:52,488 --> 02:00:55,488
right up to state of the art you'll see

2509
02:00:53,958 --> 02:00:57,979
it's basically better approaches to data

2510
02:00:55,488 --> 02:01:01,119
augmentation better approaches to

2511
02:00:57,979 --> 02:01:04,000
regularization some tweaks on ResNet

2512
02:01:01,119 --> 02:01:09,849
but it's all basically the circuit okay

2513
02:01:04,000 --> 02:01:13,779
so so is the residual training on the

2514
02:01:09,850 --> 02:01:16,150
residual method is that only looks like

2515
02:01:13,779 --> 02:01:19,119
it's a generic thing that can be applied

2516
02:01:16,149 --> 02:01:22,269
non image problems oh great question

2517
02:01:19,119 --> 02:01:25,840
yeah yes it is but it's like being

2518
02:01:22,270 --> 02:01:28,300
ignored everywhere else in NLP something

2519
02:01:25,840 --> 02:01:31,210
called the transformer architecture

2520
02:01:28,300 --> 02:01:32,940
recently appeared and you know was shown

2521
02:01:31,210 --> 02:01:36,039
to be the state of the art for

2522
02:01:32,939 --> 02:01:38,619
translation and it's got like a simple

2523
02:01:36,039 --> 02:01:40,840
resonance structure you know first time

2524
02:01:38,619 --> 02:01:42,309
I've ever seen it in NLP I haven't

2525
02:01:40,840 --> 02:01:45,520
really seen anybody else take advantage

2526
02:01:42,310 --> 02:01:47,289
of it yeah this general approach we call

2527
02:01:45,520 --> 02:01:49,510
these skip connections this idea of like

2528
02:01:47,289 --> 02:01:52,390
skipping over a layer and kind of doing

2529
02:01:49,510 --> 02:01:54,280
an identity it's yeah it's been

2530
02:01:52,390 --> 02:01:56,410
appearing a lot in computer vision and

2531
02:01:54,279 --> 02:01:57,969
nobody else much seems to be using it

2532
02:01:56,409 --> 02:02:00,039
even though there's nothing computer

2533
02:01:57,970 --> 02:02:05,740
vision specific about it so I think it's

2534
02:02:00,039 --> 02:02:09,729
a big opportunity okay so final stage I

2535
02:02:05,739 --> 02:02:12,909
want to show you is how to use an extra

2536
02:02:09,729 --> 02:02:14,709
feature of Pi torch to do something cool

2537
02:02:12,909 --> 02:02:17,439
and it's going to be a kind of a segue

2538
02:02:14,710 --> 02:02:20,590
into part two it's going to be our first

2539
02:02:17,439 --> 02:02:23,500
little hint as to what else we can build

2540
02:02:20,590 --> 02:02:24,489
on these neural nets and so and it's

2541
02:02:23,500 --> 02:02:26,800
also going to take us all the way back

2542
02:02:24,489 --> 02:02:29,679
to lesson 1 which is we're going to do

2543
02:02:26,800 --> 02:02:32,560
dogs and cats ok so going all the way

2544
02:02:29,680 --> 02:02:34,900
back to dogs and cats we're going to

2545
02:02:32,560 --> 02:02:38,890
create a resin at 34 ok so these

2546
02:02:34,899 --> 02:02:42,670
different ResNet 3450 101 they're

2547
02:02:38,890 --> 02:02:46,329
they're basically just different numbers

2548
02:02:42,670 --> 02:02:48,039
of different sized blocks it's like how

2549
02:02:46,329 --> 02:02:50,260
many of these kind of pieces do you have

2550
02:02:48,039 --> 02:02:53,050
before it bottleneck block and then how

2551
02:02:50,260 --> 02:02:54,070
many of these sets of super blocks do

2552
02:02:53,050 --> 02:02:56,590
you have right that's all these

2553
02:02:54,069 --> 02:03:00,159
different numbers mean so if you look at

2554
02:02:56,590 --> 02:03:01,779
the torch vision source code you can

2555
02:03:00,159 --> 02:03:03,189
actually see the definition of these

2556
02:03:01,779 --> 02:03:09,369
different resonates you'll see they're

2557
02:03:03,189 --> 02:03:11,669
all just different parameters right ok

2558
02:03:09,369 --> 02:03:13,079
so we're going to use rest at 34

2559
02:03:11,670 --> 02:03:17,340
and so we're going to do this a little

2560
02:03:13,079 --> 02:03:19,170
bit more by hand okay so if this is my

2561
02:03:17,340 --> 02:03:22,020
architecture this is just the name of a

2562
02:03:19,170 --> 02:03:25,649
function then I can call it to get that

2563
02:03:22,020 --> 02:03:27,989
model right and then true look at the

2564
02:03:25,649 --> 02:03:29,698
definition is do I want the pre-trained

2565
02:03:27,988 --> 02:03:31,500
so in other words is it going to load in

2566
02:03:29,698 --> 02:03:36,960
the pre-trained imagenet weights

2567
02:03:31,500 --> 02:03:39,238
okay so M now contains a model and so I

2568
02:03:36,960 --> 02:03:41,760
can take a look at it like so okay and

2569
02:03:39,238 --> 02:03:47,509
so you can see here what's going on

2570
02:03:41,760 --> 02:03:49,770
right is that inside here I've got my

2571
02:03:47,510 --> 02:03:52,380
initial two deconvolution

2572
02:03:49,770 --> 02:03:54,960
and here is that kernel size of seven by

2573
02:03:52,380 --> 02:03:56,489
seven okay and interestingly in this

2574
02:03:54,960 --> 02:03:58,890
case it actually starts out with a seven

2575
02:03:56,488 --> 02:04:00,389
by seven strobe - okay there's the

2576
02:03:58,890 --> 02:04:02,160
padding that we talked about to make

2577
02:04:00,390 --> 02:04:04,289
sure that we don't lose the edges all

2578
02:04:02,159 --> 02:04:07,019
right there's our batch naught okay

2579
02:04:04,289 --> 02:04:10,079
there's our Lu you get the idea right

2580
02:04:07,020 --> 02:04:13,530
Kong and then so here you can now see

2581
02:04:10,079 --> 02:04:16,170
there's a layer that contains a bunch of

2582
02:04:13,529 --> 02:04:19,469
blocks all right so here's a block which

2583
02:04:16,170 --> 02:04:22,560
contains a cons fetch norm rally you con

2584
02:04:19,469 --> 02:04:24,659
Bethnal you can't see it printed but

2585
02:04:22,560 --> 02:04:26,820
after this is where it does the addition

2586
02:04:24,659 --> 02:04:28,559
all right so there's like a whole ResNet

2587
02:04:26,819 --> 02:04:33,599
block and then another resident block

2588
02:04:28,560 --> 02:04:38,100
and then another ResNet block okay and

2589
02:04:33,600 --> 02:04:40,860
then you can see also sometimes you see

2590
02:04:38,100 --> 02:04:44,390
one where there's a stripe - right so

2591
02:04:40,859 --> 02:04:47,039
here's actually one of these bottleneck

2592
02:04:44,390 --> 02:04:49,530
layers okay

2593
02:04:47,039 --> 02:04:53,488
so you can kind of see how this is this

2594
02:04:49,529 --> 02:04:59,039
is structure so in our case sorry I

2595
02:04:53,488 --> 02:05:04,109
skipped over this a little bit but the

2596
02:04:59,039 --> 02:05:12,719
approach that we ended up using for real

2597
02:05:04,109 --> 02:05:18,269
you was to put it before our before our

2598
02:05:12,719 --> 02:05:24,300
Bashan on which see what they do here

2599
02:05:18,270 --> 02:05:25,050
we've got fetch norm railing you cons

2600
02:05:24,300 --> 02:05:26,699
that

2601
02:05:25,050 --> 02:05:27,960
no I'm really okay okay so you can see

2602
02:05:26,698 --> 02:05:29,308
the order that they're using it here

2603
02:05:27,960 --> 02:05:31,289
okay

2604
02:05:29,309 --> 02:05:33,779
and you'll find like there's two

2605
02:05:31,289 --> 02:05:34,710
different versions of ResNet in fact

2606
02:05:33,779 --> 02:05:37,050
there's three different versions of

2607
02:05:34,710 --> 02:05:39,270
ResNet floating around the one which

2608
02:05:37,050 --> 02:05:41,940
actually turns out to be the best it's

2609
02:05:39,270 --> 02:05:47,250
called the pre-act ResNet which has a

2610
02:05:41,939 --> 02:05:48,839
different ordering again but you can

2611
02:05:47,250 --> 02:05:50,849
look it up it's basically a different

2612
02:05:48,840 --> 02:05:53,730
order of where the value and where the

2613
02:05:50,849 --> 02:05:57,199
batch norm yeah okay so we're going to

2614
02:05:53,729 --> 02:06:01,229
start with a standard resident 34 and

2615
02:05:57,198 --> 02:06:02,879
normally what we do is we need to now

2616
02:06:01,229 --> 02:06:06,328
turn this into something that can

2617
02:06:02,880 --> 02:06:10,050
predict dogs versus class right so

2618
02:06:06,328 --> 02:06:12,448
currently the final layer has a thousand

2619
02:06:10,050 --> 02:06:14,880
features because imagenet has a thousand

2620
02:06:12,448 --> 02:06:19,678
features right so we need to get rid of

2621
02:06:14,880 --> 02:06:22,469
this so when you use confer owner from

2622
02:06:19,679 --> 02:06:25,980
pre-trained in fast AI it actually

2623
02:06:22,469 --> 02:06:30,149
deletes this layer for you and it also

2624
02:06:25,979 --> 02:06:33,238
deletes this layer and something that as

2625
02:06:30,149 --> 02:06:35,549
far as I know is unique to fast AI is we

2626
02:06:33,238 --> 02:06:37,589
replace this see this is an average

2627
02:06:35,550 --> 02:06:39,869
pooling layer of size seven by seven

2628
02:06:37,590 --> 02:06:41,909
right so this is the basically the

2629
02:06:39,868 --> 02:06:43,710
adaptive pooling layer but whoever wrote

2630
02:06:41,908 --> 02:06:45,448
this didn't know about adaptive pooling

2631
02:06:43,710 --> 02:06:47,880
so they manually said oh I know it's

2632
02:06:45,448 --> 02:06:49,618
meant to be seven by seven so in first

2633
02:06:47,880 --> 02:06:51,779
AI we replaced this with adaptive

2634
02:06:49,618 --> 02:06:54,868
pooling but we actually do both adaptive

2635
02:06:51,779 --> 02:06:56,789
average pooling and adaptive max pooling

2636
02:06:54,868 --> 02:07:00,238
and we then concatenate them two

2637
02:06:56,789 --> 02:07:02,039
together which it's it is something we

2638
02:07:00,238 --> 02:07:03,750
invented but at the same time we

2639
02:07:02,039 --> 02:07:04,260
invented it somebody wrote a paper about

2640
02:07:03,750 --> 02:07:06,210
it

2641
02:07:04,260 --> 02:07:08,489
so it's you know we don't get any credit

2642
02:07:06,210 --> 02:07:09,899
but I think we're the only library that

2643
02:07:08,488 --> 02:07:14,129
provides it and certainly only one that

2644
02:07:09,899 --> 02:07:15,420
does it by default we're going to for

2645
02:07:14,130 --> 02:07:17,400
the purpose of this exercise though

2646
02:07:15,420 --> 02:07:19,618
we're going to do a simple version where

2647
02:07:17,399 --> 02:07:21,629
we delete the last two layers so we'll

2648
02:07:19,618 --> 02:07:24,089
grab all the children of the model will

2649
02:07:21,630 --> 02:07:27,710
delete the last two layers and then

2650
02:07:24,090 --> 02:07:32,550
instead we're going to add a convolution

2651
02:07:27,710 --> 02:07:33,630
which just has two outputs right I'll

2652
02:07:32,550 --> 02:07:36,599
show you why in a moment

2653
02:07:33,630 --> 02:07:39,029
right then we're going to do our average

2654
02:07:36,599 --> 02:07:43,319
pooling and then we're going to do

2655
02:07:39,029 --> 02:07:46,259
soft mess okay so that's a model which

2656
02:07:43,319 --> 02:07:48,179
is going to have you'll see that there

2657
02:07:46,260 --> 02:07:50,640
is no this one has a fully connected

2658
02:07:48,180 --> 02:07:53,280
layer at the end this one does not have

2659
02:07:50,640 --> 02:07:55,470
a fully connected layer yet but if you

2660
02:07:53,279 --> 02:08:01,380
think about it this convolutional layer

2661
02:07:55,470 --> 02:08:03,900
is going to be two filters only right

2662
02:08:01,380 --> 02:08:06,810
and it's going to be two by seven by

2663
02:08:03,899 --> 02:08:09,269
seven and so once we then do the average

2664
02:08:06,810 --> 02:08:11,610
pooling it's going to end up being just

2665
02:08:09,270 --> 02:08:13,230
two numbers that it produces so this is

2666
02:08:11,609 --> 02:08:15,029
a different way of producing just two

2667
02:08:13,229 --> 02:08:16,919
numbers I'm not going to say it's better

2668
02:08:15,029 --> 02:08:19,469
it's going to say it's different okay

2669
02:08:16,920 --> 02:08:21,239
but there's a reason we do it I'll show

2670
02:08:19,470 --> 02:08:23,909
you the reason we can now train this

2671
02:08:21,239 --> 02:08:26,159
model in the usual way right so we can

2672
02:08:23,909 --> 02:08:29,010
say transform stock model image

2673
02:08:26,159 --> 02:08:30,840
classifier data from paths and then we

2674
02:08:29,010 --> 02:08:34,110
can use that Kampf learner from model

2675
02:08:30,840 --> 02:08:38,449
data we just learnt about I'm now going

2676
02:08:34,109 --> 02:08:41,399
to freeze every single layer except for

2677
02:08:38,449 --> 02:08:41,970
that one and this is the fourth last

2678
02:08:41,399 --> 02:08:44,729
layer

2679
02:08:41,970 --> 02:08:46,440
so we'll say freeze to minus four right

2680
02:08:44,729 --> 02:08:49,349
and so this is just training the last

2681
02:08:46,439 --> 02:08:50,789
layer okay so we get 99.1 percent

2682
02:08:49,350 --> 02:08:53,700
accuracy so that you know this

2683
02:08:50,789 --> 02:08:56,159
approaches working fine and here's what

2684
02:08:53,699 --> 02:09:02,300
we can do though we can now do something

2685
02:08:56,159 --> 02:09:06,119
called fast comm last activated Maps

2686
02:09:02,300 --> 02:09:08,610
fast activation is what we're going to

2687
02:09:06,119 --> 02:09:11,849
do is we're going to try to look at this

2688
02:09:08,609 --> 02:09:13,619
particular cat and we're going to use a

2689
02:09:11,850 --> 02:09:16,320
technique called class activation Maps

2690
02:09:13,619 --> 02:09:19,289
where we take our model and we ask you

2691
02:09:16,319 --> 02:09:22,079
which parts of this image turned out to

2692
02:09:19,289 --> 02:09:24,750
be important and when we do this it's

2693
02:09:22,079 --> 02:09:26,880
going to feed out this is got the

2694
02:09:24,750 --> 02:09:28,979
picture it's going to create alright and

2695
02:09:26,880 --> 02:09:32,579
so as you can see here it's found the

2696
02:09:28,979 --> 02:09:33,989
cat so how did it do that well the way

2697
02:09:32,579 --> 02:09:38,550
it did that will kind of work backwards

2698
02:09:33,989 --> 02:09:41,189
is to produce this matrix now you'll see

2699
02:09:38,550 --> 02:09:44,550
in this matrix there's some pretty big

2700
02:09:41,189 --> 02:09:48,449
numbers around about here which

2701
02:09:44,550 --> 02:09:52,260
correspond to our cat so what is this

2702
02:09:48,449 --> 02:10:00,210
matrix this matrix is simply

2703
02:09:52,260 --> 02:10:05,789
or to the value of this feature matrix

2704
02:10:00,210 --> 02:10:09,420
times this py vector the py vector is

2705
02:10:05,789 --> 02:10:11,970
simply equal to the predictions which in

2706
02:10:09,420 --> 02:10:14,940
this case said I am 100% confident it's

2707
02:10:11,970 --> 02:10:17,180
a cat right so this is just equal to the

2708
02:10:14,939 --> 02:10:20,729
value of if I just call the model

2709
02:10:17,180 --> 02:10:23,430
passing in our cat this is our cat

2710
02:10:20,729 --> 02:10:24,569
that's an X then we got our predictions

2711
02:10:23,430 --> 02:10:27,180
right so it's just the value of our

2712
02:10:24,569 --> 02:10:27,949
predictions so py is just the value of

2713
02:10:27,180 --> 02:10:30,650
our predictions

2714
02:10:27,949 --> 02:10:38,340
what about feet what's that equal to

2715
02:10:30,649 --> 02:10:40,739
feet is equal to the values in this

2716
02:10:38,340 --> 02:10:42,779
layer right in other words the value

2717
02:10:40,739 --> 02:10:43,949
that comes out of the final in facts

2718
02:10:42,779 --> 02:10:46,769
come out of this ladder coming out of

2719
02:10:43,949 --> 02:10:50,750
the final convolutional layer right so

2720
02:10:46,770 --> 02:10:54,570
it's actually the seven by seven by two

2721
02:10:50,750 --> 02:10:58,770
and so you can see here let's see feet

2722
02:10:54,569 --> 02:11:03,029
the shape of features is two filters by

2723
02:10:58,770 --> 02:11:09,650
seven by seven right so the idea is if

2724
02:11:03,029 --> 02:11:12,359
we multiply that vector by that tensor

2725
02:11:09,649 --> 02:11:15,239
right then it's going to end up grabbing

2726
02:11:12,359 --> 02:11:17,789
all of the first channel because that's

2727
02:11:15,239 --> 02:11:21,389
a 1 and none of the second channel

2728
02:11:17,789 --> 02:11:23,720
because that's a 0 and so therefore it's

2729
02:11:21,390 --> 02:11:28,079
going to return the value of the last

2730
02:11:23,720 --> 02:11:30,630
convolutional layer for the for the

2731
02:11:28,079 --> 02:11:33,869
section which lines up with being a cat

2732
02:11:30,630 --> 02:11:36,119
but if you think about it this the first

2733
02:11:33,869 --> 02:11:37,829
section lines up with being a cat the

2734
02:11:36,119 --> 02:11:41,099
second section lines up with being a dog

2735
02:11:37,829 --> 02:11:45,329
so if we multiply that tensor by that

2736
02:11:41,100 --> 02:11:50,010
tensor we end up with this matrix and

2737
02:11:45,329 --> 02:11:53,939
this matrix is which parts most like a

2738
02:11:50,010 --> 02:11:56,550
cat or to put it another way in our

2739
02:11:53,939 --> 02:11:59,339
model the only thing that happened after

2740
02:11:56,550 --> 02:12:01,590
the convolutional layer was an average

2741
02:11:59,340 --> 02:12:04,409
pooling layer so the average pooling

2742
02:12:01,590 --> 02:12:05,630
layer talked that 7x7 grid and said

2743
02:12:04,409 --> 02:12:08,929
average

2744
02:12:05,630 --> 02:12:10,730
how much each part is cat Lake that

2745
02:12:08,929 --> 02:12:13,449
answered my final value my final

2746
02:12:10,729 --> 02:12:17,829
prediction was the average cattiness

2747
02:12:13,448 --> 02:12:19,969
there's the whole thing right and so

2748
02:12:17,829 --> 02:12:20,988
because it had to be able to average out

2749
02:12:19,969 --> 02:12:23,448
these things to get the average

2750
02:12:20,988 --> 02:12:28,309
cattiness that means I could then just

2751
02:12:23,448 --> 02:12:30,799
take this matrix and resize it to be the

2752
02:12:28,310 --> 02:12:33,500
same size as my original cat and just

2753
02:12:30,800 --> 02:12:35,659
overlay it on top you get this heat map

2754
02:12:33,500 --> 02:12:39,020
right so the way you can use this

2755
02:12:35,658 --> 02:12:41,869
technique at home is to basically

2756
02:12:39,020 --> 02:12:43,520
calculate this matrix right on some like

2757
02:12:41,869 --> 02:12:46,488
really you've got some really big

2758
02:12:43,520 --> 02:12:48,860
picture you can calculate this matrix on

2759
02:12:46,488 --> 02:12:51,109
a quick small little cognate and then

2760
02:12:48,859 --> 02:12:54,559
zoom into the bit that has the highest

2761
02:12:51,109 --> 02:12:57,019
value and then rerun it just on that

2762
02:12:54,560 --> 02:12:58,550
part that so it's like oh this is the

2763
02:12:57,020 --> 02:13:01,850
area that seems to be the most like a

2764
02:12:58,550 --> 02:13:05,420
hat or most like a dog that zoom in to

2765
02:13:01,850 --> 02:13:06,530
that bit right so I skipped over that

2766
02:13:05,420 --> 02:13:10,940
pretty quickly because we ran out of

2767
02:13:06,529 --> 02:13:12,769
time and so we'll be learning more about

2768
02:13:10,939 --> 02:13:13,969
these kind of approaches in part two and

2769
02:13:12,770 --> 02:13:15,590
we can talk about it more on the forum

2770
02:13:13,969 --> 02:13:18,198
and hopefully you get the idea the one

2771
02:13:15,590 --> 02:13:21,710
thing that totally skipped over was how

2772
02:13:18,198 --> 02:13:23,779
do we actually ask for that particular

2773
02:13:21,710 --> 02:13:25,670
layer okay and I'll let you read about

2774
02:13:23,779 --> 02:13:31,809
this during the week but basically

2775
02:13:25,670 --> 02:13:31,810
there's a thing called a hook so we said

2776
02:13:31,960 --> 02:13:37,369
we called save features which is this

2777
02:13:35,210 --> 02:13:40,069
little class that we wrote that goes

2778
02:13:37,369 --> 02:13:42,979
register forward hook and basically a

2779
02:13:40,069 --> 02:13:45,948
forward hook is a special PI torch thing

2780
02:13:42,979 --> 02:13:48,879
that every time it calculates a layer it

2781
02:13:45,948 --> 02:13:50,809
runs this function it's like a callback

2782
02:13:48,880 --> 02:13:52,250
basically it's like a callback that

2783
02:13:50,810 --> 02:13:55,909
happens every time it calculates a layer

2784
02:13:52,250 --> 02:13:59,060
and so in this case it just saved the

2785
02:13:55,908 --> 02:14:02,359
value of the particular layer that I was

2786
02:13:59,060 --> 02:14:04,159
interested in okay and so that way I was

2787
02:14:02,359 --> 02:14:10,908
able to go inside here and grab those

2788
02:14:04,158 --> 02:14:13,729
features out look after I was done okay

2789
02:14:10,908 --> 02:14:15,979
so I call save features that gives me my

2790
02:14:13,729 --> 02:14:18,799
pork and then later on I can just grab

2791
02:14:15,979 --> 02:14:19,339
the value that I saved okay so I skipped

2792
02:14:18,800 --> 02:14:20,840
over that pretty

2793
02:14:19,340 --> 02:14:22,760
quickly but if you look in the pipe or

2794
02:14:20,840 --> 02:14:28,190
docks they have some more information

2795
02:14:22,760 --> 02:14:31,699
and help about that yes Jeremy can you

2796
02:14:28,189 --> 02:14:34,719
spend five minutes talking about your

2797
02:14:31,698 --> 02:14:39,339
journey into deep learning

2798
02:14:34,719 --> 02:14:42,050
yeah and finally how can we keep up with

2799
02:14:39,340 --> 02:14:46,069
important research that is important to

2800
02:14:42,050 --> 02:14:47,360
practice sure yeah so it's gonna that's

2801
02:14:46,069 --> 02:14:50,090
good I think I'll close more on the

2802
02:14:47,359 --> 02:14:55,098
latter bit which is like what now okay

2803
02:14:50,090 --> 02:14:57,260
so for those of you are interested you

2804
02:14:55,099 --> 02:14:59,270
should aim to come back for part two if

2805
02:14:57,260 --> 02:15:00,710
you're aiming to come back for part two

2806
02:14:59,270 --> 02:15:02,989
how many people would like to come back

2807
02:15:00,710 --> 02:15:05,810
to part two okay that's not bad I think

2808
02:15:02,988 --> 02:15:08,779
almost everybody so if you want to come

2809
02:15:05,810 --> 02:15:10,849
back to part two be aware of this by

2810
02:15:08,779 --> 02:15:12,469
that time you're expected to have

2811
02:15:10,849 --> 02:15:14,210
mastered all of the techniques we've

2812
02:15:12,469 --> 02:15:16,279
learnt in part one and there's plenty of

2813
02:15:14,210 --> 02:15:18,800
time between now and then okay even if

2814
02:15:16,279 --> 02:15:20,960
you haven't done much or any ml before

2815
02:15:18,800 --> 02:15:23,750
but it does assume that you're going to

2816
02:15:20,960 --> 02:15:25,760
be working you know at the same level of

2817
02:15:23,750 --> 02:15:27,979
intensity for now until then that you

2818
02:15:25,760 --> 02:15:30,469
have been with practicing right so

2819
02:15:27,979 --> 02:15:32,178
practicing so generally speaking the

2820
02:15:30,469 --> 02:15:34,219
people who did well in part two last

2821
02:15:32,179 --> 02:15:36,980
year had watched each of the videos

2822
02:15:34,219 --> 02:15:39,500
about three times right and some of the

2823
02:15:36,979 --> 02:15:40,609
people actually I knew had actually

2824
02:15:39,500 --> 02:15:42,590
discovered they learnt some of them off

2825
02:15:40,609 --> 02:15:44,000
by heart by mistake so they're like

2826
02:15:42,590 --> 02:15:45,770
watching the video is again is helpful

2827
02:15:44,000 --> 02:15:48,349
and make sure you get to the point that

2828
02:15:45,770 --> 02:15:50,810
you can recreate the notebooks without

2829
02:15:48,349 --> 02:15:51,710
watching the videos all right and so

2830
02:15:50,810 --> 02:15:53,630
other men make more interesting

2831
02:15:51,710 --> 02:15:57,439
obviously try and recreate them

2832
02:15:53,630 --> 02:16:00,020
notebooks using different data sets you

2833
02:15:57,439 --> 02:16:01,460
know and definitely then just keep up

2834
02:16:00,020 --> 02:16:04,159
with the forum and you'll see people

2835
02:16:01,460 --> 02:16:07,069
keep on posting more stuff about recent

2836
02:16:04,158 --> 02:16:08,539
papers and recent advances and over the

2837
02:16:07,069 --> 02:16:10,658
next couple of months you'll find

2838
02:16:08,539 --> 02:16:13,219
increasingly less and less of it seems

2839
02:16:10,658 --> 02:16:17,029
weird mysterious and more and more of it

2840
02:16:13,219 --> 02:16:19,368
makes perfect sense and so it's a bit of

2841
02:16:17,029 --> 02:16:20,719
a case with just thing staying tenacious

2842
02:16:19,368 --> 02:16:23,000
you know there's always going to be

2843
02:16:20,719 --> 02:16:25,849
stuff that you don't understand yet and

2844
02:16:23,000 --> 02:16:28,158
but you'll be surprised if you go back

2845
02:16:25,849 --> 02:16:32,380
to lesson one and two now you'll be like

2846
02:16:28,158 --> 02:16:34,359
oh that's all trivial right

2847
02:16:32,380 --> 02:16:38,680
so you know that's kind of hopefully a

2848
02:16:34,360 --> 02:16:40,780
bit of your learning journey and yeah I

2849
02:16:38,680 --> 02:16:42,251
think the main thing I've noticed is

2850
02:16:40,780 --> 02:16:44,770
that people who succeed are the ones who

2851
02:16:42,251 --> 02:16:46,719
just keep keep working at it you know so

2852
02:16:44,770 --> 02:16:48,520
not coming back here every Monday you're

2853
02:16:46,719 --> 02:16:51,460
not going to have that forcing function

2854
02:16:48,520 --> 02:16:53,711
I've noticed the forum suddenly gets

2855
02:16:51,460 --> 02:16:55,689
busy at 5:00 p.m. on a Monday you know

2856
02:16:53,710 --> 02:16:57,040
it's like Oh course is about to start

2857
02:16:55,690 --> 02:16:58,600
and suddenly these questions start

2858
02:16:57,040 --> 02:17:00,849
coming in so now that you don't have

2859
02:16:58,600 --> 02:17:03,581
that forcing function you know try and

2860
02:17:00,850 --> 02:17:05,110
use some other technique to you know

2861
02:17:03,581 --> 02:17:06,671
give yourself that little kick maybe you

2862
02:17:05,110 --> 02:17:08,440
can tell your partner at home

2863
02:17:06,671 --> 02:17:10,001
you know I'm going to try and produce

2864
02:17:08,440 --> 02:17:11,440
something every Saturday for the next

2865
02:17:10,001 --> 02:17:13,989
four weeks or I'm going to try and

2866
02:17:11,440 --> 02:17:17,381
finish reading this paper or something

2867
02:17:13,989 --> 02:17:21,190
you know anyway so I hope to see you all

2868
02:17:17,380 --> 02:17:22,479
back in March and even I regardless

2869
02:17:21,190 --> 02:17:24,190
whether I do or don't it's been a really

2870
02:17:22,479 --> 02:17:26,501
great pleasure to get to know you all

2871
02:17:24,190 --> 02:17:29,270
and I hope to keep seeing on the forum

2872
02:17:26,501 --> 02:17:32,379
thanks very much

2873
02:17:29,270 --> 02:17:32,379
[Applause]

