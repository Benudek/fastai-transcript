1
00:00:00,060 --> 00:00:09,719
welcome back less than six so this is

2
00:00:02,970 --> 00:00:15,929
our penultimate lesson and believe it or

3
00:00:09,718 --> 00:00:17,698
not a couple of weeks ago in Lesson four

4
00:00:15,929 --> 00:00:19,980
I mentioned I was going to share that

5
00:00:17,699 --> 00:00:22,800
lesson with this terrific you know P

6
00:00:19,980 --> 00:00:25,528
researcher Sebastian Reuter which I did

7
00:00:22,800 --> 00:00:29,189
and he he said he loved it and he's gone

8
00:00:25,528 --> 00:00:31,050
on to yesterday released this new post

9
00:00:29,189 --> 00:00:34,579
he called optimization for deep learning

10
00:00:31,050 --> 00:00:36,509
highlights in 2017 in which he covered

11
00:00:34,579 --> 00:00:40,289
basically everything that we talked

12
00:00:36,509 --> 00:00:42,089
about in that lesson and with some very

13
00:00:40,289 --> 00:00:43,710
nice shout outs to some of the work that

14
00:00:42,090 --> 00:00:46,920
some of the students here have done

15
00:00:43,710 --> 00:00:53,909
including when he talked about this

16
00:00:46,920 --> 00:00:58,020
separation of the separation of weight

17
00:00:53,909 --> 00:01:01,409
decay from the momentum term and so he

18
00:00:58,020 --> 00:01:04,198
actually mentions here the opportunities

19
00:01:01,409 --> 00:01:06,000
in terms of improved kind of software

20
00:01:04,198 --> 00:01:11,908
decoupling this allows and actually

21
00:01:06,000 --> 00:01:13,799
links to the commits from an answer hah

22
00:01:11,909 --> 00:01:16,710
actually showing how to implement this

23
00:01:13,799 --> 00:01:18,060
in fast AI so first a eyes code is

24
00:01:16,709 --> 00:01:23,519
actually being used as a bit of a role

25
00:01:18,060 --> 00:01:25,170
model now he then covers some of these

26
00:01:23,519 --> 00:01:31,828
learning rate tuning techniques that

27
00:01:25,170 --> 00:01:33,719
we've talked about and this is the SGD

28
00:01:31,828 --> 00:01:34,949
our schedule it looks a bit different to

29
00:01:33,719 --> 00:01:36,780
what you're used to seeing this is on a

30
00:01:34,950 --> 00:01:40,439
log curve this is the way that they show

31
00:01:36,780 --> 00:01:44,269
it on the paper and for more information

32
00:01:40,438 --> 00:01:49,649
again links to two blog posts one from

33
00:01:44,269 --> 00:01:53,429
vitaly about this topic and and again

34
00:01:49,649 --> 00:01:55,920
ananza ha is blog post on this topic so

35
00:01:53,430 --> 00:01:58,280
it's great to see that some of the work

36
00:01:55,920 --> 00:02:00,659
from faster our students is already

37
00:01:58,280 --> 00:02:02,489
getting noticed and picked up and shared

38
00:02:00,659 --> 00:02:06,990
and this blog post went on to get on the

39
00:02:02,489 --> 00:02:09,750
front page of hacker news so that's

40
00:02:06,989 --> 00:02:11,609
pretty cool and hopefully more and more

41
00:02:09,750 --> 00:02:12,740
of this work or be picked up on sisters

42
00:02:11,610 --> 00:02:19,910
released

43
00:02:12,740 --> 00:02:21,620
publicly so last week we were kind of

44
00:02:19,909 --> 00:02:27,289
doing a deep dive into collaborative

45
00:02:21,620 --> 00:02:31,090
filtering and let's remind ourselves of

46
00:02:27,289 --> 00:02:31,090
kind of what our final model looked like

47
00:02:34,300 --> 00:02:42,200
so in the end we kind of ended up

48
00:02:36,710 --> 00:02:47,629
rebuilding the model that's actually in

49
00:02:42,199 --> 00:02:49,609
the first a a library where we had an

50
00:02:47,629 --> 00:02:51,500
embedding so we had this little get

51
00:02:49,610 --> 00:02:54,470
embedding function that grabbed an

52
00:02:51,500 --> 00:02:59,360
embedding and randomly initialize the

53
00:02:54,469 --> 00:03:01,159
weights for the users and for the items

54
00:02:59,360 --> 00:03:02,480
that's the kind of generic term in our

55
00:03:01,159 --> 00:03:04,609
case the items are movies

56
00:03:02,479 --> 00:03:09,259
and the bias for the users the bias for

57
00:03:04,610 --> 00:03:11,540
the items and we had n factors embedding

58
00:03:09,259 --> 00:03:14,209
size for each for each one of course the

59
00:03:11,539 --> 00:03:15,979
biases just had a single one and then we

60
00:03:14,210 --> 00:03:18,980
grabbed the users and item in weddings

61
00:03:15,979 --> 00:03:22,729
multiply them together summed it up each

62
00:03:18,979 --> 00:03:24,560
row and add it on the bias terms pop

63
00:03:22,729 --> 00:03:27,429
that through a sigmoid to put it into

64
00:03:24,560 --> 00:03:33,020
the range that we wanted so that was our

65
00:03:27,430 --> 00:03:35,420
model and one of you asked if we can

66
00:03:33,020 --> 00:03:37,490
kind of interpret this information in

67
00:03:35,419 --> 00:03:39,589
some way and I promised this week we

68
00:03:37,490 --> 00:03:42,340
would see how to do that so let's take a

69
00:03:39,590 --> 00:03:44,870
look so we're going to start with the

70
00:03:42,340 --> 00:03:46,550
model we built here where we just used

71
00:03:44,870 --> 00:03:49,460
that fast AI library

72
00:03:46,550 --> 00:03:52,850
collaborative data set from CSP and then

73
00:03:49,460 --> 00:03:58,430
that get learner and then we fitted it

74
00:03:52,849 --> 00:04:02,180
in three epochs 19 seconds we've got a

75
00:03:58,430 --> 00:04:09,770
pretty good result so what we can now do

76
00:04:02,180 --> 00:04:11,689
is to analyze that model so you may

77
00:04:09,770 --> 00:04:15,650
remember right back when we started we

78
00:04:11,689 --> 00:04:17,779
read in the movies CSV file but that's

79
00:04:15,650 --> 00:04:19,879
just a mapping from the ID of the movie

80
00:04:17,779 --> 00:04:21,048
to the name of the movie and so we're

81
00:04:19,879 --> 00:04:24,370
just going to use that for display

82
00:04:21,048 --> 00:04:24,370
purposes so we can see what we're doing

83
00:04:24,639 --> 00:04:27,639
because

84
00:04:25,779 --> 00:04:30,069
not all of us have watched every movie

85
00:04:27,639 --> 00:04:33,519
I'm just going to limit this to the top

86
00:04:30,069 --> 00:04:34,930
500 most populous or 3,000 most popular

87
00:04:33,519 --> 00:04:36,659
movies so we might have more chance of

88
00:04:34,930 --> 00:04:39,850
recognizing the movies we're looking at

89
00:04:36,660 --> 00:04:43,270
and then I'll go ahead and change it

90
00:04:39,850 --> 00:04:46,330
from the movie IDs from movie lens to

91
00:04:43,269 --> 00:04:47,709
those unique IDs that we're using the

92
00:04:46,329 --> 00:04:57,339
contiguous IDs because that's what a

93
00:04:47,709 --> 00:05:00,789
model has alright so inside the learn

94
00:04:57,339 --> 00:05:03,129
object that we create inside alona we

95
00:05:00,790 --> 00:05:07,689
can always grab the PI torch model

96
00:05:03,129 --> 00:05:09,790
itself just by saying learn model okay

97
00:05:07,689 --> 00:05:12,819
and like I'm going to kind of show you

98
00:05:09,790 --> 00:05:14,680
more and more of the code at the moment

99
00:05:12,819 --> 00:05:20,168
so let's take a look at the definition

100
00:05:14,680 --> 00:05:21,699
of model and so a model is a property so

101
00:05:20,168 --> 00:05:23,829
if you haven't seen a property before a

102
00:05:21,699 --> 00:05:26,978
property is just something in Python

103
00:05:23,829 --> 00:05:29,490
which looks like a method when you

104
00:05:26,978 --> 00:05:32,800
define it that you can call it without

105
00:05:29,490 --> 00:05:34,689
parentheses as we do here alright and so

106
00:05:32,800 --> 00:05:36,520
it kind of looks when you call it like

107
00:05:34,689 --> 00:05:37,959
it's a regular attribute but it looks

108
00:05:36,519 --> 00:05:40,240
like when you define it like it's a

109
00:05:37,959 --> 00:05:42,668
method so every time you call it it

110
00:05:40,240 --> 00:05:44,379
actually runs this code okay and so in

111
00:05:42,668 --> 00:05:48,788
this case it's just a shortcut to grab

112
00:05:44,379 --> 00:05:49,899
something called dot models model so you

113
00:05:48,788 --> 00:05:50,459
may be interested to know what that

114
00:05:49,899 --> 00:05:54,939
looks like

115
00:05:50,459 --> 00:05:59,978
learn about models and so this is

116
00:05:54,939 --> 00:06:03,310
there's a fast AI model type is a very

117
00:05:59,978 --> 00:06:06,159
thin wrapper for pite watch models so we

118
00:06:03,310 --> 00:06:13,689
could take a look at this code filter

119
00:06:06,160 --> 00:06:19,720
model and see what that is it's only one

120
00:06:13,689 --> 00:06:21,819
line of code okay and yeah we'll talk

121
00:06:19,720 --> 00:06:23,770
more about these in part two right but

122
00:06:21,819 --> 00:06:25,209
basically that there's this very thin

123
00:06:23,769 --> 00:06:26,829
wrapper and the main thing one of the

124
00:06:25,209 --> 00:06:28,689
main things that fast i out does is we

125
00:06:26,829 --> 00:06:30,250
have this concept of layer groups where

126
00:06:28,689 --> 00:06:31,839
basically when you say here though

127
00:06:30,250 --> 00:06:33,100
different learning rates and they're

128
00:06:31,839 --> 00:06:35,019
going to apply two different sets of

129
00:06:33,100 --> 00:06:36,970
layers and that's something that's not

130
00:06:35,019 --> 00:06:39,069
in paid watch so when you say I want to

131
00:06:36,970 --> 00:06:41,230
use this PI torch model

132
00:06:39,069 --> 00:06:42,819
all this with one thing we have to do

133
00:06:41,230 --> 00:06:44,500
which is to say like okay one hour later

134
00:06:42,819 --> 00:06:47,259
groups yeah so the details aren't

135
00:06:44,500 --> 00:06:49,600
terribly important but in general if you

136
00:06:47,259 --> 00:06:51,399
want to create a little wrapper for some

137
00:06:49,600 --> 00:06:56,590
other pipe watch model you could just

138
00:06:51,399 --> 00:06:58,599
write something like this so to get to

139
00:06:56,589 --> 00:07:02,049
get inside that to grab the actual PI

140
00:06:58,600 --> 00:07:04,720
torch model itself its models dot model

141
00:07:02,050 --> 00:07:07,389
that's the PI torch model and then the

142
00:07:04,720 --> 00:07:11,020
learn object has a shortcut to that okay

143
00:07:07,389 --> 00:07:14,349
so we're going to set m to be the PI

144
00:07:11,019 --> 00:07:16,000
torch model and so when you print out a

145
00:07:14,350 --> 00:07:19,000
pipe watch model it prints it out

146
00:07:16,000 --> 00:07:22,060
basically by listing out all of the

147
00:07:19,000 --> 00:07:24,730
layers that you created in the

148
00:07:22,060 --> 00:07:26,319
constructor it's quite it's quite nifty

149
00:07:24,730 --> 00:07:28,900
actually when you kind of think about

150
00:07:26,319 --> 00:07:32,560
the way this works thanks to kind of

151
00:07:28,899 --> 00:07:34,899
some very handy stuff in Python we're

152
00:07:32,560 --> 00:07:39,459
actually able to use standard - oh wow

153
00:07:34,899 --> 00:07:41,799
to kind of define these modules in these

154
00:07:39,459 --> 00:07:44,109
layers and they basically automatically

155
00:07:41,800 --> 00:07:48,040
kind of register themselves with pipe

156
00:07:44,110 --> 00:07:49,480
which so back in our embedding bias we

157
00:07:48,040 --> 00:07:52,510
just had a bunch of things where we said

158
00:07:49,480 --> 00:07:54,850
okay each of these things are equal to

159
00:07:52,509 --> 00:07:57,819
these things and then it automatically

160
00:07:54,850 --> 00:08:00,640
knows how to represent that so you can

161
00:07:57,819 --> 00:08:02,170
see there's the name is you and so the

162
00:08:00,639 --> 00:08:05,289
name is just literally whatever we

163
00:08:02,170 --> 00:08:07,960
called it yeah you

164
00:08:05,290 --> 00:08:12,910
and then the definition is it's this

165
00:08:07,959 --> 00:08:18,310
kind of layer okay so that's our height

166
00:08:12,910 --> 00:08:21,490
watch model so we can look inside that

167
00:08:18,310 --> 00:08:24,629
basically use that so if we say m dot I

168
00:08:21,490 --> 00:08:28,660
be then that's referring to the

169
00:08:24,629 --> 00:08:31,149
embedding layer for an item which is the

170
00:08:28,660 --> 00:08:34,000
bias layer so an item bias in this case

171
00:08:31,149 --> 00:08:38,199
is the movie bias so each move either a

172
00:08:34,000 --> 00:08:40,990
9000 of them has a single bias element

173
00:08:38,200 --> 00:08:45,610
okay now the really nice thing about

174
00:08:40,990 --> 00:08:48,129
high torch layers and models is that

175
00:08:45,610 --> 00:08:50,320
they all look the same they basically

176
00:08:48,129 --> 00:08:50,710
got to use them you call them as if they

177
00:08:50,320 --> 00:08:54,190
were

178
00:08:50,710 --> 00:08:57,340
action so we can go m.i.b parenthesis

179
00:08:54,190 --> 00:09:00,820
right and that basically says I want you

180
00:08:57,340 --> 00:09:03,160
to return the value of that layer and

181
00:09:00,820 --> 00:09:06,220
that layer could be a full-on model

182
00:09:03,159 --> 00:09:08,199
right so to actually get a prediction

183
00:09:06,220 --> 00:09:12,519
from a play torch model you just I would

184
00:09:08,200 --> 00:09:17,500
go m and pass in my variable okay and so

185
00:09:12,519 --> 00:09:22,569
in this case my B and pass in my top

186
00:09:17,500 --> 00:09:26,980
movie indexes now models remember layers

187
00:09:22,570 --> 00:09:29,140
they require variables not tensors

188
00:09:26,980 --> 00:09:31,899
because it needs to keep track of the

189
00:09:29,139 --> 00:09:34,720
derivatives okay and so we use this

190
00:09:31,899 --> 00:09:37,870
capital V to turn the tensor into a

191
00:09:34,720 --> 00:09:42,399
variable and was just announced this

192
00:09:37,870 --> 00:09:44,259
week that PI torch 0.4 which is the

193
00:09:42,399 --> 00:09:46,600
version after the one that's just about

194
00:09:44,259 --> 00:09:49,330
to be released is going to get rid of

195
00:09:46,600 --> 00:09:51,610
variables and will actually be able to

196
00:09:49,330 --> 00:09:53,170
use tensors directly to keep track of

197
00:09:51,610 --> 00:09:55,269
derivatives so if you're watching this

198
00:09:53,169 --> 00:09:57,009
on the MOOC and you're looking at point

199
00:09:55,269 --> 00:09:58,539
four then you'll probably notice that

200
00:09:57,009 --> 00:09:59,610
the code doesn't have this V unit

201
00:09:58,539 --> 00:10:02,409
anymore

202
00:09:59,610 --> 00:10:04,090
and so that would be pretty exciting

203
00:10:02,409 --> 00:10:05,049
when that happens but for now we have to

204
00:10:04,090 --> 00:10:06,820
remember if we're going to pass

205
00:10:05,049 --> 00:10:09,099
something into a model to turn it into a

206
00:10:06,820 --> 00:10:12,730
variable first and remember a variable

207
00:10:09,100 --> 00:10:14,110
has a strict superset of the API of a

208
00:10:12,730 --> 00:10:16,750
tensor so anything you can do to a

209
00:10:14,110 --> 00:10:20,139
tensor you can do to a variable and it

210
00:10:16,750 --> 00:10:23,259
up will take its log or whatever okay so

211
00:10:20,139 --> 00:10:24,730
that's going to return a variable which

212
00:10:23,259 --> 00:10:26,919
consists of going through each of these

213
00:10:24,730 --> 00:10:30,909
movie IDs putting it through this

214
00:10:26,919 --> 00:10:36,429
embedding layer to get its bias okay and

215
00:10:30,909 --> 00:10:38,819
that's going to return a variable let's

216
00:10:36,429 --> 00:10:38,819
take a look

217
00:10:41,789 --> 00:10:47,679
so before I press shift down to here you

218
00:10:46,539 --> 00:10:50,409
can have a think about what I'm going to

219
00:10:47,679 --> 00:10:53,199
have I've got a list of 3,000 movies

220
00:10:50,409 --> 00:10:55,839
going in turning into variable putting

221
00:10:53,200 --> 00:10:56,950
it through this embedding layer so just

222
00:10:55,840 --> 00:11:01,240
have a think about what we expect to

223
00:10:56,950 --> 00:11:03,580
come out okay and we have a variable of

224
00:11:01,240 --> 00:11:04,720
size 3,000 by one hopefully that doesn't

225
00:11:03,580 --> 00:11:06,550
surprise you

226
00:11:04,720 --> 00:11:08,830
we had 3000 movies that we are looking

227
00:11:06,549 --> 00:11:11,379
up each one hadn't had a one long

228
00:11:08,830 --> 00:11:12,940
embedding okay so there's our three

229
00:11:11,379 --> 00:11:15,220
thousand one you'll notice it's a

230
00:11:12,940 --> 00:11:16,510
variable just not surprising because we

231
00:11:15,220 --> 00:11:18,639
fed it a variable so we've got a

232
00:11:16,509 --> 00:11:24,039
variable back and it's a variable that's

233
00:11:18,639 --> 00:11:26,230
on the GPU right doc CUDA okay so we

234
00:11:24,039 --> 00:11:28,539
have a little shortcut in fast AI

235
00:11:26,230 --> 00:11:31,000
because we we very often when I take

236
00:11:28,539 --> 00:11:33,189
variables turn them into tensors and

237
00:11:31,000 --> 00:11:33,909
move them back to the CPU so we can play

238
00:11:33,190 --> 00:11:38,110
with them more easily

239
00:11:33,909 --> 00:11:39,669
so two NP is is two numpy okay and that

240
00:11:38,110 --> 00:11:41,529
does all of those things and it works

241
00:11:39,669 --> 00:11:43,449
regardless of whether it's a tensor or a

242
00:11:41,529 --> 00:11:45,789
variable it works regardless of whether

243
00:11:43,450 --> 00:11:49,150
it's on the CPU or GPU it'll end up

244
00:11:45,789 --> 00:11:53,139
giving you a a numpy array from that

245
00:11:49,149 --> 00:11:54,669
okay so if we do that that gives us

246
00:11:53,139 --> 00:11:58,929
exactly the same thing as we just looked

247
00:11:54,669 --> 00:12:01,539
at but now in numpy form okay so that's

248
00:11:58,929 --> 00:12:04,179
a super handy thing to use when you're

249
00:12:01,539 --> 00:12:09,579
playing around with pi torch my approach

250
00:12:04,179 --> 00:12:12,939
to things is I try to use numpy for

251
00:12:09,580 --> 00:12:14,470
everything except when I explicit and

252
00:12:12,940 --> 00:12:17,170
you need something to run on the GPU or

253
00:12:14,470 --> 00:12:19,330
I need its derivatives right in which

254
00:12:17,169 --> 00:12:22,209
case I use PI torch because like none

255
00:12:19,330 --> 00:12:24,220
part like I kind of find none PI's often

256
00:12:22,210 --> 00:12:29,290
easier to work with it's been around

257
00:12:24,220 --> 00:12:32,620
many years longer than PI torch so you

258
00:12:29,289 --> 00:12:35,529
know and lots of things like the Python

259
00:12:32,620 --> 00:12:39,700
imaging library OpenCV and lots and lots

260
00:12:35,529 --> 00:12:41,529
of stuff like pandas it works with numpy

261
00:12:39,700 --> 00:12:45,460
so my approach is kind of like do as

262
00:12:41,529 --> 00:12:46,629
much as I can in num pile and finally

263
00:12:45,460 --> 00:12:49,120
when I'm ready to do something on the

264
00:12:46,629 --> 00:12:51,338
GPU or take its derivative to PI torch

265
00:12:49,120 --> 00:12:53,409
and then as soon as I can I put it back

266
00:12:51,339 --> 00:12:55,990
in vampire and you'll see that the first

267
00:12:53,409 --> 00:12:57,969
AI library really works this way like

268
00:12:55,990 --> 00:13:00,370
all the transformations and stuff happen

269
00:12:57,970 --> 00:13:03,430
in lamb pie which is different to most

270
00:13:00,370 --> 00:13:05,830
high torch computer vision libraries

271
00:13:03,429 --> 00:13:07,929
which tend to do it all as much as

272
00:13:05,830 --> 00:13:13,780
possible in pi torch I try to do as much

273
00:13:07,929 --> 00:13:15,909
as possible in non pipe so let's say we

274
00:13:13,779 --> 00:13:17,949
wanted to transfer build a model in the

275
00:13:15,909 --> 00:13:21,069
GPU with the GPU and train it

276
00:13:17,950 --> 00:13:24,580
then we want to bring this to production

277
00:13:21,070 --> 00:13:26,500
so would we call to numpy on the model

278
00:13:24,580 --> 00:13:28,330
itself or would we have to iterate

279
00:13:26,500 --> 00:13:31,659
through all the different layers and

280
00:13:28,330 --> 00:13:33,310
then call to NP yeah good question so

281
00:13:31,659 --> 00:13:36,309
it's very likely that you want to do

282
00:13:33,309 --> 00:13:38,229
inference on a cpu rather than a GPU

283
00:13:36,309 --> 00:13:39,969
it's it's more scalable you don't have

284
00:13:38,230 --> 00:13:44,009
to worry about putting things in batches

285
00:13:39,970 --> 00:13:48,220
you know and so forth so you can move a

286
00:13:44,009 --> 00:13:52,870
model onto the cpu just by typing m dot

287
00:13:48,220 --> 00:13:55,660
CPU and that model is now on the cpu and

288
00:13:52,870 --> 00:13:59,169
so therefore you can also then put your

289
00:13:55,659 --> 00:14:05,829
variable on the CPU by doing exactly the

290
00:13:59,169 --> 00:14:08,169
same thing so you can say like so now

291
00:14:05,830 --> 00:14:10,650
having said that if you're if you'll

292
00:14:08,169 --> 00:14:12,879
serve it doesn't have a GPU or CUDA GPU

293
00:14:10,649 --> 00:14:17,289
you don't have to do this because it

294
00:14:12,879 --> 00:14:19,389
won't put it on the GPU at all so if for

295
00:14:17,289 --> 00:14:22,539
inferencing on the server if you're

296
00:14:19,389 --> 00:14:24,460
running it on you know some t2 instance

297
00:14:22,539 --> 00:14:27,299
or something it'll work fine and will

298
00:14:24,460 --> 00:14:30,759
run on the on the cpu automatically

299
00:14:27,299 --> 00:14:33,849
quick follow-up and if we train the

300
00:14:30,759 --> 00:14:37,840
model on the GPU and then we save those

301
00:14:33,850 --> 00:14:40,180
embeddings and the weights would we have

302
00:14:37,840 --> 00:14:45,040
to do anything special to load you know

303
00:14:40,179 --> 00:14:46,989
you won't we have something well it kind

304
00:14:45,039 --> 00:14:49,659
of depends how much of faster I you're

305
00:14:46,990 --> 00:14:51,990
using so I'll show you how you can do

306
00:14:49,659 --> 00:14:54,159
that in case you have to do it manually

307
00:14:51,990 --> 00:15:02,169
one of the students figure this out

308
00:14:54,159 --> 00:15:05,169
which is really handy when we there's a

309
00:15:02,169 --> 00:15:07,569
load model function and you'll see what

310
00:15:05,169 --> 00:15:09,429
it does but it does torch dot load is it

311
00:15:07,570 --> 00:15:12,250
basically this is like some magic

312
00:15:09,429 --> 00:15:14,139
incantation that like normally it has to

313
00:15:12,250 --> 00:15:16,299
load it onto the same GPU or saved on

314
00:15:14,139 --> 00:15:19,439
but this will like load it into what it

315
00:15:16,299 --> 00:15:24,159
was what it is available so there's a

316
00:15:19,440 --> 00:15:27,960
Andy discovery thanks for the great

317
00:15:24,159 --> 00:15:27,959
questions and

318
00:15:28,818 --> 00:15:36,618
to put that back on the GPU I'll need to

319
00:15:32,869 --> 00:15:41,600
say doc CUDA and now there we go I can

320
00:15:36,619 --> 00:15:44,298
run it again okay so it's really

321
00:15:41,600 --> 00:15:46,939
important to know about the zip function

322
00:15:44,298 --> 00:15:50,509
in Python which iterates through a

323
00:15:46,938 --> 00:15:52,808
number of lists at the same time so in

324
00:15:50,509 --> 00:15:56,178
this case I want to grab each movie

325
00:15:52,808 --> 00:15:58,278
along with its bias term so that I can

326
00:15:56,178 --> 00:16:00,259
just pop it into our list of tuples so

327
00:15:58,278 --> 00:16:02,869
if I just go zip like that that's going

328
00:16:00,259 --> 00:16:06,318
to iterate through each movie ID and

329
00:16:02,869 --> 00:16:08,629
each bias term and so then I can use

330
00:16:06,318 --> 00:16:11,288
that in a list comprehension to grab the

331
00:16:08,629 --> 00:16:16,850
name of each movie along with its place

332
00:16:11,288 --> 00:16:20,239
okay so having done that I can then sort

333
00:16:16,850 --> 00:16:24,259
and so here are I told you that John

334
00:16:20,239 --> 00:16:26,720
John Travolta Scientology movie at the

335
00:16:24,259 --> 00:16:28,759
most negative of the quiet by a lot if

336
00:16:26,720 --> 00:16:30,798
this was a cable competition Battlefield

337
00:16:28,759 --> 00:16:34,418
Earth would have like won by miles or

338
00:16:30,798 --> 00:16:36,379
this seven seven seven ninety six so

339
00:16:34,418 --> 00:16:39,019
here's the worst movie of all time

340
00:16:36,379 --> 00:16:40,938
according to IMDB and like it's

341
00:16:39,019 --> 00:16:42,438
interesting when you think about what

342
00:16:40,938 --> 00:16:44,328
this means right because this is like a

343
00:16:42,438 --> 00:16:47,418
much more authentic way to find out how

344
00:16:44,328 --> 00:16:49,458
bad this movie is because like some

345
00:16:47,418 --> 00:16:51,588
people are just more negative about

346
00:16:49,458 --> 00:16:53,868
movies right and like it more of them

347
00:16:51,589 --> 00:16:55,489
watch your movie like you know highly

348
00:16:53,869 --> 00:16:57,319
critical audience they're gonna read it

349
00:16:55,489 --> 00:17:02,149
badly so if you take an average it's not

350
00:16:57,318 --> 00:17:04,308
quite fair right and so what this is you

351
00:17:02,149 --> 00:17:06,470
know what this is doing is saying once

352
00:17:04,308 --> 00:17:08,689
we you know remove the fact that

353
00:17:06,470 --> 00:17:10,399
different people have different overall

354
00:17:08,689 --> 00:17:11,689
positive or negative experiences and

355
00:17:10,398 --> 00:17:13,489
different people watch different kinds

356
00:17:11,689 --> 00:17:17,269
of movies and we correct for all that

357
00:17:13,490 --> 00:17:20,169
this is the worst movie of all time so

358
00:17:17,269 --> 00:17:20,169
that's a good thing to know

359
00:17:21,638 --> 00:17:29,359
so this is how we can yeah look inside

360
00:17:24,828 --> 00:17:33,069
our our model and and interpret the bias

361
00:17:29,359 --> 00:17:36,558
vectors you'll see here I've sorted by

362
00:17:33,069 --> 00:17:39,528
the zeroth element of each tuple by

363
00:17:36,558 --> 00:17:42,680
using a lambda originally I used this

364
00:17:39,528 --> 00:17:45,559
special item ghetto this is part

365
00:17:42,680 --> 00:17:47,450
of pythons operator library and this

366
00:17:45,559 --> 00:17:50,299
creates a function that returns the

367
00:17:47,450 --> 00:17:52,549
zeroth element of something in order to

368
00:17:50,299 --> 00:17:54,829
save time and then I actually realize

369
00:17:52,549 --> 00:17:57,349
that the lambda is only one more

370
00:17:54,829 --> 00:17:58,669
character to write then the item get us

371
00:17:57,349 --> 00:18:01,789
so maybe we don't need to know this

372
00:17:58,670 --> 00:18:03,980
after all so yeah really useful to make

373
00:18:01,789 --> 00:18:06,289
sure you know how to write lambdas in

374
00:18:03,980 --> 00:18:09,740
Python so this is this is a function

375
00:18:06,289 --> 00:18:11,539
okay and so sort the sort is going to

376
00:18:09,740 --> 00:18:13,309
call this function every time it decides

377
00:18:11,539 --> 00:18:15,559
like is this thing higher or lower than

378
00:18:13,309 --> 00:18:19,059
that other thing and this fact this is

379
00:18:15,559 --> 00:18:21,409
going to return the zeroth element okay

380
00:18:19,059 --> 00:18:25,899
so here's the same thing and item get a

381
00:18:21,410 --> 00:18:27,890
format and here is the reverse and

382
00:18:25,900 --> 00:18:29,000
Shawshank Redemption right at the top

383
00:18:27,890 --> 00:18:31,340
I'll definitely agree with that

384
00:18:29,000 --> 00:18:35,119
Godfather usual suspects yeah these are

385
00:18:31,339 --> 00:18:39,889
all pretty great movies twelve Angry Men

386
00:18:35,119 --> 00:18:43,099
absolutely so there you go there's how

387
00:18:39,890 --> 00:18:45,470
we can look at the base so then the

388
00:18:43,099 --> 00:18:47,329
second piece to look at would be the the

389
00:18:45,470 --> 00:18:50,269
embeddings how can we look at the

390
00:18:47,329 --> 00:18:53,179
embeddings so we can do the same thing

391
00:18:50,269 --> 00:18:55,940
so remember I was the item embeddings

392
00:18:53,180 --> 00:18:58,029
rather than IV with the item bias we can

393
00:18:55,940 --> 00:19:01,340
pass in our list of movies as a variable

394
00:18:58,029 --> 00:19:04,700
turn it into numpy and here's our movie

395
00:19:01,339 --> 00:19:07,699
embedding so for each of the 3,000 most

396
00:19:04,700 --> 00:19:12,740
popular movies here are its 50

397
00:19:07,700 --> 00:19:14,600
embeddings so it's very hard unless

398
00:19:12,740 --> 00:19:17,329
you're Geoffrey Hinton to visualize a 50

399
00:19:14,599 --> 00:19:19,309
dimensional space so what we'll do is

400
00:19:17,329 --> 00:19:23,480
we'll turn it into a three dimensional

401
00:19:19,309 --> 00:19:25,220
space so we can compress high

402
00:19:23,480 --> 00:19:26,690
dimensional spaces down into lower

403
00:19:25,220 --> 00:19:29,240
dimensional spaces using lots of

404
00:19:26,690 --> 00:19:31,720
different techniques perhaps one of the

405
00:19:29,240 --> 00:19:34,039
most common and popular is called PCA

406
00:19:31,720 --> 00:19:38,380
PCA stands for principle components

407
00:19:34,039 --> 00:19:41,299
analysis it's a linear technique but

408
00:19:38,380 --> 00:19:44,870
when your techniques generally work fine

409
00:19:41,299 --> 00:19:46,460
for this kind of embedding I'm not going

410
00:19:44,869 --> 00:19:48,889
to teach you about PCA now but I will

411
00:19:46,460 --> 00:19:50,809
say in Rachel's computation or linear

412
00:19:48,890 --> 00:19:55,610
algebra class which you can get to you

413
00:19:50,809 --> 00:19:58,309
from first at AI we cover PCA in

414
00:19:55,609 --> 00:19:59,689
detail and it's a really important

415
00:19:58,309 --> 00:20:02,029
technique it actually it turns out to be

416
00:19:59,690 --> 00:20:04,670
almost identical to something called

417
00:20:02,029 --> 00:20:06,789
singular value decomposition which is a

418
00:20:04,670 --> 00:20:10,370
type of matrix decomposition which

419
00:20:06,789 --> 00:20:13,879
actually does turn up in deep learning a

420
00:20:10,369 --> 00:20:16,789
little bit from time to time it's kind

421
00:20:13,880 --> 00:20:18,740
of somewhat worth knowing if you were

422
00:20:16,789 --> 00:20:22,399
going to dig more into linear algebra

423
00:20:18,740 --> 00:20:24,440
you know SPD and PCA along with

424
00:20:22,400 --> 00:20:26,000
eigenvalues and eigenvectors which are

425
00:20:24,440 --> 00:20:28,100
all slightly different versions is this

426
00:20:26,000 --> 00:20:30,769
kind of the same thing or all worth

427
00:20:28,099 --> 00:20:33,649
knowing but for now just know that you

428
00:20:30,769 --> 00:20:36,829
can grab PCA from SK learn to calm

429
00:20:33,650 --> 00:20:39,200
position say how much you want to reduce

430
00:20:36,829 --> 00:20:41,539
the dimensionality too so I want to find

431
00:20:39,200 --> 00:20:44,690
three components and what this is going

432
00:20:41,539 --> 00:20:48,379
to do is it's going to find three linear

433
00:20:44,690 --> 00:20:50,150
combinations of the 50 dimensions which

434
00:20:48,380 --> 00:20:52,280
capture as much as the variation as

435
00:20:50,150 --> 00:20:53,690
possible Badar is different to each

436
00:20:52,279 --> 00:20:58,119
other as possible

437
00:20:53,690 --> 00:21:02,269
okay so we would call this a lower rank

438
00:20:58,119 --> 00:21:04,969
approximation of our matrix all right

439
00:21:02,269 --> 00:21:06,490
so then we can grab the components so

440
00:21:04,970 --> 00:21:08,390
that's going to be their three

441
00:21:06,490 --> 00:21:11,620
dimensions and so once we've done that

442
00:21:08,390 --> 00:21:15,320
we've now got three by three thousand

443
00:21:11,619 --> 00:21:16,819
and so we can now take a look at the

444
00:21:15,319 --> 00:21:18,980
first of them and we'll do the same

445
00:21:16,819 --> 00:21:21,470
thing of using zip to look at each one

446
00:21:18,980 --> 00:21:24,349
along with its movie and so here's the

447
00:21:21,470 --> 00:21:28,430
thing right we we don't know ahead of

448
00:21:24,349 --> 00:21:33,649
time what this PCA thing is it's just

449
00:21:28,430 --> 00:21:37,310
it's just a bunch of latent factors you

450
00:21:33,650 --> 00:21:39,650
know it's it's kind of the the main axis

451
00:21:37,309 --> 00:21:41,679
in this space of latent factors and so

452
00:21:39,650 --> 00:21:45,290
what we can do is we can look at it and

453
00:21:41,680 --> 00:21:49,549
see if we can figure out what it's about

454
00:21:45,289 --> 00:21:51,940
right so given that police academy for

455
00:21:49,549 --> 00:21:54,799
is high up here along with water world

456
00:21:51,940 --> 00:21:57,769
where else Fargo Pulp Fiction and God

457
00:21:54,799 --> 00:22:00,200
further a high up here I'm gonna guess

458
00:21:57,769 --> 00:22:02,269
that a high value is not going to

459
00:22:00,200 --> 00:22:06,200
represent like critically acclaimed

460
00:22:02,269 --> 00:22:08,509
movies or serious watching so I kind of

461
00:22:06,200 --> 00:22:09,279
like all this yeah okay I call this easy

462
00:22:08,509 --> 00:22:11,769
what she

463
00:22:09,279 --> 00:22:13,450
is serious all right but like this is

464
00:22:11,769 --> 00:22:15,460
kind of how you have to interpret your

465
00:22:13,450 --> 00:22:18,910
embeddings is like take a look at what

466
00:22:15,460 --> 00:22:20,410
they seem to be showing and decide what

467
00:22:18,910 --> 00:22:25,060
you think it means so this is the kind

468
00:22:20,410 --> 00:22:27,910
of the the principal axis in this set of

469
00:22:25,059 --> 00:22:30,579
embedding so we can look at the next one

470
00:22:27,910 --> 00:22:34,000
so do the same thing and look at the the

471
00:22:30,579 --> 00:22:35,589
first index one embedding this one's a

472
00:22:34,000 --> 00:22:37,029
little bit harder to kind of figure out

473
00:22:35,589 --> 00:22:38,919
what's going on but with things like

474
00:22:37,029 --> 00:22:42,309
Mulholland Drive and Purple Rose of

475
00:22:38,920 --> 00:22:45,009
Cairo these look more kind of dialog II

476
00:22:42,309 --> 00:22:46,480
kind of ones or else things like Lord of

477
00:22:45,009 --> 00:22:48,490
the Rings in the Latin and Star Wars

478
00:22:46,480 --> 00:22:50,259
these book more like kind of modern CGI

479
00:22:48,490 --> 00:22:53,259
II kind of ones so you could kind of

480
00:22:50,259 --> 00:22:57,000
imagine that on that pair of dimensions

481
00:22:53,259 --> 00:22:59,140
it probably represents a lot of you know

482
00:22:57,000 --> 00:23:02,440
differences between how people read

483
00:22:59,140 --> 00:23:05,290
movies you know some people like you

484
00:23:02,440 --> 00:23:09,070
know purple rise of Cairo

485
00:23:05,289 --> 00:23:11,139
type movies you know Woody Allen kind of

486
00:23:09,069 --> 00:23:15,339
classic and some people like these you

487
00:23:11,140 --> 00:23:17,530
know big Hollywood spectacles some

488
00:23:15,339 --> 00:23:22,809
people presumably like police academy

489
00:23:17,529 --> 00:23:24,279
for more than they like Fargo so yeah so

490
00:23:22,809 --> 00:23:26,379
I'm like you can kind of get the idea of

491
00:23:24,279 --> 00:23:33,879
what's happened it's it's done a you

492
00:23:26,380 --> 00:23:36,730
know through a model which was you know

493
00:23:33,880 --> 00:23:41,380
for a model which was literally multiply

494
00:23:36,730 --> 00:23:43,990
two things together and Adam hop it's

495
00:23:41,380 --> 00:23:49,300
learnt quite a lot you know which is

496
00:23:43,990 --> 00:23:53,109
kind of cool so that's what we can do

497
00:23:49,299 --> 00:23:55,809
with with that and then we could we

498
00:23:53,109 --> 00:24:00,369
could plot them if we wanted to I just

499
00:23:55,809 --> 00:24:04,509
grabbed a small subset to plot on those

500
00:24:00,369 --> 00:24:09,009
first two asses all right so that's that

501
00:24:04,509 --> 00:24:12,009
so I wanted to next kind of dig in a

502
00:24:09,009 --> 00:24:18,299
layer deeper into what actually happens

503
00:24:12,009 --> 00:24:23,700
when we say fit alright so when we said

504
00:24:18,299 --> 00:24:23,700
learn fit what's it doing

505
00:24:24,849 --> 00:24:30,789
for something like the store model is it

506
00:24:27,519 --> 00:24:33,099
a way to interpret the embeddings for

507
00:24:30,789 --> 00:24:35,230
something like this the rustman one yes

508
00:24:33,099 --> 00:24:36,579
yeah we'll see that in a moment well

509
00:24:35,230 --> 00:24:51,309
let's jump straight there what the hell

510
00:24:36,579 --> 00:24:52,929
okay so so for the rustman how much are

511
00:24:51,309 --> 00:25:04,269
we going to sell at each store on each

512
00:24:52,930 --> 00:25:06,310
date model we this is from the paper

513
00:25:04,269 --> 00:25:09,339
gore and burke on it so it's a great

514
00:25:06,309 --> 00:25:12,429
paper by the way well worth you know

515
00:25:09,339 --> 00:25:14,949
like pretty accessible I think any of

516
00:25:12,430 --> 00:25:16,810
you would at this point be able to at

517
00:25:14,950 --> 00:25:18,789
least get the gist of it if you know and

518
00:25:16,809 --> 00:25:20,379
much of the detail as well particularly

519
00:25:18,789 --> 00:25:23,259
as you've also done the machine learning

520
00:25:20,380 --> 00:25:25,500
course and they actually make this point

521
00:25:23,259 --> 00:25:28,089
in the paper this is in the paper that

522
00:25:25,500 --> 00:25:30,849
the equivalent of what they call entity

523
00:25:28,089 --> 00:25:33,369
embedding layers so an embedding of a

524
00:25:30,849 --> 00:25:37,599
categorical variable is identical to a

525
00:25:33,369 --> 00:25:39,549
one hot encoding followed by a matrix

526
00:25:37,599 --> 00:25:41,709
multiply that's why they're basically

527
00:25:39,549 --> 00:25:44,139
saying if you've got three embeddings

528
00:25:41,710 --> 00:25:45,700
that's the same as doing three one hot

529
00:25:44,140 --> 00:25:48,370
encodings putting each through one

530
00:25:45,700 --> 00:25:50,680
through a matrix multiply and then put

531
00:25:48,369 --> 00:25:53,500
that through a a dense layer

532
00:25:50,680 --> 00:25:56,039
well what pi torch would call a linear

533
00:25:53,500 --> 00:25:58,210
oh yeah right

534
00:25:56,039 --> 00:26:00,220
one of the nice things here is because

535
00:25:58,210 --> 00:26:01,870
this is kind of like well they thought

536
00:26:00,220 --> 00:26:03,730
it was the first paper is actually the

537
00:26:01,869 --> 00:26:06,759
second I think paper to show the idea of

538
00:26:03,730 --> 00:26:08,500
using categorical embeddings for this

539
00:26:06,759 --> 00:26:10,809
kind of data set they really go to clean

540
00:26:08,500 --> 00:26:13,509
too quite a lot of detail to you know

541
00:26:10,809 --> 00:26:15,009
right back to the the detailed stuff

542
00:26:13,509 --> 00:26:15,519
that we learnt about so it's kind of a

543
00:26:15,009 --> 00:26:17,049
second

544
00:26:15,519 --> 00:26:21,609
you know a second cat of thinking about

545
00:26:17,049 --> 00:26:23,710
what embeddings are doing so one of the

546
00:26:21,609 --> 00:26:25,419
interesting things that they did was

547
00:26:23,710 --> 00:26:31,299
they said okay after we've trained a

548
00:26:25,420 --> 00:26:34,930
neural net with these embeddings what

549
00:26:31,299 --> 00:26:37,899
else could we do with it so

550
00:26:34,930 --> 00:26:41,170
they got a winning result with a neural

551
00:26:37,900 --> 00:26:42,430
network where the entity meetings but

552
00:26:41,170 --> 00:26:45,070
then they said hey you know what

553
00:26:42,430 --> 00:26:48,070
we could take those empty embeddings and

554
00:26:45,069 --> 00:26:50,319
replace each categorical variable with

555
00:26:48,069 --> 00:26:54,309
the learnt entity embeddings and then

556
00:26:50,319 --> 00:26:56,169
feed that into a GBM right so in other

557
00:26:54,309 --> 00:26:58,839
words like rather than passing into the

558
00:26:56,170 --> 00:27:01,870
GBM a one modern coded version or an

559
00:26:58,839 --> 00:27:04,539
ordinal version let's actually replace

560
00:27:01,869 --> 00:27:08,259
the categorical variable with its

561
00:27:04,539 --> 00:27:11,399
embedding for the appropriate level for

562
00:27:08,259 --> 00:27:13,569
that row right so it's actually a way of

563
00:27:11,400 --> 00:27:18,250
create you know feature engineering and

564
00:27:13,569 --> 00:27:23,079
so the main average percent error

565
00:27:18,250 --> 00:27:25,960
without that for gbms I'm using just 100

566
00:27:23,079 --> 00:27:30,220
codings was 0.15 but with that it was

567
00:27:25,960 --> 00:27:34,180
0.11 that random forests without that

568
00:27:30,220 --> 00:27:37,690
was point one six with that 0.108 nearly

569
00:27:34,180 --> 00:27:39,220
as good as the neural net right so this

570
00:27:37,690 --> 00:27:42,029
is kind of an interesting technique

571
00:27:39,220 --> 00:27:45,190
because what it means is in your

572
00:27:42,029 --> 00:27:47,410
organization you can train a neural net

573
00:27:45,190 --> 00:27:50,049
that has an embedding of stores and an

574
00:27:47,410 --> 00:27:52,660
embedding of product types and an

575
00:27:50,049 --> 00:27:54,339
embedding of I don't know whatever kind

576
00:27:52,660 --> 00:27:56,110
of high cardinality or even medium

577
00:27:54,339 --> 00:27:58,149
cardinality categorical variables you

578
00:27:56,109 --> 00:28:00,639
have and then everybody else in the

579
00:27:58,150 --> 00:28:03,580
organization can now like chuck those

580
00:28:00,640 --> 00:28:07,210
into their you know JVM or random forest

581
00:28:03,579 --> 00:28:09,819
or whatever and I'm use them and what

582
00:28:07,210 --> 00:28:11,670
this is saying is they won't get in fact

583
00:28:09,819 --> 00:28:13,990
you can even use K nearest neighbors

584
00:28:11,670 --> 00:28:17,769
with this technique and get nearly as

585
00:28:13,990 --> 00:28:19,359
good a result right so this is a good

586
00:28:17,769 --> 00:28:21,369
way of kind of giving the power of

587
00:28:19,359 --> 00:28:24,089
neural nets to everybody in your

588
00:28:21,369 --> 00:28:26,469
organization without having them do the

589
00:28:24,089 --> 00:28:28,059
faster idea of learning course first you

590
00:28:26,470 --> 00:28:30,730
know they can just use whatever SK learn

591
00:28:28,059 --> 00:28:33,309
or R or whatever that they're used to

592
00:28:30,730 --> 00:28:35,740
and like those those embeddings could

593
00:28:33,309 --> 00:28:37,299
literally be in a database table because

594
00:28:35,740 --> 00:28:40,210
if you think about an embedding is just

595
00:28:37,299 --> 00:28:43,629
an index lookup right which is the same

596
00:28:40,210 --> 00:28:45,940
as an inner join in SQL right so if

597
00:28:43,630 --> 00:28:48,340
you've got a table on each product along

598
00:28:45,940 --> 00:28:48,830
with its embedding vector then you can

599
00:28:48,339 --> 00:28:51,829
literally do

600
00:28:48,829 --> 00:28:53,240
in a joint and now you have every row in

601
00:28:51,829 --> 00:28:55,879
your table along with its product

602
00:28:53,240 --> 00:28:59,500
embedding vector so that's a really this

603
00:28:55,880 --> 00:29:02,930
is this is a really useful idea and

604
00:28:59,500 --> 00:29:05,599
gbm's and random forests learn a lot

605
00:29:02,930 --> 00:29:07,400
quicker than neural nets do all right so

606
00:29:05,599 --> 00:29:08,649
that's like even if you do know how to

607
00:29:07,400 --> 00:29:12,530
train your on its this is still

608
00:29:08,650 --> 00:29:14,120
potentially quite handy so here's what

609
00:29:12,529 --> 00:29:17,299
happened when they took the various

610
00:29:14,119 --> 00:29:19,399
different states of Germany and plotted

611
00:29:17,299 --> 00:29:21,379
the first two principal components of

612
00:29:19,400 --> 00:29:23,420
their embedding vectors and they

613
00:29:21,380 --> 00:29:26,990
basically here is where they were in

614
00:29:23,420 --> 00:29:30,259
that 2d space and wacken lee enough i've

615
00:29:26,990 --> 00:29:31,880
circled in red three cities and i've

616
00:29:30,259 --> 00:29:34,789
circled here the three cities in Germany

617
00:29:31,880 --> 00:29:38,000
and here I've circled in purple so blue

618
00:29:34,789 --> 00:29:41,599
here at the blue here's the green here's

619
00:29:38,000 --> 00:29:44,660
the green so it's actually drawn a map

620
00:29:41,599 --> 00:29:47,869
of Germany even though it never was told

621
00:29:44,660 --> 00:29:49,730
anything about how far these states are

622
00:29:47,869 --> 00:29:52,459
away from each other or the very concept

623
00:29:49,730 --> 00:29:57,740
of geography didn't exist so that's

624
00:29:52,460 --> 00:30:01,610
pretty crazy so that was from there

625
00:29:57,740 --> 00:30:03,289
paper so I went ahead and looked well

626
00:30:01,609 --> 00:30:05,089
here's another thing I think this is

627
00:30:03,289 --> 00:30:10,190
also from their paper they took every

628
00:30:05,089 --> 00:30:13,579
pair of places and they looked at how

629
00:30:10,190 --> 00:30:15,980
far away they are on a map versus how

630
00:30:13,579 --> 00:30:19,699
far away are they in embedding space and

631
00:30:15,980 --> 00:30:23,240
they've got this beautiful correlation

632
00:30:19,700 --> 00:30:24,710
alright so again it kind of apparently

633
00:30:23,240 --> 00:30:30,339
you know it's doors that are near by

634
00:30:24,710 --> 00:30:32,779
each other physically have similar

635
00:30:30,339 --> 00:30:36,049
characteristics in terms of when people

636
00:30:32,779 --> 00:30:38,779
buy more or less stuff from them so I

637
00:30:36,049 --> 00:30:41,059
looked at the same thing four days of

638
00:30:38,779 --> 00:30:43,549
the week right so here's an embedding of

639
00:30:41,059 --> 00:30:45,259
the days of the week from our model and

640
00:30:43,549 --> 00:30:46,250
I just kind of joined up Monday Tuesday

641
00:30:45,259 --> 00:30:48,170
Wednesday Tuesday Thursday Friday

642
00:30:46,250 --> 00:30:51,200
Saturday Sunday I did the same thing for

643
00:30:48,170 --> 00:30:52,850
the months of the year all right again

644
00:30:51,200 --> 00:30:58,640
you can see you know here's here's

645
00:30:52,849 --> 00:31:01,119
winter here's summer so yeah I think

646
00:30:58,640 --> 00:31:03,100
like visualize

647
00:31:01,119 --> 00:31:06,069
embeddings can be interesting like it's

648
00:31:03,099 --> 00:31:08,319
good to like first of all check you can

649
00:31:06,069 --> 00:31:10,720
see things you would expect to see you

650
00:31:08,319 --> 00:31:12,579
know and then you could like try and see

651
00:31:10,720 --> 00:31:14,279
like maybe things you didn't expect to

652
00:31:12,579 --> 00:31:20,949
see so you could try all kinds of

653
00:31:14,279 --> 00:31:23,558
clusterings or or whatever and this is

654
00:31:20,950 --> 00:31:25,509
not something which has been widely

655
00:31:23,558 --> 00:31:27,099
studied at all right so I'm not going to

656
00:31:25,509 --> 00:31:32,259
tell you what the limitations are of

657
00:31:27,099 --> 00:31:34,209
this technique or whatever oh yes so

658
00:31:32,259 --> 00:31:37,210
I've heard of other ways to generate

659
00:31:34,210 --> 00:31:40,058
embeddings like skip grams uh-huh

660
00:31:37,210 --> 00:31:41,410
wondering if you could say is there one

661
00:31:40,058 --> 00:31:44,829
better than the other using your own

662
00:31:41,410 --> 00:31:49,679
Network sir skip grams so screwed grams

663
00:31:44,829 --> 00:31:49,678
is quite specific to NLP right so like

664
00:31:49,859 --> 00:31:56,409
I'm not sure if we'll cover it in this

665
00:31:51,940 --> 00:31:58,750
course but basically the the approach to

666
00:31:56,410 --> 00:32:01,600
original kind of word to vac approach to

667
00:31:58,750 --> 00:32:07,299
generating embeddings was to say you

668
00:32:01,599 --> 00:32:10,539
know what we actually don't have we

669
00:32:07,299 --> 00:32:12,399
don't actually have our labelled data

670
00:32:10,539 --> 00:32:15,399
set you know they said all we have is

671
00:32:12,400 --> 00:32:16,720
like google books and so they have an

672
00:32:15,400 --> 00:32:19,000
unsupervised learning problem

673
00:32:16,720 --> 00:32:21,190
unlabeled problem and so the best way in

674
00:32:19,000 --> 00:32:23,289
my opinion to turn an unlabeled problem

675
00:32:21,190 --> 00:32:25,570
into a labelled problem is to kind of

676
00:32:23,289 --> 00:32:27,700
invent some labels and so what they did

677
00:32:25,569 --> 00:32:30,519
in the word to vet case was they said

678
00:32:27,700 --> 00:32:33,490
okay here's a sentence with 11 words in

679
00:32:30,519 --> 00:32:39,569
it right and then they said okay let's

680
00:32:33,490 --> 00:32:44,500
delete the middle word and replace it

681
00:32:39,569 --> 00:32:48,159
for the random word and so you know

682
00:32:44,500 --> 00:32:53,500
originally it said cat and they say no

683
00:32:48,160 --> 00:32:55,960
let's replace that with justice all

684
00:32:53,500 --> 00:32:58,929
right so before it said the cute little

685
00:32:55,960 --> 00:33:01,360
cat sat on the fuzzy mat and now it says

686
00:32:58,929 --> 00:33:01,840
the cute little justice sat on the fuzzy

687
00:33:01,359 --> 00:33:03,969
man

688
00:33:01,839 --> 00:33:06,759
right and what they do is they do that

689
00:33:03,970 --> 00:33:10,170
so they have one sentence where they

690
00:33:06,759 --> 00:33:10,170
keep exactly as is

691
00:33:12,119 --> 00:33:16,709
and then they make a copy of it and they

692
00:33:14,609 --> 00:33:20,729
do the replacement and so then they have

693
00:33:16,710 --> 00:33:22,680
a label where they say it's a one if it

694
00:33:20,730 --> 00:33:27,210
was unchanged it was the original and

695
00:33:22,680 --> 00:33:29,279
zero otherwise okay and so basically

696
00:33:27,210 --> 00:33:32,279
then you now have something you can

697
00:33:29,279 --> 00:33:33,329
build a machine learning model on and so

698
00:33:32,279 --> 00:33:35,369
they went and build a machine learning

699
00:33:33,329 --> 00:33:40,799
model on this so the model was like try

700
00:33:35,369 --> 00:33:42,929
and find the effect sentences not

701
00:33:40,799 --> 00:33:44,909
because they were interested in a fake

702
00:33:42,930 --> 00:33:46,529
sentence binder but because as a result

703
00:33:44,910 --> 00:33:48,360
they now have embeddings that just like

704
00:33:46,529 --> 00:33:50,720
we discussed you can now use for other

705
00:33:48,359 --> 00:33:55,199
purposes and that became word to vet now

706
00:33:50,720 --> 00:33:58,289
it turns out that if you do this as just

707
00:33:55,200 --> 00:33:59,850
a kind of a effectively like a single

708
00:33:58,289 --> 00:34:01,859
matrix multiply rather than make it a

709
00:33:59,849 --> 00:34:05,519
deep neural net you can train this super

710
00:34:01,859 --> 00:34:06,719
quickly and so that's basically what

711
00:34:05,519 --> 00:34:08,460
they did with they'd met there though

712
00:34:06,720 --> 00:34:11,699
they kind of decided we're going to make

713
00:34:08,460 --> 00:34:13,760
a pretty crappy model like a shallow

714
00:34:11,699 --> 00:34:15,989
learning model rather than a deep model

715
00:34:13,760 --> 00:34:18,150
you know with the downside it's a less

716
00:34:15,989 --> 00:34:20,159
powerful model but a number of upsides

717
00:34:18,150 --> 00:34:22,860
the first thing we can train it on a

718
00:34:20,159 --> 00:34:24,480
really large data set and then also

719
00:34:22,860 --> 00:34:27,710
really importantly we're going to end up

720
00:34:24,480 --> 00:34:30,150
with embeddings which have really very

721
00:34:27,710 --> 00:34:32,190
linear characteristics so we can like

722
00:34:30,150 --> 00:34:37,380
add them together and subtract them and

723
00:34:32,190 --> 00:34:39,329
stuff like that okay so that so there's

724
00:34:37,380 --> 00:34:41,610
a lot of stuff we can learn about there

725
00:34:39,329 --> 00:34:44,250
from like for other types of embedding

726
00:34:41,610 --> 00:34:45,930
like categorical embeddings and

727
00:34:44,250 --> 00:34:48,809
specifically if we want categorical

728
00:34:45,929 --> 00:34:51,119
embeddings which we can kind of draw

729
00:34:48,809 --> 00:34:52,710
nicely and expect them to us to be able

730
00:34:51,119 --> 00:34:56,009
to add and subtract them and behave

731
00:34:52,710 --> 00:34:57,809
linearly you know probably if we want to

732
00:34:56,010 --> 00:35:00,330
use them in k-nearest neighbors and

733
00:34:57,809 --> 00:35:03,989
stuff we should probably use shallow

734
00:35:00,329 --> 00:35:06,000
learning if we want something that's

735
00:35:03,989 --> 00:35:09,299
going to be more predictive we probably

736
00:35:06,000 --> 00:35:14,670
want to use a neural net and so actually

737
00:35:09,300 --> 00:35:17,130
an NLP I'm really pushing the idea that

738
00:35:14,670 --> 00:35:19,440
we need to move past word to backhand

739
00:35:17,130 --> 00:35:21,960
glove these linear based methods because

740
00:35:19,440 --> 00:35:24,450
it turns out that those embeddings are

741
00:35:21,960 --> 00:35:25,260
way less predictive than embeddings

742
00:35:24,449 --> 00:35:27,899
learnt from

743
00:35:25,260 --> 00:35:29,220
models and so the language model that we

744
00:35:27,900 --> 00:35:30,750
learned about which ended up getting a

745
00:35:29,219 --> 00:35:33,209
state-of-the-art on sentiment analysis

746
00:35:30,750 --> 00:35:35,789
didn't used a lot more work to vet that

747
00:35:33,210 --> 00:35:38,220
instead we pre trained a deep recurrent

748
00:35:35,789 --> 00:35:40,469
neural network and we ended up with not

749
00:35:38,219 --> 00:35:46,349
just a pre trained word vectors but a

750
00:35:40,469 --> 00:35:48,869
for pre-trained model so it looks like

751
00:35:46,349 --> 00:35:51,569
Duke creates embeddings for entities we

752
00:35:48,869 --> 00:35:53,250
need like a dummy task not necessarily a

753
00:35:51,570 --> 00:35:55,140
dummy task like in this case we had a

754
00:35:53,250 --> 00:35:56,820
real task right so we created the

755
00:35:55,139 --> 00:36:01,259
embeddings for Rossmann by trying to

756
00:35:56,820 --> 00:36:02,900
predict store sales you only need this

757
00:36:01,260 --> 00:36:05,850
isn't just in this isn't just for

758
00:36:02,900 --> 00:36:09,869
learning embeddings for learning any

759
00:36:05,849 --> 00:36:14,549
kind of feature space you either need

760
00:36:09,869 --> 00:36:16,199
label data or you need to invent some

761
00:36:14,550 --> 00:36:18,960
kind of fake task

762
00:36:16,199 --> 00:36:20,639
so does that task matter like if I

763
00:36:18,960 --> 00:36:22,050
choose a task and train and lettings if

764
00:36:20,639 --> 00:36:26,549
I choose another task and train and

765
00:36:22,050 --> 00:36:28,380
lettings like which one is it's a great

766
00:36:26,550 --> 00:36:31,050
question and it's not something that's

767
00:36:28,380 --> 00:36:32,730
been studied nearly enough right I'm not

768
00:36:31,050 --> 00:36:34,850
sure that many people even quite

769
00:36:32,730 --> 00:36:37,500
understand that when they say

770
00:36:34,849 --> 00:36:40,799
unsupervised learning now about nowadays

771
00:36:37,500 --> 00:36:45,030
they almost nearly always mean fake

772
00:36:40,800 --> 00:36:48,300
tasks labeled learning and so the idea

773
00:36:45,030 --> 00:36:49,650
of like what makes a good fake task I

774
00:36:48,300 --> 00:36:54,210
don't know that I've seen a paper on

775
00:36:49,650 --> 00:36:56,780
that right that intuitively you know we

776
00:36:54,210 --> 00:36:59,849
need something where the kinds of

777
00:36:56,780 --> 00:37:01,500
relationships it's going to learn likely

778
00:36:59,849 --> 00:37:05,130
to be the kinds of relationships that

779
00:37:01,500 --> 00:37:09,929
you probably care about right so for

780
00:37:05,130 --> 00:37:13,820
example in in computer vision one kind

781
00:37:09,929 --> 00:37:16,769
of fake task people use is to say like

782
00:37:13,820 --> 00:37:20,010
let's take some images and use some kind

783
00:37:16,769 --> 00:37:23,190
of like unreal and unreasonable data

784
00:37:20,010 --> 00:37:25,470
augmentation like like recolor them too

785
00:37:23,190 --> 00:37:27,389
much or whatever and then we'll ask the

786
00:37:25,469 --> 00:37:29,129
neural net to like predict which one was

787
00:37:27,389 --> 00:37:37,199
the Augmented which one was not you

788
00:37:29,130 --> 00:37:38,519
admitted yeah so it's I think it's a

789
00:37:37,199 --> 00:37:41,099
fascinating area

790
00:37:38,519 --> 00:37:43,139
one which you know would be really

791
00:37:41,099 --> 00:37:44,130
interesting for people to you know maybe

792
00:37:43,139 --> 00:37:45,868
some of the students here they're

793
00:37:44,130 --> 00:37:47,309
looking to further it's like take some

794
00:37:45,869 --> 00:37:49,079
interesting semi-supervised tour

795
00:37:47,309 --> 00:37:54,150
unsupervised datasets and try and come

796
00:37:49,079 --> 00:37:56,759
up with some like more clever fake tasks

797
00:37:54,150 --> 00:37:59,639
and see like does it matter you know how

798
00:37:56,760 --> 00:38:01,619
much does it matter in general like if

799
00:37:59,639 --> 00:38:04,099
you can't come up with a fake task that

800
00:38:01,619 --> 00:38:06,420
you think seems great I would say use it

801
00:38:04,099 --> 00:38:09,868
use the best you can it's an often

802
00:38:06,420 --> 00:38:12,570
surprising how how little you need like

803
00:38:09,869 --> 00:38:15,119
the ultimately crappy fake task is

804
00:38:12,570 --> 00:38:19,050
called the auto encoder and the auto

805
00:38:15,119 --> 00:38:20,940
encoder is the thing which which one the

806
00:38:19,050 --> 00:38:23,760
claims prediction competition that just

807
00:38:20,940 --> 00:38:27,090
finished on cattle they had lots of

808
00:38:23,760 --> 00:38:28,800
examples of insurance policies where we

809
00:38:27,090 --> 00:38:30,360
knew this was how much was claimed and

810
00:38:28,800 --> 00:38:32,220
then lots of examples of insurance

811
00:38:30,360 --> 00:38:34,559
policies where I guess there must have

812
00:38:32,219 --> 00:38:36,959
been still still open we didn't yet know

813
00:38:34,559 --> 00:38:39,719
how much they claimed right and so what

814
00:38:36,960 --> 00:38:41,940
they did was they said okay so for all

815
00:38:39,719 --> 00:38:44,129
of the ones so let's basically start off

816
00:38:41,940 --> 00:38:46,200
by grabbing every policy right and we'll

817
00:38:44,130 --> 00:38:50,940
take a single policy and we'll put it

818
00:38:46,199 --> 00:38:55,829
through a neural net right and we'll try

819
00:38:50,940 --> 00:38:57,990
and have it reconstruct itself but in

820
00:38:55,829 --> 00:38:59,579
these intermediate layers and at least

821
00:38:57,989 --> 00:39:01,529
one of those intermediate layers will

822
00:38:59,579 --> 00:39:03,690
make sure there's less activations and

823
00:39:01,530 --> 00:39:06,150
there were inputs so let's say if there

824
00:39:03,690 --> 00:39:08,579
was a hundred variables on the insurance

825
00:39:06,150 --> 00:39:09,869
policy you know we'll have something in

826
00:39:08,579 --> 00:39:13,590
the middle that only has like twenty

827
00:39:09,869 --> 00:39:16,019
activations all right and so when you

828
00:39:13,590 --> 00:39:18,090
basically are saying hey reconstruct

829
00:39:16,019 --> 00:39:20,070
your own input like it's not a different

830
00:39:18,090 --> 00:39:23,970
kind of model doesn't require any

831
00:39:20,070 --> 00:39:25,619
special code it's literally just passing

832
00:39:23,969 --> 00:39:28,649
you can use any standard pipe torch or

833
00:39:25,619 --> 00:39:31,710
fast AI learner you just say my output

834
00:39:28,650 --> 00:39:35,670
equals my input right and that's that's

835
00:39:31,710 --> 00:39:37,650
like the the most uncreated you know

836
00:39:35,670 --> 00:39:38,700
invented task you can create and that's

837
00:39:37,650 --> 00:39:41,430
called an autoencoder

838
00:39:38,699 --> 00:39:43,589
and it works surprisingly well in fact

839
00:39:41,429 --> 00:39:45,029
to the point that it literally just won

840
00:39:43,590 --> 00:39:47,100
a cackle competition they took the

841
00:39:45,030 --> 00:39:49,980
features that it learnt and chucked it

842
00:39:47,099 --> 00:39:54,269
into another neural net and

843
00:39:49,980 --> 00:39:56,429
yeah and one you know maybe if we have

844
00:39:54,269 --> 00:39:59,608
enough students taking an interest in

845
00:39:56,429 --> 00:40:01,559
this then you know we'll be able to

846
00:39:59,608 --> 00:40:03,449
cover covered unsupervised learning in

847
00:40:01,559 --> 00:40:10,588
more detail in in part two specially

848
00:40:03,449 --> 00:40:11,879
given this cattle have a win I think

849
00:40:10,588 --> 00:40:15,739
this may be related to the previous

850
00:40:11,880 --> 00:40:18,150
question when training language models

851
00:40:15,739 --> 00:40:19,979
is the language model example trained on

852
00:40:18,150 --> 00:40:26,070
the archive data is that useful at all

853
00:40:19,980 --> 00:40:29,639
in the movie great question you know I

854
00:40:26,070 --> 00:40:31,170
was just talking to Sebastian about this

855
00:40:29,639 --> 00:40:32,670
question read about this this week and

856
00:40:31,170 --> 00:40:35,519
we thought would try and do some

857
00:40:32,670 --> 00:40:36,838
research on this in January it's it's

858
00:40:35,519 --> 00:40:39,420
again it's not well done

859
00:40:36,838 --> 00:40:42,900
we know that in computer vision it's

860
00:40:39,420 --> 00:40:45,119
shockingly effective to train on cats

861
00:40:42,900 --> 00:40:47,970
and dogs and use that fruit train

862
00:40:45,119 --> 00:40:52,230
network to do lung cancer diagnosis and

863
00:40:47,969 --> 00:40:54,480
CT scans in the NLP world nobody much

864
00:40:52,230 --> 00:40:56,490
seems to have tried this the NLP

865
00:40:54,480 --> 00:40:58,949
research as I've spoken to other than

866
00:40:56,489 --> 00:41:00,328
Sebastian about this assume that it

867
00:40:58,949 --> 00:41:01,679
wouldn't work and they generally haven't

868
00:41:00,329 --> 00:41:07,079
bother trying I think it would work

869
00:41:01,679 --> 00:41:12,419
great so so since we're talking about

870
00:41:07,079 --> 00:41:14,460
ruspin I just mentioned during the week

871
00:41:12,420 --> 00:41:17,809
I was interested to see like how good

872
00:41:14,460 --> 00:41:20,039
this solution actually actually was

873
00:41:17,809 --> 00:41:21,150
because I noticed that on the public

874
00:41:20,039 --> 00:41:23,550
leader board it didn't look like it was

875
00:41:21,150 --> 00:41:25,769
going to be that great and I also

876
00:41:23,550 --> 00:41:28,560
thought it'd be good to see like what

877
00:41:25,769 --> 00:41:30,690
does it actually take to use a test set

878
00:41:28,559 --> 00:41:32,699
properly with this kind of structured

879
00:41:30,690 --> 00:41:34,380
data so if you have a look at ruspin now

880
00:41:32,699 --> 00:41:36,899
I've pushed some changes that actually

881
00:41:34,380 --> 00:41:38,430
run the test set through as well and so

882
00:41:36,900 --> 00:41:40,440
you can get a sense of how to do this

883
00:41:38,429 --> 00:41:44,149
so you'll see basically every line

884
00:41:40,440 --> 00:41:46,650
appears twice one for tests and one-foot

885
00:41:44,150 --> 00:41:48,720
one for train when we get there yeah

886
00:41:46,650 --> 00:41:50,608
test train test trains history obviously

887
00:41:48,719 --> 00:41:53,459
you could do this on a lot fewer lines

888
00:41:50,608 --> 00:41:55,409
of code by putting all of the steps into

889
00:41:53,460 --> 00:41:57,389
a method and then pass either the train

890
00:41:55,409 --> 00:42:00,379
data set well the test data set up

891
00:41:57,389 --> 00:42:02,969
dataframe to it in this case i wanted to

892
00:42:00,380 --> 00:42:03,750
kind of put for teaching purposes you'd

893
00:42:02,969 --> 00:42:05,699
be able to see

894
00:42:03,750 --> 00:42:08,010
step and to experiment to see what each

895
00:42:05,699 --> 00:42:12,509
step looks like but you could certainly

896
00:42:08,010 --> 00:42:16,470
simplify this code so yeah so we do this

897
00:42:12,510 --> 00:42:17,580
for every data frame and then some of

898
00:42:16,469 --> 00:42:20,159
these you can see I kind of lived

899
00:42:17,579 --> 00:42:24,480
through the data frame in joined and the

900
00:42:20,159 --> 00:42:27,779
joint test right training just this

901
00:42:24,480 --> 00:42:30,150
whole thing about the durations I

902
00:42:27,780 --> 00:42:32,400
basically put two lines here one that

903
00:42:30,150 --> 00:42:34,139
said data frame equals train columns one

904
00:42:32,400 --> 00:42:36,150
that says data frame equals test columns

905
00:42:34,139 --> 00:42:39,059
and so my you know basically ideas you'd

906
00:42:36,150 --> 00:42:40,769
run this line first and then you would

907
00:42:39,059 --> 00:42:42,599
skip the next one and you'd run

908
00:42:40,769 --> 00:42:45,000
everything beneath it and then you'd go

909
00:42:42,599 --> 00:42:45,900
back and run this line and then run

910
00:42:45,000 --> 00:42:48,000
everything believe it

911
00:42:45,900 --> 00:42:49,950
so some people on the forum were asking

912
00:42:48,000 --> 00:42:52,260
how come this code wasn't working this

913
00:42:49,949 --> 00:42:54,329
week which is a good reminder that the

914
00:42:52,260 --> 00:42:56,340
code is not designed to be code that you

915
00:42:54,329 --> 00:42:57,840
always run top to bottom without

916
00:42:56,340 --> 00:43:00,180
thinking right you're meant to like

917
00:42:57,840 --> 00:43:03,960
think like what is this code here should

918
00:43:00,179 --> 00:43:05,549
I be running it right now okay and so

919
00:43:03,960 --> 00:43:07,559
like the early lessons I tried to make

920
00:43:05,550 --> 00:43:09,480
it so you can run it top to bottom but

921
00:43:07,559 --> 00:43:10,799
increasingly as we go along I kind of

922
00:43:09,480 --> 00:43:11,789
make it more and more that like you

923
00:43:10,800 --> 00:43:17,340
actually have to think about what's

924
00:43:11,789 --> 00:43:20,190
going on so Jimmy you're talking about

925
00:43:17,340 --> 00:43:22,620
shadow learning and deep learning could

926
00:43:20,190 --> 00:43:24,000
you define that a bit better by sure I'm

927
00:43:22,619 --> 00:43:25,829
learning I think I just mean anything

928
00:43:24,000 --> 00:43:28,260
that doesn't have a hidden layer so

929
00:43:25,829 --> 00:43:40,799
something that's like a dot product

930
00:43:28,260 --> 00:43:43,590
matrix multiplier basically okay so so

931
00:43:40,800 --> 00:43:46,230
we end up with a training and a test

932
00:43:43,590 --> 00:43:50,039
version and then everything else is

933
00:43:46,230 --> 00:43:51,840
basically the same one thing to note on

934
00:43:50,039 --> 00:43:53,190
a lot of these details of this we cover

935
00:43:51,840 --> 00:43:54,180
in the machine learning course by the

936
00:43:53,190 --> 00:43:55,650
way because it's not really deep

937
00:43:54,179 --> 00:43:58,109
learning specific so check that out if

938
00:43:55,650 --> 00:44:00,450
you're just in the details

939
00:43:58,110 --> 00:44:02,970
I should mention you know we use apply

940
00:44:00,449 --> 00:44:04,769
cats rather than train cats to make sure

941
00:44:02,969 --> 00:44:09,539
the test set and the training set have

942
00:44:04,769 --> 00:44:14,039
the same categorical codes and that they

943
00:44:09,539 --> 00:44:15,989
join too we also need to make sure that

944
00:44:14,039 --> 00:44:17,179
we keep track of the mapper this is the

945
00:44:15,989 --> 00:44:18,500
thing which basically says

946
00:44:17,179 --> 00:44:22,068
what's the mean and standard deviation

947
00:44:18,500 --> 00:44:27,440
of each continuous column and then apply

948
00:44:22,068 --> 00:44:28,880
that same method test set and so when we

949
00:44:27,440 --> 00:44:30,950
do all that that's basically it then the

950
00:44:28,880 --> 00:44:33,608
rest is easy we just have to pass you in

951
00:44:30,949 --> 00:44:39,098
the test data frame in the usual way

952
00:44:33,608 --> 00:44:42,049
when we create our model data object and

953
00:44:39,099 --> 00:44:43,970
there's no changes through all here we

954
00:44:42,050 --> 00:44:51,380
trained it in the same way and then once

955
00:44:43,969 --> 00:44:55,699
we finish training it we can then call

956
00:44:51,380 --> 00:44:57,710
predict as per usual passing in true to

957
00:44:55,699 --> 00:45:00,699
say this is the test set rather than the

958
00:44:57,710 --> 00:45:03,650
validation set and pass that off to

959
00:45:00,699 --> 00:45:08,868
cattle and so it was really interesting

960
00:45:03,650 --> 00:45:14,838
because this was my submission it got a

961
00:45:08,869 --> 00:45:21,108
public score of 103 which would put us

962
00:45:14,838 --> 00:45:25,338
in about 300 and some things place which

963
00:45:21,108 --> 00:45:37,880
looks awful right and our private score

964
00:45:25,338 --> 00:45:38,889
of 107 need a board private here's about

965
00:45:37,880 --> 00:45:42,289
fifth

966
00:45:38,889 --> 00:45:45,798
right so like if you're competing in a

967
00:45:42,289 --> 00:45:47,989
cable competition and you don't haven't

968
00:45:45,798 --> 00:45:49,818
thoughtfully created a validation set of

969
00:45:47,989 --> 00:45:52,039
your own and you're relying on publicly

970
00:45:49,818 --> 00:45:53,509
the board feedback this could totally

971
00:45:52,039 --> 00:45:55,699
happen to you but the other way around

972
00:45:53,510 --> 00:45:56,420
you'll be like oh I'm in the top ten I'm

973
00:45:55,699 --> 00:45:59,078
doing great

974
00:45:56,420 --> 00:46:01,970
and then oh for example at the moment

975
00:45:59,079 --> 00:46:04,940
the ice Berg's competition recognizing

976
00:46:01,969 --> 00:46:07,730
icebergs a very large percentage of the

977
00:46:04,940 --> 00:46:10,639
public leaderboard set is synthetically

978
00:46:07,730 --> 00:46:13,490
generated data augmentation data like

979
00:46:10,639 --> 00:46:15,289
totally meaningless and so your

980
00:46:13,489 --> 00:46:17,269
validation set is going to be much more

981
00:46:15,289 --> 00:46:21,858
helpful and the public leaderboard

982
00:46:17,269 --> 00:46:24,380
feedback right so yeah be very careful

983
00:46:21,858 --> 00:46:26,269
so our final score here is kind of

984
00:46:24,380 --> 00:46:28,460
within statistical noise of the actual

985
00:46:26,269 --> 00:46:31,039
third-place getters so I'm pretty

986
00:46:28,460 --> 00:46:35,829
confident that we've we've captured

987
00:46:31,039 --> 00:46:40,699
their approach and so that's that's

988
00:46:35,829 --> 00:46:43,340
pretty interesting something to mention

989
00:46:40,699 --> 00:46:45,019
there's a nice kernel about the rustman

990
00:46:43,340 --> 00:46:46,070
I quite a few nice kernels actually but

991
00:46:45,019 --> 00:46:47,150
you can go back and see like

992
00:46:46,070 --> 00:46:48,680
particularly if you're doing the

993
00:46:47,150 --> 00:46:50,180
groceries competition go and have a look

994
00:46:48,679 --> 00:46:51,889
at the Rossmann kernels because actually

995
00:46:50,179 --> 00:46:53,719
quite a few of them a higher quality

996
00:46:51,889 --> 00:46:56,089
than the ones for the Ecuadorian

997
00:46:53,719 --> 00:46:58,459
groceries competition one of them for

998
00:46:56,090 --> 00:47:02,480
example showed how on four particular

999
00:46:58,460 --> 00:47:05,210
stores like straw eighty five the sales

1000
00:47:02,480 --> 00:47:08,329
for non Sundays and the sale for

1001
00:47:05,210 --> 00:47:09,860
Sunday's looked very different where

1002
00:47:08,329 --> 00:47:11,869
else there are some other stores where

1003
00:47:09,860 --> 00:47:13,400
the sales on Sunday don't look any

1004
00:47:11,869 --> 00:47:14,719
different and it can kind of like get a

1005
00:47:13,400 --> 00:47:16,309
sense of why you need these kind of

1006
00:47:14,719 --> 00:47:18,289
interactions the one I particularly

1007
00:47:16,309 --> 00:47:20,360
wanted to point out is the one I think I

1008
00:47:18,289 --> 00:47:22,849
briefly mentioned that the third-place

1009
00:47:20,360 --> 00:47:25,519
winners whose approach we used they

1010
00:47:22,849 --> 00:47:29,989
didn't notice is this one and here's a

1011
00:47:25,519 --> 00:47:33,280
really cool visualization here you can

1012
00:47:29,989 --> 00:47:37,279
see that the store this store is closed

1013
00:47:33,280 --> 00:47:38,330
right and just after oh my god we run a

1014
00:47:37,280 --> 00:47:41,540
we run out of eggs

1015
00:47:38,329 --> 00:47:44,869
and just before oh my god go and get the

1016
00:47:41,539 --> 00:47:48,800
milk before the store closes alright

1017
00:47:44,869 --> 00:47:52,190
and here again closed bang right so this

1018
00:47:48,800 --> 00:47:55,100
third-place winner actually deleted all

1019
00:47:52,190 --> 00:47:57,230
of the closed store rows before they

1020
00:47:55,099 --> 00:47:59,360
started doing any analysis right so

1021
00:47:57,230 --> 00:48:02,380
remember how we talked about like don't

1022
00:47:59,360 --> 00:48:04,970
touch your data unless you first of all

1023
00:48:02,380 --> 00:48:08,720
analyze to see whether that thing you're

1024
00:48:04,969 --> 00:48:11,779
doing is actually okay no assumptions

1025
00:48:08,719 --> 00:48:13,159
right so in this case I am sure like I

1026
00:48:11,780 --> 00:48:15,260
haven't tried it but I'm sure they would

1027
00:48:13,159 --> 00:48:16,849
have one otherwise right because like

1028
00:48:15,260 --> 00:48:19,670
well though there weren't actually any

1029
00:48:16,849 --> 00:48:22,670
store closures to my knowledge in the

1030
00:48:19,670 --> 00:48:25,099
test set period the problem is that

1031
00:48:22,670 --> 00:48:27,559
their model was trying to fit to these

1032
00:48:25,099 --> 00:48:28,549
like really extreme things and so and

1033
00:48:27,559 --> 00:48:30,679
because it wasn't able to do it very

1034
00:48:28,550 --> 00:48:32,720
well it was gonna end up getting a

1035
00:48:30,679 --> 00:48:34,879
little bit confused it's not gonna break

1036
00:48:32,719 --> 00:48:36,409
the model but it's definitely gonna harm

1037
00:48:34,880 --> 00:48:38,480
it because it's kind of trying to do

1038
00:48:36,409 --> 00:48:40,699
computations to fit something which it

1039
00:48:38,480 --> 00:48:42,869
literally doesn't have the data for your

1040
00:48:40,699 --> 00:48:50,739
neck can you pass that back there

1041
00:48:42,869 --> 00:48:52,778
all right so that Russman model again

1042
00:48:50,739 --> 00:48:54,818
like it's nice to kind of look inside to

1043
00:48:52,778 --> 00:49:02,559
see what's actually going on right and

1044
00:48:54,818 --> 00:49:04,028
so that Russman model I want to make

1045
00:49:02,559 --> 00:49:05,170
sure you kind of know how to find your

1046
00:49:04,028 --> 00:49:06,489
way around the code so you can answer

1047
00:49:05,170 --> 00:49:12,309
these questions for yourself so it's

1048
00:49:06,489 --> 00:49:13,838
inside columnar model data now um we

1049
00:49:12,309 --> 00:49:15,190
started out by kind of saying hey if you

1050
00:49:13,838 --> 00:49:17,889
want to look at the code for something

1051
00:49:15,190 --> 00:49:21,999
you couldn't like a question mark

1052
00:49:17,889 --> 00:49:23,828
question mark like this and oh okay I

1053
00:49:21,998 --> 00:49:25,268
need to I haven't got this reading but

1054
00:49:23,829 --> 00:49:27,999
you can use question mark question mark

1055
00:49:25,268 --> 00:49:31,258
to get the source code for something

1056
00:49:27,998 --> 00:49:33,578
right but obviously like that's not

1057
00:49:31,259 --> 00:49:35,228
really a great way because often you

1058
00:49:33,579 --> 00:49:36,190
look at that source code and it turns

1059
00:49:35,228 --> 00:49:38,169
out you need to look at something else

1060
00:49:36,190 --> 00:49:40,929
right and so for those of you that

1061
00:49:38,170 --> 00:49:43,349
haven't done much coding you might not

1062
00:49:40,929 --> 00:49:45,879
be aware that almost certainly the

1063
00:49:43,349 --> 00:49:48,219
editor you're using probably has the

1064
00:49:45,880 --> 00:49:51,940
ability to both open up stuff directly

1065
00:49:48,219 --> 00:49:53,229
off SSH and to navigate through it so

1066
00:49:51,940 --> 00:49:55,028
you can jump straight from place to

1067
00:49:53,228 --> 00:49:57,159
place right so want to show you what I

1068
00:49:55,028 --> 00:50:00,130
mean so if I were to find columnar model

1069
00:49:57,159 --> 00:50:03,368
data and I have to be using vim here I

1070
00:50:00,130 --> 00:50:05,710
can basically say tag columnar model

1071
00:50:03,369 --> 00:50:09,278
data and it will jump straight to the

1072
00:50:05,710 --> 00:50:10,960
definition of that plus right and so

1073
00:50:09,278 --> 00:50:12,460
then I notice here that like oh it's

1074
00:50:10,960 --> 00:50:15,068
actually building up a data loader

1075
00:50:12,460 --> 00:50:17,588
that's interesting if I get control

1076
00:50:15,068 --> 00:50:19,568
right square bracket it'll jump to the

1077
00:50:17,588 --> 00:50:21,429
definition of the thing that was under

1078
00:50:19,568 --> 00:50:22,449
my cursor and after I finished reading

1079
00:50:21,429 --> 00:50:25,328
it for a while

1080
00:50:22,449 --> 00:50:26,169
I can hit ctrl T to jump back up to

1081
00:50:25,329 --> 00:50:28,450
where I came from

1082
00:50:26,170 --> 00:50:30,729
right and you kind of get the idea right

1083
00:50:28,449 --> 00:50:34,710
or if I want to find it for usage of

1084
00:50:30,728 --> 00:50:38,618
this in this file of columnar model data

1085
00:50:34,710 --> 00:50:41,139
I can hit star to jump to the next place

1086
00:50:38,619 --> 00:50:46,660
it's new used you know and so forth

1087
00:50:41,139 --> 00:50:49,739
alright so in this case get learner was

1088
00:50:46,659 --> 00:50:51,670
the thing which actually got the model

1089
00:50:49,739 --> 00:50:55,880
and we want to find out what kind of

1090
00:50:51,670 --> 00:50:56,990
model it is and apparently it uses a

1091
00:50:55,880 --> 00:50:59,420
I'm not using collaborative filtering

1092
00:50:56,989 --> 00:51:08,078
are we were using columnar model data

1093
00:50:59,420 --> 00:51:10,970
sorry columnar model data okay learner

1094
00:51:08,079 --> 00:51:13,548
which users and so here you can see

1095
00:51:10,969 --> 00:51:16,969
mixed input model is the PI torch model

1096
00:51:13,548 --> 00:51:20,989
and then it wraps it in the structured

1097
00:51:16,969 --> 00:51:23,239
learner which is the the first day I

1098
00:51:20,989 --> 00:51:25,368
learn a type which wraps the data and

1099
00:51:23,239 --> 00:51:27,469
the model together so if we want to see

1100
00:51:25,369 --> 00:51:30,200
the definition of this actual PI torch

1101
00:51:27,469 --> 00:51:34,518
model I can go to control right square

1102
00:51:30,199 --> 00:51:39,768
bracket to see it right and so here is

1103
00:51:34,518 --> 00:51:42,889
the model right and nearly all of this

1104
00:51:39,768 --> 00:51:52,008
we can now understand right so we got

1105
00:51:42,889 --> 00:52:04,818
past we got past a list of embedding

1106
00:51:52,009 --> 00:52:09,380
sizes in the mixed model that we saw

1107
00:52:04,818 --> 00:52:15,409
does it always expect categorical and

1108
00:52:09,380 --> 00:52:19,460
continuous together yes it does

1109
00:52:15,409 --> 00:52:21,858
and the the model data behind the scenes

1110
00:52:19,460 --> 00:52:26,000
if there are no none of the other type

1111
00:52:21,858 --> 00:52:30,078
it creates a column of ones or zeros or

1112
00:52:26,000 --> 00:52:35,000
something okay so if it is null it can

1113
00:52:30,079 --> 00:52:36,950
still work yeah yeah yeah it's kind of

1114
00:52:35,000 --> 00:52:39,289
ugly and hacky and will you know

1115
00:52:36,949 --> 00:52:42,139
hopefully improve it but yeah you can

1116
00:52:39,289 --> 00:52:43,759
pass in an empty list of categorical or

1117
00:52:42,139 --> 00:52:46,368
continuous variables to the model data

1118
00:52:43,759 --> 00:52:49,719
and it will basically yeah it'll

1119
00:52:46,369 --> 00:52:54,650
basically pass an unused column of zeros

1120
00:52:49,719 --> 00:52:56,598
to avoid things breaking and I'm I'm

1121
00:52:54,650 --> 00:52:59,990
leaving fixing some of these slightly

1122
00:52:56,599 --> 00:53:01,970
hacky edge cases because height or 0.4

1123
00:52:59,989 --> 00:53:04,788
as well as you're getting rid of

1124
00:53:01,969 --> 00:53:08,179
variables they're going to also add rank

1125
00:53:04,789 --> 00:53:08,839
0 tensors which is to say if you grab a

1126
00:53:08,179 --> 00:53:11,478
single

1127
00:53:08,838 --> 00:53:14,058
thing out of like a rent 110 sir rather

1128
00:53:11,478 --> 00:53:16,818
than getting back at a number which is

1129
00:53:14,059 --> 00:53:18,349
like qualitatively different you're

1130
00:53:16,818 --> 00:53:20,509
actually going to get back like a tensor

1131
00:53:18,349 --> 00:53:21,680
that just happens to have no rank now it

1132
00:53:20,509 --> 00:53:23,210
turns out that a lot of this kind of

1133
00:53:21,679 --> 00:53:25,489
codes gonna be actually easier to write

1134
00:53:23,210 --> 00:53:29,019
then so and for now it's it's a little

1135
00:53:25,489 --> 00:53:29,019
bit more happier than it needs to be

1136
00:53:30,400 --> 00:53:35,269
Jeremy you talk about this a little bit

1137
00:53:33,349 --> 00:53:38,960
before where maybe it's a good time at

1138
00:53:35,268 --> 00:53:41,058
some points talk about how can we write

1139
00:53:38,960 --> 00:53:44,749
something that is slightly different for

1140
00:53:41,059 --> 00:53:47,329
worries in the library yeah I think

1141
00:53:44,748 --> 00:53:49,338
we'll cover that a little bit next week

1142
00:53:47,329 --> 00:53:53,778
that I'm mainly going to do that in part

1143
00:53:49,338 --> 00:53:55,548
to like Pat who's going to cover quite a

1144
00:53:53,778 --> 00:53:57,079
lot of stuff one of the main things were

1145
00:53:55,548 --> 00:53:59,059
cover in part two is what it called

1146
00:53:57,079 --> 00:54:00,710
generative models so things where the

1147
00:53:59,059 --> 00:54:03,469
output is a whole sentence or a whole

1148
00:54:00,710 --> 00:54:08,929
image but you know I also dig into like

1149
00:54:03,469 --> 00:54:11,869
powder really either customize the first

1150
00:54:08,929 --> 00:54:16,248
day I library or use it on more custom

1151
00:54:11,869 --> 00:54:22,400
models but if we have time we'll touch

1152
00:54:16,248 --> 00:54:25,068
on it a little bit next week okay so the

1153
00:54:22,400 --> 00:54:27,318
the learner we were passing in a list of

1154
00:54:25,068 --> 00:54:28,998
embedding sizes and as you can see that

1155
00:54:27,318 --> 00:54:30,349
embedding sizes list was literally just

1156
00:54:28,998 --> 00:54:32,568
the number of rows and the number of

1157
00:54:30,349 --> 00:54:35,559
columns in each embedding right and the

1158
00:54:32,568 --> 00:54:38,808
number of code rose was just coming from

1159
00:54:35,559 --> 00:54:42,048
literally how many stores are there in

1160
00:54:38,809 --> 00:54:44,298
the store category for example and the

1161
00:54:42,048 --> 00:54:47,329
number of columns was just a quarter

1162
00:54:44,298 --> 00:54:49,309
that divided by two and a maximum of 50

1163
00:54:47,329 --> 00:54:51,469
so that thing that list of tuples was

1164
00:54:49,309 --> 00:54:53,210
coming in and so you can see here how we

1165
00:54:51,469 --> 00:54:57,229
use it right we go through each of those

1166
00:54:53,210 --> 00:55:00,199
tuples grab the number of categories and

1167
00:54:57,228 --> 00:55:03,078
the size of the embedding and construct

1168
00:55:00,199 --> 00:55:05,868
an embedding all right and so that's a

1169
00:55:03,079 --> 00:55:07,700
that's a list right one minor thing

1170
00:55:05,869 --> 00:55:11,269
height or specific thing we haven't

1171
00:55:07,699 --> 00:55:13,759
talked about before is for it to be able

1172
00:55:11,268 --> 00:55:15,618
to like register remember how we kind of

1173
00:55:13,759 --> 00:55:18,108
said like it registers your parameters

1174
00:55:15,619 --> 00:55:19,700
it registers your your layers like

1175
00:55:18,108 --> 00:55:21,199
someone we like listed the model it

1176
00:55:19,699 --> 00:55:24,619
actually printed out the Novation

1177
00:55:21,199 --> 00:55:27,019
varying an age bias it can't do that if

1178
00:55:24,619 --> 00:55:29,359
they're hidden inside a list right they

1179
00:55:27,019 --> 00:55:33,469
have to be like a there have to be a an

1180
00:55:29,360 --> 00:55:35,059
actual n n dot module subclass so

1181
00:55:33,469 --> 00:55:38,119
there's a special thing called an N n

1182
00:55:35,059 --> 00:55:40,070
dot module list which takes a list and

1183
00:55:38,119 --> 00:55:42,650
it basically says I want you to register

1184
00:55:40,070 --> 00:55:47,360
everything in here has been part of this

1185
00:55:42,650 --> 00:55:49,369
model okay so it's just a minor tweak so

1186
00:55:47,360 --> 00:55:53,539
yeah so our mixed input model has a list

1187
00:55:49,369 --> 00:55:56,960
of embeddings and then I do the same

1188
00:55:53,539 --> 00:56:03,920
thing for a list of linear layers right

1189
00:55:56,960 --> 00:56:06,320
so when I said here 1000 comma 500 this

1190
00:56:03,920 --> 00:56:09,980
was saying how many activations I wanted

1191
00:56:06,320 --> 00:56:13,400
featured my lineal is okay and so here I

1192
00:56:09,980 --> 00:56:16,849
just go through that list and create a

1193
00:56:13,400 --> 00:56:19,610
linear layer that goes from this size to

1194
00:56:16,849 --> 00:56:21,619
the next size okay so you can see like

1195
00:56:19,610 --> 00:56:24,710
how easy it is to kind of construct your

1196
00:56:21,619 --> 00:56:26,389
own not just your own model but a kind

1197
00:56:24,710 --> 00:56:28,220
of a model which you can pass parameters

1198
00:56:26,389 --> 00:56:30,829
to have a constructed on the fly

1199
00:56:28,219 --> 00:56:34,459
dynamically and that's normal talk about

1200
00:56:30,829 --> 00:56:36,380
next week this is initialization we've

1201
00:56:34,460 --> 00:56:40,699
mentioned climbing her initialization

1202
00:56:36,380 --> 00:56:44,750
before and we mentioned it last week and

1203
00:56:40,699 --> 00:56:46,369
then drop out same thing right we have

1204
00:56:44,750 --> 00:56:48,920
here a list of how much drop out to

1205
00:56:46,369 --> 00:56:50,630
apply to each layer right so again here

1206
00:56:48,920 --> 00:56:52,970
it's just like go through each thing in

1207
00:56:50,630 --> 00:56:55,910
that list and create a drop out layer

1208
00:56:52,969 --> 00:56:57,980
for it okay so this constructor we

1209
00:56:55,909 --> 00:57:00,469
understand everything in it except for

1210
00:56:57,980 --> 00:57:03,349
batch norm which we don't have to worry

1211
00:57:00,469 --> 00:57:08,179
about for now so that's the constructor

1212
00:57:03,349 --> 00:57:10,549
and so then the forward also you know

1213
00:57:08,179 --> 00:57:11,899
all stuff we're aware of go through each

1214
00:57:10,550 --> 00:57:14,300
of those embedding layers that we just

1215
00:57:11,900 --> 00:57:16,550
saw and remember we've just treated like

1216
00:57:14,300 --> 00:57:18,890
as a function so call it with the ithe

1217
00:57:16,550 --> 00:57:22,250
categorical variable and then

1218
00:57:18,889 --> 00:57:26,420
concatenate them all together put that

1219
00:57:22,250 --> 00:57:29,239
through drop out and then go through

1220
00:57:26,420 --> 00:57:32,170
each one of our linear layers and call

1221
00:57:29,239 --> 00:57:33,939
it apply relia to it

1222
00:57:32,170 --> 00:57:36,710
apply dropout

1223
00:57:33,940 --> 00:57:40,159
and then finally apply the final linear

1224
00:57:36,710 --> 00:57:47,030
layer and the final linear layer has

1225
00:57:40,159 --> 00:57:49,879
this as its size which is here right

1226
00:57:47,030 --> 00:57:52,250
size one there's a single unit sales

1227
00:57:49,880 --> 00:57:55,010
okay so we're kind of getting to the

1228
00:57:52,250 --> 00:57:57,170
point where oh and then of course at the

1229
00:57:55,010 --> 00:57:59,960
end if this I mentioned would come back

1230
00:57:57,170 --> 00:58:02,030
to this if you passed in a Y underscore

1231
00:57:59,960 --> 00:58:03,260
range parameter then we're going to do

1232
00:58:02,030 --> 00:58:06,050
the thing we just learned about last

1233
00:58:03,260 --> 00:58:07,730
week which is to use a sigmoid right and

1234
00:58:06,050 --> 00:58:09,260
this is a cool little trick to make

1235
00:58:07,730 --> 00:58:11,119
you're not just to make your

1236
00:58:09,260 --> 00:58:14,660
collaborative filtering better but in

1237
00:58:11,119 --> 00:58:17,889
this case my basic idea was you know

1238
00:58:14,659 --> 00:58:21,379
sales are going to be greater than zero

1239
00:58:17,889 --> 00:58:25,730
and probably less than the largest sale

1240
00:58:21,380 --> 00:58:29,000
they've ever had so I just pass in that

1241
00:58:25,730 --> 00:58:30,949
as Y range and so we do a sigmoid and

1242
00:58:29,000 --> 00:58:33,789
multiply with the sigmoid by the range

1243
00:58:30,949 --> 00:58:37,579
that I passed it all right and so

1244
00:58:33,789 --> 00:58:39,739
hopefully we can find that here yeah

1245
00:58:37,579 --> 00:58:43,250
here it is right so I actually said hey

1246
00:58:39,739 --> 00:58:45,529
maybe the range is between zero and you

1247
00:58:43,250 --> 00:58:47,960
know the highest x one point two you

1248
00:58:45,530 --> 00:58:49,670
know cuz maybe maybe the next two weeks

1249
00:58:47,960 --> 00:58:51,199
we have one bigger but this is kind of

1250
00:58:49,670 --> 00:58:53,900
like again try to make it a little bit

1251
00:58:51,199 --> 00:58:56,629
easier for it to give us the kind of

1252
00:58:53,900 --> 00:58:59,809
results that it thinks is right so like

1253
00:58:56,630 --> 00:59:04,210
increasingly you know I'd love your wall

1254
00:58:59,809 --> 00:59:07,579
to kind of try to not treat these

1255
00:59:04,210 --> 00:59:09,320
learners and models as black boxes but

1256
00:59:07,579 --> 00:59:10,909
to feel like you now have the

1257
00:59:09,320 --> 00:59:12,500
information you need to look inside them

1258
00:59:10,909 --> 00:59:16,029
and remember you could then copy and

1259
00:59:12,500 --> 00:59:19,400
paste this plus paste it into a cell in

1260
00:59:16,030 --> 00:59:27,580
duple notebook and start fiddling with

1261
00:59:19,400 --> 00:59:27,579
it to create your own versions okay

1262
00:59:30,639 --> 00:59:35,318
I think what I might do is we might take

1263
00:59:33,130 --> 00:59:37,150
a bit of a early break because we've got

1264
00:59:35,318 --> 00:59:42,159
a lot to cover and I want to do it all

1265
00:59:37,150 --> 00:59:46,150
in one big go so let's take a let's take

1266
00:59:42,159 --> 00:59:47,649
a break until 7:45 and then we're going

1267
00:59:46,150 --> 00:59:56,160
to come back and talk about recurrent

1268
00:59:47,650 --> 00:59:56,160
neural networks all right

1269
00:59:56,670 --> 01:00:02,079
so we're going to talk about Aaron ends

1270
00:59:58,750 --> 01:00:05,829
before we do we've got to kind of dig a

1271
01:00:02,079 --> 01:00:06,760
little bit deeper into SGD because I

1272
01:00:05,829 --> 01:00:11,579
just want to make sure everybody's

1273
01:00:06,760 --> 01:00:13,390
totally comfortable with with SGD and so

1274
01:00:11,579 --> 01:00:17,548
what we're going to look at is we're

1275
01:00:13,389 --> 01:00:19,480
going to look at lesson six SGD notebook

1276
01:00:17,548 --> 01:00:26,500
and we're going to look at a really

1277
01:00:19,480 --> 01:00:29,528
simple example of using SGD to learn y

1278
01:00:26,500 --> 01:00:32,159
equals ax plus B and so what we're going

1279
01:00:29,528 --> 01:00:36,699
to do here is we're going to create like

1280
01:00:32,159 --> 01:00:38,710
the simplest possible model y equals ax

1281
01:00:36,699 --> 01:00:43,509
plus B okay and then we're going to

1282
01:00:38,710 --> 01:00:46,750
generate some random data that looks

1283
01:00:43,509 --> 01:00:52,778
like so so here's our X and here's our Y

1284
01:00:46,750 --> 01:00:56,018
we want to predict Y from X and we

1285
01:00:52,778 --> 01:00:57,068
passed in 3 &amp; 8 as our a and B so we're

1286
01:00:56,018 --> 01:01:00,608
going to kind of try and recover that

1287
01:00:57,068 --> 01:01:03,099
right and so the idea is that if we can

1288
01:01:00,608 --> 01:01:05,500
solve something like this which has two

1289
01:01:03,099 --> 01:01:09,490
parameters we can use the same technique

1290
01:01:05,500 --> 01:01:10,989
to solve we can use the same technique

1291
01:01:09,489 --> 01:01:14,068
to solve something with a hundred

1292
01:01:10,989 --> 01:01:22,000
million parameters right without any

1293
01:01:14,068 --> 01:01:23,558
changes at all so in order to find a and

1294
01:01:22,000 --> 01:01:25,989
a B that fits this we need a loss

1295
01:01:23,559 --> 01:01:27,819
function and this is a regression

1296
01:01:25,989 --> 01:01:30,129
problem because we have a continuous

1297
01:01:27,818 --> 01:01:31,838
output so for continuous output

1298
01:01:30,130 --> 01:01:33,849
regression we tend to use mean squared

1299
01:01:31,838 --> 01:01:34,869
error all right and obviously all of

1300
01:01:33,849 --> 01:01:36,338
this stuff there's there's

1301
01:01:34,869 --> 01:01:37,809
implementations in non pious

1302
01:01:36,338 --> 01:01:39,969
implementations in flight or we're just

1303
01:01:37,809 --> 01:01:42,640
doing stuff by hand so you can see all

1304
01:01:39,969 --> 01:01:44,019
the steps right so there's MSE okay

1305
01:01:42,639 --> 01:01:45,940
y hat is

1306
01:01:44,019 --> 01:01:48,250
we often call our predictions Y hat

1307
01:01:45,940 --> 01:01:50,980
mitis y squared mean there's I meant

1308
01:01:48,250 --> 01:01:54,159
whatever okay so for example if we had

1309
01:01:50,980 --> 01:01:56,710
ten and five where a and B then there's

1310
01:01:54,159 --> 01:01:59,109
our mean square R squared error three

1311
01:01:56,710 --> 01:02:00,789
point two five okay so if we've got an A

1312
01:01:59,110 --> 01:02:02,500
and a B and we've got an x and a y then

1313
01:02:00,789 --> 01:02:05,739
our mean square error loss is just the

1314
01:02:02,500 --> 01:02:07,420
mean squared error of our linear that's

1315
01:02:05,739 --> 01:02:11,199
our predictions and our way okay so

1316
01:02:07,420 --> 01:02:13,510
there's a last four ten five X Y all

1317
01:02:11,199 --> 01:02:18,750
right so that's a loss function right

1318
01:02:13,510 --> 01:02:21,520
and so when we talk about combining

1319
01:02:18,750 --> 01:02:24,519
linear layers and loss functions and

1320
01:02:21,519 --> 01:02:26,590
optionally nonlinear layers this is all

1321
01:02:24,519 --> 01:02:29,559
we're doing right is we're putting a

1322
01:02:26,590 --> 01:02:31,500
function inside a function yeah that's

1323
01:02:29,559 --> 01:02:35,349
that's all like I know people draw these

1324
01:02:31,500 --> 01:02:36,670
clever looking dots and lines all over

1325
01:02:35,349 --> 01:02:38,110
the screen when they're saying this is

1326
01:02:36,670 --> 01:02:39,730
what a neural network is but it's just

1327
01:02:38,110 --> 01:02:41,260
it's just a function of a function of a

1328
01:02:39,730 --> 01:02:43,059
function okay so here we've got a

1329
01:02:41,260 --> 01:02:45,700
prediction function being a linear layer

1330
01:02:43,059 --> 01:02:47,440
followed by a loss function being MSE

1331
01:02:45,699 --> 01:02:49,269
and now we can say like oh well let's

1332
01:02:47,440 --> 01:02:51,970
just define this as MSA Lost's and we'll

1333
01:02:49,269 --> 01:02:53,980
use that in the future okay so there's

1334
01:02:51,969 --> 01:02:57,009
our loss function which incorporates our

1335
01:02:53,980 --> 01:03:00,550
prediction function okay so let's

1336
01:02:57,010 --> 01:03:02,500
generate 10,000 items or thick data and

1337
01:03:00,550 --> 01:03:03,940
let's show them in two variables so we

1338
01:03:02,500 --> 01:03:05,440
can use them with PI torch because

1339
01:03:03,940 --> 01:03:07,139
Jeremy doesn't like taking derivatives

1340
01:03:05,440 --> 01:03:11,320
so we're going to use PI torch for that

1341
01:03:07,139 --> 01:03:14,529
and let's create random wait for a and B

1342
01:03:11,320 --> 01:03:16,480
so a single random number and we want

1343
01:03:14,530 --> 01:03:19,060
the gradients of these to be calculated

1344
01:03:16,480 --> 01:03:20,559
as we start computing with them because

1345
01:03:19,059 --> 01:03:24,340
these are the actual things we need to

1346
01:03:20,559 --> 01:03:31,090
update in our SGD okay so here's our a

1347
01:03:24,340 --> 01:03:34,360
and B 0.029 0.111 all right so let's

1348
01:03:31,090 --> 01:03:40,600
pick a learning rate okay and let let's

1349
01:03:34,360 --> 01:03:42,519
do 10,000 epochs of SGD in fact this

1350
01:03:40,599 --> 01:03:43,690
isn't really SGD it's not stochastic

1351
01:03:42,519 --> 01:03:45,570
gradient it said this is actually full

1352
01:03:43,690 --> 01:03:48,730
gradient descent we're going to each

1353
01:03:45,570 --> 01:03:52,390
each loop is going to look at all of the

1354
01:03:48,730 --> 01:03:56,329
data okay stochastic gradient descent

1355
01:03:52,389 --> 01:03:58,369
would be looking at a subset each time

1356
01:03:56,329 --> 01:04:01,039
so to do gradient descent we basically

1357
01:03:58,369 --> 01:04:03,799
calculate loss right so remember we've

1358
01:04:01,039 --> 01:04:05,420
started out with a random a and B okay

1359
01:04:03,800 --> 01:04:08,269
and so this is going to compute some

1360
01:04:05,420 --> 01:04:10,159
amount of loss and then it's nice from

1361
01:04:08,269 --> 01:04:13,579
time to time so one way of saying from

1362
01:04:10,159 --> 01:04:15,949
time to time is if the epoch number mod

1363
01:04:13,579 --> 01:04:17,900
a thousand is zero right so every

1364
01:04:15,949 --> 01:04:21,049
thousand epochs just print out the loss

1365
01:04:17,900 --> 01:04:23,690
so you have it do it okay

1366
01:04:21,050 --> 01:04:26,090
so now that we've computed the loss we

1367
01:04:23,690 --> 01:04:28,840
can compute our gradients right and so

1368
01:04:26,090 --> 01:04:32,180
you just remember this thing here is

1369
01:04:28,840 --> 01:04:34,130
both a number a single number that is

1370
01:04:32,179 --> 01:04:35,899
our lost something we can print but it's

1371
01:04:34,130 --> 01:04:38,210
also a variable because we passed

1372
01:04:35,900 --> 01:04:41,059
variables into it and therefore it also

1373
01:04:38,210 --> 01:04:43,789
has a method type backward which means

1374
01:04:41,059 --> 01:04:45,409
calculate the gradients of everything

1375
01:04:43,789 --> 01:04:48,829
that we asked it to everything where we

1376
01:04:45,409 --> 01:04:52,940
said requires radical is true okay so at

1377
01:04:48,829 --> 01:04:57,619
this point we now have a dot grad

1378
01:04:52,940 --> 01:04:59,619
property inside a and inside P and here

1379
01:04:57,619 --> 01:05:02,389
they are here is that grant grad

1380
01:04:59,619 --> 01:05:04,609
property okay so now that we've

1381
01:05:02,389 --> 01:05:07,879
calculated the gradients for a and B we

1382
01:05:04,610 --> 01:05:10,280
can update them by saying a is equal to

1383
01:05:07,880 --> 01:05:14,530
whatever it used to be - the learning

1384
01:05:10,280 --> 01:05:18,590
rate times the gradient okay dot data

1385
01:05:14,530 --> 01:05:21,380
because a is a variable and a variable

1386
01:05:18,590 --> 01:05:24,410
contains a tensor and it's dot data

1387
01:05:21,380 --> 01:05:26,750
property and we again this is going to

1388
01:05:24,409 --> 01:05:28,759
disappear in height which point four but

1389
01:05:26,750 --> 01:05:31,340
for now it's actually the ten so that we

1390
01:05:28,760 --> 01:05:33,560
need to update okay so update the tensor

1391
01:05:31,340 --> 01:05:37,030
inside here with whatever it used to be

1392
01:05:33,559 --> 01:05:40,400
- the learning rate times the gradient

1393
01:05:37,030 --> 01:05:43,220
okay and that's basically it

1394
01:05:40,400 --> 01:05:45,980
all right that's basically all gradient

1395
01:05:43,219 --> 01:05:49,309
descent is okay so it's it's as simple

1396
01:05:45,980 --> 01:05:51,949
as we claimed there's one extra step in

1397
01:05:49,309 --> 01:05:54,110
pi torch which is that you might have

1398
01:05:51,949 --> 01:05:56,559
like multiple different loss functions

1399
01:05:54,110 --> 01:05:59,599
or like lots of lots of output layers

1400
01:05:56,559 --> 01:06:00,829
all contributing to the gradient and you

1401
01:05:59,599 --> 01:06:04,279
like to have to add them all together

1402
01:06:00,829 --> 01:06:05,960
and so if you've got multiple loss

1403
01:06:04,280 --> 01:06:07,640
functions you could be calling loss stop

1404
01:06:05,960 --> 01:06:08,429
backward on each of them and what it

1405
01:06:07,639 --> 01:06:10,889
does is an ad

1406
01:06:08,429 --> 01:06:12,509
sit to the gradients right and so you

1407
01:06:10,889 --> 01:06:15,389
have to tell it when to set the

1408
01:06:12,510 --> 01:06:18,860
gradients back to zero okay so that's

1409
01:06:15,389 --> 01:06:21,629
where you just go okay set a to zero and

1410
01:06:18,860 --> 01:06:25,710
gradients in set B gradients to zero

1411
01:06:21,630 --> 01:06:31,440
okay and so this is wrapped up inside

1412
01:06:25,710 --> 01:06:34,260
the you know op TMS JD class right so

1413
01:06:31,440 --> 01:06:37,050
when we say up Tim dot SGD and we just

1414
01:06:34,260 --> 01:06:38,850
say you know dot step it's just doing

1415
01:06:37,050 --> 01:06:41,850
these for us so when we say dot zero

1416
01:06:38,849 --> 01:06:46,380
gradients is just doing this force and

1417
01:06:41,849 --> 01:06:48,599
this underscore here every pretty much

1418
01:06:46,380 --> 01:06:50,970
every function that applies to a tensor

1419
01:06:48,599 --> 01:06:53,429
in pi torch if you stick an underscore

1420
01:06:50,969 --> 01:06:54,809
on the end it means do it in place okay

1421
01:06:53,429 --> 01:06:56,730
so this is actually going to not return

1422
01:06:54,809 --> 01:06:58,289
a bunch of zeros but it's going to

1423
01:06:56,730 --> 01:07:04,320
change this in place to be a bunch of

1424
01:06:58,289 --> 01:07:09,420
zeros so that's basically it we can look

1425
01:07:04,320 --> 01:07:10,740
at the same thing without PI torch which

1426
01:07:09,420 --> 01:07:12,960
means we actually do have to do some

1427
01:07:10,739 --> 01:07:16,589
calculus so if we generate some fake

1428
01:07:12,960 --> 01:07:18,059
data again we're just going to create 50

1429
01:07:16,590 --> 01:07:21,660
data points this time just to make this

1430
01:07:18,059 --> 01:07:23,969
fast and easy to look at and so let's

1431
01:07:21,659 --> 01:07:25,440
create a function called update right

1432
01:07:23,969 --> 01:07:28,230
we're just going to use numpy no pi

1433
01:07:25,440 --> 01:07:31,530
torch okay so our predictions is equal

1434
01:07:28,230 --> 01:07:32,760
to again linear and in this case we

1435
01:07:31,530 --> 01:07:34,980
actually gonna calculate the derivatives

1436
01:07:32,760 --> 01:07:37,920
so the derivative of the square of the

1437
01:07:34,980 --> 01:07:39,539
loss is just two times and then the

1438
01:07:37,920 --> 01:07:41,400
derivative is the vector a is just that

1439
01:07:39,539 --> 01:07:44,219
you can confirm that yourself if you

1440
01:07:41,400 --> 01:07:46,260
want to and so here our we're going to

1441
01:07:44,219 --> 01:07:49,469
update a minus equals learning rate

1442
01:07:46,260 --> 01:07:52,770
times the derivative of loss with

1443
01:07:49,469 --> 01:07:54,809
respect to a and for B it's learning

1444
01:07:52,769 --> 01:07:59,130
rate times derivative with respect to B

1445
01:07:54,809 --> 01:08:02,670
okay and so what we can do let's just

1446
01:07:59,130 --> 01:08:04,740
run all this so just for fun

1447
01:08:02,670 --> 01:08:07,590
rather than looping through manually we

1448
01:08:04,739 --> 01:08:12,750
can use the map flop matplotlib func

1449
01:08:07,590 --> 01:08:14,820
animation command to run the animate

1450
01:08:12,750 --> 01:08:16,789
function a bunch of times and the

1451
01:08:14,820 --> 01:08:20,369
animate function is going to run 30

1452
01:08:16,789 --> 01:08:22,048
epochs and at the end of each epoch it's

1453
01:08:20,369 --> 01:08:24,028
going to print out

1454
01:08:22,048 --> 01:08:27,509
on the plot where the line currently is

1455
01:08:24,029 --> 01:08:30,119
and that creates this at all movie okay

1456
01:08:27,509 --> 01:08:33,238
so you can actually see that the line

1457
01:08:30,118 --> 01:08:35,969
moving at a place right so if you want

1458
01:08:33,238 --> 01:08:40,250
to play around with like understanding

1459
01:08:35,969 --> 01:08:42,239
how high torque gradients actually work

1460
01:08:40,250 --> 01:08:46,738
step-by-step here's like the world's

1461
01:08:42,238 --> 01:08:49,798
simplest at all example okay and you

1462
01:08:46,738 --> 01:08:52,198
know it's kind of like it's kind of

1463
01:08:49,798 --> 01:08:53,729
weird to say like that's that's it like

1464
01:08:52,198 --> 01:08:55,408
when you're optimizing a hundred million

1465
01:08:53,729 --> 01:08:57,298
parameters in a neural net it's doing

1466
01:08:55,408 --> 01:08:59,219
the same thing but it it actually is

1467
01:08:57,298 --> 01:09:01,948
alright you can actually look at the PI

1468
01:08:59,219 --> 01:09:04,710
torch code and see it's this is it right

1469
01:09:01,948 --> 01:09:07,078
there's no trick

1470
01:09:04,710 --> 01:09:09,779
well we load a couple of minor tricks

1471
01:09:07,078 --> 01:09:13,289
last time which was like momentum and

1472
01:09:09,779 --> 01:09:15,839
atom right that if you could do it in

1473
01:09:13,289 --> 01:09:19,439
Excel you can do it invite them so okay

1474
01:09:15,838 --> 01:09:20,838
so let's do talk about our lens so we're

1475
01:09:19,439 --> 01:09:29,629
now in less than six hour and in

1476
01:09:20,838 --> 01:09:34,519
notebook and we're going to study

1477
01:09:29,630 --> 01:09:34,520
Nietzsche as you should

1478
01:09:34,548 --> 01:09:41,960
so Nietzsche says supposing that truth

1479
01:09:38,939 --> 01:09:46,318
is a woman what then I love this

1480
01:09:41,960 --> 01:09:47,338
apparently all philosophers have failed

1481
01:09:46,319 --> 01:09:48,449
to understand women

1482
01:09:47,338 --> 01:09:49,798
so apparently at the point that

1483
01:09:48,448 --> 01:09:52,048
Nietzsche was alive there was no female

1484
01:09:49,798 --> 01:09:54,599
philosophers or at least those that were

1485
01:09:52,048 --> 01:09:56,158
around didn't understand women either so

1486
01:09:54,599 --> 01:09:59,909
anyway so this is the philosopher

1487
01:09:56,158 --> 01:10:02,158
apparently we've chosen to study it

1488
01:09:59,908 --> 01:10:05,189
leech is actually much less worse than

1489
01:10:02,158 --> 01:10:07,799
people think he is but it's a different

1490
01:10:05,189 --> 01:10:13,069
era I guess alright so we're going to

1491
01:10:07,800 --> 01:10:16,110
learn to write philosophy like Nietzsche

1492
01:10:13,069 --> 01:10:18,118
and so we're going to do it one

1493
01:10:16,109 --> 01:10:20,098
character at a time so this is like the

1494
01:10:18,118 --> 01:10:21,569
language model that we did in Lesson

1495
01:10:20,099 --> 01:10:22,949
four where we did it a word at the time

1496
01:10:21,569 --> 01:10:26,759
but this time we're going to do a

1497
01:10:22,948 --> 01:10:28,319
character at a time and so the main

1498
01:10:26,760 --> 01:10:31,469
thing I'm going to try and convince you

1499
01:10:28,319 --> 01:10:34,259
is an RNN is no different to anything

1500
01:10:31,469 --> 01:10:35,699
you've already learned okay and so to

1501
01:10:34,260 --> 01:10:39,630
show you that

1502
01:10:35,699 --> 01:10:41,130
going to build it from plain PI torch

1503
01:10:39,630 --> 01:10:43,680
layers all of which are extremely

1504
01:10:41,130 --> 01:10:44,369
familiar already okay and eventually

1505
01:10:43,680 --> 01:10:47,190
we're going to use something really

1506
01:10:44,369 --> 01:10:48,779
complex which is a for loop okay so

1507
01:10:47,189 --> 01:10:52,169
that's when we're going to make a really

1508
01:10:48,779 --> 01:10:55,639
sophisticated so the basic idea of our n

1509
01:10:52,170 --> 01:10:57,659
ends is that you want to keep track of

1510
01:10:55,640 --> 01:10:59,789
the main thing is you want to keep track

1511
01:10:57,659 --> 01:11:01,889
of kind of state over long term

1512
01:10:59,789 --> 01:11:04,619
dependencies so for example if you're

1513
01:11:01,890 --> 01:11:08,430
trying to model something like this kind

1514
01:11:04,619 --> 01:11:10,619
of template language right then at the

1515
01:11:08,430 --> 01:11:13,020
end of your percent comment blue percent

1516
01:11:10,619 --> 01:11:15,630
you need a percent common end percent

1517
01:11:13,020 --> 01:11:16,920
right and so somehow your model needs to

1518
01:11:15,630 --> 01:11:19,710
keep track of the fact that it's like

1519
01:11:16,920 --> 01:11:21,930
inside a comment over all of these

1520
01:11:19,710 --> 01:11:23,550
different characters right and so this

1521
01:11:21,930 --> 01:11:25,860
is this idea of state it's kind of

1522
01:11:23,550 --> 01:11:29,670
memory right and this is quite a

1523
01:11:25,859 --> 01:11:31,829
difficult thing to do with like just a

1524
01:11:29,670 --> 01:11:34,890
calm confident it turns out actually to

1525
01:11:31,829 --> 01:11:36,239
be possible but it's it's you know a

1526
01:11:34,890 --> 01:11:38,520
little bit tricky

1527
01:11:36,239 --> 01:11:40,619
where elsewhere as an iron in it turns

1528
01:11:38,520 --> 01:11:42,030
out to be pretty straightforward all

1529
01:11:40,619 --> 01:11:43,529
right so these are the basic ideas if

1530
01:11:42,029 --> 01:11:44,729
you want the stateful representation

1531
01:11:43,529 --> 01:11:47,369
where you kind of keeping track of like

1532
01:11:44,729 --> 01:11:49,739
where are we now have memory have long

1533
01:11:47,369 --> 01:11:53,309
term dependencies and potentially even

1534
01:11:49,739 --> 01:11:54,539
have variable length sequences these are

1535
01:11:53,310 --> 01:11:57,030
all difficult things to do with

1536
01:11:54,539 --> 01:12:02,250
confidence they're very straightforward

1537
01:11:57,029 --> 01:12:04,529
with arid ends so for example SwiftKey a

1538
01:12:02,250 --> 01:12:06,840
year or so ago did a blog post about how

1539
01:12:04,529 --> 01:12:08,429
they had a new language model where they

1540
01:12:06,840 --> 01:12:10,949
basically this is from their blog post

1541
01:12:08,430 --> 01:12:14,190
we basically said like of course this is

1542
01:12:10,949 --> 01:12:15,779
what their neural net looks like somehow

1543
01:12:14,189 --> 01:12:17,939
they always look like this on the

1544
01:12:15,779 --> 01:12:19,529
internet you know you've got a bunch of

1545
01:12:17,939 --> 01:12:21,539
words and it's basically going to take

1546
01:12:19,529 --> 01:12:23,189
your particular words in their

1547
01:12:21,539 --> 01:12:25,019
particular orders and try and figure out

1548
01:12:23,189 --> 01:12:27,179
what the next words going to be which is

1549
01:12:25,020 --> 01:12:28,020
to say they built a language model they

1550
01:12:27,180 --> 01:12:30,600
actually have a pretty good language

1551
01:12:28,020 --> 01:12:31,920
model if you've used SwiftKey they seem

1552
01:12:30,600 --> 01:12:35,310
to do better predictions than anybody

1553
01:12:31,920 --> 01:12:36,690
else still another cool example was

1554
01:12:35,310 --> 01:12:39,630
andre capaci a couple of years ago

1555
01:12:36,689 --> 01:12:42,259
showed that he could use character level

1556
01:12:39,630 --> 01:12:45,420
are a 10 to actually create an entire

1557
01:12:42,260 --> 01:12:48,000
latex document so he didn't actually

1558
01:12:45,420 --> 01:12:49,550
tell it in any way what life looks like

1559
01:12:48,000 --> 01:12:52,890
he just passed the

1560
01:12:49,550 --> 01:12:54,600
some may tech text like this and said

1561
01:12:52,890 --> 01:12:56,460
generate more low text text and it

1562
01:12:54,600 --> 01:12:58,890
literally started writing something

1563
01:12:56,460 --> 01:13:05,489
which means about as much to me as most

1564
01:12:58,890 --> 01:13:07,890
math papers do this okay so we're gonna

1565
01:13:05,488 --> 01:13:11,488
start with something that's not an RN

1566
01:13:07,890 --> 01:13:15,329
and I'm going to introduce Jeremy's

1567
01:13:11,488 --> 01:13:20,759
patented neural network notation

1568
01:13:15,329 --> 01:13:23,969
involving boxes circles and triangles so

1569
01:13:20,760 --> 01:13:29,460
let me explain what's going on as a

1570
01:13:23,969 --> 01:13:35,489
rectangle is an input an arrow is a

1571
01:13:29,460 --> 01:13:38,670
layer as a circle in fact every square

1572
01:13:35,488 --> 01:13:41,339
is a bunch of activate so every shape is

1573
01:13:38,670 --> 01:13:43,980
a bunch of activations right the

1574
01:13:41,340 --> 01:13:47,960
rectangle is the input activations the

1575
01:13:43,979 --> 01:13:51,319
circle is a hidden activations and a

1576
01:13:47,960 --> 01:13:56,340
triangle is an output activations and

1577
01:13:51,319 --> 01:13:58,649
arrow is a layer operation right or

1578
01:13:56,340 --> 01:14:02,340
possibly more than one all right so here

1579
01:13:58,649 --> 01:14:04,619
my rectangle is an input of number of

1580
01:14:02,340 --> 01:14:06,659
rows equal a batch size and number of

1581
01:14:04,619 --> 01:14:08,488
columns equal to the number of number of

1582
01:14:06,659 --> 01:14:11,969
inputs number of variables all right and

1583
01:14:08,488 --> 01:14:13,619
so my first arrow my first operation is

1584
01:14:11,969 --> 01:14:17,039
going to represent a matrix product

1585
01:14:13,619 --> 01:14:20,988
followed by our Lu and that's going to

1586
01:14:17,039 --> 01:14:23,939
generate a set of activation remember

1587
01:14:20,988 --> 01:14:27,209
activations like an activation is a

1588
01:14:23,939 --> 01:14:29,669
number that an activation is a number a

1589
01:14:27,210 --> 01:14:32,698
number that's being calculated by a

1590
01:14:29,670 --> 01:14:35,760
value or a matrix product or whatever

1591
01:14:32,698 --> 01:14:39,509
it's a number right so this circle here

1592
01:14:35,760 --> 01:14:41,250
represents a matrix of activations all

1593
01:14:39,510 --> 01:14:43,409
of the numbers that come out when we

1594
01:14:41,250 --> 01:14:45,119
take the inputs we do a matrix product

1595
01:14:43,409 --> 01:14:47,579
followed by a value so we started with

1596
01:14:45,119 --> 01:14:50,189
batch size byte number of inputs and so

1597
01:14:47,579 --> 01:14:54,210
after we do this matrix operation we now

1598
01:14:50,189 --> 01:14:56,939
have batch size by you know whatever the

1599
01:14:54,210 --> 01:15:00,090
number of columns in our matrix product

1600
01:14:56,939 --> 01:15:02,519
was by number of hidden units okay and

1601
01:15:00,090 --> 01:15:04,949
so if we now take these activations

1602
01:15:02,520 --> 01:15:06,540
but it's the matrix and we put it

1603
01:15:04,949 --> 01:15:08,449
through another operation in this case

1604
01:15:06,539 --> 01:15:10,739
another matrix product and the softmax

1605
01:15:08,449 --> 01:15:12,539
we get a triangle that's our output

1606
01:15:10,739 --> 01:15:14,849
activations another matrix of

1607
01:15:12,539 --> 01:15:17,519
activations and again number of roses

1608
01:15:14,850 --> 01:15:18,960
batch size number of columns number is

1609
01:15:17,520 --> 01:15:21,300
equal to the number of classes again

1610
01:15:18,960 --> 01:15:27,329
however many columns our matrix in this

1611
01:15:21,300 --> 01:15:29,520
matrix product head so that's a that's a

1612
01:15:27,329 --> 01:15:34,319
neuro net right that's our basic kind of

1613
01:15:29,520 --> 01:15:35,910
one hidden layer neural net and if you

1614
01:15:34,319 --> 01:15:39,719
haven't written one of these from

1615
01:15:35,909 --> 01:15:41,579
scratch try it you know and in fact in

1616
01:15:39,720 --> 01:15:43,949
lessons nine ten and eleven of the

1617
01:15:41,579 --> 01:15:46,800
machine learning course we do this right

1618
01:15:43,949 --> 01:15:47,909
we create one of these from scratch so

1619
01:15:46,800 --> 01:15:49,529
if you're not quite sure how to do it

1620
01:15:47,909 --> 01:15:51,899
you can check out the machine learning

1621
01:15:49,529 --> 01:15:54,000
costs yeah in general the machine

1622
01:15:51,899 --> 01:15:56,069
learning cost is much more like building

1623
01:15:54,000 --> 01:15:58,199
stuff up from the foundations where else

1624
01:15:56,069 --> 01:16:02,880
this course is much more like best

1625
01:15:58,199 --> 01:16:05,760
practices kind of top-down all right so

1626
01:16:02,880 --> 01:16:08,460
if we were doing like a cognate with a

1627
01:16:05,760 --> 01:16:11,820
single dense hidden layer our input

1628
01:16:08,460 --> 01:16:13,380
would be equal to actually number yeah

1629
01:16:11,819 --> 01:16:16,739
that's very implied watch number of

1630
01:16:13,380 --> 01:16:18,720
channels by height by width right and

1631
01:16:16,739 --> 01:16:20,699
notice that here batch size appeared

1632
01:16:18,720 --> 01:16:21,570
every time so I'm not gonna I'm not

1633
01:16:20,699 --> 01:16:26,069
gonna write it anymore

1634
01:16:21,569 --> 01:16:28,619
okay so I've removed the batch size also

1635
01:16:26,069 --> 01:16:30,779
the activation function it's always

1636
01:16:28,619 --> 01:16:33,059
basically value or something similar for

1637
01:16:30,779 --> 01:16:34,619
all the hidden layers and softmax at the

1638
01:16:33,060 --> 01:16:36,450
end for classification so I'm not going

1639
01:16:34,619 --> 01:16:38,670
to write that either okay so I'm kind of

1640
01:16:36,449 --> 01:16:41,099
edge picture I'm going to simplify it a

1641
01:16:38,670 --> 01:16:42,720
little bit alright so I'm not gonna

1642
01:16:41,100 --> 01:16:43,950
mention batch size it's still there

1643
01:16:42,720 --> 01:16:45,960
we're not going to mention real you or

1644
01:16:43,949 --> 01:16:47,909
softmax but it's still there so here's

1645
01:16:45,960 --> 01:16:50,430
our input and so in this case rather

1646
01:16:47,909 --> 01:16:53,189
than a matrix product will do a

1647
01:16:50,430 --> 01:16:56,100
convolution let's drive to convolution

1648
01:16:53,189 --> 01:16:58,379
so we'll skip over every second one or

1649
01:16:56,100 --> 01:17:01,320
could be a convolution followed by a mac

1650
01:16:58,380 --> 01:17:03,390
spool in either case we end up with

1651
01:17:01,319 --> 01:17:05,880
something which is replaced number of

1652
01:17:03,390 --> 01:17:08,070
channels with number of filters right

1653
01:17:05,880 --> 01:17:09,270
and we have now height divided by two

1654
01:17:08,069 --> 01:17:12,750
and width divided by 2

1655
01:17:09,270 --> 01:17:15,420
okay and then we can flatten that out

1656
01:17:12,750 --> 01:17:15,869
somehow we'll talk next week about the

1657
01:17:15,420 --> 01:17:17,579
main way

1658
01:17:15,869 --> 01:17:19,349
we do that nowadays which is basically

1659
01:17:17,579 --> 01:17:21,809
to do something called an adaptive max

1660
01:17:19,350 --> 01:17:24,770
pooling where we basically get an

1661
01:17:21,810 --> 01:17:28,020
average across the height and the width

1662
01:17:24,770 --> 01:17:29,700
and turn that into a vector anyway

1663
01:17:28,020 --> 01:17:33,240
somehow we flatten it out into a vector

1664
01:17:29,699 --> 01:17:34,920
we can do a matrix product or a couple

1665
01:17:33,239 --> 01:17:38,760
of matrix products we actually tend to

1666
01:17:34,920 --> 01:17:40,770
do in fast AI so that'll be our fully

1667
01:17:38,760 --> 01:17:43,500
connected layer with some number of

1668
01:17:40,770 --> 01:17:46,260
activations final matrix product give us

1669
01:17:43,500 --> 01:17:48,979
some number of classes okay so this is

1670
01:17:46,260 --> 01:17:51,800
our basic component remembering

1671
01:17:48,979 --> 01:17:54,839
rectangles input circle is hidden

1672
01:17:51,800 --> 01:17:59,190
triangle is output all other shapes

1673
01:17:54,840 --> 01:18:02,220
represent a tensor of activations all of

1674
01:17:59,189 --> 01:18:04,559
the arrows represent a operation or lay

1675
01:18:02,220 --> 01:18:06,360
operation all right

1676
01:18:04,560 --> 01:18:07,500
so now that's going to jump to the one

1677
01:18:06,359 --> 01:18:12,210
the first one that we're going to

1678
01:18:07,500 --> 01:18:14,250
actually try to try to create for NLP

1679
01:18:12,210 --> 01:18:17,460
and we're going to basically do exactly

1680
01:18:14,250 --> 01:18:18,930
the same thing as here right and we're

1681
01:18:17,460 --> 01:18:21,480
going to try and predict the third

1682
01:18:18,930 --> 01:18:25,640
character in a three character sequence

1683
01:18:21,479 --> 01:18:29,219
based on the previous two characters so

1684
01:18:25,640 --> 01:18:33,810
our input and again remember we've

1685
01:18:29,220 --> 01:18:35,699
removed the batch size dimension we're

1686
01:18:33,810 --> 01:18:39,390
not saying that we're still here okay

1687
01:18:35,699 --> 01:18:41,099
and also here I've removed the names of

1688
01:18:39,390 --> 01:18:43,680
the layer operations entirely

1689
01:18:41,100 --> 01:18:46,110
okay just keeping simplifying things so

1690
01:18:43,680 --> 01:18:50,700
for example our first import would be

1691
01:18:46,109 --> 01:18:54,659
the first character of each string in

1692
01:18:50,699 --> 01:18:57,869
our mini batch okay and assuming this is

1693
01:18:54,659 --> 01:18:59,489
one hot encoded then the width is just

1694
01:18:57,869 --> 01:19:01,109
however many items there are in the

1695
01:18:59,489 --> 01:19:04,019
vocabulary how many unique characters

1696
01:19:01,109 --> 01:19:06,420
could we have okay we probably won't

1697
01:19:04,020 --> 01:19:08,400
really one hot encoder will feed it in

1698
01:19:06,420 --> 01:19:09,720
as an integer and pretend it's one hot

1699
01:19:08,399 --> 01:19:12,269
encoded by using an embedding layer

1700
01:19:09,720 --> 01:19:15,000
which is mathematically identical okay

1701
01:19:12,270 --> 01:19:16,770
and then we that's going to give us some

1702
01:19:15,000 --> 01:19:23,880
activations which we can stick through a

1703
01:19:16,770 --> 01:19:25,740
fully connected layer okay so we we put

1704
01:19:23,880 --> 01:19:27,029
that through if we click through a fully

1705
01:19:25,739 --> 01:19:29,029
connected layer to get some activations

1706
01:19:27,029 --> 01:19:31,670
we can then put that

1707
01:19:29,029 --> 01:19:34,009
another fully connected layer and now

1708
01:19:31,670 --> 01:19:36,409
we're going to bring in the input of

1709
01:19:34,010 --> 01:19:37,730
character to alright so the character to

1710
01:19:36,409 --> 01:19:39,409
input will be exactly the same

1711
01:19:37,729 --> 01:19:42,379
dimensionality as the character one

1712
01:19:39,409 --> 01:19:45,050
input and we now need to somehow combine

1713
01:19:42,380 --> 01:19:47,989
these two arrows together so we could

1714
01:19:45,050 --> 01:19:50,230
just add them up for instance right

1715
01:19:47,989 --> 01:19:53,599
because remember this arrow here

1716
01:19:50,229 --> 01:19:55,250
represents a matrix product so this

1717
01:19:53,600 --> 01:19:56,600
matrix product is going to spit out the

1718
01:19:55,250 --> 01:19:59,479
same dimensionality as this matrix

1719
01:19:56,600 --> 01:20:02,960
product so we could just add them up to

1720
01:19:59,479 --> 01:20:04,250
create these activations and so now we

1721
01:20:02,960 --> 01:20:05,689
can put that through another matrix

1722
01:20:04,250 --> 01:20:08,060
product and of course remember all these

1723
01:20:05,689 --> 01:20:10,519
metrics products have a RAL you as well

1724
01:20:08,060 --> 01:20:13,850
and this final one will have a softmax

1725
01:20:10,520 --> 01:20:18,350
instead to create our predicted set of

1726
01:20:13,850 --> 01:20:20,329
characters right so it's a standard you

1727
01:20:18,350 --> 01:20:23,090
know two hidden layer

1728
01:20:20,329 --> 01:20:27,050
I guess it's actually three matrix

1729
01:20:23,090 --> 01:20:29,239
products neural net this first one is

1730
01:20:27,050 --> 01:20:32,239
coming through an embedding layer the

1731
01:20:29,239 --> 01:20:34,760
only difference is that we're also got a

1732
01:20:32,239 --> 01:20:36,619
second input coming in here that we're

1733
01:20:34,760 --> 01:20:39,530
just adding in right but it's kind of

1734
01:20:36,619 --> 01:20:46,159
conceptually identical so let's let's

1735
01:20:39,529 --> 01:20:48,979
implement that for Nietzsche all right

1736
01:20:46,159 --> 01:20:51,050
so I'm not going to use torch text I'm

1737
01:20:48,979 --> 01:20:53,449
gonna try not to use almost any fast AI

1738
01:20:51,050 --> 01:20:56,029
so we can see it all kind of again from

1739
01:20:53,449 --> 01:20:59,179
raw right so here's the first 400

1740
01:20:56,029 --> 01:21:02,389
characters of the collected works let's

1741
01:20:59,180 --> 01:21:06,230
grab a set of all of the letters that we

1742
01:21:02,390 --> 01:21:08,090
see there and sort them okay and so a

1743
01:21:06,229 --> 01:21:12,459
set creates all the unique letters so

1744
01:21:08,090 --> 01:21:15,110
we've got 85 unique letters in our vocab

1745
01:21:12,460 --> 01:21:17,210
let's pop up it's nice to put an empty

1746
01:21:15,109 --> 01:21:18,859
kind of a null or some some kind of

1747
01:21:17,210 --> 01:21:20,180
padding character in there for padding

1748
01:21:18,859 --> 01:21:23,589
so we're gonna put a parenting character

1749
01:21:20,180 --> 01:21:29,030
at the start right and so here is what

1750
01:21:23,590 --> 01:21:33,350
our vocab looks like okay so so Kars is

1751
01:21:29,029 --> 01:21:37,460
our bouquet so as per usual we want some

1752
01:21:33,350 --> 01:21:41,200
way to map every character to a unique

1753
01:21:37,460 --> 01:21:42,600
ID and every unique ID to a character

1754
01:21:41,199 --> 01:21:44,210
and

1755
01:21:42,600 --> 01:21:48,390
so now we can just go through our

1756
01:21:44,210 --> 01:21:51,119
collected works of niche and grab the

1757
01:21:48,390 --> 01:21:54,720
index of each one of those characters so

1758
01:21:51,119 --> 01:22:01,140
now we've just turned it into this right

1759
01:21:54,720 --> 01:22:08,010
so rather than quote PR e we now have 40

1760
01:22:01,140 --> 01:22:10,500
42 29 okay so so that's basically the

1761
01:22:08,010 --> 01:22:13,560
first step and just to confirm we can

1762
01:22:10,500 --> 01:22:15,539
now take each of those indexes and turn

1763
01:22:13,560 --> 01:22:20,060
them back into characters and join them

1764
01:22:15,539 --> 01:22:21,930
together and yeah there it is okay so

1765
01:22:20,060 --> 01:22:23,430
from now on we're just going to work

1766
01:22:21,930 --> 01:22:26,310
with this IDX

1767
01:22:23,430 --> 01:22:30,329
list the list of character members in

1768
01:22:26,310 --> 01:22:33,360
the connected works of Nietzsche yes so

1769
01:22:30,329 --> 01:22:36,539
Jeremy why are we doing like a model of

1770
01:22:33,359 --> 01:22:38,429
characters and not a model of words I

1771
01:22:36,539 --> 01:22:42,180
just thought it seemed simpler you know

1772
01:22:38,430 --> 01:22:47,510
with a vocab of 80-ish items we can kind

1773
01:22:42,180 --> 01:22:50,100
of see it better character level models

1774
01:22:47,510 --> 01:22:51,600
turn out to be potentially quite useful

1775
01:22:50,100 --> 01:22:54,390
in a number of situations but we'll

1776
01:22:51,600 --> 01:22:57,030
cover that in part two the short answer

1777
01:22:54,390 --> 01:22:58,560
is like you generally want to combine

1778
01:22:57,029 --> 01:22:59,849
both the word level model and a connect

1779
01:22:58,560 --> 01:23:02,250
character level model like if you're

1780
01:22:59,850 --> 01:23:04,140
doing say translation it's a great way

1781
01:23:02,250 --> 01:23:05,970
to deal with unknown like unusual words

1782
01:23:04,140 --> 01:23:07,440
rather than treating it as unknown

1783
01:23:05,970 --> 01:23:09,390
anytime you see a word you haven't seen

1784
01:23:07,439 --> 01:23:11,819
before you could use a character level

1785
01:23:09,390 --> 01:23:13,200
model for that and there's actually

1786
01:23:11,819 --> 01:23:15,809
something in between the two quarter

1787
01:23:13,199 --> 01:23:18,569
byte pair and coding vpe which basically

1788
01:23:15,810 --> 01:23:22,830
looks at at all engrams of characters

1789
01:23:18,569 --> 01:23:24,019
but we'll cover all that in part two if

1790
01:23:22,829 --> 01:23:27,029
you want to look at it right now

1791
01:23:24,020 --> 01:23:30,540
then part two of the existing course

1792
01:23:27,029 --> 01:23:33,170
already has this stuff taught and part

1793
01:23:30,539 --> 01:23:35,819
two of the version 1 of this course

1794
01:23:33,170 --> 01:23:38,430
although the NLP stuff is in flight

1795
01:23:35,819 --> 01:23:41,639
which by the way so you'll understand it

1796
01:23:38,430 --> 01:23:43,110
straight away it was actually the thing

1797
01:23:41,640 --> 01:23:44,940
that inspired us to move to piped watch

1798
01:23:43,109 --> 01:23:48,750
because trying to do it in chaos turned

1799
01:23:44,939 --> 01:23:52,649
out to be a nightmare all right so let's

1800
01:23:48,750 --> 01:23:53,939
create the inputs to this we're actually

1801
01:23:52,649 --> 01:23:55,679
going to do something slightly different

1802
01:23:53,939 --> 01:23:58,018
what I said we're actually going to

1803
01:23:55,679 --> 01:24:00,658
I predict the fourth character that

1804
01:23:58,019 --> 01:24:02,340
actually this the fifth character using

1805
01:24:00,658 --> 01:24:04,558
the first four so the index four

1806
01:24:02,340 --> 01:24:06,538
character using the index zero one two

1807
01:24:04,559 --> 01:24:08,458
and three okay so it was exactly the

1808
01:24:06,538 --> 01:24:11,609
same thing but with just a couple more

1809
01:24:08,458 --> 01:24:17,908
layers so that means that we need a list

1810
01:24:11,609 --> 01:24:19,649
of the zeroth first second and third

1811
01:24:17,908 --> 01:24:21,929
characters that's why I'm just cutting

1812
01:24:19,649 --> 01:24:24,978
every character from the start from the

1813
01:24:21,929 --> 01:24:28,559
one from two from three skipping over

1814
01:24:24,979 --> 01:24:32,610
three at a time okay

1815
01:24:28,559 --> 01:24:36,840
so hmm

1816
01:24:32,609 --> 01:24:39,179
this is I I said this wrong so we're

1817
01:24:36,840 --> 01:24:41,069
going to predict the third character the

1818
01:24:39,179 --> 01:24:42,479
fourth character from the third for the

1819
01:24:41,069 --> 01:24:46,109
first story okay

1820
01:24:42,479 --> 01:24:50,489
the fourth character is history

1821
01:24:46,109 --> 01:24:53,908
all right so our inputs will be these

1822
01:24:50,488 --> 01:24:57,538
three lists right so we can just use n P

1823
01:24:53,908 --> 01:25:01,638
dot stack to pop them together all right

1824
01:24:57,538 --> 01:25:04,050
so here's the zero one and two

1825
01:25:01,639 --> 01:25:05,519
characters that are going to feed into a

1826
01:25:04,050 --> 01:25:13,729
model and then here is the next

1827
01:25:05,519 --> 01:25:23,248
character in the list so for example X 1

1828
01:25:13,729 --> 01:25:25,829
X 2 X 3 and Y all right so you can see

1829
01:25:23,248 --> 01:25:33,748
for example we start off the first the

1830
01:25:25,828 --> 01:25:36,538
very first item would be 40 42 and 29

1831
01:25:33,748 --> 01:25:40,948
right so that's characters naught 1 and

1832
01:25:36,538 --> 01:25:44,788
2 and then we'd be predicting 30 that's

1833
01:25:40,948 --> 01:25:46,288
the fourth character which is the start

1834
01:25:44,788 --> 01:25:49,978
of the next row

1835
01:25:46,288 --> 01:25:52,738
all right so then 30 25 27 we need to

1836
01:25:49,979 --> 01:25:54,900
predict 29 which is the start of next

1837
01:25:52,738 --> 01:25:59,029
row and so forth so we're always using

1838
01:25:54,899 --> 01:26:05,188
three characters to predict the fourth

1839
01:25:59,029 --> 01:26:08,550
so there are 200,000 of these that we're

1840
01:26:05,189 --> 01:26:09,298
going to try and model right so we're

1841
01:26:08,550 --> 01:26:11,159
going to build this

1842
01:26:09,298 --> 01:26:18,298
which means we need to decide how many

1843
01:26:11,158 --> 01:26:19,948
activations so I'm going to use 256 okay

1844
01:26:18,298 --> 01:26:21,899
and we need to decide how big our

1845
01:26:19,948 --> 01:26:24,028
embeddings are going to be and so I

1846
01:26:21,899 --> 01:26:27,449
decided to use 42 so about half the

1847
01:26:24,029 --> 01:26:28,709
number of characters I have and you can

1848
01:26:27,448 --> 01:26:30,148
play around these so you can come up

1849
01:26:28,708 --> 01:26:32,729
with better numbers it's just a kind of

1850
01:26:30,149 --> 01:26:35,579
experimental and now we're going to

1851
01:26:32,729 --> 01:26:38,579
build our model now I'm gonna change my

1852
01:26:35,578 --> 01:26:41,248
model slightly and so here is the the

1853
01:26:38,578 --> 01:26:44,398
full version so predicting character for

1854
01:26:41,248 --> 01:26:45,868
using characters 1 2 &amp; 3 as you can see

1855
01:26:44,399 --> 01:26:48,298
it's the same picture as a previous page

1856
01:26:45,868 --> 01:26:51,298
but I put some very important coloured

1857
01:26:48,298 --> 01:26:54,628
arrows here all the arrows of the same

1858
01:26:51,298 --> 01:26:58,319
color are going to use the same matrix

1859
01:26:54,628 --> 01:27:00,958
the same weight matrix right so all of

1860
01:26:58,319 --> 01:27:05,609
our input embeddings are going to use

1861
01:27:00,958 --> 01:27:07,528
the same matrix all of our layers that

1862
01:27:05,609 --> 01:27:09,689
go from one layer to the next they're

1863
01:27:07,529 --> 01:27:13,199
going to use the same orange arrow

1864
01:27:09,689 --> 01:27:15,359
weight matrix and then our output will

1865
01:27:13,198 --> 01:27:19,858
have its own matrix so we're going to

1866
01:27:15,359 --> 01:27:22,499
have one two three weight matrices right

1867
01:27:19,859 --> 01:27:24,088
and the idea here is the reason I'm not

1868
01:27:22,498 --> 01:27:27,358
gonna have a separate one but every

1869
01:27:24,088 --> 01:27:29,368
everything here is that like why would

1870
01:27:27,359 --> 01:27:31,048
kind of semantically a carrot to have a

1871
01:27:29,368 --> 01:27:33,028
different meaning depending if it's the

1872
01:27:31,048 --> 01:27:34,559
first or the second or the third item in

1873
01:27:33,029 --> 01:27:36,599
a sequence like it's not like we're even

1874
01:27:34,559 --> 01:27:37,949
starting every sequence at the start of

1875
01:27:36,599 --> 01:27:39,958
a sentence we're just arbitrarily

1876
01:27:37,948 --> 01:27:41,338
chopped it into groups of three right so

1877
01:27:39,958 --> 01:27:44,188
you would expect these to all have the

1878
01:27:41,338 --> 01:27:45,809
same kind of conceptual mapping and

1879
01:27:44,189 --> 01:27:48,659
ditto like when we're moving from

1880
01:27:45,809 --> 01:27:51,149
claritin or character one you know to

1881
01:27:48,658 --> 01:27:52,498
kind of say build up some state here why

1882
01:27:51,149 --> 01:27:53,819
would that be any different kind of

1883
01:27:52,498 --> 01:27:56,938
operation to moving from character

1884
01:27:53,819 --> 01:28:00,599
wonder character to so that's the basic

1885
01:27:56,939 --> 01:28:04,260
idea so let's create a three character

1886
01:28:00,599 --> 01:28:06,899
model and so we're going to create one

1887
01:28:04,260 --> 01:28:09,269
linear layer for our Green Arrow one

1888
01:28:06,899 --> 01:28:11,819
linear layer fat orange arrow and one

1889
01:28:09,269 --> 01:28:16,529
linear layer for our blue arrow and then

1890
01:28:11,819 --> 01:28:18,959
also one embedding okay so the embedding

1891
01:28:16,529 --> 01:28:20,609
is going to bring in something with size

1892
01:28:18,958 --> 01:28:22,139
whatever it was 84

1893
01:28:20,609 --> 01:28:23,010
I think vocab size and spit out

1894
01:28:22,139 --> 01:28:26,429
something with an

1895
01:28:23,010 --> 01:28:29,519
factors in the embedding well then put

1896
01:28:26,429 --> 01:28:30,750
that through a linear layer and then

1897
01:28:29,519 --> 01:28:34,650
we've got our hidden layers before the

1898
01:28:30,750 --> 01:28:37,109
output layer so when we call forward

1899
01:28:34,649 --> 01:28:40,259
they're going to be passing in one two

1900
01:28:37,109 --> 01:28:42,479
three characters so if each one will

1901
01:28:40,260 --> 01:28:44,639
stick it through an embedding we'll

1902
01:28:42,479 --> 01:28:46,768
stick it through a linear layer and

1903
01:28:44,639 --> 01:28:48,599
we'll stick it through a value just to

1904
01:28:46,769 --> 01:28:51,920
do it the character one character - and

1905
01:28:48,599 --> 01:29:05,099
character three okay

1906
01:28:51,920 --> 01:29:07,769
then I'm going to create this circle of

1907
01:29:05,099 --> 01:29:10,319
activations here okay and that matrix

1908
01:29:07,769 --> 01:29:12,539
I'm going to call H right and so it's

1909
01:29:10,319 --> 01:29:17,250
going to be equal to my input

1910
01:29:12,538 --> 01:29:19,590
activations okay after going through the

1911
01:29:17,250 --> 01:29:21,510
value and the linear layer and the

1912
01:29:19,590 --> 01:29:26,819
embedding right and then I'm going to

1913
01:29:21,510 --> 01:29:30,239
apply this l hidden so the orange arrow

1914
01:29:26,819 --> 01:29:32,488
and that's going to get me to here okay

1915
01:29:30,238 --> 01:29:34,709
so that's what this layer here does and

1916
01:29:32,488 --> 01:29:36,959
then to get to the next one I need to

1917
01:29:34,710 --> 01:29:40,170
reply the same thing and it apply the

1918
01:29:36,960 --> 01:29:43,710
orange arrow to that okay but I also

1919
01:29:40,170 --> 01:29:48,480
have to add in this second input right

1920
01:29:43,710 --> 01:29:52,618
so take my second input and add in okay

1921
01:29:48,479 --> 01:29:58,859
my previous layer your neck could you

1922
01:29:52,618 --> 01:30:01,979
pass it back three rows I don't really

1923
01:29:58,859 --> 01:30:05,250
see how these dimensions are the same

1924
01:30:01,979 --> 01:30:07,979
from eight and in2 from literature which

1925
01:30:05,250 --> 01:30:10,309
from yeah okay let's go through so let's

1926
01:30:07,979 --> 01:30:16,078
figure out the dimensions together so

1927
01:30:10,309 --> 01:30:18,480
self dot E is gonna be of length 42 okay

1928
01:30:16,078 --> 01:30:24,000
and then it's gonna go through L in I'm

1929
01:30:18,479 --> 01:30:26,459
just gonna make it of size n hidden okay

1930
01:30:24,000 --> 01:30:30,868
and so then we're going to pass that

1931
01:30:26,460 --> 01:30:34,349
which is now size n hidden through this

1932
01:30:30,868 --> 01:30:35,969
which is also going to return something

1933
01:30:34,349 --> 01:30:37,619
of size n hidden

1934
01:30:35,970 --> 01:30:39,180
okay so it's a really important to

1935
01:30:37,619 --> 01:30:43,229
notice that this is square this is a

1936
01:30:39,180 --> 01:30:46,289
square weight matrix okay so we now know

1937
01:30:43,229 --> 01:30:47,849
that this is of size n hidden into it's

1938
01:30:46,289 --> 01:30:50,460
going to be exactly the same size as in

1939
01:30:47,850 --> 01:30:53,670
one was which is n hidden so we can now

1940
01:30:50,460 --> 01:30:56,399
sum together two sets of activations

1941
01:30:53,670 --> 01:30:59,190
both the size n hidden passing it into

1942
01:30:56,399 --> 01:31:01,769
here and again it returns something of

1943
01:30:59,189 --> 01:31:03,779
size n hidden so basically the trick was

1944
01:31:01,770 --> 01:31:05,250
to make this a square matrix and to make

1945
01:31:03,779 --> 01:31:06,899
sure that it's square matrix was the

1946
01:31:05,250 --> 01:31:09,329
same size as the output of this hidden

1947
01:31:06,899 --> 01:31:16,729
well thanks for the great question can

1948
01:31:09,329 --> 01:31:19,920
you pass that out to you now Jeremy is

1949
01:31:16,729 --> 01:31:21,959
summing the only thing people can do in

1950
01:31:19,920 --> 01:31:25,500
these cases I'll come back to that in a

1951
01:31:21,960 --> 01:31:28,260
moment that's great point okay um I

1952
01:31:25,500 --> 01:31:29,789
don't like it when I have like three

1953
01:31:28,260 --> 01:31:31,050
bits of code that look identical and

1954
01:31:29,789 --> 01:31:32,970
then three bits of codes that look

1955
01:31:31,050 --> 01:31:35,400
nearly identical but aren't quiet

1956
01:31:32,970 --> 01:31:40,199
because it's harder to refactor so I'm

1957
01:31:35,399 --> 01:31:45,389
going to put a make H into a bunch of

1958
01:31:40,199 --> 01:31:49,109
zeros so that I can then put H here and

1959
01:31:45,390 --> 01:31:51,600
these are now identical okay so that the

1960
01:31:49,109 --> 01:31:54,359
hugely complex trick that we're going to

1961
01:31:51,600 --> 01:31:57,270
do very shortly is to replace these

1962
01:31:54,359 --> 01:32:00,809
three things with a for loop okay and

1963
01:31:57,270 --> 01:32:02,940
it's going to loop through one two and

1964
01:32:00,810 --> 01:32:05,130
three that's that's going to be the for

1965
01:32:02,939 --> 01:32:06,329
loop or actually zero one two okay at

1966
01:32:05,130 --> 01:32:09,000
that point we'll be able to call it a

1967
01:32:06,329 --> 01:32:12,119
recurrent neural network okay so just to

1968
01:32:09,000 --> 01:32:16,470
skip ahead a little bit alright so we

1969
01:32:12,119 --> 01:32:17,640
create that that model make sure I've

1970
01:32:16,470 --> 01:32:24,869
run all these so we can actually run

1971
01:32:17,640 --> 01:32:26,730
this thing okay so we can now just use

1972
01:32:24,869 --> 01:32:28,710
the same columnar model data class that

1973
01:32:26,729 --> 01:32:31,649
we've used before and if we use from

1974
01:32:28,710 --> 01:32:33,569
arrays then it's basically it's going to

1975
01:32:31,649 --> 01:32:36,029
spit back the exact arrays we gave it

1976
01:32:33,569 --> 01:32:38,489
right so if we pass if we stack together

1977
01:32:36,029 --> 01:32:40,619
those three arrays then it's going to

1978
01:32:38,489 --> 01:32:43,429
feed us those three things back to our

1979
01:32:40,619 --> 01:32:47,640
forward method so if you want to like

1980
01:32:43,430 --> 01:32:49,889
play around with training models using

1981
01:32:47,640 --> 01:32:51,840
like you know as roar

1982
01:32:49,889 --> 01:32:53,578
approach as possible but without writing

1983
01:32:51,840 --> 01:32:55,649
lots of boilerplate this is kind of how

1984
01:32:53,578 --> 01:32:58,349
to do it here's column Namit model data

1985
01:32:55,649 --> 01:33:01,530
from arrays and then if you pass in

1986
01:32:58,349 --> 01:33:06,958
whatever you pass in here right you're

1987
01:33:01,529 --> 01:33:09,090
going to get back here okay so I've

1988
01:33:06,958 --> 01:33:11,368
passed in three things which means I'm

1989
01:33:09,090 --> 01:33:14,059
going to get sent three things okay so

1990
01:33:11,368 --> 01:33:14,058
that's how that works

1991
01:33:14,189 --> 01:33:18,209
batch size 512 because this is you know

1992
01:33:16,590 --> 01:33:23,309
this data is tiny so I can use a bigger

1993
01:33:18,208 --> 01:33:25,229
batch size so I'm not using really much

1994
01:33:23,309 --> 01:33:27,059
faster i stuff at all I'm using fast AI

1995
01:33:25,229 --> 01:33:28,530
stuff just to save me fiddling around

1996
01:33:27,059 --> 01:33:30,239
with data loaders and data sets and

1997
01:33:28,529 --> 01:33:33,208
stuff but I'm actually going to create a

1998
01:33:30,238 --> 01:33:35,129
standard ply torch model I'm not going

1999
01:33:33,208 --> 01:33:36,929
to create a loner okay so this is a

2000
01:33:35,130 --> 01:33:38,248
standard paper model and because I'm

2001
01:33:36,929 --> 01:33:40,618
using ply towards that means I have to

2002
01:33:38,248 --> 01:33:49,019
remember to write CUDA okay let's tick

2003
01:33:40,618 --> 01:33:50,938
it on the GPU so here is how we can look

2004
01:33:49,019 --> 01:33:54,269
inside at what's going on right so we

2005
01:33:50,939 --> 01:33:56,429
can say it er MD train data loader to

2006
01:33:54,269 --> 01:33:59,309
grab the iterator to iterate through the

2007
01:33:56,429 --> 01:34:01,529
training set we can then call next on

2008
01:33:59,309 --> 01:34:05,369
that to grab a mini batch and that's

2009
01:34:01,529 --> 01:34:08,279
going to return all of our X's and why

2010
01:34:05,368 --> 01:34:13,618
tensor and so we can then take a look at

2011
01:34:08,279 --> 01:34:16,168
you know here's our X's for example all

2012
01:34:13,618 --> 01:34:17,488
right and so you would expect have a

2013
01:34:16,168 --> 01:34:22,050
think about what you would expect for

2014
01:34:17,488 --> 01:34:24,299
this length three not surprisingly

2015
01:34:22,050 --> 01:34:32,458
because these are the three things okay

2016
01:34:24,300 --> 01:34:37,110
and so then XS 0 not surprisingly okay

2017
01:34:32,458 --> 01:34:38,578
is of length 512 and it's not actually

2018
01:34:37,109 --> 01:34:41,398
one hot encoded because we use an

2019
01:34:38,578 --> 01:34:42,958
embedding to pretend it is okay and so

2020
01:34:41,399 --> 01:34:46,099
then we can use a model as if it's a

2021
01:34:42,958 --> 01:34:49,078
function okay by passing to it

2022
01:34:46,099 --> 01:34:51,958
the variable eyes version of our tensors

2023
01:34:49,078 --> 01:34:56,279
and so have a think about what you would

2024
01:34:51,958 --> 01:34:58,349
expect to be returned here okay so not

2025
01:34:56,279 --> 01:35:01,438
surprisingly we had a mini batch of 512

2026
01:34:58,349 --> 01:35:03,460
so we still have 5 12 and then 85 is the

2027
01:35:01,439 --> 01:35:05,739
probability of each of the possible

2028
01:35:03,460 --> 01:35:07,480
vocab items and of course we've got the

2029
01:35:05,738 --> 01:35:09,819
log of them because that's kind of what

2030
01:35:07,479 --> 01:35:12,250
we do in pi torch okay you can see here

2031
01:35:09,819 --> 01:35:14,289
the softmax alright so that's how you

2032
01:35:12,250 --> 01:35:15,880
can look inside alright so you can see

2033
01:35:14,289 --> 01:35:19,630
here how to do everything really very

2034
01:35:15,880 --> 01:35:22,270
much by hand so we can create an

2035
01:35:19,630 --> 01:35:24,969
optimizer again using standard pipe

2036
01:35:22,270 --> 01:35:27,160
torch so with PI torch when you use a

2037
01:35:24,969 --> 01:35:29,739
plate or optimizer you have to pass in a

2038
01:35:27,159 --> 01:35:31,510
list of the things to optimize and so if

2039
01:35:29,738 --> 01:35:34,718
you call m dot parameters that will

2040
01:35:31,510 --> 01:35:37,750
return that list for you and then we can

2041
01:35:34,719 --> 01:35:43,899
fit and there it goes

2042
01:35:37,750 --> 01:35:46,180
okay and so we don't have learning rate

2043
01:35:43,899 --> 01:35:47,469
finders and sttr and all that stuff

2044
01:35:46,180 --> 01:35:48,940
because we're not using a learner so

2045
01:35:47,469 --> 01:35:50,529
we'll have to manually do learning rate

2046
01:35:48,939 --> 01:35:57,629
annealing so set the learning rate a

2047
01:35:50,529 --> 01:36:01,149
little bit lower and fit again okay and

2048
01:35:57,630 --> 01:36:03,730
so now we can write a little function to

2049
01:36:01,149 --> 01:36:10,629
to test this thing out okay

2050
01:36:03,729 --> 01:36:14,859
so here's something called getnext where

2051
01:36:10,630 --> 01:36:17,980
we can pass in three characters like why

2052
01:36:14,859 --> 01:36:19,750
full top space right and so I can then

2053
01:36:17,979 --> 01:36:24,250
go through and turn that into a tensor

2054
01:36:19,750 --> 01:36:26,590
with capital T of an array of the

2055
01:36:24,250 --> 01:36:27,159
character index for each character in

2056
01:36:26,590 --> 01:36:28,420
that list

2057
01:36:27,159 --> 01:36:31,689
so basically turn those into the

2058
01:36:28,420 --> 01:36:35,079
integers turn those into variables pass

2059
01:36:31,689 --> 01:36:38,139
that to our model right and then we can

2060
01:36:35,079 --> 01:36:40,719
do an Arg max on that to grab which

2061
01:36:38,140 --> 01:36:43,360
character number is it and in order to

2062
01:36:40,719 --> 01:36:45,730
do stuff in none pile and I use two NP

2063
01:36:43,359 --> 01:36:47,319
to turn that variable into a lumpy array

2064
01:36:45,729 --> 01:36:49,750
right and then I can return that

2065
01:36:47,319 --> 01:36:51,309
character and so for example a capital T

2066
01:36:49,750 --> 01:36:55,090
because what it thinks would be

2067
01:36:51,310 --> 01:36:56,200
reasonable after seeing why . space that

2068
01:36:55,090 --> 01:37:00,219
seems like a very reasonable way to

2069
01:36:56,199 --> 01:37:02,590
start a sentence if it was ppl a that

2070
01:37:00,219 --> 01:37:05,109
sounds reasonable space th a that's

2071
01:37:02,590 --> 01:37:05,710
bouncer e small a and D space that

2072
01:37:05,109 --> 01:37:08,819
sounds reasonable

2073
01:37:05,710 --> 01:37:11,890
so it seems to reflect created something

2074
01:37:08,819 --> 01:37:15,000
sensible alright so you know the

2075
01:37:11,890 --> 01:37:17,320
important thing to note here is our

2076
01:37:15,000 --> 01:37:21,609
character model

2077
01:37:17,319 --> 01:37:23,889
is a totally standard fully connected

2078
01:37:21,609 --> 01:37:27,189
model right the only slightly

2079
01:37:23,890 --> 01:37:31,869
interesting thing we did was to kind of

2080
01:37:27,189 --> 01:37:36,539
do this addition of each of the inputs

2081
01:37:31,869 --> 01:37:39,579
one at a time okay but there's nothing

2082
01:37:36,539 --> 01:37:42,510
new conceptually here we're training it

2083
01:37:39,579 --> 01:37:42,510
in the usual way

2084
01:37:42,960 --> 01:37:59,439
all right let's now create an errand in

2085
01:37:49,500 --> 01:38:03,310
so an iron in is when we do exactly the

2086
01:37:59,439 --> 01:38:05,799
same thing that we did here all right

2087
01:38:03,310 --> 01:38:07,210
but I could draw this more simply by

2088
01:38:05,800 --> 01:38:10,210
saying you know what if we've got a

2089
01:38:07,210 --> 01:38:12,189
green arrow going to a circle let's not

2090
01:38:10,210 --> 01:38:14,710
draw a green arrow go into a circle

2091
01:38:12,189 --> 01:38:16,359
again and again and again so let's just

2092
01:38:14,710 --> 01:38:18,550
draw it like this green arrow going to a

2093
01:38:16,359 --> 01:38:20,439
circle right and rather than drawing an

2094
01:38:18,550 --> 01:38:24,279
orange arrow going to a circle let's

2095
01:38:20,439 --> 01:38:27,369
just draw it like this okay so this is

2096
01:38:24,279 --> 01:38:32,529
the same picture exactly the same

2097
01:38:27,369 --> 01:38:33,819
picture as this one right and so you

2098
01:38:32,529 --> 01:38:35,949
just have to say how many times to go

2099
01:38:33,819 --> 01:38:37,539
around this circle right so in this case

2100
01:38:35,949 --> 01:38:39,789
if we were to predict character number n

2101
01:38:37,539 --> 01:38:41,769
from characters one through n minus one

2102
01:38:39,789 --> 01:38:44,710
then we can take the character one input

2103
01:38:41,770 --> 01:38:46,870
get some activations feed that to some

2104
01:38:44,710 --> 01:38:49,270
new activations that go through remember

2105
01:38:46,869 --> 01:38:51,729
orange is the hidden to hidden weight

2106
01:38:49,270 --> 01:38:53,560
matrix right and each time we'll also

2107
01:38:51,729 --> 01:38:57,369
bring in the next character of input

2108
01:38:53,560 --> 01:39:01,480
through its embeddings okay so that

2109
01:38:57,369 --> 01:39:04,210
picture and that picture I have two ways

2110
01:39:01,479 --> 01:39:06,309
of writing the same thing but this one

2111
01:39:04,210 --> 01:39:07,930
is more flexible because rather than me

2112
01:39:06,310 --> 01:39:10,360
having to say hey let's do it for H I

2113
01:39:07,930 --> 01:39:16,329
don't have to draw eight circles right I

2114
01:39:10,359 --> 01:39:17,969
can just say I'll just repeat this so I

2115
01:39:16,329 --> 01:39:21,670
could simplify this a little bit further

2116
01:39:17,970 --> 01:39:23,760
by saying you know what rather than

2117
01:39:21,670 --> 01:39:26,170
having this thing as a special case

2118
01:39:23,760 --> 01:39:28,480
let's actually start out with a bunch of

2119
01:39:26,170 --> 01:39:30,699
zeros right and then let's have all of

2120
01:39:28,479 --> 01:39:39,549
our characters

2121
01:39:30,698 --> 01:39:41,888
inside here yes yeah so I was wondering

2122
01:39:39,550 --> 01:39:45,909
if you can explain it be better why are

2123
01:39:41,889 --> 01:39:47,800
you reusing those why you think oh

2124
01:39:45,908 --> 01:39:50,498
they're the same yeah where are you you

2125
01:39:47,800 --> 01:39:52,929
kind of seem to be reusing the same same

2126
01:39:50,498 --> 01:39:55,118
weight matrices weight matrices yeah

2127
01:39:52,929 --> 01:39:57,368
maybe this is kind of similar to what we

2128
01:39:55,118 --> 01:40:00,009
did in convolution your Nets like if

2129
01:39:57,368 --> 01:40:02,679
somehow no I don't think so

2130
01:40:00,010 --> 01:40:05,679
at least not that I can see so the idea

2131
01:40:02,679 --> 01:40:12,578
is just kind of semantically speaking

2132
01:40:05,679 --> 01:40:17,498
like this arrow here this this arrow

2133
01:40:12,578 --> 01:40:21,998
here is saying take a character of

2134
01:40:17,498 --> 01:40:24,908
import and represented as some says some

2135
01:40:21,998 --> 01:40:26,019
set of features right and this arrow is

2136
01:40:24,908 --> 01:40:27,248
saying the same thing take some

2137
01:40:26,019 --> 01:40:30,179
character and represent as a set of

2138
01:40:27,248 --> 01:40:32,920
features and so is this one okay so like

2139
01:40:30,179 --> 01:40:34,300
why would the three be represented with

2140
01:40:32,920 --> 01:40:36,519
different weight matrices because it's

2141
01:40:34,300 --> 01:40:40,900
all doing the same thing right and this

2142
01:40:36,519 --> 01:40:44,019
orange arrow is saying kind of

2143
01:40:40,899 --> 01:40:46,768
transition from character 0 state to

2144
01:40:44,019 --> 01:40:49,539
character 1 state 2 characters to state

2145
01:40:46,769 --> 01:40:51,550
again it's it's the same thing it's like

2146
01:40:49,538 --> 01:40:53,408
why would the transition from character

2147
01:40:51,550 --> 01:40:55,630
0 to 1 be different to character from

2148
01:40:53,408 --> 01:40:59,978
transition from one or two so the idea

2149
01:40:55,630 --> 01:41:02,949
is like but is to like say hey if if

2150
01:40:59,979 --> 01:41:07,360
it's doing the same conceptual thing

2151
01:41:02,948 --> 01:41:09,009
let's use the exact same white matrix my

2152
01:41:07,359 --> 01:41:12,158
comment on convolution neural networks

2153
01:41:09,010 --> 01:41:16,599
is that a filter or so this apply to

2154
01:41:12,158 --> 01:41:18,698
multiple places I think something like a

2155
01:41:16,599 --> 01:41:21,130
convolution is almost like a kind of a

2156
01:41:18,698 --> 01:41:22,629
special dot product with shared weights

2157
01:41:21,130 --> 01:41:25,989
yeah no that's okay

2158
01:41:22,630 --> 01:41:27,340
that's very good point and in fact one

2159
01:41:25,988 --> 01:41:29,049
of our students actually wrote a good

2160
01:41:27,340 --> 01:41:31,300
blog post about that last year we should

2161
01:41:29,050 --> 01:41:32,559
dig that up okay I totally see where

2162
01:41:31,300 --> 01:41:37,150
you're coming from and I totally agree

2163
01:41:32,559 --> 01:41:41,498
with you all right so let's let's

2164
01:41:37,149 --> 01:41:43,359
implement this version so this time

2165
01:41:41,498 --> 01:41:48,760
we're going to do eight

2166
01:41:43,359 --> 01:41:52,839
as eight sees okay and so let's create a

2167
01:41:48,760 --> 01:41:55,119
list of every eighth character from zero

2168
01:41:52,840 --> 01:41:57,400
through seven and then our outputs will

2169
01:41:55,118 --> 01:42:00,158
be the next character and so we can

2170
01:41:57,399 --> 01:42:05,289
stack them together and so now we've got

2171
01:42:00,158 --> 01:42:11,129
six hundred thousand by eight so here's

2172
01:42:05,289 --> 01:42:15,698
an example so for example after this

2173
01:42:11,130 --> 01:42:17,230
series of eight characters right so this

2174
01:42:15,698 --> 01:42:20,049
is characters north through eight

2175
01:42:17,229 --> 01:42:21,789
this is characters one through nine this

2176
01:42:20,050 --> 01:42:25,119
is two through ten these are all

2177
01:42:21,789 --> 01:42:27,189
overlapping okay so after characters one

2178
01:42:25,118 --> 01:42:30,189
north through eight this is going to be

2179
01:42:27,189 --> 01:42:33,129
the next one okay and then after these

2180
01:42:30,189 --> 01:42:35,169
characters this will be the next one all

2181
01:42:33,130 --> 01:42:39,069
right so you can see that this one here

2182
01:42:35,170 --> 01:42:42,880
has 43 is its Y value right because

2183
01:42:39,069 --> 01:42:46,380
after those the next one will be 43 okay

2184
01:42:42,880 --> 01:42:49,868
so so this is the first eight characters

2185
01:42:46,380 --> 01:42:51,279
this is two through nine three through

2186
01:42:49,868 --> 01:42:53,139
ten and so forth right so these are

2187
01:42:51,279 --> 01:42:57,269
overlapping groups of eight characters

2188
01:42:53,139 --> 01:43:06,639
and then this is the the next one okay

2189
01:42:57,270 --> 01:43:09,130
so let's create that model okay so again

2190
01:43:06,639 --> 01:43:12,250
we use from arrays to create a model

2191
01:43:09,130 --> 01:43:14,109
data class and so you'll see here we

2192
01:43:12,250 --> 01:43:14,500
have exactly the same code as we had

2193
01:43:14,109 --> 01:43:18,219
before

2194
01:43:14,500 --> 01:43:20,100
there's our embedding Linea hidden

2195
01:43:18,219 --> 01:43:25,210
output these are literally identical

2196
01:43:20,100 --> 01:43:27,280
okay and then we've replaced our value

2197
01:43:25,210 --> 01:43:30,489
of the linear input of the embedding

2198
01:43:27,279 --> 01:43:33,309
with something that's inside a loop okay

2199
01:43:30,488 --> 01:43:35,049
and then we've replaced the cell hidden

2200
01:43:33,310 --> 01:43:38,730
thing okay

2201
01:43:35,050 --> 01:43:38,730
also inside the loop

2202
01:43:43,939 --> 01:43:48,698
I just realize didn't mentioned last

2203
01:43:45,439 --> 01:43:57,229
time the use of the hyperbolic tan

2204
01:43:48,698 --> 01:43:59,719
hyperbolic tan looks like this okay so

2205
01:43:57,229 --> 01:44:02,509
it's just a sigmoid that's offset right

2206
01:43:59,719 --> 01:44:05,270
and it's very common to use a hyperbolic

2207
01:44:02,510 --> 01:44:07,159
tan inside this trend this state to

2208
01:44:05,270 --> 01:44:09,800
state transition because it kind of

2209
01:44:07,159 --> 01:44:14,059
stops it from flying off too high or too

2210
01:44:09,800 --> 01:44:15,710
low you know it's nicely controlled back

2211
01:44:14,060 --> 01:44:19,360
in the old days we used to use

2212
01:44:15,710 --> 01:44:22,969
hyperbolic tanh or the equivalent

2213
01:44:19,359 --> 01:44:24,829
sigmoid a lot as most of our activation

2214
01:44:22,969 --> 01:44:28,430
functions nowadays we tend to use value

2215
01:44:24,829 --> 01:44:31,609
but in these hidden state to here in the

2216
01:44:28,430 --> 01:44:33,949
hidden state transition weight matrices

2217
01:44:31,609 --> 01:44:36,649
we still tend to use hyperbolic tanh

2218
01:44:33,948 --> 01:44:42,169
quite a lot so you'll see I've done that

2219
01:44:36,649 --> 01:44:43,549
also yeah hyperbolic tanh okay so this

2220
01:44:42,170 --> 01:44:45,739
is exactly the same as before but I've

2221
01:44:43,550 --> 01:44:50,719
just replaced it with a Pollard and then

2222
01:44:45,738 --> 01:44:52,209
here's my output yes you know so a does

2223
01:44:50,719 --> 01:44:57,500
he have to do anything with convergence

2224
01:44:52,210 --> 01:45:00,560
these networks yeah we'll talk about

2225
01:44:57,500 --> 01:45:03,140
that a little bit over time let's let's

2226
01:45:00,560 --> 01:45:04,130
let's come back to that though for now

2227
01:45:03,140 --> 01:45:06,980
we're not really going to do anything

2228
01:45:04,130 --> 01:45:09,050
special at all you know recognizing this

2229
01:45:06,979 --> 01:45:11,718
is just a standard fully connected

2230
01:45:09,050 --> 01:45:16,010
Network you know interestingly it's

2231
01:45:11,719 --> 01:45:19,219
quite a deep one right like because this

2232
01:45:16,010 --> 01:45:22,159
is actually this that we've got eight of

2233
01:45:19,219 --> 01:45:24,399
these things now we've now got a deep

2234
01:45:22,159 --> 01:45:26,630
eight layer Network which is why units

2235
01:45:24,399 --> 01:45:28,670
starting suggest we should be concerned

2236
01:45:26,630 --> 01:45:29,989
as you know as we get deeper and deeper

2237
01:45:28,670 --> 01:45:34,029
networks they can be harder and harder

2238
01:45:29,988 --> 01:45:34,029
to train but let's try training this

2239
01:45:37,529 --> 01:45:45,029
all right so when it goes as before

2240
01:45:41,670 --> 01:45:49,529
we've got a batch size of 512 we're

2241
01:45:45,029 --> 01:45:51,300
using Adam and where it goes so we will

2242
01:45:49,529 --> 01:45:53,929
sit there watching it so we can then set

2243
01:45:51,300 --> 01:45:58,710
the learning rate down back to 20 neg 3

2244
01:45:53,930 --> 01:46:01,740
we can fit it again and yeah it's

2245
01:45:58,710 --> 01:46:05,489
actually it seems to be training fun

2246
01:46:01,739 --> 01:46:07,170
okay but we're gonna try something else

2247
01:46:05,489 --> 01:46:09,510
which is we're going to use this a trick

2248
01:46:07,170 --> 01:46:11,760
that your net rather hinted at before

2249
01:46:09,510 --> 01:46:14,789
which is maybe we shouldn't be adding

2250
01:46:11,760 --> 01:46:15,989
these things together and so the reason

2251
01:46:14,789 --> 01:46:18,029
you might want to be feeling a little

2252
01:46:15,989 --> 01:46:23,670
uncomfortable about adding these things

2253
01:46:18,029 --> 01:46:26,130
together is that the input state and the

2254
01:46:23,670 --> 01:46:28,470
hidden state are kind of qualitatively

2255
01:46:26,130 --> 01:46:31,260
different kinds of things right the

2256
01:46:28,470 --> 01:46:34,110
input state is the is the encoding of

2257
01:46:31,260 --> 01:46:36,900
this character for us H represents the

2258
01:46:34,109 --> 01:46:39,779
encoding of the series of characters so

2259
01:46:36,899 --> 01:46:42,109
far and so adding them together is kind

2260
01:46:39,779 --> 01:46:44,789
of potentially going to lose information

2261
01:46:42,109 --> 01:46:46,229
so I think what your net was going to

2262
01:46:44,789 --> 01:46:47,939
prefer that we might do is maybe to

2263
01:46:46,229 --> 01:46:49,949
concatenate these instead of adding them

2264
01:46:47,939 --> 01:46:53,759
so it sound good to you you know she's

2265
01:46:49,949 --> 01:46:56,130
not it okay so let's now make a copy of

2266
01:46:53,760 --> 01:46:59,869
the previous cell all the same right

2267
01:46:56,130 --> 01:47:03,510
rather than using plus let's use cat

2268
01:46:59,869 --> 01:47:06,649
okay now if we can cat then we need to

2269
01:47:03,510 --> 01:47:11,159
make sure now that our input layer is

2270
01:47:06,649 --> 01:47:12,629
not from n fac-2 hidden which is what we

2271
01:47:11,159 --> 01:47:15,090
had before but because we're

2272
01:47:12,630 --> 01:47:19,590
concatenated it needs to be in fact plus

2273
01:47:15,090 --> 01:47:20,880
and hidden to end hidden okay and so now

2274
01:47:19,590 --> 01:47:26,159
that's going to make all the dimensions

2275
01:47:20,880 --> 01:47:30,210
work nicely so this now is of size n

2276
01:47:26,159 --> 01:47:33,269
fact plus and hidden this now makes it

2277
01:47:30,210 --> 01:47:35,279
back to size n hidden again okay and

2278
01:47:33,270 --> 01:47:37,170
then this is putting it through the same

2279
01:47:35,279 --> 01:47:42,439
square matrix as before so it's still a

2280
01:47:37,170 --> 01:47:45,149
size n here okay so this is like a good

2281
01:47:42,439 --> 01:47:47,129
design heuristic if you're designing an

2282
01:47:45,149 --> 01:47:49,439
architecture is if you've got different

2283
01:47:47,130 --> 01:47:51,460
types of information that you want to

2284
01:47:49,439 --> 01:47:54,549
combine you generally want

2285
01:47:51,460 --> 01:47:56,319
concatenate it okay you know adding

2286
01:47:54,550 --> 01:48:00,850
things together even if they're the same

2287
01:47:56,319 --> 01:48:02,229
shape is losing information okay and so

2288
01:48:00,850 --> 01:48:05,460
once you've concatenated things together

2289
01:48:02,229 --> 01:48:08,619
you can always convert it back down to a

2290
01:48:05,460 --> 01:48:11,560
fixed size by just tracking it through a

2291
01:48:08,619 --> 01:48:12,909
matrix product okay so that's what we've

2292
01:48:11,560 --> 01:48:17,469
done here again it's the same thing but

2293
01:48:12,909 --> 01:48:19,899
now we're concatenating instead and so

2294
01:48:17,469 --> 01:48:23,619
we can fit that and so last time we got

2295
01:48:19,899 --> 01:48:25,449
one point seven two this time you go at

2296
01:48:23,619 --> 01:48:26,439
one point six six so it's not setting

2297
01:48:25,449 --> 01:48:28,630
the world on fire but it's an

2298
01:48:26,439 --> 01:48:29,649
improvement and the improvements of it

2299
01:48:28,630 --> 01:48:32,739
okay

2300
01:48:29,649 --> 01:48:34,719
so we can now test that with get next

2301
01:48:32,738 --> 01:48:38,349
and so now we can pass in eight things

2302
01:48:34,719 --> 01:48:41,760
right so it's no before those let's go

2303
01:48:38,350 --> 01:48:45,070
to a part of that sounds good as well so

2304
01:48:41,760 --> 01:48:49,440
Queens and that sounds good too all

2305
01:48:45,069 --> 01:48:53,859
right so great so that's enough

2306
01:48:49,439 --> 01:48:55,929
manual hackery let's see if pi torch

2307
01:48:53,859 --> 01:48:58,839
couldn't do some of this for us and so

2308
01:48:55,930 --> 01:49:02,130
basically what pi torch will do for us

2309
01:48:58,840 --> 01:49:05,289
is it will write this loop automatically

2310
01:49:02,130 --> 01:49:08,680
okay and it will create these linear

2311
01:49:05,289 --> 01:49:11,680
input layers automatically okay and so

2312
01:49:08,680 --> 01:49:15,430
to ask it to do that we can use the n n

2313
01:49:11,680 --> 01:49:19,180
dot R and n plus so here's the exact

2314
01:49:15,430 --> 01:49:21,219
same thing in less code by taking

2315
01:49:19,180 --> 01:49:23,829
advantage of height choice and again I'm

2316
01:49:21,219 --> 01:49:25,689
not using a conceptual analogy to say

2317
01:49:23,829 --> 01:49:28,029
player torches doing something like it

2318
01:49:25,689 --> 01:49:30,599
I'm saying play torch is doing it now

2319
01:49:28,029 --> 01:49:32,500
this is just the code you just saw

2320
01:49:30,600 --> 01:49:33,760
wrapped up a little bit

2321
01:49:32,500 --> 01:49:36,340
reflect it a little bit for your

2322
01:49:33,760 --> 01:49:39,960
convenience right so where we say we now

2323
01:49:36,340 --> 01:49:43,659
want to create an era ten call our it n

2324
01:49:39,960 --> 01:49:47,770
then what this does is it does that for

2325
01:49:43,659 --> 01:49:51,550
live now notice that our for loop needed

2326
01:49:47,770 --> 01:49:53,410
a starting point you remember why right

2327
01:49:51,550 --> 01:49:54,850
because otherwise our for loop didn't

2328
01:49:53,409 --> 01:49:56,889
quite work we couldn't quite refactor it

2329
01:49:54,850 --> 01:49:59,739
out and because this is exactly the same

2330
01:49:56,890 --> 01:50:01,480
this needs our starting point to and so

2331
01:49:59,738 --> 01:50:03,549
let's give it a starting point and so

2332
01:50:01,479 --> 01:50:05,138
you have to pass in your initial hidden

2333
01:50:03,550 --> 01:50:07,239
State

2334
01:50:05,139 --> 01:50:14,079
for reasons that will become apparent

2335
01:50:07,238 --> 01:50:17,649
later on it turns out to be quite useful

2336
01:50:14,079 --> 01:50:20,408
to be able to get back that here in the

2337
01:50:17,649 --> 01:50:21,849
state at the end and just like we could

2338
01:50:20,408 --> 01:50:25,539
here we could actually keep track of the

2339
01:50:21,850 --> 01:50:28,329
hidden state we get back to things we

2340
01:50:25,539 --> 01:50:30,519
get back both the output and the hidden

2341
01:50:28,329 --> 01:50:32,019
state right so we pass in the input in

2342
01:50:30,520 --> 01:50:36,340
the hidden State when we get back the

2343
01:50:32,020 --> 01:50:38,380
output and the hidden state yes could

2344
01:50:36,340 --> 01:50:42,940
you remind us what the hint state

2345
01:50:38,380 --> 01:50:49,960
represents the hidden state is H so it's

2346
01:50:42,939 --> 01:50:54,509
the it's the orange circle ellipse of

2347
01:50:49,960 --> 01:51:06,010
activations okay and so it is of size

2348
01:50:54,510 --> 01:51:07,659
256 okay all right so we can okay

2349
01:51:06,010 --> 01:51:11,980
there's one other thing too to know

2350
01:51:07,658 --> 01:51:16,359
which is in our case we were replacing H

2351
01:51:11,979 --> 01:51:19,089
with a new hidden state the one minor

2352
01:51:16,359 --> 01:51:22,329
difference in pi torch is they append

2353
01:51:19,090 --> 01:51:24,400
the new hidden state to a list or to a

2354
01:51:22,329 --> 01:51:25,929
tensor which gets bigger and bigger so

2355
01:51:24,399 --> 01:51:27,729
they actually give you back all of the

2356
01:51:25,929 --> 01:51:29,399
hidden states so in other words rather

2357
01:51:27,729 --> 01:51:32,019
than just giving you back the final

2358
01:51:29,399 --> 01:51:33,309
ellipse they give you back all the

2359
01:51:32,020 --> 01:51:35,350
ellipses stacked on top of each other

2360
01:51:33,310 --> 01:51:38,110
and so because we just want the final

2361
01:51:35,350 --> 01:51:40,989
one I was got indexed into it with minus

2362
01:51:38,109 --> 01:51:44,079
one here okay other than that this is

2363
01:51:40,988 --> 01:51:46,299
the same code as before put that through

2364
01:51:44,079 --> 01:51:50,939
our output layer to get the correct

2365
01:51:46,300 --> 01:51:50,940
vocab size and then we can train that

2366
01:51:57,630 --> 01:52:01,239
alright so you can see here I can do it

2367
01:51:59,560 --> 01:52:03,520
manually I can create some hidden state

2368
01:52:01,238 --> 01:52:08,368
I can pass it to that area and I can see

2369
01:52:03,520 --> 01:52:12,429
the stuff I get back you'll see that the

2370
01:52:08,368 --> 01:52:16,179
dimensionality of H it's actually a rank

2371
01:52:12,429 --> 01:52:18,239
3 tensor where else in my version it was

2372
01:52:16,179 --> 01:52:18,239
a

2373
01:52:19,020 --> 01:52:25,180
let's see it was a rank two tensor okay

2374
01:52:23,140 --> 01:52:28,480
and the difference is here we've got

2375
01:52:25,180 --> 01:52:30,280
just a unit axis at the front we'll

2376
01:52:28,479 --> 01:52:32,439
learn more about why that is later but

2377
01:52:30,279 --> 01:52:34,840
basically it turns out you can have a

2378
01:52:32,439 --> 01:52:36,939
second R and n that goes backwards

2379
01:52:34,840 --> 01:52:38,739
alright one that goes forwards one that

2380
01:52:36,939 --> 01:52:40,529
goes backwards from the idea is neck and

2381
01:52:38,739 --> 01:52:43,329
then it's going to be better at finding

2382
01:52:40,529 --> 01:52:44,920
relationships that kind of go backwards

2383
01:52:43,329 --> 01:52:47,019
that's quite a bi-directional eridan

2384
01:52:44,920 --> 01:52:48,730
also it turns out you can have an error

2385
01:52:47,020 --> 01:52:51,580
in feed to an iron in that's got a

2386
01:52:48,729 --> 01:52:53,939
multi-layer eridan so basically if you

2387
01:52:51,579 --> 01:52:56,170
have those things you need an additional

2388
01:52:53,939 --> 01:52:58,449
access on your tensor to keep track of

2389
01:52:56,170 --> 01:53:01,690
those additional layers of hidden state

2390
01:52:58,449 --> 01:53:05,649
but for now we'll always have a one yeah

2391
01:53:01,689 --> 01:53:10,210
and we'll always also get back a one at

2392
01:53:05,649 --> 01:53:13,929
the end okay so if we go ahead and fit

2393
01:53:10,210 --> 01:53:14,489
this now let's actually trade it for a

2394
01:53:13,930 --> 01:53:17,950
bit longer

2395
01:53:14,489 --> 01:53:20,229
okay so last time we only kind of did a

2396
01:53:17,949 --> 01:53:21,970
couple of epochs this time we're due for

2397
01:53:20,229 --> 01:53:24,779
a pox

2398
01:53:21,970 --> 01:53:28,060
what have we sit at one in egg three and

2399
01:53:24,779 --> 01:53:31,090
then we'll do another to epochs at one

2400
01:53:28,060 --> 01:53:34,330
in egg four and so we've now got our

2401
01:53:31,090 --> 01:53:39,610
lost down to one point five so getting

2402
01:53:34,329 --> 01:53:43,059
better and better so here's our get next

2403
01:53:39,609 --> 01:53:45,250
again okay and you know let's just it

2404
01:53:43,060 --> 01:53:49,090
was the same thing so what we can now do

2405
01:53:45,250 --> 01:53:52,090
is we can look through like forty times

2406
01:53:49,090 --> 01:53:54,460
calling get next each time and then each

2407
01:53:52,090 --> 01:53:56,710
time will replace our input by removing

2408
01:53:54,460 --> 01:53:58,779
the first character and adding the thing

2409
01:53:56,710 --> 01:54:00,970
that we just predicted and so that way

2410
01:53:58,779 --> 01:54:02,609
we can like feed in a new set of eight

2411
01:54:00,970 --> 01:54:05,260
characters that get them again and again

2412
01:54:02,609 --> 01:54:09,009
and so that way we'll call that get next

2413
01:54:05,260 --> 01:54:11,380
in so here are 40 characters that we've

2414
01:54:09,010 --> 01:54:14,890
generated so we started out with four th

2415
01:54:11,380 --> 01:54:17,289
OS so we got four those of the same -

2416
01:54:14,890 --> 01:54:18,190
the same - the same you can probably

2417
01:54:17,289 --> 01:54:20,619
guess what happens if you can't

2418
01:54:18,189 --> 01:54:25,689
predicting the same - the same all right

2419
01:54:20,619 --> 01:54:31,420
so it's you know it's doing okay we we

2420
01:54:25,689 --> 01:54:33,609
now have something which you know

2421
01:54:31,420 --> 01:54:37,239
we've basically built from scratch and

2422
01:54:33,609 --> 01:54:39,609
then we've said here's how high torture

2423
01:54:37,239 --> 01:54:41,260
effected it for us so if you want to

2424
01:54:39,609 --> 01:54:44,409
like have an interesting little homework

2425
01:54:41,260 --> 01:54:48,159
assignment this week try to write your

2426
01:54:44,409 --> 01:54:51,550
own version of an RNN plus all right

2427
01:54:48,159 --> 01:54:53,559
like try to like literally like create

2428
01:54:51,550 --> 01:54:55,840
your like you know Jeremy's aren't in

2429
01:54:53,560 --> 01:54:58,690
and then like type in here

2430
01:54:55,840 --> 01:55:00,810
Jeremy's aren't in or in your case maybe

2431
01:54:58,689 --> 01:55:04,210
your name's not Jeremy which is okay too

2432
01:55:00,810 --> 01:55:06,100
and then get it to run writing your

2433
01:55:04,210 --> 01:55:07,420
implementation that's fast from scratch

2434
01:55:06,100 --> 01:55:10,210
without looking at the piped water

2435
01:55:07,420 --> 01:55:12,010
source code you know like basically it's

2436
01:55:10,210 --> 01:55:14,680
just a case of like going up and seeing

2437
01:55:12,010 --> 01:55:17,289
what we did back here right and like

2438
01:55:14,680 --> 01:55:19,119
make sure you get the same answers and

2439
01:55:17,289 --> 01:55:21,760
confirm that you do so that's kind of a

2440
01:55:19,119 --> 01:55:24,130
good little test simply simple at all

2441
01:55:21,760 --> 01:55:25,630
assignment but I think you'll feel

2442
01:55:24,130 --> 01:55:29,909
really good when you seem like oh I've

2443
01:55:25,630 --> 01:55:29,909
just reimplemented an end alone in

2444
01:55:30,390 --> 01:55:37,750
alright so I'm going to do one other

2445
01:55:36,159 --> 01:55:40,029
thing when I switched from this one when

2446
01:55:37,750 --> 01:55:41,529
I've moved the car one input inside the

2447
01:55:40,029 --> 01:55:44,069
dotted line right this dotted rectangle

2448
01:55:41,529 --> 01:55:47,579
represents the thing I'm repeating I

2449
01:55:44,069 --> 01:55:51,130
also watch the triangle the output I

2450
01:55:47,579 --> 01:55:51,930
moved that inside as well now that's a

2451
01:55:51,130 --> 01:55:55,800
big difference

2452
01:55:51,930 --> 01:55:59,140
because now what I've actually done is

2453
01:55:55,800 --> 01:56:03,670
I'm actually saying spit out an output

2454
01:55:59,140 --> 01:56:07,200
after every one of these circles so spit

2455
01:56:03,670 --> 01:56:09,730
out an output here and here and here

2456
01:56:07,199 --> 01:56:12,069
alright so in other words if I have a

2457
01:56:09,729 --> 01:56:13,779
three character input I'm going to spit

2458
01:56:12,069 --> 01:56:15,579
out a three character output I'm saying

2459
01:56:13,779 --> 01:56:17,469
half the character 1 this will be next

2460
01:56:15,579 --> 01:56:20,640
after character to this be next after

2461
01:56:17,470 --> 01:56:24,240
character 3 this will be next

2462
01:56:20,640 --> 01:56:26,470
so again nothing different

2463
01:56:24,239 --> 01:56:28,869
and again this you know if you want to

2464
01:56:26,470 --> 01:56:31,449
go a bit further with the assignment you

2465
01:56:28,869 --> 01:56:33,130
could write this by hand as well but

2466
01:56:31,449 --> 01:56:37,590
basically what we're saying is in the

2467
01:56:33,130 --> 01:56:41,350
for loop would be saying like you know

2468
01:56:37,590 --> 01:56:42,730
results equals some empty list right and

2469
01:56:41,350 --> 01:56:44,960
then would be going through and rather

2470
01:56:42,729 --> 01:56:48,949
than returning that

2471
01:56:44,960 --> 01:56:55,399
we're instead be saying you know results

2472
01:56:48,949 --> 01:57:00,679
dot append that right and then like

2473
01:56:55,399 --> 01:57:02,479
return whatever torch dot stat something

2474
01:57:00,680 --> 01:57:05,750
like that right that it made me right in

2475
01:57:02,479 --> 01:57:09,609
my question so now you know we now have

2476
01:57:05,750 --> 01:57:13,130
like every step we've created an output

2477
01:57:09,609 --> 01:57:16,609
okay so which is basically this picture

2478
01:57:13,130 --> 01:57:19,400
and so the reason was lots of reasons

2479
01:57:16,609 --> 01:57:21,559
that's interesting but I think the main

2480
01:57:19,399 --> 01:57:29,119
reason right now that's interesting is

2481
01:57:21,560 --> 01:57:32,060
that you probably noticed this this

2482
01:57:29,119 --> 01:57:34,510
approach to dealing with our data seems

2483
01:57:32,060 --> 01:57:38,660
terribly inefficient like we're grabbing

2484
01:57:34,510 --> 01:57:41,960
the first eight right but then this next

2485
01:57:38,659 --> 01:57:45,369
set all but one of them overlap the

2486
01:57:41,960 --> 01:57:48,230
previous one right so we're kind of like

2487
01:57:45,369 --> 01:57:50,210
recalculating the exact set of

2488
01:57:48,229 --> 01:57:51,669
embeddings seven out of eight of them

2489
01:57:50,210 --> 01:57:55,789
are going to be exact same embeddings

2490
01:57:51,670 --> 01:57:58,159
right exact same transitions it kind of

2491
01:57:55,789 --> 01:58:00,739
seems weird to like do all this

2492
01:57:58,159 --> 01:58:02,840
calculation to just predict one thing

2493
01:58:00,739 --> 01:58:04,219
and then go back and recalculate seven

2494
01:58:02,840 --> 01:58:06,680
out of eight of them and add one more to

2495
01:58:04,220 --> 01:58:06,980
the end to calculate the next thing all

2496
01:58:06,680 --> 01:58:09,590
right

2497
01:58:06,979 --> 01:58:16,429
so the basic idea then is to say well

2498
01:58:09,590 --> 01:58:18,079
let's not do it that way instead let's

2499
01:58:16,430 --> 01:58:22,730
taking non overlapping sets of

2500
01:58:18,079 --> 01:58:25,460
characters all right so like so here is

2501
01:58:22,729 --> 01:58:27,469
our first eight characters here is the

2502
01:58:25,460 --> 01:58:29,859
next day characters here are the next

2503
01:58:27,470 --> 01:58:32,780
day characters so like if you read this

2504
01:58:29,859 --> 01:58:38,239
top left to bottom right that would be

2505
01:58:32,779 --> 01:58:39,849
the whole nature right and so then if

2506
01:58:38,239 --> 01:58:43,939
these are the first eight characters

2507
01:58:39,850 --> 01:58:48,829
then offset this by one starting here

2508
01:58:43,939 --> 01:58:51,109
that's a list of outputs right so after

2509
01:58:48,829 --> 01:58:54,039
we see characters zero through seven

2510
01:58:51,109 --> 01:58:58,439
we should predict characters 1 through 8

2511
01:58:54,039 --> 01:59:02,819
the XS so after 40 should come 42

2512
01:58:58,439 --> 01:59:05,669
as it did after 42 should come 29 as it

2513
01:59:02,819 --> 01:59:11,099
did okay and so now that can be our

2514
01:59:05,670 --> 01:59:14,489
inputs and labels for that model and so

2515
01:59:11,100 --> 01:59:17,250
it shouldn't be any more or less

2516
01:59:14,488 --> 01:59:20,939
accurate it should just be the same

2517
01:59:17,250 --> 01:59:27,390
right pretty much but it should allow us

2518
01:59:20,939 --> 01:59:34,579
to do it more efficiently so let's try

2519
01:59:27,390 --> 01:59:39,800
that all right

2520
01:59:34,579 --> 01:59:39,800
so I mentioned last time that we had a

2521
01:59:40,488 --> 01:59:48,238
minus 1 index here because we just

2522
01:59:43,500 --> 01:59:50,250
wanted to grab the last triangle okay so

2523
01:59:48,238 --> 01:59:52,409
in this case we're going to grab all the

2524
01:59:50,250 --> 01:59:55,948
triangles so this this is actually the

2525
01:59:52,409 --> 01:59:58,349
way it end on RNN creates things we we

2526
01:59:55,948 --> 02:00:06,000
only kept the last one but this time

2527
01:59:58,350 --> 02:00:07,890
we're going to keep all of them so we've

2528
02:00:06,000 --> 02:00:12,090
made one change which is to remove that

2529
02:00:07,890 --> 02:00:22,739
minus one other than that this is the

2530
02:00:12,090 --> 02:00:24,119
exact same code as before okay so but

2531
02:00:22,738 --> 02:00:26,549
there's nothing much to show you here I

2532
02:00:24,119 --> 02:00:31,260
mean except of course at this time if we

2533
02:00:26,550 --> 02:00:33,600
look at the labels it's now 512 by eight

2534
02:00:31,260 --> 02:00:38,670
factors we're trying to predict eight

2535
02:00:33,600 --> 02:00:42,120
things every time through so there is

2536
02:00:38,670 --> 02:00:46,670
one complexity here which is that we

2537
02:00:42,119 --> 02:00:50,909
want to use the negative log likelihood

2538
02:00:46,670 --> 02:00:53,399
loss function as before right but the

2539
02:00:50,909 --> 02:00:56,869
ligand if lost likelihood loss function

2540
02:00:53,399 --> 02:00:59,909
just like our MSE expects to receive to

2541
02:00:56,869 --> 02:01:02,550
rank one tensors actually with the

2542
02:00:59,909 --> 02:01:05,550
mini-batch access to rank two tensors

2543
02:01:02,550 --> 02:01:11,960
all right so two to mini-batches of

2544
02:01:05,550 --> 02:01:11,960
vectors problem is that we've got

2545
02:01:12,329 --> 02:01:18,189
eight-time steps you know it characters

2546
02:01:15,369 --> 02:01:20,948
in an RNN we call it a time step right

2547
02:01:18,189 --> 02:01:24,369
we have eight time steps and then for

2548
02:01:20,948 --> 02:01:27,988
each one we have 84 probabilities we

2549
02:01:24,369 --> 02:01:30,939
have the probability for every single

2550
02:01:27,988 --> 02:01:34,928
one of those eight times deaths and then

2551
02:01:30,939 --> 02:01:37,388
we have that for each of our 512 items

2552
02:01:34,929 --> 02:01:42,279
in the mini batch so we have a rank 3

2553
02:01:37,389 --> 02:01:44,230
tensor not a rank two tensor um so that

2554
02:01:42,279 --> 02:01:46,179
means that the negative log likelihood

2555
02:01:44,229 --> 02:01:48,759
loss function is going to spit out an

2556
02:01:46,179 --> 02:01:50,230
error now frankly I think this is kind

2557
02:01:48,760 --> 02:01:54,429
of dumb you know I think it would be

2558
02:01:50,229 --> 02:01:56,738
better if PI torch had written the loss

2559
02:01:54,429 --> 02:01:59,109
functions in such a way that they didn't

2560
02:01:56,738 --> 02:02:01,869
care at all about rank and they just

2561
02:01:59,109 --> 02:02:05,259
applied it to whatever rank you gave it

2562
02:02:01,869 --> 02:02:07,719
but for now at least it does care about

2563
02:02:05,260 --> 02:02:09,310
rick but the nice thing is I get to show

2564
02:02:07,719 --> 02:02:11,550
you how to write a custom loss function

2565
02:02:09,310 --> 02:02:14,530
okay so we're going to create a special

2566
02:02:11,550 --> 02:02:17,110
negative log likelihood loss function

2567
02:02:14,529 --> 02:02:19,179
for sequences okay and so it's going to

2568
02:02:17,109 --> 02:02:21,549
take an input in the target and it's got

2569
02:02:19,179 --> 02:02:25,199
a call f dot negative log likelihood

2570
02:02:21,550 --> 02:02:28,000
lost so the pipe launched one all right

2571
02:02:25,198 --> 02:02:33,189
but what we're going to do is we're

2572
02:02:28,000 --> 02:02:37,289
going to flatten our input and we're

2573
02:02:33,189 --> 02:02:40,299
going to flatten our targets right and

2574
02:02:37,289 --> 02:02:45,340
so and it turns out these are going to

2575
02:02:40,300 --> 02:02:50,500
be the first two axes that I have to be

2576
02:02:45,340 --> 02:02:54,069
transposed so the way PI torch handles

2577
02:02:50,500 --> 02:02:56,859
are and end data by default is the first

2578
02:02:54,069 --> 02:02:59,198
axis is the sequence length in this case

2579
02:02:56,859 --> 02:03:02,769
eight right so the sequence length of an

2580
02:02:59,198 --> 02:03:04,448
R and n is how many times deaths so we

2581
02:03:02,770 --> 02:03:06,850
have eight characters so a sequence

2582
02:03:04,448 --> 02:03:10,089
length of eight the second axis is the

2583
02:03:06,850 --> 02:03:12,520
batch size and then as would expect the

2584
02:03:10,090 --> 02:03:15,989
third axis is the actual hidden state

2585
02:03:12,520 --> 02:03:22,830
itself okay so this is going to be eight

2586
02:03:15,988 --> 02:03:24,649
by 512 by n hidden which I think was 256

2587
02:03:22,829 --> 02:03:28,550
yeah

2588
02:03:24,649 --> 02:03:31,369
okay so we can grab the size and unpack

2589
02:03:28,550 --> 02:03:39,550
it into each of these sequence length

2590
02:03:31,369 --> 02:03:39,550
batch size and I'm hidden now target

2591
02:03:39,789 --> 02:03:53,269
mighty dot size is 512 by 8 where else

2592
02:03:49,850 --> 02:03:55,070
this one here was 8 by 512 so to make

2593
02:03:53,270 --> 02:04:00,360
them match we're going to have to

2594
02:03:55,069 --> 02:04:01,539
transpose the first two axis okay

2595
02:04:00,359 --> 02:04:04,099
[Music]

2596
02:04:01,539 --> 02:04:06,500
hi torch when you do something like

2597
02:04:04,100 --> 02:04:09,380
transpose doesn't generally actually

2598
02:04:06,500 --> 02:04:11,529
shuffle the memory order but instead it

2599
02:04:09,380 --> 02:04:13,940
just kind of keeps some internal

2600
02:04:11,529 --> 02:04:17,829
metadata to say like hey you should

2601
02:04:13,939 --> 02:04:20,629
treat this as if it's transposed and

2602
02:04:17,829 --> 02:04:23,269
some things in pi torch will give you an

2603
02:04:20,630 --> 02:04:26,239
error if you try and use it when it has

2604
02:04:23,270 --> 02:04:30,949
these like this internal state and I

2605
02:04:26,238 --> 02:04:33,259
basically say error this tensor is not

2606
02:04:30,948 --> 02:04:35,509
contiguous if you ever see that error at

2607
02:04:33,260 --> 02:04:38,150
the word contiguous after it and it goes

2608
02:04:35,510 --> 02:04:39,739
away so I don't know they can't do that

2609
02:04:38,149 --> 02:04:41,509
for you apparently so in this particular

2610
02:04:39,738 --> 02:04:44,238
case I got that error so I wrote the

2611
02:04:41,510 --> 02:04:46,340
code contiguous after it okay and so

2612
02:04:44,238 --> 02:04:49,129
then finally we need to flatten it out

2613
02:04:46,340 --> 02:04:50,900
into a single vector and so we can just

2614
02:04:49,130 --> 02:04:53,719
go a dot view which is the same as non

2615
02:04:50,899 --> 02:04:58,849
PI dot reshape and minus one means as

2616
02:04:53,719 --> 02:05:02,119
long as it needs to be okay and then the

2617
02:04:58,850 --> 02:05:05,870
input again we also reshape that right

2618
02:05:02,119 --> 02:05:09,920
but remember the input sorry the the the

2619
02:05:05,869 --> 02:05:11,869
predictions also have this axis of

2620
02:05:09,920 --> 02:05:15,850
length 84 all of the predicted

2621
02:05:11,869 --> 02:05:18,439
probabilities okay so so here's a custom

2622
02:05:15,850 --> 02:05:20,420
these are custom lost function that's it

2623
02:05:18,439 --> 02:05:22,849
right so if you ever want to play around

2624
02:05:20,420 --> 02:05:27,230
with your own loss functions you can

2625
02:05:22,850 --> 02:05:30,710
just do that like so and then pass that

2626
02:05:27,229 --> 02:05:34,399
to fit okay so it's important to

2627
02:05:30,710 --> 02:05:37,480
remember that Fitch is this like lowest

2628
02:05:34,399 --> 02:05:39,789
level fast AI abstraction

2629
02:05:37,479 --> 02:05:42,459
that's--it's that this is the thing that

2630
02:05:39,789 --> 02:05:45,970
implements the training look okay and so

2631
02:05:42,460 --> 02:05:50,500
like you're the stuff you pass it in is

2632
02:05:45,970 --> 02:05:53,289
all standard pi torch stuff except for

2633
02:05:50,500 --> 02:05:56,010
this this is our model data object this

2634
02:05:53,289 --> 02:05:58,600
is the thing that wraps up the test set

2635
02:05:56,010 --> 02:06:01,420
the training set and the validation set

2636
02:05:58,600 --> 02:06:06,460
to get that okay your neck could you

2637
02:06:01,420 --> 02:06:09,819
pass that back so when we pull the

2638
02:06:06,460 --> 02:06:13,480
triangle into the replicator structure

2639
02:06:09,819 --> 02:06:15,909
right so the the first n minus one

2640
02:06:13,479 --> 02:06:18,219
iterations of the sequence length we

2641
02:06:15,909 --> 02:06:20,500
don't see the whole sequence length yeah

2642
02:06:18,220 --> 02:06:24,760
so does that mean that the batch size

2643
02:06:20,500 --> 02:06:26,079
should be much bigger so that be careful

2644
02:06:24,760 --> 02:06:28,210
you don't mean that size you main

2645
02:06:26,079 --> 02:06:31,930
sequence length right because the batch

2646
02:06:28,210 --> 02:06:34,989
size is like some firing yeah okay so

2647
02:06:31,930 --> 02:06:37,119
yes yes if you have a short sequence

2648
02:06:34,988 --> 02:06:41,109
length like eight yeah

2649
02:06:37,119 --> 02:06:46,479
the first character has nothing to go on

2650
02:06:41,109 --> 02:06:49,719
it starts with an empty hidden state of

2651
02:06:46,479 --> 02:06:51,309
zeros okay so what we're going to start

2652
02:06:49,720 --> 02:06:52,989
with next week

2653
02:06:51,310 --> 02:06:55,390
is we're going to learn how to avoid

2654
02:06:52,988 --> 02:06:59,169
that problem right and so it's a really

2655
02:06:55,390 --> 02:07:01,900
insightful question or concern right and

2656
02:06:59,170 --> 02:07:05,909
but if you think about it the basic idea

2657
02:07:01,899 --> 02:07:09,909
is why should we reset this to zero

2658
02:07:05,909 --> 02:07:13,750
every time you know like if we can kind

2659
02:07:09,909 --> 02:07:16,779
of line up these mini batches somehow so

2660
02:07:13,750 --> 02:07:18,640
that the next mini batch joins up

2661
02:07:16,779 --> 02:07:22,000
correctly it represents like the next

2662
02:07:18,640 --> 02:07:26,880
letter in leaches works then we'd want

2663
02:07:22,000 --> 02:07:32,340
to move this up into the constructor

2664
02:07:26,880 --> 02:07:38,020
right and then like pass that here and

2665
02:07:32,340 --> 02:07:40,210
then store it here right and now we're

2666
02:07:38,020 --> 02:07:43,000
not resetting the hidden state each time

2667
02:07:40,210 --> 02:07:45,220
we're actually we're actually keeping

2668
02:07:43,000 --> 02:07:48,340
the hidden state from call to call and

2669
02:07:45,220 --> 02:07:51,338
so the only time that it would be

2670
02:07:48,340 --> 02:07:53,109
failing to benefit from

2671
02:07:51,338 --> 02:07:55,568
learning state would be like literally

2672
02:07:53,109 --> 02:07:56,559
at the very start of the document so

2673
02:07:55,569 --> 02:07:58,989
that's where but that's where we're

2674
02:07:56,559 --> 02:08:09,069
going to try and ahead next week

2675
02:07:58,988 --> 02:08:11,168
I feel like this lesson every time I've

2676
02:08:09,069 --> 02:08:13,149
got a punch line coming somebody asks me

2677
02:08:11,168 --> 02:08:17,648
a question where I have to like do the

2678
02:08:13,149 --> 02:08:21,338
punch line ahead of time okay so we can

2679
02:08:17,649 --> 02:08:22,899
fit that and we can fit that and I want

2680
02:08:21,338 --> 02:08:25,868
to show you something interesting and

2681
02:08:22,899 --> 02:08:27,369
this is coming to the punch line that

2682
02:08:25,868 --> 02:08:33,908
another punch line that you net try to

2683
02:08:27,368 --> 02:08:35,978
spoil which is when we're you know

2684
02:08:33,908 --> 02:08:37,688
remember this is just doing a loop right

2685
02:08:35,979 --> 02:08:42,668
applying the same matrix multiply again

2686
02:08:37,689 --> 02:08:46,418
and again if that matrix multiply tends

2687
02:08:42,668 --> 02:08:48,878
to increase the activations each time

2688
02:08:46,418 --> 02:08:50,439
then effectively we're doing that to the

2689
02:08:48,878 --> 02:08:53,318
power of eight right so it's going to

2690
02:08:50,439 --> 02:08:54,869
like to shoot off really high or if it's

2691
02:08:53,319 --> 02:08:57,760
decreasing it a little bit each time

2692
02:08:54,868 --> 02:08:58,958
that's going to shoot off really low so

2693
02:08:57,760 --> 02:09:01,659
this is what we call a gradient

2694
02:08:58,958 --> 02:09:07,208
explosion right and so we really want to

2695
02:09:01,658 --> 02:09:11,888
make sure that the initial H naught H

2696
02:09:07,208 --> 02:09:16,358
the initial but if we call it the

2697
02:09:11,889 --> 02:09:19,958
initial L hidden that we create is is

2698
02:09:16,359 --> 02:09:22,149
like oversize that's not going to cause

2699
02:09:19,958 --> 02:09:25,300
our activations on average to increase

2700
02:09:22,149 --> 02:09:29,939
or decrease right and there's actually a

2701
02:09:25,300 --> 02:09:33,458
very nice matrix that does exactly that

2702
02:09:29,939 --> 02:09:35,588
called the identity matrix so the

2703
02:09:33,458 --> 02:09:39,038
identity matrix for those that don't

2704
02:09:35,588 --> 02:09:42,309
quite remember their linear algebra is

2705
02:09:39,038 --> 02:09:46,958
this this would be a size 3 identity

2706
02:09:42,309 --> 02:09:49,838
matrix all right and so the trick about

2707
02:09:46,958 --> 02:09:52,929
an identity matrix is anything times an

2708
02:09:49,838 --> 02:09:54,519
identity matrix is itself right and so

2709
02:09:52,929 --> 02:09:56,439
therefore you could multiply it by this

2710
02:09:54,519 --> 02:09:59,289
again and again and again and again and

2711
02:09:56,439 --> 02:10:03,280
still end up with itself right so

2712
02:09:59,288 --> 02:10:04,689
there's no gradient explosion so what we

2713
02:10:03,279 --> 02:10:08,079
could do is instead

2714
02:10:04,689 --> 02:10:11,229
of using whatever the default random in

2715
02:10:08,079 --> 02:10:14,350
it is for this matrix we could instead

2716
02:10:11,229 --> 02:10:15,069
after we create our errand in is we can

2717
02:10:14,350 --> 02:10:17,410
go into that

2718
02:10:15,069 --> 02:10:22,769
Erol in right and notice this right we

2719
02:10:17,409 --> 02:10:25,988
can go m dot RN n right and if we now go

2720
02:10:22,770 --> 02:10:29,110
like so we can get the docs for m dot R

2721
02:10:25,988 --> 02:10:31,929
and M right and as well as the arguments

2722
02:10:29,109 --> 02:10:33,819
for constructing it it also tells you

2723
02:10:31,930 --> 02:10:35,770
the inputs and outputs for calling the

2724
02:10:33,819 --> 02:10:37,960
layer and it also tells you the

2725
02:10:35,770 --> 02:10:39,610
attributes and so it tells you there's

2726
02:10:37,960 --> 02:10:42,069
something called weight

2727
02:10:39,609 --> 02:10:43,479
H H and these are the learn about hidden

2728
02:10:42,069 --> 02:10:46,389
to hidden weights that's that square

2729
02:10:43,479 --> 02:10:49,119
matrix right so after we've constructed

2730
02:10:46,390 --> 02:10:55,030
our M we can just go in and say all

2731
02:10:49,119 --> 02:10:58,680
right m dot R and n dot weight h HL dot

2732
02:10:55,029 --> 02:11:03,819
data that's the tensor dot copy

2733
02:10:58,680 --> 02:11:07,920
underscore in place torch I that is I

2734
02:11:03,819 --> 02:11:11,139
for identity in case you are wondering

2735
02:11:07,920 --> 02:11:14,949
so this is an identity matrix of size n

2736
02:11:11,140 --> 02:11:18,310
hidden so this both puts into this

2737
02:11:14,948 --> 02:11:24,089
weight matrix and returns the identity

2738
02:11:18,310 --> 02:11:29,110
matrix and so this was like actually a

2739
02:11:24,090 --> 02:11:31,449
Geoffrey Hinton paper was like hey you

2740
02:11:29,109 --> 02:11:34,269
know after it was it's 2015

2741
02:11:31,448 --> 02:11:38,189
so after recurrent neural Nets have been

2742
02:11:34,270 --> 02:11:40,449
around for decades here's like hey gang

2743
02:11:38,189 --> 02:11:44,439
maybe we should just use the identity

2744
02:11:40,448 --> 02:11:46,529
matrix to initialize this and like it

2745
02:11:44,439 --> 02:11:49,869
actually turns out to work really well

2746
02:11:46,529 --> 02:11:52,000
and so that was a 2015 paper believe it

2747
02:11:49,869 --> 02:11:54,550
or not from the father of neural

2748
02:11:52,000 --> 02:11:56,439
networks and so here is the here is our

2749
02:11:54,550 --> 02:11:58,090
implementation of his paper and this is

2750
02:11:56,439 --> 02:11:59,738
an important thing to know right when

2751
02:11:58,090 --> 02:12:02,619
very famous people like Geoffrey Hinton

2752
02:11:59,738 --> 02:12:04,809
write a paper sometimes in entire

2753
02:12:02,619 --> 02:12:07,890
implementation of that paper looks like

2754
02:12:04,810 --> 02:12:10,660
one line of code okay so let's do it

2755
02:12:07,890 --> 02:12:12,969
before we got point six one two five

2756
02:12:10,659 --> 02:12:16,630
seven we'll fit it with exactly the same

2757
02:12:12,969 --> 02:12:16,899
parameters and now we get 0.5 1 and in

2758
02:12:16,630 --> 02:12:19,329
fact

2759
02:12:16,899 --> 02:12:22,359
can keep training 0.50 so like this

2760
02:12:19,329 --> 02:12:24,309
tweak really really really helped okay

2761
02:12:22,359 --> 02:12:26,738
now one of the nice things about this

2762
02:12:24,310 --> 02:12:29,530
tweak was before I could only use a

2763
02:12:26,738 --> 02:12:32,619
learning rate of one in x3 before it

2764
02:12:29,529 --> 02:12:34,300
started going crazy but after identity

2765
02:12:32,619 --> 02:12:36,429
matrix I found I could use one in egg

2766
02:12:34,300 --> 02:12:38,770
too because it's you know it's better

2767
02:12:36,429 --> 02:12:41,890
behaved weight initialization I found I

2768
02:12:38,770 --> 02:12:46,750
could use a higher learning rate okay

2769
02:12:41,890 --> 02:12:49,179
and honestly these things you know

2770
02:12:46,750 --> 02:12:51,369
increasingly we're trying to incorporate

2771
02:12:49,179 --> 02:12:53,649
into the defaults in first day I you

2772
02:12:51,369 --> 02:12:56,019
know you don't necessarily personally

2773
02:12:53,649 --> 02:12:59,139
need to actually know them but you know

2774
02:12:56,020 --> 02:13:00,730
at this point we're still at a point

2775
02:12:59,140 --> 02:13:02,289
where you know most things in most

2776
02:13:00,729 --> 02:13:03,879
libraries most of the time don't have

2777
02:13:02,289 --> 02:13:05,679
great defaults it's good to know all

2778
02:13:03,880 --> 02:13:07,170
these little tricks it's also nice to

2779
02:13:05,679 --> 02:13:09,310
know if you want to improve something

2780
02:13:07,170 --> 02:13:11,350
what kind of tricks people have used

2781
02:13:09,310 --> 02:13:15,250
elsewhere because you can often borrow

2782
02:13:11,350 --> 02:13:17,560
them yourself all right well that's the

2783
02:13:15,250 --> 02:13:20,560
end of the lesson today and so next week

2784
02:13:17,560 --> 02:13:22,030
we will look at this idea of a stateful

2785
02:13:20,560 --> 02:13:23,830
RNN that's going to keep this hidden

2786
02:13:22,029 --> 02:13:27,069
state around and then we're going to go

2787
02:13:23,829 --> 02:13:28,569
back to looking at language models again

2788
02:13:27,069 --> 02:13:30,069
and then finally we're going to go all

2789
02:13:28,569 --> 02:13:32,529
the way back to computer vision and

2790
02:13:30,069 --> 02:13:35,349
learn about things like rez nets and

2791
02:13:32,529 --> 02:13:38,439
batch norm and all the tricks that were

2792
02:13:35,350 --> 02:13:40,679
in figured out in cats versus dogs see

2793
02:13:38,439 --> 02:13:40,678
you then

2794
02:13:41,039 --> 02:13:43,960
[Applause]

