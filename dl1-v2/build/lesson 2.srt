1
00:00:00,030 --> 00:00:10,379
okay so welcome back to deep learning

2
00:00:03,658 --> 00:00:12,149
lesson 2 last week we got to the point

3
00:00:10,380 --> 00:00:16,500
where we had successfully trained a

4
00:00:12,150 --> 00:00:20,719
pretty accurate image classifier and so

5
00:00:16,500 --> 00:00:20,719
just to remind you about how we did that

6
00:00:21,079 --> 00:00:25,768
can you guys see okay I think the

7
00:00:24,118 --> 00:00:26,879
actually we can turn the phone once all

8
00:00:25,768 --> 00:00:30,448
right

9
00:00:26,879 --> 00:00:35,340
can you guys all see the screen okay we

10
00:00:30,449 --> 00:00:37,469
can to adjust these ones can we some

11
00:00:35,340 --> 00:00:41,250
pictures all into darkness but if that

12
00:00:37,469 --> 00:00:46,160
works then is that okay that's better

13
00:00:41,250 --> 00:00:47,969
isn't it yeah can I dream the other two

14
00:00:46,159 --> 00:00:53,819
and maybe that one as well

15
00:00:47,969 --> 00:00:55,698
oh but that one oh that's great sorry I

16
00:00:53,820 --> 00:01:03,420
don't know your renders oh okay great

17
00:00:55,698 --> 00:01:05,429
that's better isn't it me so just to

18
00:01:03,420 --> 00:01:08,820
remind you the way that we built this

19
00:01:05,430 --> 00:01:11,850
image classifier was we used a small

20
00:01:08,819 --> 00:01:14,250
amount of code basically three lines of

21
00:01:11,849 --> 00:01:17,489
code and these three lines of code

22
00:01:14,250 --> 00:01:19,590
pointed at a particular path which

23
00:01:17,489 --> 00:01:21,599
already had some data in it and so the

24
00:01:19,590 --> 00:01:25,020
key thing for this to know how to train

25
00:01:21,599 --> 00:01:27,989
this model was that this path which was

26
00:01:25,019 --> 00:01:30,269
data dogs cats and had to have a

27
00:01:27,989 --> 00:01:33,359
particular structure which is that it

28
00:01:30,269 --> 00:01:35,670
had a train folder and a valid folder

29
00:01:33,359 --> 00:01:37,259
and in each of those trained and valid

30
00:01:35,670 --> 00:01:39,659
folders there was a cats folder in the

31
00:01:37,259 --> 00:01:42,269
dogs folder and if the cats on the docs

32
00:01:39,659 --> 00:01:44,820
folders was a bunch of images of cats

33
00:01:42,269 --> 00:01:48,359
and votes but this is like a pretty

34
00:01:44,819 --> 00:01:51,239
standard it's one of two main structures

35
00:01:48,359 --> 00:01:52,799
that are used to say here is the data

36
00:01:51,239 --> 00:01:55,289
that I want you to train an image model

37
00:01:52,799 --> 00:01:58,500
from so I know some of you during the

38
00:01:55,290 --> 00:02:01,079
week went away and tried different data

39
00:01:58,500 --> 00:02:02,578
sets where you had folders with

40
00:02:01,078 --> 00:02:05,368
different sets of images and in credit

41
00:02:02,578 --> 00:02:06,750
your own image classifiers and generally

42
00:02:05,368 --> 00:02:08,669
that seems to be working pretty well

43
00:02:06,750 --> 00:02:11,989
from what I can see on the forums so to

44
00:02:08,669 --> 00:02:13,889
make it clear at this point this is

45
00:02:11,989 --> 00:02:16,920
everything you need

46
00:02:13,889 --> 00:02:18,599
to get started so if you create your own

47
00:02:16,919 --> 00:02:23,759
folders with different sets of images

48
00:02:18,599 --> 00:02:28,169
you know a few hundred or a few thousand

49
00:02:23,759 --> 00:02:30,929
at each folder and run the same three

50
00:02:28,169 --> 00:02:32,489
lines of code that will give you an

51
00:02:30,930 --> 00:02:35,159
image classifier and you'll be able to

52
00:02:32,490 --> 00:02:42,750
see this third column tells you how

53
00:02:35,159 --> 00:02:46,139
accurate is so we looked at some kind of

54
00:02:42,750 --> 00:02:48,629
simple visualizations to see like what

55
00:02:46,139 --> 00:02:50,909
was it uncertain about what was it wrong

56
00:02:48,629 --> 00:02:55,109
about and so forth and that's always a

57
00:02:50,909 --> 00:02:57,329
really good idea and then we learned

58
00:02:55,110 --> 00:02:59,100
about the one key number you have to

59
00:02:57,330 --> 00:03:02,340
pick so this is this number here is the

60
00:02:59,099 --> 00:03:05,009
one key number is 0.01 and this is

61
00:03:02,340 --> 00:03:07,979
called the learning rate and so I wanted

62
00:03:05,009 --> 00:03:10,099
to go over this again and we'll learn

63
00:03:07,979 --> 00:03:12,629
about the theory behind what this is

64
00:03:10,099 --> 00:03:14,459
during the rest of the course in quite a

65
00:03:12,629 --> 00:03:21,680
lot of detail and for now I just wanted

66
00:03:14,459 --> 00:03:21,680
to talk about the practice yes you know

67
00:03:23,180 --> 00:03:31,650
they cannot see you in the medium

68
00:03:25,729 --> 00:03:34,379
turnout I just turned it around you tell

69
00:03:31,650 --> 00:03:38,849
us about the other three numbers being

70
00:03:34,379 --> 00:03:40,079
bad we did these three here we're going

71
00:03:38,849 --> 00:03:41,519
to talk about the other other ones

72
00:03:40,079 --> 00:03:43,650
shortly so the main one we're going to

73
00:03:41,519 --> 00:03:48,030
look at for now is is the last column

74
00:03:43,650 --> 00:03:50,549
which is the accuracy the first column

75
00:03:48,030 --> 00:03:52,199
as you can see is the epoch number so

76
00:03:50,549 --> 00:03:55,260
this tells us how many times has it been

77
00:03:52,199 --> 00:03:57,449
through the entire dataset trying to

78
00:03:55,259 --> 00:03:58,889
learn a better classifier and in the

79
00:03:57,449 --> 00:04:00,708
next two columns is what's called the

80
00:03:58,889 --> 00:04:03,539
loss which we'll be learning about

81
00:04:00,709 --> 00:04:05,069
either later today or next week the

82
00:04:03,539 --> 00:04:07,019
first point is the loss on the training

83
00:04:05,069 --> 00:04:08,459
set these are the images that we're

84
00:04:07,019 --> 00:04:10,469
looking at in order to try to make a

85
00:04:08,459 --> 00:04:12,479
better classifier and the second is the

86
00:04:10,469 --> 00:04:14,099
loss of the validation set these are the

87
00:04:12,479 --> 00:04:15,419
images that we're not looking at and

88
00:04:14,099 --> 00:04:17,969
we're training but we're just sitting on

89
00:04:15,419 --> 00:04:19,349
the side to see how accurate we are so

90
00:04:17,970 --> 00:04:23,239
we'll learn about littering loss in

91
00:04:19,350 --> 00:04:23,239
accuracy later

92
00:04:24,899 --> 00:04:33,099
okay so so we've got the epoch number

93
00:04:30,970 --> 00:04:36,970
the training loss is the second column

94
00:04:33,100 --> 00:04:40,879
the validation loss is the third column

95
00:04:36,970 --> 00:04:42,940
and the accuracy is the fourth column

96
00:04:40,879 --> 00:04:42,939
you

97
00:04:44,899 --> 00:04:56,519
okay so the basic idea of the loading

98
00:04:48,449 --> 00:04:58,259
rate so the basic idea of the learning

99
00:04:56,519 --> 00:05:01,378
rate is it's the thing that's going to

100
00:04:58,259 --> 00:05:05,158
decide how quickly do we zoom do we kind

101
00:05:01,379 --> 00:05:06,838
of hone in on the solution and so I find

102
00:05:05,158 --> 00:05:08,759
that a good way to think about this is

103
00:05:06,838 --> 00:05:13,740
to think about like well what if we were

104
00:05:08,759 --> 00:05:15,899
trying to fit to a function that looks

105
00:05:13,740 --> 00:05:17,759
something like this right we're trying

106
00:05:15,899 --> 00:05:20,459
to say okay where's where abouts is the

107
00:05:17,759 --> 00:05:23,158
minimum point this is basically what we

108
00:05:20,459 --> 00:05:26,848
do when we do deep learning is we try to

109
00:05:23,158 --> 00:05:29,579
find the minimum point of a function now

110
00:05:26,848 --> 00:05:30,808
our function happens to have millions or

111
00:05:29,579 --> 00:05:32,818
hundreds of millions of parameters but

112
00:05:30,809 --> 00:05:33,838
it works the same basic way and so when

113
00:05:32,819 --> 00:05:36,419
we look at it you know we can

114
00:05:33,838 --> 00:05:40,439
immediately see that the lowest point is

115
00:05:36,418 --> 00:05:42,448
here but how would you do that if you

116
00:05:40,439 --> 00:05:45,119
are a computer algorithm and what we do

117
00:05:42,449 --> 00:05:47,728
is we we start out at some point at

118
00:05:45,119 --> 00:05:50,099
random so you pick say here and we have

119
00:05:47,728 --> 00:05:52,498
a look and we say okay what's the what's

120
00:05:50,098 --> 00:05:53,819
the loss or the error at this point and

121
00:05:52,499 --> 00:05:55,409
we say what's the gradient in other

122
00:05:53,819 --> 00:05:58,680
words which way is up and which way is

123
00:05:55,408 --> 00:06:00,748
down and it tells us that down is going

124
00:05:58,680 --> 00:06:03,838
to be in that direction and it also

125
00:06:00,749 --> 00:06:05,430
tells us how fast is it going down which

126
00:06:03,838 --> 00:06:09,028
is at this point is going down pretty

127
00:06:05,430 --> 00:06:11,699
quickly and so then we take a step in

128
00:06:09,028 --> 00:06:13,829
the direction that's down and the

129
00:06:11,699 --> 00:06:15,538
distance we travel is going to be

130
00:06:13,829 --> 00:06:17,218
proportional to the gradient sort of

131
00:06:15,538 --> 00:06:20,038
unfortunately how steep it is the idea

132
00:06:17,218 --> 00:06:22,408
is if it's deeper then we're probably

133
00:06:20,038 --> 00:06:25,110
further away that's the general idea

134
00:06:22,408 --> 00:06:26,879
right and so specifically what we do is

135
00:06:25,110 --> 00:06:28,740
we take the gradient which is how steep

136
00:06:26,879 --> 00:06:30,360
is it at this point and we multiply it

137
00:06:28,740 --> 00:06:31,889
by some number and that number is called

138
00:06:30,360 --> 00:06:35,788
the learning rate okay

139
00:06:31,889 --> 00:06:38,249
so if we pick a number that is very

140
00:06:35,788 --> 00:06:39,598
small then we're guaranteed that we're

141
00:06:38,249 --> 00:06:40,740
going to go a little bit closer and a

142
00:06:39,598 --> 00:06:43,019
little bit closer and a little bit

143
00:06:40,740 --> 00:06:45,658
closer each time right but it's going to

144
00:06:43,019 --> 00:06:48,418
take us a very long time to eventually

145
00:06:45,658 --> 00:06:51,478
get to the bottom if we dig a number

146
00:06:48,418 --> 00:06:53,128
that's very big we could actually step

147
00:06:51,478 --> 00:06:54,930
too far could go in the right direction

148
00:06:53,129 --> 00:06:57,270
but we could step all the way over to

149
00:06:54,930 --> 00:06:59,220
here right as

150
00:06:57,269 --> 00:07:01,799
result of which we end up further away

151
00:06:59,220 --> 00:07:04,590
than we started and we could oscillate

152
00:07:01,800 --> 00:07:06,720
and get worse and worse so if you start

153
00:07:04,589 --> 00:07:08,699
training a neural net and you find that

154
00:07:06,720 --> 00:07:10,740
your accuracy or your loss is like

155
00:07:08,699 --> 00:07:12,779
spitting off into infinity

156
00:07:10,740 --> 00:07:17,519
almost certainly your learning rates too

157
00:07:12,779 --> 00:07:20,579
high so in a sense learning rate too low

158
00:07:17,519 --> 00:07:21,569
is is a better problem to have because

159
00:07:20,579 --> 00:07:23,099
you're going to have to wait a long time

160
00:07:21,569 --> 00:07:25,560
but wouldn't it be nice if there was a

161
00:07:23,100 --> 00:07:27,450
way to figure out like what's the best

162
00:07:25,560 --> 00:07:30,689
learning rate something where you could

163
00:07:27,449 --> 00:07:34,469
kind of go quickly go like Bom Bom Bom

164
00:07:30,689 --> 00:07:36,269
right and so that's why we use this

165
00:07:34,470 --> 00:07:39,300
thing called a learning rate finder and

166
00:07:36,269 --> 00:07:41,819
what the learning rate finder does is it

167
00:07:39,300 --> 00:07:44,160
tries each each time it looks at another

168
00:07:41,819 --> 00:07:46,860
remember the mini-batch how many batches

169
00:07:44,160 --> 00:07:48,900
a few images that we look at each time

170
00:07:46,860 --> 00:07:50,850
so that we're using the parallel

171
00:07:48,899 --> 00:07:53,759
processing power of the GPU effectively

172
00:07:50,850 --> 00:07:56,370
we look generally at around 64 128

173
00:07:53,759 --> 00:07:59,339
images at a time for each mini batch

174
00:07:56,370 --> 00:08:01,040
which is labeled here as an iteration we

175
00:07:59,339 --> 00:08:02,639
gradually increase the learning rate

176
00:08:01,040 --> 00:08:04,230
multiplicatively increase the learning

177
00:08:02,639 --> 00:08:06,449
rate we started really really tiny

178
00:08:04,230 --> 00:08:09,090
learning rates to make sure that we

179
00:08:06,449 --> 00:08:11,789
don't start at something too high and we

180
00:08:09,089 --> 00:08:14,489
gradually increase it and so the idea is

181
00:08:11,790 --> 00:08:16,710
that eventually the learning rate will

182
00:08:14,490 --> 00:08:17,730
be so big that the loss will start

183
00:08:16,709 --> 00:08:19,259
getting worse

184
00:08:17,730 --> 00:08:22,470
and so what we're going to do then is

185
00:08:19,259 --> 00:08:25,319
we're a look at the plot of learning

186
00:08:22,470 --> 00:08:28,620
rate against loss right so when the

187
00:08:25,319 --> 00:08:29,909
learning rates tiny it increases slowly

188
00:08:28,620 --> 00:08:33,029
then it's that's where increase a bit

189
00:08:29,910 --> 00:08:34,500
faster and then eventually it starts not

190
00:08:33,029 --> 00:08:36,839
increasing as quickly and in fact it

191
00:08:34,500 --> 00:08:38,850
starts getting worse right so clearly

192
00:08:36,840 --> 00:08:42,180
here and make sure you're you want to be

193
00:08:38,850 --> 00:08:45,870
familiar with this scientific notation

194
00:08:42,179 --> 00:08:48,509
okay so ten to the negative one is 0.1

195
00:08:45,870 --> 00:08:52,620
10 to 50 or is 1 10 to the negative 2 is

196
00:08:48,509 --> 00:08:53,819
0.001 and when we write this in Python

197
00:08:52,620 --> 00:08:56,429
we'll generally write it like this

198
00:08:53,820 --> 00:08:59,220
rather than writing 10 to the negative 1

199
00:08:56,429 --> 00:09:05,429
or 10 to the negative 2 we'll just write

200
00:08:59,220 --> 00:09:06,600
1 a neg 1 or 1 e neg - okay I mean the

201
00:09:05,429 --> 00:09:10,829
same thing you're going to see that all

202
00:09:06,600 --> 00:09:19,620
the time and remember that equals 0.1

203
00:09:10,830 --> 00:09:21,990
Oh point O one okay so don't be confused

204
00:09:19,620 --> 00:09:25,139
by this text that it prints out here

205
00:09:21,990 --> 00:09:27,600
this this loss here is the the final

206
00:09:25,139 --> 00:09:29,189
loss at the very at the end of it's not

207
00:09:27,600 --> 00:09:31,019
of any interest right so ignore this

208
00:09:29,190 --> 00:09:32,880
this is only interesting when we're

209
00:09:31,019 --> 00:09:34,379
doing regular trading that's not

210
00:09:32,879 --> 00:09:35,730
interesting for the learning rate finder

211
00:09:34,379 --> 00:09:38,220
the thing that's interesting for the

212
00:09:35,730 --> 00:09:41,730
learning rate finder is this loan shed

213
00:09:38,220 --> 00:09:43,350
plot and specifically we're not looking

214
00:09:41,730 --> 00:09:44,519
for the point where it's the lowest back

215
00:09:43,350 --> 00:09:46,139
to the point where it's the lowest it's

216
00:09:44,519 --> 00:09:47,909
actually not getting better anymore so

217
00:09:46,139 --> 00:09:49,319
that's to higher learning rate so I

218
00:09:47,909 --> 00:09:51,689
generally look to see like where is it

219
00:09:49,320 --> 00:09:56,760
the lowest and then I go back like one

220
00:09:51,690 --> 00:09:59,670
for magnitude so one enoch two would be

221
00:09:56,759 --> 00:10:05,460
a pretty good choice yeah okay so that's

222
00:09:59,669 --> 00:10:09,679
why you saw when we ran our fit here we

223
00:10:05,460 --> 00:10:12,870
picked 0.01 right which is one a neg two

224
00:10:09,679 --> 00:10:15,059
so important point to make here is like

225
00:10:12,870 --> 00:10:21,000
this this is the one key number that

226
00:10:15,059 --> 00:10:22,859
we've learnt to adjust and if you just

227
00:10:21,000 --> 00:10:24,120
adjust this number at nothing else most

228
00:10:22,860 --> 00:10:26,340
of the time you're going to be able to

229
00:10:24,120 --> 00:10:28,139
get pretty good results and this is like

230
00:10:26,340 --> 00:10:31,350
a very different message to what you

231
00:10:28,139 --> 00:10:35,879
would hear or see in any textbook or any

232
00:10:31,350 --> 00:10:38,310
video or any course because up until now

233
00:10:35,879 --> 00:10:39,570
there's been like dozens and dozens of

234
00:10:38,309 --> 00:10:41,309
these they're called hyper parameters

235
00:10:39,570 --> 00:10:43,110
dozens and dozens of hyper parameters to

236
00:10:41,309 --> 00:10:45,359
set and they've been thought of as

237
00:10:43,110 --> 00:10:48,210
highly sensitive and difficult to set so

238
00:10:45,360 --> 00:10:51,450
inside the first AI library we kind of

239
00:10:48,210 --> 00:10:53,490
do all that stuff for you as much as we

240
00:10:51,450 --> 00:10:55,110
can and during the course we're going to

241
00:10:53,490 --> 00:10:58,259
learn that there are some more we can

242
00:10:55,110 --> 00:11:01,350
quake to get slightly better results but

243
00:10:58,259 --> 00:11:03,210
it's kind of like it's kind of in a

244
00:11:01,350 --> 00:11:04,290
funny situation here because for those

245
00:11:03,210 --> 00:11:06,540
of you that haven't done anything

246
00:11:04,289 --> 00:11:09,179
learning before is kind of like oh this

247
00:11:06,539 --> 00:11:10,829
is that's all there is to it this is

248
00:11:09,179 --> 00:11:12,449
very easy and then when you talk to

249
00:11:10,830 --> 00:11:14,190
people outside this class they'll be

250
00:11:12,450 --> 00:11:16,140
like deep learning so difficult as

251
00:11:14,190 --> 00:11:18,000
someone to say it's a real art form and

252
00:11:16,139 --> 00:11:19,769
so that's why there's this as is

253
00:11:18,000 --> 00:11:21,360
difference right and so that the truth

254
00:11:19,769 --> 00:11:24,000
is that the learning rate really is the

255
00:11:21,360 --> 00:11:24,649
key thing to set and this ability to use

256
00:11:24,000 --> 00:11:26,509
this

257
00:11:24,649 --> 00:11:29,948
to figure out how to set it well though

258
00:11:26,509 --> 00:11:33,438
the paper is now probably 18 months old

259
00:11:29,948 --> 00:11:35,118
almost nobody knows about this paper it

260
00:11:33,438 --> 00:11:36,860
was from a guy who's not from a famous

261
00:11:35,119 --> 00:11:38,509
research labs so most people kind of

262
00:11:36,860 --> 00:11:40,639
ignored it and in fact even this

263
00:11:38,509 --> 00:11:42,829
particular technique was one subpart of

264
00:11:40,639 --> 00:11:45,110
a paper that was about something else

265
00:11:42,828 --> 00:11:47,688
so again this idea of like this is how

266
00:11:45,110 --> 00:11:49,610
you can set the learning rate really

267
00:11:47,688 --> 00:11:51,980
nobody outside this classroom just about

268
00:11:49,610 --> 00:11:53,480
knows about it obviously the guy who

269
00:11:51,980 --> 00:11:56,869
wrote it Leslie Smith knows about it

270
00:11:53,480 --> 00:11:58,999
yeah so it's a good thing to tell your

271
00:11:56,869 --> 00:12:00,290
colleagues about is like here is

272
00:11:58,999 --> 00:12:03,230
actually a great way to set the learning

273
00:12:00,289 --> 00:12:04,818
rate and there's even been papers caught

274
00:12:03,230 --> 00:12:07,399
like one of the famous papers is called

275
00:12:04,818 --> 00:12:09,528
no more pesky learning rates which

276
00:12:07,399 --> 00:12:11,028
actually is a less effective technique

277
00:12:09,528 --> 00:12:13,220
than this one but this idea that like

278
00:12:11,028 --> 00:12:15,678
setting learning rates is is very

279
00:12:13,220 --> 00:12:17,600
difficult and thirdly is has been true

280
00:12:15,678 --> 00:12:20,568
for most of the kind of deep learning

281
00:12:17,600 --> 00:12:23,028
history so here's the trick right go

282
00:12:20,568 --> 00:12:24,948
look at this plot find kind of the

283
00:12:23,028 --> 00:12:28,068
lowest to go back about a multiple of

284
00:12:24,948 --> 00:12:29,508
ten and try that all right and if that

285
00:12:28,068 --> 00:12:31,219
doesn't quite work you can always try

286
00:12:29,509 --> 00:12:34,539
you know going back another multiple ten

287
00:12:31,220 --> 00:12:34,540
but this is always worked for me so far

288
00:12:40,159 --> 00:12:45,860
once why does this learning rate this

289
00:12:43,490 --> 00:12:47,749
method work versus something else like

290
00:12:45,860 --> 00:12:50,120
momentum base or what's like the

291
00:12:47,749 --> 00:12:52,009
advantages a disadvantage with just

292
00:12:50,120 --> 00:13:00,188
learning rate rate like technique we're

293
00:12:52,009 --> 00:13:02,209
just feels that's a great question so

294
00:13:00,188 --> 00:13:03,498
we're going to learn during this course

295
00:13:02,208 --> 00:13:05,748
about a number of ways of improving

296
00:13:03,499 --> 00:13:08,778
gradient percent like you mentioned

297
00:13:05,749 --> 00:13:10,970
momentum and atom and so forth this is

298
00:13:08,778 --> 00:13:13,039
orthogonal in fact so one of the things

299
00:13:10,970 --> 00:13:15,439
the faster a library tries to do is

300
00:13:13,039 --> 00:13:17,149
figure out the right gradient descent

301
00:13:15,438 --> 00:13:18,379
version and in fact behind the scenes

302
00:13:17,149 --> 00:13:21,169
this is actually using something called

303
00:13:18,379 --> 00:13:23,139
atom and so this technique is telling us

304
00:13:21,169 --> 00:13:26,389
this is the best learning rate to use

305
00:13:23,139 --> 00:13:29,028
given what I thought other tweaks you're

306
00:13:26,389 --> 00:13:31,339
using in this case the atom optimizer so

307
00:13:29,028 --> 00:13:32,899
it's not that there's some compromise

308
00:13:31,339 --> 00:13:34,759
between this and some other approaches

309
00:13:32,899 --> 00:13:36,019
who sits on top of those approaches and

310
00:13:34,759 --> 00:13:38,178
you still have to set the learning rate

311
00:13:36,019 --> 00:13:38,659
when you use with other approaches so

312
00:13:38,178 --> 00:13:41,028
we're trying to

313
00:13:38,659 --> 00:13:42,649
find the best kind of optimizer to use

314
00:13:41,028 --> 00:13:44,269
for a problem that you still have to set

315
00:13:42,649 --> 00:13:46,458
the learning rate and this is how we can

316
00:13:44,269 --> 00:13:49,100
do it and in fact this idea of using

317
00:13:46,458 --> 00:13:50,568
this technique on top of more advanced

318
00:13:49,100 --> 00:13:52,579
optimizers like Adam might haven't even

319
00:13:50,568 --> 00:13:54,349
seen mentioned in a paper before so I

320
00:13:52,578 --> 00:13:56,269
think this is like a I mean it's not a

321
00:13:54,350 --> 00:13:59,629
huge breakthrough it seems obvious but

322
00:13:56,269 --> 00:14:02,589
nobody else seems to tried it so as you

323
00:13:59,629 --> 00:14:02,589
can see it was well

324
00:14:05,799 --> 00:14:10,689
when we use optimizers like Adam ditched

325
00:14:08,230 --> 00:14:12,339
Harvick adaptive learning rate so and he

326
00:14:10,690 --> 00:14:14,050
said this learning rate is Italy initial

327
00:14:12,339 --> 00:14:21,880
learning rate because it changes during

328
00:14:14,049 --> 00:14:24,250
the people so we're going to be learning

329
00:14:21,879 --> 00:14:26,350
about things like Adam the details about

330
00:14:24,250 --> 00:14:28,269
it later in the class but the basic

331
00:14:26,350 --> 00:14:29,909
answer is no even with even the Adam

332
00:14:28,269 --> 00:14:35,860
that there actually is a learning rate

333
00:14:29,909 --> 00:14:39,519
it's just being it's being basically

334
00:14:35,860 --> 00:14:41,950
divided by the the gradient the average

335
00:14:39,519 --> 00:14:43,750
previous gradient and also the recent

336
00:14:41,950 --> 00:14:45,220
summer Squared's of gradients so there's

337
00:14:43,750 --> 00:14:47,589
still like a number called the learning

338
00:14:45,220 --> 00:14:49,750
rate there there isn't a even these so

339
00:14:47,589 --> 00:15:00,339
called dynamic learning rate methods

340
00:14:49,750 --> 00:15:04,509
still have unlearning rate okay so the

341
00:15:00,339 --> 00:15:07,660
most important thing that you can do to

342
00:15:04,509 --> 00:15:10,990
make your model better and is to give it

343
00:15:07,659 --> 00:15:13,719
more data so the challenge that happens

344
00:15:10,990 --> 00:15:16,810
is that these models have hundreds of

345
00:15:13,720 --> 00:15:19,870
millions of parameters and if you train

346
00:15:16,809 --> 00:15:21,969
them for a while they start to do what's

347
00:15:19,870 --> 00:15:23,980
called overfitting and so overfitting

348
00:15:21,970 --> 00:15:26,079
means that they're going to start to see

349
00:15:23,980 --> 00:15:27,789
like the specific details of the images

350
00:15:26,078 --> 00:15:31,569
you're giving them rather than the more

351
00:15:27,789 --> 00:15:34,299
general learning that can transfer

352
00:15:31,570 --> 00:15:36,040
across to the validation set so the best

353
00:15:34,299 --> 00:15:39,399
thing we can do to avoid overfitting is

354
00:15:36,039 --> 00:15:41,078
to find more data now obviously one way

355
00:15:39,399 --> 00:15:42,399
to do that would just be to collect more

356
00:15:41,078 --> 00:15:45,069
data from where you're getting it from

357
00:15:42,399 --> 00:15:47,200
or label more data but a really easy way

358
00:15:45,070 --> 00:15:51,040
that we should always do is to use

359
00:15:47,200 --> 00:15:53,200
something called data augmentation so

360
00:15:51,039 --> 00:15:55,958
they don't open tuition is one of these

361
00:15:53,200 --> 00:15:57,520
things that's key in many courses it's

362
00:15:55,958 --> 00:15:58,838
not even mentioned at all or if it is

363
00:15:57,519 --> 00:16:00,730
it's kind of like an advanced topic

364
00:15:58,839 --> 00:16:03,250
right at the end but actually it's like

365
00:16:00,730 --> 00:16:05,320
the most important thing that you can do

366
00:16:03,250 --> 00:16:07,000
to make a better model okay and so it's

367
00:16:05,320 --> 00:16:08,890
built into the faster you library to

368
00:16:07,000 --> 00:16:10,419
make it very easy to do and so we're

369
00:16:08,889 --> 00:16:14,740
going to look at the details of the code

370
00:16:10,419 --> 00:16:19,929
shortly but the basic idea is that

371
00:16:14,740 --> 00:16:22,240
as in our initial code we had a line

372
00:16:19,929 --> 00:16:23,949
that said image classified data from

373
00:16:22,240 --> 00:16:26,769
parts and we passed in the path to our

374
00:16:23,950 --> 00:16:30,009
data and for transforms we passed in

375
00:16:26,769 --> 00:16:31,360
basically the sizing the architecture

376
00:16:30,009 --> 00:16:34,149
we'll look at this in more detail

377
00:16:31,360 --> 00:16:36,759
shortly we just add one more parameter

378
00:16:34,149 --> 00:16:41,019
which is what kind of data augmentation

379
00:16:36,759 --> 00:16:43,240
do you want to do and so to understand

380
00:16:41,019 --> 00:16:44,620
data augmentation it's may be easiest to

381
00:16:43,240 --> 00:16:47,139
look at some pictures of data

382
00:16:44,620 --> 00:16:48,820
augmentation so what I've done here

383
00:16:47,139 --> 00:16:50,799
again we'll look at the code in more

384
00:16:48,820 --> 00:16:56,560
detail later but the basic idea is oh

385
00:16:50,799 --> 00:16:58,269
I've run I've built a data class like

386
00:16:56,559 --> 00:17:00,549
multiple times I'm going to do it six

387
00:16:58,269 --> 00:17:03,909
times and each time I'm going to plot

388
00:17:00,549 --> 00:17:07,328
the same catch and you can see that what

389
00:17:03,909 --> 00:17:09,099
happens is that this cap here is further

390
00:17:07,328 --> 00:17:10,509
over to the left this one here is

391
00:17:09,099 --> 00:17:13,838
further over to the right and this one

392
00:17:10,509 --> 00:17:17,730
here is fit horizontally and so forth so

393
00:17:13,838 --> 00:17:19,568
data augmentation different types of

394
00:17:17,730 --> 00:17:22,660
image you're going to want different

395
00:17:19,568 --> 00:17:25,019
types of data augmentation right so for

396
00:17:22,660 --> 00:17:27,730
example if you were trying to recognize

397
00:17:25,019 --> 00:17:29,470
letters and digits you wouldn't want to

398
00:17:27,730 --> 00:17:31,539
flip horizontally because like it's

399
00:17:29,470 --> 00:17:33,929
actually has a different meaning whereas

400
00:17:31,539 --> 00:17:36,308
on the other hand if you're looking at

401
00:17:33,929 --> 00:17:38,140
photos of cats and dogs you probably

402
00:17:36,308 --> 00:17:40,240
don't want to fit vertically because

403
00:17:38,140 --> 00:17:42,220
cats aren't generally upside down all

404
00:17:40,240 --> 00:17:44,049
right where else if you're looking at

405
00:17:42,220 --> 00:17:47,529
there's a current Cargill competition

406
00:17:44,049 --> 00:17:50,079
which is recognizing icebergs in

407
00:17:47,529 --> 00:17:52,359
satellite images you probably do want to

408
00:17:50,079 --> 00:17:54,399
fit them upside down because it's really

409
00:17:52,359 --> 00:17:58,959
matter which area around the iceberg or

410
00:17:54,400 --> 00:18:00,940
the satellite was right so one of the

411
00:17:58,960 --> 00:18:03,700
examples of the transform sets we have

412
00:18:00,940 --> 00:18:05,200
is transforms sidon so in other words if

413
00:18:03,700 --> 00:18:07,420
you have photos that are like generally

414
00:18:05,200 --> 00:18:08,679
taken from the side which generally

415
00:18:07,420 --> 00:18:10,690
means you want to be able to flip them

416
00:18:08,679 --> 00:18:12,130
horizontally but not vertically this is

417
00:18:10,690 --> 00:18:13,509
going to give you all the transforms you

418
00:18:12,130 --> 00:18:16,179
need for that so it'll flip them

419
00:18:13,509 --> 00:18:19,000
sideways rotate them by small amounts

420
00:18:16,179 --> 00:18:22,630
but not too much and slightly bury their

421
00:18:19,000 --> 00:18:24,190
contrast and brightness and slightly

422
00:18:22,630 --> 00:18:25,160
zoom in and out a little bit and move

423
00:18:24,190 --> 00:18:26,650
them around a little

424
00:18:25,160 --> 00:18:30,980
so each time it's a slightly different

425
00:18:26,650 --> 00:18:33,610
fight billionaires we're getting a

426
00:18:30,980 --> 00:18:36,620
couple of questions from people about

427
00:18:33,609 --> 00:18:38,929
you explaining in the reason why you

428
00:18:36,619 --> 00:18:41,750
don't think the minimum of the loss

429
00:18:38,930 --> 00:18:44,840
curve yeah but it's like the higher rate

430
00:18:41,750 --> 00:18:48,650
so yeah and also could you people

431
00:18:44,839 --> 00:18:52,909
understand if this works for every CNN

432
00:18:48,650 --> 00:18:58,550
for CNN every minute there's a running

433
00:18:52,910 --> 00:18:58,850
right fine done yeah exactly yeah okay

434
00:18:58,549 --> 00:19:02,809
great

435
00:18:58,849 --> 00:19:11,480
um could you put your hand up if there's

436
00:19:02,809 --> 00:19:13,159
a spare seat next to you so there was a

437
00:19:11,480 --> 00:19:15,470
question about the learning rate finder

438
00:19:13,160 --> 00:19:18,350
about why do we use the learning rate

439
00:19:15,470 --> 00:19:20,150
that's less than the lowest point and so

440
00:19:18,349 --> 00:19:23,709
the reason why is to understand what's

441
00:19:20,150 --> 00:19:23,710
going on with this learning rate finder

442
00:19:24,160 --> 00:19:31,190
so let's go back to our picture here

443
00:19:28,029 --> 00:19:33,470
like how do we figure out what learning

444
00:19:31,190 --> 00:19:35,750
rate to use right and so what we're

445
00:19:33,470 --> 00:19:39,829
going to do is we're going to take steps

446
00:19:35,750 --> 00:19:41,299
and each time we're going to double the

447
00:19:39,829 --> 00:19:43,099
learning rate so kind of double the

448
00:19:41,299 --> 00:19:45,769
amount by which we multiply the grander

449
00:19:43,099 --> 00:19:47,779
gradient so in other words would go tiny

450
00:19:45,769 --> 00:19:51,980
step slightly bigger slightly bigger

451
00:19:47,779 --> 00:19:57,349
slightly bigger slightly bigger slightly

452
00:19:51,980 --> 00:19:59,360
bigger slightly bigger okay and so the

453
00:19:57,349 --> 00:20:01,490
question is of the purpose of this is

454
00:19:59,359 --> 00:20:03,740
not to find the minimum the purpose of

455
00:20:01,490 --> 00:20:07,029
this is to figure out what learning rate

456
00:20:03,740 --> 00:20:09,829
is allowing us to decrease quickly right

457
00:20:07,029 --> 00:20:13,309
so the point at which the loss was

458
00:20:09,829 --> 00:20:15,169
lowest here is actually there right but

459
00:20:13,309 --> 00:20:16,669
that learning rate actually looks like

460
00:20:15,170 --> 00:20:19,070
it's probably too high it's going to

461
00:20:16,670 --> 00:20:22,250
just jump like probably backwards and

462
00:20:19,069 --> 00:20:24,289
forwards okay so instead what we do is

463
00:20:22,250 --> 00:20:26,450
we go back to the point where the

464
00:20:24,289 --> 00:20:33,019
learning rates quickly are giving us a

465
00:20:26,450 --> 00:20:34,610
quick increase in the loss so here is so

466
00:20:33,019 --> 00:20:36,769
here is the actual learning rate

467
00:20:34,609 --> 00:20:38,089
increasing every single time we look at

468
00:20:36,769 --> 00:20:40,009
a new mini batch

469
00:20:38,089 --> 00:20:42,468
so mini-batch reiteration versus

470
00:20:40,009 --> 00:20:44,450
learning right and then here is learning

471
00:20:42,469 --> 00:20:47,288
rate versus loss so here's that point at

472
00:20:44,450 --> 00:20:49,519
the bottom where is now already too high

473
00:20:47,288 --> 00:20:51,319
okay and so here's the point where we go

474
00:20:49,519 --> 00:20:55,339
back a little bit and it's increasing

475
00:20:51,319 --> 00:20:57,259
nice and quickly we're going to learn

476
00:20:55,339 --> 00:20:59,658
about something called stochastic

477
00:20:57,259 --> 00:21:01,249
gradient descent with restarts shortly

478
00:20:59,659 --> 00:21:03,379
where we're going to see like in a sense

479
00:21:01,249 --> 00:21:05,329
you might want to go back to 1 enoch 3

480
00:21:03,378 --> 00:21:07,189
where it's actually even steeper still

481
00:21:05,329 --> 00:21:11,329
and maybe we would actually find this

482
00:21:07,190 --> 00:21:13,009
book actually learn even quicker you

483
00:21:11,329 --> 00:21:14,868
could try it but we're going to see

484
00:21:13,009 --> 00:21:16,729
later why actually using a higher number

485
00:21:14,868 --> 00:21:19,579
is going to give us better

486
00:21:16,729 --> 00:21:22,129
generalization so for now let's put that

487
00:21:19,579 --> 00:21:24,259
aside do you mean higher learning rate

488
00:21:22,128 --> 00:21:25,579
when you say I know I mean higher

489
00:21:24,259 --> 00:21:31,429
letting retina say higher

490
00:21:25,579 --> 00:21:33,259
yeah yeah I mean I am learning rate so

491
00:21:31,429 --> 00:21:35,179
as we increase the iterations from the

492
00:21:33,259 --> 00:21:37,909
learning rate finder the learning rate

493
00:21:35,179 --> 00:21:41,298
is going up this is iterations versus

494
00:21:37,909 --> 00:21:43,249
learning ready okay so as we do that as

495
00:21:41,298 --> 00:21:46,579
the learning rate increases and we plot

496
00:21:43,249 --> 00:21:47,778
it here the loss Goes Down and here we

497
00:21:46,579 --> 00:21:50,329
get to the point where the learning rate

498
00:21:47,778 --> 00:21:52,759
is too high and at that point the most

499
00:21:50,329 --> 00:21:54,019
is now getting worse because I asked the

500
00:21:52,759 --> 00:21:55,639
question because you were just

501
00:21:54,019 --> 00:21:58,519
indicating that you know even though the

502
00:21:55,638 --> 00:22:00,558
minimum was at 10 to the minus 1 you

503
00:21:58,519 --> 00:22:03,288
were gonna you suggest that we should

504
00:22:00,558 --> 00:22:04,668
choose 10 to the minus 2 but now you're

505
00:22:03,288 --> 00:22:07,038
saying I mean we should go back the

506
00:22:04,669 --> 00:22:08,749
other way higher so I didn't mean to say

507
00:22:07,038 --> 00:22:11,499
that I'm sorry if I said something

508
00:22:08,749 --> 00:22:14,899
backwards I want to go back down to the

509
00:22:11,499 --> 00:22:17,778
lower learning rate so possibly I said a

510
00:22:14,898 --> 00:22:19,668
higher when I meant higher into this

511
00:22:17,778 --> 00:22:22,909
lower OS do you know I'm learning right

512
00:22:19,669 --> 00:22:26,390
okay thanks yep

513
00:22:22,909 --> 00:22:29,690
last class is said that the local all

514
00:22:26,390 --> 00:22:32,270
the local minima are the same and this

515
00:22:29,690 --> 00:22:33,769
graph also shows the same is that is

516
00:22:32,269 --> 00:22:37,788
this something that was observed or is

517
00:22:33,769 --> 00:22:40,220
the logic theory behind it that's not

518
00:22:37,788 --> 00:22:41,750
what this graph is showing this graph is

519
00:22:40,220 --> 00:22:43,460
simply showing that there's a point

520
00:22:41,750 --> 00:22:46,099
where if we increase the learning rate

521
00:22:43,460 --> 00:22:48,230
more then it stops getting better than

522
00:22:46,099 --> 00:22:53,019
actually starts getting worse the idea

523
00:22:48,230 --> 00:22:57,470
that all local minima are the same is a

524
00:22:53,019 --> 00:22:58,788
totally separate issue and it's actually

525
00:22:57,470 --> 00:23:02,110
something else we'll see a picture of

526
00:22:58,788 --> 00:23:02,109
shortly so let's come back to that

527
00:23:02,409 --> 00:23:08,710
Jeremy do we have to find the base

528
00:23:05,630 --> 00:23:13,400
learning rate every time we are going to

529
00:23:08,710 --> 00:23:15,919
run a poke third time we're running on a

530
00:23:13,400 --> 00:23:19,220
poke and a pop so how many times should

531
00:23:15,919 --> 00:23:27,410
I run this like let me write find my

532
00:23:19,220 --> 00:23:31,930
training that's a great question unit um

533
00:23:27,410 --> 00:23:34,070
I certainly run it once when I start

534
00:23:31,930 --> 00:23:37,970
later on in this class we're going to

535
00:23:34,069 --> 00:23:40,009
learn about unfreezing layers and after

536
00:23:37,970 --> 00:23:42,799
I unfreeze layers I sometimes run it

537
00:23:40,009 --> 00:23:44,750
again if I do something to like change

538
00:23:42,799 --> 00:23:46,519
the thing I'm training or change the way

539
00:23:44,750 --> 00:23:50,539
I'm training it you may want to run it

540
00:23:46,519 --> 00:23:51,769
again basically or you know if you

541
00:23:50,539 --> 00:23:54,139
particularly if you've changed something

542
00:23:51,769 --> 00:23:55,490
about how you train like unfreezing

543
00:23:54,140 --> 00:23:57,500
layers which we're gonna soon learn

544
00:23:55,490 --> 00:24:01,069
about and you're finding the other

545
00:23:57,500 --> 00:24:03,259
training is unstable or too slow

546
00:24:01,069 --> 00:24:04,269
well again you can run it again there's

547
00:24:03,259 --> 00:24:07,660
never any harm

548
00:24:04,269 --> 00:24:11,539
in running it it doesn't take very long

549
00:24:07,660 --> 00:24:14,660
that's great question okay so back to

550
00:24:11,539 --> 00:24:17,710
data augmentation so if we add to a when

551
00:24:14,660 --> 00:24:20,480
we run this little transforms from model

552
00:24:17,710 --> 00:24:23,319
function we pass in orientation

553
00:24:20,480 --> 00:24:26,329
transforms we can pass in the main to a

554
00:24:23,319 --> 00:24:28,339
transform side on or transforms top down

555
00:24:26,329 --> 00:24:31,669
later on we'll learn about creating your

556
00:24:28,339 --> 00:24:32,959
own custom transform lists as well but

557
00:24:31,670 --> 00:24:35,450
for now because we're taking pictures

558
00:24:32,960 --> 00:24:38,720
from the side cats and dogs will say

559
00:24:35,450 --> 00:24:41,029
transform side on and now each time we

560
00:24:38,720 --> 00:24:42,829
look at an image it's going to be zoomed

561
00:24:41,029 --> 00:24:43,399
in or out a little bit moved around a

562
00:24:42,829 --> 00:24:47,740
little bit

563
00:24:43,400 --> 00:24:50,720
rotated a little bit possibly flipped

564
00:24:47,740 --> 00:24:53,210
okay and so what this does is it's not

565
00:24:50,720 --> 00:24:55,640
exactly creating new data but as far as

566
00:24:53,210 --> 00:24:57,110
the convolutional neural net is

567
00:24:55,640 --> 00:24:59,330
concerned it's a different way of

568
00:24:57,109 --> 00:25:02,839
looking at this thing and it actually

569
00:24:59,329 --> 00:25:06,049
therefore allows it to learn how to

570
00:25:02,839 --> 00:25:07,849
recognize cats or dogs from somewhat

571
00:25:06,049 --> 00:25:09,409
different angles right so when we do

572
00:25:07,849 --> 00:25:12,849
data orientation we're basically trying

573
00:25:09,410 --> 00:25:15,590
to say based on our domain knowledge

574
00:25:12,849 --> 00:25:18,949
here here are different ways that we can

575
00:25:15,589 --> 00:25:20,899
mess with this image that we know still

576
00:25:18,950 --> 00:25:22,490
make it the same image you know and that

577
00:25:20,900 --> 00:25:25,330
we could expect that you might actually

578
00:25:22,490 --> 00:25:28,910
see that kind of image in the real world

579
00:25:25,329 --> 00:25:31,309
so what we can do now is when we call

580
00:25:28,910 --> 00:25:33,410
this from parts function which we'll

581
00:25:31,309 --> 00:25:35,960
learn more about shortly we can now pass

582
00:25:33,410 --> 00:25:40,450
in this set of transforms which actually

583
00:25:35,960 --> 00:25:40,450
have these augmentations in now

584
00:25:41,410 --> 00:25:47,250
so that's going to we're going to start

585
00:25:42,819 --> 00:25:51,039
from scratch here we do a fit and

586
00:25:47,250 --> 00:25:53,589
initially the augmentations actually

587
00:25:51,039 --> 00:25:55,210
don't do anything and the reason

588
00:25:53,589 --> 00:25:56,859
initially they don't do anything is

589
00:25:55,210 --> 00:25:59,350
because we've got here something that

590
00:25:56,859 --> 00:26:03,669
says precompute equals true we're going

591
00:25:59,349 --> 00:26:06,129
to come back to these lots of times but

592
00:26:03,670 --> 00:26:08,410
basically what this is doing is do you

593
00:26:06,130 --> 00:26:11,320
remember this picture we saw where we

594
00:26:08,410 --> 00:26:13,750
learn each different layer has these

595
00:26:11,319 --> 00:26:16,029
activations that basically look for it

596
00:26:13,750 --> 00:26:21,279
or anything from the middle of flowers

597
00:26:16,029 --> 00:26:25,049
to eyeballs of birds or whatever right

598
00:26:21,279 --> 00:26:28,000
and so literally what happens is that

599
00:26:25,049 --> 00:26:30,039
the the later layers of this

600
00:26:28,000 --> 00:26:32,289
convolutional neural network have these

601
00:26:30,039 --> 00:26:34,899
things called activations and activation

602
00:26:32,289 --> 00:26:38,339
literally it's a number an activation is

603
00:26:34,900 --> 00:26:42,640
a number that says this feature like

604
00:26:38,339 --> 00:26:44,259
eyeball of bird is in this location with

605
00:26:42,640 --> 00:26:46,780
this level of confidence with its

606
00:26:44,259 --> 00:26:50,319
probability right and so we're going to

607
00:26:46,779 --> 00:26:52,869
see a lot of this later but what we can

608
00:26:50,319 --> 00:26:55,689
do is we can say all right well in this

609
00:26:52,869 --> 00:26:57,549
we've got a pre trained network remember

610
00:26:55,690 --> 00:26:58,690
and a pre trained network is one where

611
00:26:57,549 --> 00:27:00,430
it's already learned to recognize

612
00:26:58,690 --> 00:27:02,380
certain things in this case it's learnt

613
00:27:00,430 --> 00:27:05,560
to recognize the one and a half million

614
00:27:02,380 --> 00:27:07,990
images in the imagenet dataset and so

615
00:27:05,559 --> 00:27:10,240
what we could do is we could take the

616
00:27:07,990 --> 00:27:12,609
the second last layer so the one which

617
00:27:10,240 --> 00:27:14,740
is like got all of the information

618
00:27:12,609 --> 00:27:17,439
necessary to figure out what kind of

619
00:27:14,740 --> 00:27:19,509
thing a thing is and we can save those

620
00:27:17,440 --> 00:27:21,970
activations so basically saving things

621
00:27:19,509 --> 00:27:24,009
saying you know there's this level of

622
00:27:21,970 --> 00:27:25,809
eyeball nurse here in this level of dogs

623
00:27:24,009 --> 00:27:29,410
facing us here or in this level of

624
00:27:25,809 --> 00:27:32,769
fluffy ear there and so forth and so we

625
00:27:29,410 --> 00:27:35,140
save for every image these activations

626
00:27:32,769 --> 00:27:38,109
and that we call them the pre computed

627
00:27:35,140 --> 00:27:41,020
activations and so the idea is now that

628
00:27:38,109 --> 00:27:43,990
when we want to create a new classifier

629
00:27:41,019 --> 00:27:46,569
which can basically take advantage of

630
00:27:43,990 --> 00:27:48,910
these pre computed applications we can

631
00:27:46,569 --> 00:27:50,559
just very quickly train when all the

632
00:27:48,910 --> 00:27:52,740
details there shortly we can very

633
00:27:50,559 --> 00:27:54,558
quickly train a simple linear model

634
00:27:52,740 --> 00:27:55,878
based on those

635
00:27:54,558 --> 00:27:58,759
and so that's what happens when we say

636
00:27:55,878 --> 00:28:01,278
pre-compute equals true and that's why

637
00:27:58,759 --> 00:28:05,389
you may have noticed this week the first

638
00:28:01,278 --> 00:28:08,509
time that you run a model a new model it

639
00:28:05,388 --> 00:28:10,128
takes a minute or two where else you saw

640
00:28:08,509 --> 00:28:12,139
when I ran it it took like five or ten

641
00:28:10,128 --> 00:28:14,288
seconds took you a minute or two and

642
00:28:12,138 --> 00:28:16,668
that's because it had to pre-compute

643
00:28:14,288 --> 00:28:19,339
these activations and just has to do

644
00:28:16,669 --> 00:28:21,350
that once if you're using like your own

645
00:28:19,339 --> 00:28:24,980
computer or AWS it just has to do it

646
00:28:21,349 --> 00:28:28,368
once ever if you're using Kressel it

647
00:28:24,980 --> 00:28:30,919
actually has to do it once every single

648
00:28:28,368 --> 00:28:33,740
time you rerun press all because press

649
00:28:30,919 --> 00:28:35,509
or uses are just for these pre computed

650
00:28:33,740 --> 00:28:37,730
activations it uses a special that all

651
00:28:35,509 --> 00:28:39,740
had a scratch space that disappears each

652
00:28:37,730 --> 00:28:42,440
time you restart your press or instance

653
00:28:39,740 --> 00:28:44,028
so other than the special case of cresol

654
00:28:42,440 --> 00:28:44,570
generally speak he does have to run at

655
00:28:44,028 --> 00:28:50,628
once

656
00:28:44,569 --> 00:28:53,058
ever for a data set okay so the issue

657
00:28:50,628 --> 00:28:56,148
with that is that since we pre computed

658
00:28:53,058 --> 00:28:57,888
for each image you know how much does it

659
00:28:56,148 --> 00:29:00,859
have an EI here and how much does it

660
00:28:57,888 --> 00:29:02,839
have a lizard's eyeball there and so

661
00:29:00,859 --> 00:29:04,939
forth that means that data augmentations

662
00:29:02,839 --> 00:29:05,839
don't work right in other words even

663
00:29:04,940 --> 00:29:07,548
though we're trying to show at a

664
00:29:05,839 --> 00:29:09,949
different version of the cat each time

665
00:29:07,548 --> 00:29:13,368
we've pre computed the activations for a

666
00:29:09,950 --> 00:29:15,860
particular version of that cat so in

667
00:29:13,368 --> 00:29:17,628
order to use data augmentation we just

668
00:29:15,859 --> 00:29:20,748
have to go and learn pre compute equals

669
00:29:17,628 --> 00:29:25,428
false okay and then we can run a few

670
00:29:20,749 --> 00:29:29,149
more APIs right and so you can see here

671
00:29:25,429 --> 00:29:31,580
that as we run more a Potts the accuracy

672
00:29:29,148 --> 00:29:32,148
isn't particularly getting better that's

673
00:29:31,579 --> 00:29:35,778
the bad news

674
00:29:32,148 --> 00:29:39,378
the good news is that you can see that

675
00:29:35,778 --> 00:29:40,849
the train loss practices like the way of

676
00:29:39,378 --> 00:29:42,888
measuring the error of this model

677
00:29:40,849 --> 00:29:45,378
although that's getting better the

678
00:29:42,888 --> 00:29:48,798
errors going down the validation error

679
00:29:45,378 --> 00:29:50,928
isn't going down and but we're not

680
00:29:48,798 --> 00:29:53,898
overfitting and overfitting would mean

681
00:29:50,929 --> 00:29:56,360
that the training loss is much lower

682
00:29:53,898 --> 00:29:57,739
than the validation loss and we're going

683
00:29:56,359 --> 00:30:00,079
to talk about that a lot during this

684
00:29:57,740 --> 00:30:02,720
course but the general idea here is if

685
00:30:00,079 --> 00:30:04,398
you're doing much better job on the

686
00:30:02,720 --> 00:30:06,440
training set then you are on the

687
00:30:04,398 --> 00:30:07,559
validation set that means your models

688
00:30:06,440 --> 00:30:10,250
not generalize

689
00:30:07,559 --> 00:30:14,759
so we're not at that point which is good

690
00:30:10,250 --> 00:30:15,930
but we're not really improving so we're

691
00:30:14,759 --> 00:30:19,170
going to have to figure out how to deal

692
00:30:15,930 --> 00:30:21,740
with that before we do I want to show

693
00:30:19,170 --> 00:30:25,289
you one other cool trick I've added here

694
00:30:21,740 --> 00:30:29,009
cycle length equals one and this is

695
00:30:25,289 --> 00:30:30,089
another really interesting idea here's

696
00:30:29,009 --> 00:30:33,750
the basic idea

697
00:30:30,089 --> 00:30:35,849
cycle length equals one enables a recent

698
00:30:33,750 --> 00:30:37,079
fairly recent discovery and deep

699
00:30:35,849 --> 00:30:40,230
learning called stochastic gradient

700
00:30:37,079 --> 00:30:45,089
descent with restarts and the basic idea

701
00:30:40,230 --> 00:30:49,019
is this as you as you get closer and

702
00:30:45,089 --> 00:30:52,829
closer as you get closer and closer to

703
00:30:49,019 --> 00:30:55,109
the right spot right now getting closer

704
00:30:52,829 --> 00:30:57,389
and closer I may want to start to

705
00:30:55,109 --> 00:30:59,219
decrease my learning rate right because

706
00:30:57,390 --> 00:31:01,500
as I get closer I'm kind of like oh I'm

707
00:30:59,220 --> 00:31:04,620
pretty close down so let's let's slow

708
00:31:01,500 --> 00:31:08,250
down my steps to try to get executive

709
00:31:04,619 --> 00:31:11,659
the right spot right and so as we do

710
00:31:08,250 --> 00:31:11,660
more iterations

711
00:31:12,430 --> 00:31:19,360
our learning rate perhaps should

712
00:31:15,400 --> 00:31:20,980
actually go down right because as we go

713
00:31:19,359 --> 00:31:22,119
along we're getting closer and closer to

714
00:31:20,980 --> 00:31:25,599
where we want to be and we want to like

715
00:31:22,119 --> 00:31:27,969
get exactly to the right spot okay so

716
00:31:25,599 --> 00:31:29,529
the idea of decreasing the learning rate

717
00:31:27,970 --> 00:31:33,430
as you train is called

718
00:31:29,529 --> 00:31:36,089
learning rate annealing and it's it's

719
00:31:33,430 --> 00:31:39,420
very very common very very popular

720
00:31:36,089 --> 00:31:41,829
everybody uses it basically all the time

721
00:31:39,420 --> 00:31:45,100
the most common kind of learning rate

722
00:31:41,829 --> 00:31:47,289
annealing is really horrendously hacky

723
00:31:45,099 --> 00:31:49,599
it's basically that researchers like

724
00:31:47,289 --> 00:31:51,159
pick a learning rate that seems to work

725
00:31:49,599 --> 00:31:53,349
for a while and then when it stops

726
00:31:51,160 --> 00:31:55,360
learning well they drop it down by about

727
00:31:53,349 --> 00:31:56,649
10 times and then they keep learning a

728
00:31:55,359 --> 00:31:58,089
bit more until it doesn't seem to be

729
00:31:56,650 --> 00:32:00,220
improving and they drop it down by

730
00:31:58,089 --> 00:32:02,470
another ten times that's what most

731
00:32:00,220 --> 00:32:04,569
academic research papers and most people

732
00:32:02,470 --> 00:32:07,509
in industry do so this would be like

733
00:32:04,569 --> 00:32:11,859
stepwise annealing very manual very

734
00:32:07,509 --> 00:32:14,890
annoying a better approach is simply to

735
00:32:11,859 --> 00:32:17,469
pick some kind of functional form like a

736
00:32:14,890 --> 00:32:19,809
line it turns out that a really good

737
00:32:17,470 --> 00:32:22,279
functional form is one half of the

738
00:32:19,808 --> 00:32:25,789
cosine curve

739
00:32:22,279 --> 00:32:27,769
right and the reason why is that for a

740
00:32:25,789 --> 00:32:29,269
while when you're not very close you

741
00:32:27,769 --> 00:32:31,190
kind of have a really high learning rate

742
00:32:29,269 --> 00:32:33,289
and that is you do get close you kind of

743
00:32:31,190 --> 00:32:35,390
quickly drop down and do a few

744
00:32:33,289 --> 00:32:37,430
iterations with a really low learning

745
00:32:35,390 --> 00:32:40,160
rate and so this is called cosine

746
00:32:37,430 --> 00:32:41,480
annealing so to those of you who haven't

747
00:32:40,160 --> 00:32:44,210
done trigonometry for a while

748
00:32:41,480 --> 00:32:47,480
cosine basically looks something like

749
00:32:44,210 --> 00:32:52,279
this right so we've picked one little

750
00:32:47,480 --> 00:32:56,230
half piece okay so we're going to use

751
00:32:52,279 --> 00:33:00,710
cosine annealing but here's the thing

752
00:32:56,230 --> 00:33:02,660
when you're in a very high dimensional

753
00:33:00,710 --> 00:33:05,090
space right near we're only able to show

754
00:33:02,660 --> 00:33:06,500
three dimensions right but in reality

755
00:33:05,089 --> 00:33:10,929
we've got hundreds of millions of

756
00:33:06,500 --> 00:33:13,460
dimensions we've got lots of different

757
00:33:10,930 --> 00:33:14,990
fairly flat points there no not the

758
00:33:13,460 --> 00:33:17,480
actual local minima but they're fairly

759
00:33:14,990 --> 00:33:20,359
flat points all of which are pretty good

760
00:33:17,480 --> 00:33:22,519
right but they might differ in a really

761
00:33:20,359 --> 00:33:27,579
interesting way which is that some of

762
00:33:22,519 --> 00:33:27,579
those flat points let me show you

763
00:33:32,369 --> 00:33:37,949
let's imagine we've got a surface that

764
00:33:35,170 --> 00:33:37,950
looks something like this

765
00:33:40,359 --> 00:33:47,439
right now imagine that where you kind of

766
00:33:43,890 --> 00:33:49,419
random guest started here and our

767
00:33:47,440 --> 00:33:51,778
initial therefore kind of learning rate

768
00:33:49,419 --> 00:33:55,720
annealing schedule got us down to here

769
00:33:51,778 --> 00:33:57,849
now indeed that's a pretty nice low

770
00:33:55,720 --> 00:34:00,190
error right but it probably doesn't

771
00:33:57,849 --> 00:34:02,379
generalize very well which is to say if

772
00:34:00,190 --> 00:34:04,750
we use a different data set where things

773
00:34:02,380 --> 00:34:07,149
are just kind of slightly different in

774
00:34:04,750 --> 00:34:10,358
one of these directions suddenly is a

775
00:34:07,148 --> 00:34:13,898
terrible solution right where else over

776
00:34:10,358 --> 00:34:16,179
here is basically equally good in terms

777
00:34:13,898 --> 00:34:18,039
of loss right but it rather suggests

778
00:34:16,179 --> 00:34:19,750
that if you move if you have slightly

779
00:34:18,039 --> 00:34:21,668
different data sets that are slightly

780
00:34:19,750 --> 00:34:23,409
moved in different directions it's still

781
00:34:21,668 --> 00:34:25,418
going to be good right so in other words

782
00:34:23,409 --> 00:34:29,619
we would expect this solution here is

783
00:34:25,418 --> 00:34:33,158
probably going to generalize better than

784
00:34:29,619 --> 00:34:35,379
this by key one so here's what we do is

785
00:34:33,159 --> 00:34:39,700
we've got like a bunch of different low

786
00:34:35,378 --> 00:34:41,648
bits right then our standard loading

787
00:34:39,699 --> 00:34:43,719
rate annealing approach will start of go

788
00:34:41,648 --> 00:34:47,289
down here or downhill downhill downhill

789
00:34:43,719 --> 00:34:49,779
downhill to one spot right but what we

790
00:34:47,289 --> 00:34:54,219
could do instead is use a learning rate

791
00:34:49,780 --> 00:34:56,349
schedule that looks like this which is

792
00:34:54,219 --> 00:34:58,029
to say we do a cosign annealing and then

793
00:34:56,349 --> 00:34:59,530
suddenly jump up again into a cosign

794
00:34:58,030 --> 00:35:02,440
annealing and then jump up again

795
00:34:59,530 --> 00:35:04,210
and so each time we jump up it means

796
00:35:02,440 --> 00:35:06,039
that if they're going to spiky bit and

797
00:35:04,210 --> 00:35:08,320
then we subtly increase the learning

798
00:35:06,039 --> 00:35:10,690
rate and it jumps now all the way over

799
00:35:08,320 --> 00:35:11,890
to here and so then we kind of learning

800
00:35:10,690 --> 00:35:13,659
right in your learning right near death

801
00:35:11,889 --> 00:35:16,690
down to here and then we jump up again

802
00:35:13,659 --> 00:35:19,299
to a high learning rate oh and it stays

803
00:35:16,690 --> 00:35:21,639
here right so in other words each time

804
00:35:19,300 --> 00:35:24,160
we jump up the learning rate that means

805
00:35:21,639 --> 00:35:26,049
that if it's in a nasty spiky part of

806
00:35:24,159 --> 00:35:28,149
the surface it's going to hop out of the

807
00:35:26,050 --> 00:35:30,580
spiky part and hopefully if we do that

808
00:35:28,150 --> 00:35:34,500
enough times it'll eventually find a

809
00:35:30,579 --> 00:35:34,500
nice smooth Bowl

810
00:35:40,070 --> 00:35:44,730
could you get the same effect by running

811
00:35:42,989 --> 00:35:46,259
multiple iterations through the

812
00:35:44,730 --> 00:35:48,990
different ground of my starting point so

813
00:35:46,260 --> 00:35:52,560
that eventually you explore all possible

814
00:35:48,989 --> 00:35:58,729
minimize yeah so in fact that that's a

815
00:35:52,559 --> 00:35:58,730
great question and before this approach

816
00:35:59,630 --> 00:36:05,240
which is called stochastic gradient

817
00:36:01,829 --> 00:36:07,469
descent with restarts was was created

818
00:36:05,239 --> 00:36:08,669
that's exactly what people used to do

819
00:36:07,469 --> 00:36:11,809
they used to create these things called

820
00:36:08,670 --> 00:36:15,480
ensembles where they would basically

821
00:36:11,809 --> 00:36:17,909
relearn a whole new model ten times in

822
00:36:15,480 --> 00:36:22,500
the hope that one of them's like but it

823
00:36:17,909 --> 00:36:24,089
ended up being better and so the cool

824
00:36:22,500 --> 00:36:26,099
thing about this decosta gradient

825
00:36:24,090 --> 00:36:29,010
descent with restarts is that the model

826
00:36:26,099 --> 00:36:30,809
once we're in a reasonably good spot

827
00:36:29,010 --> 00:36:34,110
each time we jump up the learning rate

828
00:36:30,809 --> 00:36:36,299
it doesn't restart it actually hangs out

829
00:36:34,110 --> 00:36:37,920
in this nice part part of the space and

830
00:36:36,300 --> 00:36:39,420
then keeps getting better so

831
00:36:37,920 --> 00:36:42,690
interestingly it turns out that this

832
00:36:39,420 --> 00:36:45,960
approach where we do this a bunch of

833
00:36:42,690 --> 00:36:49,260
separate cosine annealing steps we end

834
00:36:45,960 --> 00:36:50,940
up with a better result as then if we

835
00:36:49,260 --> 00:36:54,300
just randomly try it a few different

836
00:36:50,940 --> 00:36:58,200
starting points so it's a super neat

837
00:36:54,300 --> 00:37:00,930
trick and it's a fairly recent

838
00:36:58,199 --> 00:37:04,609
development but and again almost

839
00:37:00,929 --> 00:37:07,739
nobody's heard of it but I found like

840
00:37:04,610 --> 00:37:10,110
it's now like my superpower like using

841
00:37:07,739 --> 00:37:14,489
this along with the learning rate finder

842
00:37:10,110 --> 00:37:16,050
like I can get better results than

843
00:37:14,489 --> 00:37:18,179
nearly anybody like in a casual

844
00:37:16,050 --> 00:37:20,789
competition you know in the first week

845
00:37:18,179 --> 00:37:23,129
or two I can like jump in it's been an

846
00:37:20,789 --> 00:37:26,070
arrow to and back I've got a

847
00:37:23,130 --> 00:37:29,550
fantastically good result and so this is

848
00:37:26,070 --> 00:37:31,650
why I didn't pick the point where it's

849
00:37:29,550 --> 00:37:33,480
got the steepest slope I actually trying

850
00:37:31,650 --> 00:37:35,490
to pick something kind of aggressively

851
00:37:33,480 --> 00:37:36,990
high it's still getting down but maybe

852
00:37:35,489 --> 00:37:39,029
like getting to the point where it's

853
00:37:36,989 --> 00:37:40,589
nearly too high not because I want to

854
00:37:39,030 --> 00:37:43,650
make sure because that's because when we

855
00:37:40,590 --> 00:37:46,340
do this stochastic gradient descent with

856
00:37:43,650 --> 00:37:48,090
restarts this ten to the negative two

857
00:37:46,340 --> 00:37:51,780
represents the

858
00:37:48,090 --> 00:37:54,120
a highest number that it uses so it goes

859
00:37:51,780 --> 00:37:56,130
up to ten to the negative two and then

860
00:37:54,119 --> 00:37:58,829
goes down and then up to ten negative

861
00:37:56,130 --> 00:38:01,349
two and down so if I use to lower

862
00:37:58,829 --> 00:38:06,509
learning rate it's not going to jump to

863
00:38:01,349 --> 00:38:08,369
a different part of the function so I

864
00:38:06,510 --> 00:38:10,290
have a few questions but the first one

865
00:38:08,369 --> 00:38:13,380
is how many times do you change your

866
00:38:10,289 --> 00:38:15,360
learning rate you want to work we don't

867
00:38:13,380 --> 00:38:17,460
change the learning rate all three how

868
00:38:15,360 --> 00:38:19,019
many times - okay so in terms of this

869
00:38:17,460 --> 00:38:20,789
part here where it's going down we

870
00:38:19,019 --> 00:38:23,730
change the learning rate every single

871
00:38:20,789 --> 00:38:27,690
mini - all right and then the number of

872
00:38:23,730 --> 00:38:30,690
times we reset it is set by the cycle

873
00:38:27,690 --> 00:38:33,840
length parameter and so 1 means reset it

874
00:38:30,690 --> 00:38:36,090
up to every epoch so if I had to there

875
00:38:33,840 --> 00:38:38,280
it would reset it up to every to epochs

876
00:38:36,090 --> 00:38:39,780
and interestingly this this point that

877
00:38:38,280 --> 00:38:41,070
when we do the learning rate and

878
00:38:39,780 --> 00:38:43,880
kneeling that we actually change it

879
00:38:41,070 --> 00:38:47,190
every single batch it turns out to be

880
00:38:43,880 --> 00:38:48,900
really critical to making this work and

881
00:38:47,190 --> 00:38:50,639
it again is very different to what

882
00:38:48,900 --> 00:38:52,970
nearly everybody in industry in academia

883
00:38:50,639 --> 00:38:54,989
has done before

884
00:38:52,969 --> 00:38:58,500
what do you get a chance could you

885
00:38:54,989 --> 00:39:02,189
explain recompute it was true because

886
00:38:58,500 --> 00:39:04,769
it's still yeah we're going to come back

887
00:39:02,190 --> 00:39:06,329
to that multiple times in this course so

888
00:39:04,769 --> 00:39:07,380
the way this course has been a work is

889
00:39:06,329 --> 00:39:09,900
we're going to like do a really

890
00:39:07,380 --> 00:39:11,400
high-level version of each thing and

891
00:39:09,900 --> 00:39:12,930
then we're going to like come back to it

892
00:39:11,400 --> 00:39:14,220
in two or three lessons and then come

893
00:39:12,929 --> 00:39:15,839
back to it at the end of the course and

894
00:39:14,219 --> 00:39:18,239
each time we're going to see like more

895
00:39:15,840 --> 00:39:20,519
of the math more of the code and get a

896
00:39:18,239 --> 00:39:25,339
deeper view okay and we can talk about

897
00:39:20,519 --> 00:39:25,340
it also in the forums during the week

898
00:39:26,539 --> 00:39:31,969
our main goal is to generalize and we

899
00:39:29,579 --> 00:39:34,590
don't want to get those like narrow

900
00:39:31,969 --> 00:39:37,709
demas yeah that's a it's a very short

901
00:39:34,590 --> 00:39:40,760
summary this method are we keeping track

902
00:39:37,710 --> 00:39:44,010
off to minimize and averaging them ah

903
00:39:40,760 --> 00:39:45,750
that's that's another level of

904
00:39:44,010 --> 00:39:47,280
sophistication and indeed you can see

905
00:39:45,750 --> 00:39:50,039
there's something here called snapshot

906
00:39:47,280 --> 00:39:53,190
ensemble so we're not doing it in the

907
00:39:50,039 --> 00:39:55,409
code right now but yes if you wanted to

908
00:39:53,190 --> 00:39:58,590
make us generalize even better you can

909
00:39:55,409 --> 00:40:00,980
save the weights here and here and here

910
00:39:58,590 --> 00:40:03,650
and then take the average

911
00:40:00,980 --> 00:40:10,940
ishes but for now we're just going to

912
00:40:03,650 --> 00:40:16,309
pick the last one if you want to skip

913
00:40:10,940 --> 00:40:19,970
ahead if you want to skip ahead there's

914
00:40:16,309 --> 00:40:21,529
a parameter called cycle safe name which

915
00:40:19,969 --> 00:40:23,629
you can add as well as cycle them and

916
00:40:21,530 --> 00:40:26,359
that will save a set of weights at the

917
00:40:23,630 --> 00:40:35,150
end of every learning rate cycle and

918
00:40:26,358 --> 00:40:38,358
then you can ensemble them ok so we've

919
00:40:35,150 --> 00:40:41,240
got a pretty decent model here ninety

920
00:40:38,358 --> 00:40:42,858
nine point three percent accuracy and

921
00:40:41,239 --> 00:40:44,719
we've gone through of you know a few

922
00:40:42,858 --> 00:40:47,329
steps that is taken you know a minute or

923
00:40:44,719 --> 00:40:49,009
two to run and so from time to time I

924
00:40:47,329 --> 00:40:51,259
tend to save my weight so if you go

925
00:40:49,010 --> 00:40:53,510
learn dot save and then pass in a file

926
00:40:51,260 --> 00:40:55,670
name it's going to go ahead and save

927
00:40:53,510 --> 00:40:57,890
that for you later on if you go learn

928
00:40:55,670 --> 00:41:00,530
load you'll be straight back to where

929
00:40:57,889 --> 00:41:03,349
you came from okay so it's a good idea

930
00:41:00,530 --> 00:41:06,410
to do that from time to time this is a

931
00:41:03,349 --> 00:41:10,039
good time to mention what happens when

932
00:41:06,409 --> 00:41:12,379
you do this when you go learn dot save

933
00:41:10,039 --> 00:41:14,269
when you create precomputed activations

934
00:41:12,380 --> 00:41:16,760
another thing we learn about soon when

935
00:41:14,269 --> 00:41:20,090
you create resized images these are all

936
00:41:16,760 --> 00:41:28,070
creating various temporary files okay

937
00:41:20,090 --> 00:41:34,579
and so what happens is if we go to data

938
00:41:28,070 --> 00:41:36,710
and we go to dogs cats this is my data

939
00:41:34,579 --> 00:41:41,029
folder and you'll see there's a folder

940
00:41:36,710 --> 00:41:43,608
here called TMP or - and so this is

941
00:41:41,030 --> 00:41:46,430
automatically created and all of my pre

942
00:41:43,608 --> 00:41:50,420
computed activations end up in here I

943
00:41:46,429 --> 00:41:52,549
mention this because if if things are if

944
00:41:50,420 --> 00:41:54,019
you're getting weird errors that might

945
00:41:52,550 --> 00:41:56,090
be because you've got some Oh pre

946
00:41:54,019 --> 00:41:59,090
computed activations like we're only

947
00:41:56,090 --> 00:42:01,130
half completed or are in some way

948
00:41:59,090 --> 00:42:03,079
incompatible with what you're doing so

949
00:42:01,130 --> 00:42:05,990
you can always go ahead and just delete

950
00:42:03,079 --> 00:42:08,090
this TMP this temporary directory and

951
00:42:05,989 --> 00:42:10,819
see if that causes your error to go away

952
00:42:08,090 --> 00:42:12,920
this is the faster I equivalent of

953
00:42:10,820 --> 00:42:14,990
turning it off and then on again

954
00:42:12,920 --> 00:42:16,730
you'll also see there's a directory

955
00:42:14,989 --> 00:42:19,159
called models and that's where all of

956
00:42:16,730 --> 00:42:22,519
these when you say dot save with a model

957
00:42:19,159 --> 00:42:23,899
that's where that's going to go actually

958
00:42:22,519 --> 00:42:25,610
it reminds me when the stochastic

959
00:42:23,900 --> 00:42:27,800
gradient descent with restarts paper

960
00:42:25,610 --> 00:42:29,150
came out I saw a tweet that was somebody

961
00:42:27,800 --> 00:42:30,620
was like Oh to make your deep learning

962
00:42:29,150 --> 00:42:38,180
work better turn it off and then on

963
00:42:30,619 --> 00:42:40,099
again question it so if I want to see I

964
00:42:38,179 --> 00:42:43,399
want to retrain my model fuselage again

965
00:42:40,099 --> 00:42:51,230
do I just do everything the 10 folder if

966
00:42:43,400 --> 00:42:54,260
you want if you want to train your model

967
00:42:51,230 --> 00:42:56,570
from scratch there's generally no reason

968
00:42:54,260 --> 00:42:59,260
to delete the pre computed activations

969
00:42:56,570 --> 00:43:02,480
because the pre computed activations are

970
00:42:59,260 --> 00:43:06,410
without any training that's what the pre

971
00:43:02,480 --> 00:43:07,579
trained model created with the with the

972
00:43:06,409 --> 00:43:12,440
weights that you downloaded off the

973
00:43:07,579 --> 00:43:14,119
internet the only yeah I mean the only

974
00:43:12,440 --> 00:43:15,769
reason you want to delete the pre

975
00:43:14,119 --> 00:43:18,409
computed activations is that there was

976
00:43:15,769 --> 00:43:20,809
some error caused by like half creating

977
00:43:18,409 --> 00:43:23,569
them and crashing or some something like

978
00:43:20,809 --> 00:43:25,400
that as you change the size of your

979
00:43:23,570 --> 00:43:26,870
import change different architectures

980
00:43:25,400 --> 00:43:28,490
and so forth they all create different

981
00:43:26,869 --> 00:43:30,319
sets of activations with different file

982
00:43:28,489 --> 00:43:32,539
names so you don't generally you

983
00:43:30,320 --> 00:43:33,680
shouldn't have to worry about it if you

984
00:43:32,539 --> 00:43:35,960
want to start training again from

985
00:43:33,679 --> 00:43:40,250
scratch all you have to do is create a

986
00:43:35,960 --> 00:43:42,530
new learn object so each time you go

987
00:43:40,250 --> 00:43:45,619
like conch learner dot pre-trained that

988
00:43:42,530 --> 00:43:50,960
creates a new object with with new sets

989
00:43:45,619 --> 00:43:52,849
of weights fever train from okay so

990
00:43:50,960 --> 00:43:56,769
before our break we'll finish off by

991
00:43:52,849 --> 00:44:00,679
talking about about fine-tuning and

992
00:43:56,769 --> 00:44:05,780
differential learning rates and so so

993
00:44:00,679 --> 00:44:08,179
far everything we've done has not

994
00:44:05,780 --> 00:44:09,950
changed any of these free trained

995
00:44:08,179 --> 00:44:12,109
filters right so we've used a pre

996
00:44:09,949 --> 00:44:17,509
trained model that already knows how to

997
00:44:12,110 --> 00:44:21,400
find at the early stages edges

998
00:44:17,510 --> 00:44:26,090
ingredients and then corners and curves

999
00:44:21,400 --> 00:44:26,660
and then repeating patterns and bits of

1000
00:44:26,090 --> 00:44:30,559
text

1001
00:44:26,659 --> 00:44:35,868
and eventually eyeballs right we have

1002
00:44:30,559 --> 00:44:38,569
not retrained any of those activations

1003
00:44:35,869 --> 00:44:40,640
any of those features well specifically

1004
00:44:38,568 --> 00:44:43,190
any of those weights in the

1005
00:44:40,639 --> 00:44:46,548
convolutional kernels all we've done is

1006
00:44:43,190 --> 00:44:48,200
we've learnt some new layers that we've

1007
00:44:46,548 --> 00:44:50,119
added on top of these things we've

1008
00:44:48,199 --> 00:44:54,528
learned how to mix and match these

1009
00:44:50,119 --> 00:44:58,460
pre-trained features now obviously it

1010
00:44:54,528 --> 00:45:00,679
may turn out that your pictures have you

1011
00:44:58,460 --> 00:45:03,380
know different kinds of eyeballs or

1012
00:45:00,679 --> 00:45:05,719
faces or if you're using different kinds

1013
00:45:03,380 --> 00:45:07,068
of images like satellite images totally

1014
00:45:05,719 --> 00:45:10,068
different kinds of features altogether

1015
00:45:07,068 --> 00:45:12,650
right so if you're like training to

1016
00:45:10,068 --> 00:45:15,469
recognize icebergs you'll probably want

1017
00:45:12,650 --> 00:45:16,670
to go all the way back and learn you

1018
00:45:15,469 --> 00:45:18,649
know all the way back to kind of

1019
00:45:16,670 --> 00:45:22,430
different combinations of these simple

1020
00:45:18,650 --> 00:45:24,829
gradients and edges in our cases dogs

1021
00:45:22,429 --> 00:45:26,719
vs. cats we're going to have some minor

1022
00:45:24,829 --> 00:45:29,740
differences but we still may find it's

1023
00:45:26,719 --> 00:45:34,068
helpful to slightly tune some of these

1024
00:45:29,739 --> 00:45:36,199
later layers as well so to tell the

1025
00:45:34,068 --> 00:45:38,750
learner that we now want to start

1026
00:45:36,199 --> 00:45:41,389
actually changing the convolutional

1027
00:45:38,750 --> 00:45:44,539
filters themselves we simply say

1028
00:45:41,389 --> 00:45:46,429
unfreeze okay so a frozen layer is a

1029
00:45:44,539 --> 00:45:49,640
layer which is not trained is not

1030
00:45:46,429 --> 00:45:51,949
updated okay so unfreeze unfreezes all

1031
00:45:49,639 --> 00:45:56,920
of the layers now when you think about

1032
00:45:51,949 --> 00:45:59,808
it it's pretty obvious that layer one

1033
00:45:56,920 --> 00:46:02,509
right which is like a diagonal edge or a

1034
00:45:59,809 --> 00:46:05,089
gradient probably doesn't need to change

1035
00:46:02,509 --> 00:46:06,920
by much if at all right from the 1 and a

1036
00:46:05,088 --> 00:46:08,298
half million images on image net it

1037
00:46:06,920 --> 00:46:10,849
probably already is figured out pretty

1038
00:46:08,298 --> 00:46:13,099
well how to find like edges of gradients

1039
00:46:10,849 --> 00:46:15,318
it probably already knows also like

1040
00:46:13,099 --> 00:46:17,390
which kind of corners to look for and

1041
00:46:15,318 --> 00:46:19,759
how to find which kinds of curves and so

1042
00:46:17,389 --> 00:46:22,449
forth so in other words these early

1043
00:46:19,759 --> 00:46:26,269
layers probably need little if any

1044
00:46:22,449 --> 00:46:28,548
learning where else these later ones are

1045
00:46:26,268 --> 00:46:30,649
much more likely to need more learning

1046
00:46:28,548 --> 00:46:32,869
and this is universally true regardless

1047
00:46:30,650 --> 00:46:34,940
of whether you're looking for satellite

1048
00:46:32,869 --> 00:46:36,200
images of rainforests or icebergs or

1049
00:46:34,940 --> 00:46:38,809
whether you're looking for cats versus

1050
00:46:36,199 --> 00:46:41,808
dogs right

1051
00:46:38,809 --> 00:46:45,109
so what we do is we create an array of

1052
00:46:41,809 --> 00:46:47,869
learning rates where we say okay these

1053
00:46:45,108 --> 00:46:50,418
are the learning rates to use for our

1054
00:46:47,869 --> 00:46:53,239
additional layers that we've added on

1055
00:46:50,418 --> 00:46:56,658
top these are the learning rates to use

1056
00:46:53,239 --> 00:46:58,759
in the middle few layers and these are

1057
00:46:56,659 --> 00:47:00,798
the learning rates to use for the first

1058
00:46:58,759 --> 00:47:02,568
few layers so these are the ones for the

1059
00:47:00,798 --> 00:47:05,568
layers that represent like very basic

1060
00:47:02,568 --> 00:47:08,719
geometric features these are the ones

1061
00:47:05,568 --> 00:47:11,509
that are used to for the more complex

1062
00:47:08,719 --> 00:47:13,369
kind of sophisticated convolutional

1063
00:47:11,509 --> 00:47:14,688
features and these are the ones that are

1064
00:47:13,369 --> 00:47:17,539
used for the features that we've added

1065
00:47:14,688 --> 00:47:19,899
and went from stretch right so you can

1066
00:47:17,539 --> 00:47:22,729
create a array of learning rates and

1067
00:47:19,900 --> 00:47:24,409
then when we called up fit and pass an

1068
00:47:22,728 --> 00:47:25,728
array of learning rates it's now going

1069
00:47:24,409 --> 00:47:30,489
to use those different learning rates

1070
00:47:25,728 --> 00:47:34,759
for different parts of the model this is

1071
00:47:30,489 --> 00:47:37,429
not something that we've like invented

1072
00:47:34,759 --> 00:47:39,650
but I'd also say it's like it's so not

1073
00:47:37,429 --> 00:47:42,499
that common that it doesn't even have a

1074
00:47:39,650 --> 00:47:46,519
name as far as I know so we're going to

1075
00:47:42,498 --> 00:47:48,379
call it differential learning rates if

1076
00:47:46,518 --> 00:47:49,518
it actually has a name or indeed if

1077
00:47:48,380 --> 00:47:51,769
somebody's actually written a paper

1078
00:47:49,518 --> 00:47:54,828
specifically talking about it I don't

1079
00:47:51,768 --> 00:47:56,808
know there's a great researcher called

1080
00:47:54,829 --> 00:47:58,429
Jason your Sinskey who who did write a

1081
00:47:56,809 --> 00:47:59,719
paper about the kind of the idea that

1082
00:47:58,429 --> 00:48:02,479
you might want different learning rates

1083
00:47:59,719 --> 00:48:05,208
and showing why but I don't think any

1084
00:48:02,478 --> 00:48:07,578
other libraries support it and yeah I

1085
00:48:05,208 --> 00:48:10,608
don't know of a name for it having said

1086
00:48:07,579 --> 00:48:12,919
that though this ability to like

1087
00:48:10,608 --> 00:48:14,958
unfreeze and then use these differential

1088
00:48:12,918 --> 00:48:17,929
learning rates I found it's like the

1089
00:48:14,958 --> 00:48:25,908
secret to taking a pretty good model and

1090
00:48:17,929 --> 00:48:29,449
putting it into an awesome model so just

1091
00:48:25,909 --> 00:48:32,358
to clarify so you have three numbers

1092
00:48:29,449 --> 00:48:36,139
there okay three hyper parameters the

1093
00:48:32,358 --> 00:48:41,199
first one is the photo late model so the

1094
00:48:36,139 --> 00:48:44,208
mall that are late layers the so with

1095
00:48:41,199 --> 00:48:46,309
the it's your answer is many many right

1096
00:48:44,208 --> 00:48:47,448
and they're kind of in groups and we're

1097
00:48:46,309 --> 00:48:49,159
going to learn about the architecture

1098
00:48:47,449 --> 00:48:50,130
this is called a ResNet for residual

1099
00:48:49,159 --> 00:48:53,190
network

1100
00:48:50,130 --> 00:48:54,778
it kind of has ResNet blocks and so what

1101
00:48:53,190 --> 00:48:58,259
we're doing is we're grouping the blocks

1102
00:48:54,778 --> 00:49:00,018
into three groups and so this one is

1103
00:48:58,259 --> 00:49:04,588
actually this first number is for the

1104
00:49:00,018 --> 00:49:06,118
earliest layers yeah they're ones

1105
00:49:04,588 --> 00:49:09,058
closest to the pixels that represent

1106
00:49:06,119 --> 00:49:12,930
like corners and edges and gradients but

1107
00:49:09,059 --> 00:49:15,569
why why do you well I thought those

1108
00:49:12,929 --> 00:49:17,338
layers are frozen at first so yeah right

1109
00:49:15,568 --> 00:49:19,018
so we just said unfreeze the streets

1110
00:49:17,338 --> 00:49:20,460
also we so yeah I'm freezing them

1111
00:49:19,018 --> 00:49:24,058
because you have kind of partially

1112
00:49:20,460 --> 00:49:26,579
trained although lately we've trained

1113
00:49:24,059 --> 00:49:28,548
we've trained our added layers yes now

1114
00:49:26,579 --> 00:49:30,869
you are we training the Oh step exactly

1115
00:49:28,548 --> 00:49:33,329
obviously so it waits and the learning

1116
00:49:30,869 --> 00:49:34,829
rate is particularly small for the early

1117
00:49:33,329 --> 00:49:37,890
layers that's right because you just

1118
00:49:34,829 --> 00:49:39,210
find a want to find food yeah yeah we

1119
00:49:37,889 --> 00:49:42,568
probably don't want to change them at

1120
00:49:39,210 --> 00:49:50,159
all but you know if it does need to then

1121
00:49:42,568 --> 00:49:51,929
it can thanks no problem so using the

1122
00:49:50,159 --> 00:49:56,068
differential in rates a little different

1123
00:49:51,929 --> 00:49:58,108
from like grid search there's no

1124
00:49:56,068 --> 00:50:00,179
similarity to grid search so grid search

1125
00:49:58,108 --> 00:50:03,719
is where we're trying to find the best

1126
00:50:00,179 --> 00:50:07,219
hyper parameter for something so for

1127
00:50:03,719 --> 00:50:09,539
example you could kind of think of the

1128
00:50:07,219 --> 00:50:11,460
learning rate finder as a really

1129
00:50:09,539 --> 00:50:12,869
sophisticated grid search which is like

1130
00:50:11,460 --> 00:50:15,349
trying lots and lots of learning rates

1131
00:50:12,869 --> 00:50:16,490
to find which one is best

1132
00:50:15,349 --> 00:50:18,650
but this has nothing to do with that

1133
00:50:16,489 --> 00:50:20,989
this is actually for the entire training

1134
00:50:18,650 --> 00:50:25,180
from now on it's actually going to use a

1135
00:50:20,989 --> 00:50:25,179
different learning rate for each layer

1136
00:50:28,480 --> 00:50:35,630
and so I was wondering so you give a pre

1137
00:50:33,349 --> 00:50:39,230
train model then you have to use the

1138
00:50:35,630 --> 00:50:40,640
same input dimensions right because I

1139
00:50:39,230 --> 00:50:43,818
was thinking okay let's say you have

1140
00:50:40,639 --> 00:50:45,078
this big they use like big machines to

1141
00:50:43,818 --> 00:50:46,880
train these things and you want to take

1142
00:50:45,079 --> 00:50:48,500
advantage of it how would you go about

1143
00:50:46,880 --> 00:50:50,930
you know you have like images that are

1144
00:50:48,500 --> 00:50:52,789
like bigger than the ones that they used

1145
00:50:50,929 --> 00:50:54,949
or we're going to be talking about sizes

1146
00:50:52,789 --> 00:50:56,769
later but the short answer is that with

1147
00:50:54,949 --> 00:50:58,848
this library and the modern

1148
00:50:56,768 --> 00:51:04,818
architectures were using we can use any

1149
00:50:58,849 --> 00:51:08,119
size we like so did I mean do we need

1150
00:51:04,818 --> 00:51:09,650
can we at least just a specific layer we

1151
00:51:08,119 --> 00:51:12,108
can we're not doing it yet but if you

1152
00:51:09,650 --> 00:51:14,420
wanted to you can learn dot freeze

1153
00:51:12,108 --> 00:51:21,380
underscore two and pass into layer

1154
00:51:14,420 --> 00:51:23,838
number much to my surprise or at least

1155
00:51:21,380 --> 00:51:26,568
initial my surprise it turns out I

1156
00:51:23,838 --> 00:51:28,489
almost never need to do that I almost

1157
00:51:26,568 --> 00:51:29,568
never find it helpful and I think it's

1158
00:51:28,489 --> 00:51:33,439
because we're using differential

1159
00:51:29,568 --> 00:51:35,719
learning rates the the optimizer can

1160
00:51:33,440 --> 00:51:45,650
kind of learn just as much as it needs

1161
00:51:35,719 --> 00:51:49,278
to so yeah it's a little data like very

1162
00:51:45,650 --> 00:51:51,019
little data yeah it still doesn't seem

1163
00:51:49,278 --> 00:51:54,380
to help the one place I have found it

1164
00:51:51,018 --> 00:51:56,449
helpful is if I'm using like a really

1165
00:51:54,380 --> 00:52:01,430
big memory intensive model and I'm like

1166
00:51:56,449 --> 00:52:03,439
running out of GPU crazy having the the

1167
00:52:01,429 --> 00:52:05,210
less layers you unfreeze the less memory

1168
00:52:03,440 --> 00:52:07,278
it takes and the less time it takes so

1169
00:52:05,210 --> 00:52:09,619
there's that kind of practical aspect so

1170
00:52:07,278 --> 00:52:13,518
to me she'll say I asked the question

1171
00:52:09,619 --> 00:52:16,400
right can I just like unfreezes specific

1172
00:52:13,518 --> 00:52:21,768
layer no you you can only unfreeze

1173
00:52:16,400 --> 00:52:23,180
layers from layer n onwards you could

1174
00:52:21,768 --> 00:52:24,739
probably delve inside the library in

1175
00:52:23,179 --> 00:52:27,639
phase one phase one layer but I don't

1176
00:52:24,739 --> 00:52:27,639
know why you would

1177
00:52:28,088 --> 00:52:31,489
okay so I'm really excited to be showing

1178
00:52:30,199 --> 00:52:32,629
you guys this stuff because it's like

1179
00:52:31,489 --> 00:52:34,429
it's something we've been kind of

1180
00:52:32,630 --> 00:52:37,369
researching all year it's figuring out

1181
00:52:34,429 --> 00:52:39,618
how to train state of the art models and

1182
00:52:37,369 --> 00:52:41,930
we've kind of found these like tiny

1183
00:52:39,619 --> 00:52:45,410
number of tricks and so once we do that

1184
00:52:41,929 --> 00:52:46,879
we now go learn about fit right and you

1185
00:52:45,409 --> 00:52:51,279
can see look at this we get right up to

1186
00:52:46,880 --> 00:52:54,170
that 99.5% accuracy which is crazy

1187
00:52:51,280 --> 00:52:55,849
there's one other trick you might see

1188
00:52:54,170 --> 00:52:58,490
here that as well as using stochastic

1189
00:52:55,849 --> 00:53:00,800
gradient descent with restarts a cycle

1190
00:52:58,489 --> 00:53:04,069
length equals one we've done three

1191
00:53:00,800 --> 00:53:05,480
cycles so earlier on I lied to you I

1192
00:53:04,070 --> 00:53:07,130
said this is this is the number of

1193
00:53:05,480 --> 00:53:09,199
epochs it's actually the number of

1194
00:53:07,130 --> 00:53:11,420
cyclists right so if you said cycle

1195
00:53:09,199 --> 00:53:14,899
length equals two it would do three

1196
00:53:11,420 --> 00:53:17,240
cycles of each of two epochs or do six

1197
00:53:14,900 --> 00:53:19,250
because so here I've said two three

1198
00:53:17,239 --> 00:53:22,039
cycles yet somehow it's done seven

1199
00:53:19,250 --> 00:53:23,300
epochs and the reason why is I've got

1200
00:53:22,039 --> 00:53:26,179
one last trick to show you which is

1201
00:53:23,300 --> 00:53:27,859
cycle mult equals two and to tell you

1202
00:53:26,179 --> 00:53:30,739
what that does I'm simply going to draw

1203
00:53:27,858 --> 00:53:32,480
you a picture you the picture if I go

1204
00:53:30,739 --> 00:53:34,000
learn Dutch share top plot learning rate

1205
00:53:32,480 --> 00:53:36,440
there it is

1206
00:53:34,000 --> 00:53:40,130
now you can see what cycle mode equals

1207
00:53:36,440 --> 00:53:42,619
to is doing okay it's it's doubling the

1208
00:53:40,130 --> 00:53:44,630
length of the cycle after each cycle and

1209
00:53:42,619 --> 00:53:46,010
so in the paper that introduced this

1210
00:53:44,630 --> 00:53:46,760
stochastic gradient descent with

1211
00:53:46,010 --> 00:53:49,520
restarts

1212
00:53:46,760 --> 00:53:51,650
the researcher kind of said hey this is

1213
00:53:49,519 --> 00:53:53,630
something that seems to sometimes work

1214
00:53:51,650 --> 00:53:57,309
pretty well and I've certainly found

1215
00:53:53,630 --> 00:54:00,170
that often to be the case so basically

1216
00:53:57,309 --> 00:54:04,068
intuitively speaking if your cycle

1217
00:54:00,170 --> 00:54:06,108
length is too short right then it's kind

1218
00:54:04,068 --> 00:54:08,300
of starts going down to find a good spot

1219
00:54:06,108 --> 00:54:09,949
and then it pops out and it goes to a

1220
00:54:08,300 --> 00:54:11,300
try and photographs button pops out it

1221
00:54:09,949 --> 00:54:14,480
never actually gets to find a good spot

1222
00:54:11,300 --> 00:54:16,039
right so earlier on you want it to do

1223
00:54:14,480 --> 00:54:18,469
that because it's trying to find the bit

1224
00:54:16,039 --> 00:54:20,300
that's like smoother but then later on

1225
00:54:18,469 --> 00:54:22,909
you want it to fight do more exploring

1226
00:54:20,300 --> 00:54:26,300
and then more exploring right so that's

1227
00:54:22,909 --> 00:54:28,969
why this cycle mole equals two thing

1228
00:54:26,300 --> 00:54:32,660
often seems to be a pretty good approach

1229
00:54:28,969 --> 00:54:34,159
right so suddenly we're introducing more

1230
00:54:32,659 --> 00:54:36,348
and more hyper parameters having told

1231
00:54:34,159 --> 00:54:38,420
you that there aren't that many but

1232
00:54:36,349 --> 00:54:40,579
the reason is that like you can really

1233
00:54:38,420 --> 00:54:42,950
get away with just taking a good

1234
00:54:40,579 --> 00:54:47,869
learning rate but then adding these

1235
00:54:42,949 --> 00:54:50,689
extra tweaks really helps get that extra

1236
00:54:47,869 --> 00:54:55,579
level up without any effort right and so

1237
00:54:50,690 --> 00:54:59,690
in practice I find this kind of three

1238
00:54:55,579 --> 00:55:02,180
cycles starting at 1 mode equals 2 works

1239
00:54:59,690 --> 00:55:07,130
very very often to get a pretty decent

1240
00:55:02,179 --> 00:55:10,669
model if it does doesn't then often I'll

1241
00:55:07,130 --> 00:55:11,660
just do 3 cycles of length 2 with no

1242
00:55:10,670 --> 00:55:12,979
molt

1243
00:55:11,659 --> 00:55:14,989
okay there's kind of like two things

1244
00:55:12,978 --> 00:55:17,689
that seem to work a lot and there's not

1245
00:55:14,989 --> 00:55:19,579
too much fiddling I find necessary and

1246
00:55:17,690 --> 00:55:21,979
as I say even even if you just if you

1247
00:55:19,579 --> 00:55:23,660
use this line every time I'd be

1248
00:55:21,978 --> 00:55:29,538
surprised if you didn't get a reasonable

1249
00:55:23,659 --> 00:55:32,478
result so a question here why does a

1250
00:55:29,539 --> 00:55:40,849
smoother services correlate to more

1251
00:55:32,478 --> 00:55:44,239
generalize networks so it's kind of this

1252
00:55:40,849 --> 00:55:46,338
some this intuitive explanation I try to

1253
00:55:44,239 --> 00:55:51,369
just kill the whole thing I try to give

1254
00:55:46,338 --> 00:55:58,420
back here which is that if you've got

1255
00:55:51,369 --> 00:56:02,440
something spiky right and so what this

1256
00:55:58,420 --> 00:56:05,088
what this x-axis is showing is like how

1257
00:56:02,440 --> 00:56:06,739
how good is this at recognizing dogs

1258
00:56:05,088 --> 00:56:10,068
versus cats as you change this

1259
00:56:06,739 --> 00:56:12,588
particular parameter right and so so

1260
00:56:10,068 --> 00:56:14,150
something to be generalizable that means

1261
00:56:12,588 --> 00:56:15,259
that we wanted to work when we give it

1262
00:56:14,150 --> 00:56:17,329
when we give it a slightly different

1263
00:56:15,259 --> 00:56:20,509
data set and so a slightly different

1264
00:56:17,329 --> 00:56:22,880
data set may have a slightly different

1265
00:56:20,509 --> 00:56:25,400
relationship between this parameter and

1266
00:56:22,880 --> 00:56:28,660
how caddy versus dog it is it may

1267
00:56:25,400 --> 00:56:28,660
instead look a little bit like this

1268
00:56:30,059 --> 00:56:37,239
right so in other words if we end up at

1269
00:56:33,820 --> 00:56:38,800
this point right then it's not going to

1270
00:56:37,239 --> 00:56:40,509
do a good job on this slightly different

1271
00:56:38,800 --> 00:56:42,160
data set for else if we end up on this

1272
00:56:40,510 --> 00:56:45,090
point it's still going to do a good job

1273
00:56:42,159 --> 00:56:45,089
on this data set

1274
00:56:49,230 --> 00:56:54,309
okay so that's what psychomotor equals

1275
00:56:52,480 --> 00:56:55,900
doing okay so we've got one last thing

1276
00:56:54,309 --> 00:56:58,929
before we going to take a break which is

1277
00:56:55,900 --> 00:57:01,539
we're now going to take this model which

1278
00:56:58,929 --> 00:57:02,589
has 99.5 percent accuracy and we're

1279
00:57:01,539 --> 00:57:05,019
going to try and make it better still

1280
00:57:02,590 --> 00:57:06,550
and what we're going to do is we're not

1281
00:57:05,019 --> 00:57:08,858
actually going to change the model at

1282
00:57:06,550 --> 00:57:13,330
all right but instead we're going to

1283
00:57:08,858 --> 00:57:15,369
look back at the original virtual

1284
00:57:13,329 --> 00:57:22,480
visualization we did where we looked at

1285
00:57:15,369 --> 00:57:24,039
some of our incorrect pictures now what

1286
00:57:22,480 --> 00:57:26,590
I've done is I've printed out the whole

1287
00:57:24,039 --> 00:57:30,759
of these incorrect pictures but the key

1288
00:57:26,590 --> 00:57:34,050
thing to realize is that particularly in

1289
00:57:30,760 --> 00:57:37,960
fact when we do the the validation set

1290
00:57:34,050 --> 00:57:40,990
all of our inputs to our model all the

1291
00:57:37,960 --> 00:57:43,750
time have to be square right and the

1292
00:57:40,989 --> 00:57:45,819
reason for that is it's kind of a minor

1293
00:57:43,750 --> 00:57:48,489
technical detail but basically the GPU

1294
00:57:45,820 --> 00:57:50,200
doesn't go very quickly if you have like

1295
00:57:48,489 --> 00:57:51,639
different dimensions for different

1296
00:57:50,199 --> 00:57:53,710
images because it needs seems to be

1297
00:57:51,639 --> 00:57:55,838
consistent so that every part of the GPU

1298
00:57:53,710 --> 00:57:57,429
can do the same thing and I think this

1299
00:57:55,838 --> 00:57:59,049
is probably fixable but it now that's

1300
00:57:57,429 --> 00:58:01,659
the state of the technology we have so

1301
00:57:59,050 --> 00:58:03,310
our validation set when we actually say

1302
00:58:01,659 --> 00:58:05,379
for this particular thing is it's a dog

1303
00:58:03,309 --> 00:58:08,409
what we actually do to make it square as

1304
00:58:05,380 --> 00:58:10,510
we just pick out the square in the

1305
00:58:08,409 --> 00:58:12,819
middle right so we would take off its

1306
00:58:10,510 --> 00:58:15,010
two edges and so we take the whole

1307
00:58:12,820 --> 00:58:16,930
height and then as much of the middle as

1308
00:58:15,010 --> 00:58:19,380
we can and so you can see in this case

1309
00:58:16,929 --> 00:58:21,730
we wouldn't actually see this dog's head

1310
00:58:19,380 --> 00:58:24,130
right so I think the reason this was

1311
00:58:21,730 --> 00:58:26,230
actually not correctly classified was

1312
00:58:24,130 --> 00:58:29,470
because the validation set only got to

1313
00:58:26,230 --> 00:58:31,719
see the body and the body doesn't look

1314
00:58:29,469 --> 00:58:35,139
particularly doglike or cat-like it's

1315
00:58:31,719 --> 00:58:38,139
not at all punctual what it is so what

1316
00:58:35,139 --> 00:58:39,789
we're going to do when we calculate the

1317
00:58:38,139 --> 00:58:40,960
predictions for our validation set is

1318
00:58:39,789 --> 00:58:43,779
we're going to use something called test

1319
00:58:40,960 --> 00:58:46,119
time augmentation and what this means is

1320
00:58:43,780 --> 00:58:48,070
that every time we decide is this cat or

1321
00:58:46,119 --> 00:58:50,140
a dog not in the training but after

1322
00:58:48,070 --> 00:58:56,349
we've trained the model is we're going

1323
00:58:50,139 --> 00:58:58,329
to actually take four random data

1324
00:58:56,349 --> 00:59:00,309
augmentations and remember the data

1325
00:58:58,329 --> 00:59:03,400
augmentations move around

1326
00:59:00,309 --> 00:59:04,869
and zoom in and out and flip okay

1327
00:59:03,400 --> 00:59:06,849
so we're going to take four of them at

1328
00:59:04,869 --> 00:59:09,789
random and we're going to take the

1329
00:59:06,849 --> 00:59:11,440
original and augmented sent a cropped

1330
00:59:09,789 --> 00:59:13,570
image and we're going to do a prediction

1331
00:59:11,440 --> 00:59:16,630
for all of those and then we're going to

1332
00:59:13,570 --> 00:59:18,880
take the average of those predictions so

1333
00:59:16,630 --> 00:59:21,460
I'm going to say is this a cat is this a

1334
00:59:18,880 --> 00:59:24,369
cat is this a cat is this a cat but and

1335
00:59:21,460 --> 00:59:26,019
so hopefully in one of those random ones

1336
00:59:24,369 --> 00:59:28,480
we actually make sure that the face is

1337
00:59:26,019 --> 00:59:30,340
there zoomed in by a similar amount to

1338
00:59:28,480 --> 00:59:31,840
other dogs faces at sea and it's rotated

1339
00:59:30,340 --> 00:59:36,039
by the amount that it expects to see it

1340
00:59:31,840 --> 00:59:38,890
and so forth and so do that all we have

1341
00:59:36,039 --> 00:59:41,199
to do is just call tt8

1342
00:59:38,889 --> 00:59:43,960
TTA stands for Test time augmentation

1343
00:59:41,199 --> 00:59:45,609
this term of like what a what do we call

1344
00:59:43,960 --> 00:59:47,110
up when we're making predictions from up

1345
00:59:45,610 --> 00:59:48,730
from a model we've trained sometimes

1346
00:59:47,110 --> 00:59:50,590
it's called inference time sometimes

1347
00:59:48,730 --> 00:59:53,199
it's called test time everybody since

1348
00:59:50,590 --> 00:59:55,600
have a different name so TTA and so when

1349
00:59:53,199 --> 00:59:57,250
we do that we go learn TTA check the

1350
00:59:55,599 --> 00:59:59,289
accuracy and lo and behold

1351
00:59:57,250 --> 01:00:01,960
we're now at ninety nine point six five

1352
00:59:59,289 --> 01:00:07,420
percent which is kind of crazy where's

1353
01:00:01,960 --> 01:00:11,530
our green box but for every park we are

1354
01:00:07,420 --> 01:00:13,840
only showing one type of augmentation or

1355
01:00:11,530 --> 01:00:16,540
for particular image right so when we

1356
01:00:13,840 --> 01:00:20,170
are training back here we're not doing

1357
01:00:16,539 --> 01:00:22,659
any TTA right so TTA is not like you

1358
01:00:20,170 --> 01:00:24,730
could and sometimes like I've written

1359
01:00:22,659 --> 01:00:26,679
libraries where after a cheap up I run

1360
01:00:24,730 --> 01:00:27,760
TTA to see how well it's going but

1361
01:00:26,679 --> 01:00:31,029
that's not what's happening here I

1362
01:00:27,760 --> 01:00:33,190
trained the whole thing with training

1363
01:00:31,030 --> 01:00:34,510
time organization which doesn't have a

1364
01:00:33,190 --> 01:00:36,070
special name because that's what we mean

1365
01:00:34,510 --> 01:00:38,230
when we say data augmentation we need

1366
01:00:36,070 --> 01:00:39,850
training time augmentation so here every

1367
01:00:38,230 --> 01:00:41,800
time we showed a picture

1368
01:00:39,849 --> 01:00:43,900
we were randomly changing it a little

1369
01:00:41,800 --> 01:00:45,519
bit so each epoch each of these seven

1370
01:00:43,900 --> 01:00:47,829
epochs it was seen slightly different

1371
01:00:45,519 --> 01:00:50,469
versions of the picture having done that

1372
01:00:47,829 --> 01:00:52,179
we now have a fully trained model we

1373
01:00:50,469 --> 01:00:54,399
then said okay let's look at the

1374
01:00:52,179 --> 01:00:56,349
validation set so TTA by default uses

1375
01:00:54,400 --> 01:00:57,670
the validation set and said okay what

1376
01:00:56,349 --> 01:00:59,589
are your predictions of which ones are

1377
01:00:57,670 --> 01:01:02,409
cats and which ones are dogs and it did

1378
01:00:59,590 --> 01:01:04,539
4 predictions with different random

1379
01:01:02,409 --> 01:01:06,099
orientations plus one on the organ under

1380
01:01:04,539 --> 01:01:07,869
Augmented version average them all

1381
01:01:06,099 --> 01:01:10,199
together and that's what we got and

1382
01:01:07,869 --> 01:01:12,500
that's what we can't clear the accurate

1383
01:01:10,199 --> 01:01:15,358
so is there a high probability of having

1384
01:01:12,500 --> 01:01:19,619
sample in TTA that was not shown in

1385
01:01:15,358 --> 01:01:22,799
doing trained yeah actually every data

1386
01:01:19,619 --> 01:01:24,869
augmented for image is is unique because

1387
01:01:22,800 --> 01:01:28,380
the rotation could be like point zero

1388
01:01:24,869 --> 01:01:30,780
three four degrees and zoom could be 1.0

1389
01:01:28,380 --> 01:01:32,970
one sixty five so every time it's

1390
01:01:30,780 --> 01:01:39,480
slightly different no problem

1391
01:01:32,969 --> 01:01:40,980
was behind you what's your might not use

1392
01:01:39,480 --> 01:01:44,369
white padding or something like that

1393
01:01:40,980 --> 01:01:46,170
just one of your white padding like just

1394
01:01:44,369 --> 01:01:48,990
you know put like a white water around

1395
01:01:46,170 --> 01:01:50,220
oh padding's not yes so like there's

1396
01:01:48,989 --> 01:01:52,259
lots of different types of a better

1397
01:01:50,219 --> 01:01:54,299
orientation you can do and so one of the

1398
01:01:52,260 --> 01:01:57,150
things you can do is to add a border

1399
01:01:54,300 --> 01:01:58,859
around it basically adding a border

1400
01:01:57,150 --> 01:02:00,980
around it in my experiments doesn't

1401
01:01:58,858 --> 01:02:03,659
doesn't help it doesn't make it any less

1402
01:02:00,980 --> 01:02:04,829
cat-like it's not the convolutional

1403
01:02:03,659 --> 01:02:07,289
neural network doesn't seem to find it

1404
01:02:04,829 --> 01:02:08,909
very interesting basically something

1405
01:02:07,289 --> 01:02:10,380
that I do do we'll see later is I do

1406
01:02:08,909 --> 01:02:12,420
something called reflection padding

1407
01:02:10,380 --> 01:02:15,059
which is where I add some borders that

1408
01:02:12,420 --> 01:02:16,829
are the outside just reflected it's a

1409
01:02:15,059 --> 01:02:18,809
way to kind of make some bigger images

1410
01:02:16,829 --> 01:02:21,598
works well with satellite imagery in

1411
01:02:18,809 --> 01:02:23,219
particular but yeah in general I don't

1412
01:02:21,599 --> 01:02:26,838
do I have a lot of padding instead I do

1413
01:02:23,219 --> 01:02:26,838
a bit of zooming

1414
01:02:28,380 --> 01:02:33,599
it's kind of follow-up to that last one

1415
01:02:30,119 --> 01:02:35,549
but rather than cropping just at white

1416
01:02:33,599 --> 01:02:37,799
space because when you crop you lose the

1417
01:02:35,550 --> 01:02:40,710
dog's face but if you added white space

1418
01:02:37,800 --> 01:02:43,260
you wouldn't yeah so that's that's where

1419
01:02:40,710 --> 01:02:44,909
the kind of the reflection padding or

1420
01:02:43,260 --> 01:02:46,200
the zooming or whatever can help so

1421
01:02:44,909 --> 01:02:48,089
there are ways in the faster you know

1422
01:02:46,199 --> 01:02:58,619
library when you do custom transforms of

1423
01:02:48,090 --> 01:03:00,720
of making that happen I find that it

1424
01:02:58,619 --> 01:03:03,329
kind of depends on the image size you

1425
01:03:00,719 --> 01:03:06,419
know but generally speaking it seems

1426
01:03:03,329 --> 01:03:08,429
that using TTA plus data augmentation

1427
01:03:06,420 --> 01:03:10,440
the best thing to do is to try to use

1428
01:03:08,429 --> 01:03:11,879
this larger image as possible and so if

1429
01:03:10,440 --> 01:03:13,860
you kind of crop the thing down and put

1430
01:03:11,880 --> 01:03:16,710
white borders on top and bottom it's now

1431
01:03:13,860 --> 01:03:18,480
quite a lot smaller and so to make it as

1432
01:03:16,710 --> 01:03:20,130
big as it was before you now have to use

1433
01:03:18,480 --> 01:03:21,480
more GPU and if you're going to use more

1434
01:03:20,130 --> 01:03:24,030
that multi figure you could have zoomed

1435
01:03:21,480 --> 01:03:25,740
in and used a bigger image so in my

1436
01:03:24,030 --> 01:03:36,060
playing around that doesn't seem to be

1437
01:03:25,739 --> 01:03:38,459
generally as successful there is a

1438
01:03:36,059 --> 01:03:41,690
little interest on the topic of how do

1439
01:03:38,460 --> 01:03:47,940
the domain tation in older than images

1440
01:03:41,690 --> 01:03:53,130
indeed at least not images um yeah um no

1441
01:03:47,940 --> 01:03:54,210
one seems to know I actually um I asked

1442
01:03:53,130 --> 01:03:55,710
some of my friends in the natural

1443
01:03:54,210 --> 01:03:57,150
language processing community about this

1444
01:03:55,710 --> 01:03:59,909
we'll get to natural language processing

1445
01:03:57,150 --> 01:04:01,200
in a couple of lessons you know it seems

1446
01:03:59,909 --> 01:04:04,259
like it'd be really helpful there's been

1447
01:04:01,199 --> 01:04:06,029
a few example I carry very few number

1448
01:04:04,260 --> 01:04:08,490
examples of people where papers would

1449
01:04:06,030 --> 01:04:10,800
like try replacing synonyms for instance

1450
01:04:08,489 --> 01:04:12,869
but on the whole and understanding of

1451
01:04:10,800 --> 01:04:16,830
like appropriate data augmentation for

1452
01:04:12,869 --> 01:04:24,449
non image domains is under-researched in

1453
01:04:16,829 --> 01:04:26,219
under under developed the question was

1454
01:04:24,449 --> 01:04:28,799
could couldn't we just use a sliding

1455
01:04:26,219 --> 01:04:30,959
window to generate on the images so in

1456
01:04:28,800 --> 01:04:33,840
that dog thank you couldn't we generate

1457
01:04:30,960 --> 01:04:34,079
three parts of it wouldn't that be

1458
01:04:33,840 --> 01:04:37,559
better

1459
01:04:34,079 --> 01:04:40,348
yeah PTI you mean just just in general

1460
01:04:37,559 --> 01:04:42,179
when you're creating your so

1461
01:04:40,349 --> 01:04:43,680
training time I would say no that

1462
01:04:42,179 --> 01:04:45,690
wouldn't be better because we're not

1463
01:04:43,679 --> 01:04:48,210
gonna get as much variation you know we

1464
01:04:45,690 --> 01:04:49,950
want to have it like like one degree off

1465
01:04:48,210 --> 01:04:51,420
five you know five degrees off ten

1466
01:04:49,949 --> 01:04:52,978
pixels up like lots of slightly

1467
01:04:51,420 --> 01:04:56,548
different versions and so if you just

1468
01:04:52,978 --> 01:04:58,379
have three standard ways then you're not

1469
01:04:56,548 --> 01:05:00,568
giving it as many different ways of

1470
01:04:58,380 --> 01:05:03,900
looking at the data for testing

1471
01:05:00,568 --> 01:05:07,079
augmentation having fixed cropped

1472
01:05:03,900 --> 01:05:09,809
locations I think probably would be

1473
01:05:07,079 --> 01:05:12,210
better and I just haven't gotten around

1474
01:05:09,809 --> 01:05:13,979
to writing that yet I have a version in

1475
01:05:12,210 --> 01:05:19,199
an old library I think having fixed

1476
01:05:13,978 --> 01:05:21,239
cropped locations plus random contrast

1477
01:05:19,199 --> 01:05:23,118
brightness rotation changes might be

1478
01:05:21,239 --> 01:05:25,318
better

1479
01:05:23,119 --> 01:05:27,630
the reason I've got around to it yet is

1480
01:05:25,318 --> 01:05:29,400
because in my testing it didn't seem to

1481
01:05:27,630 --> 01:05:31,289
help him practice very much and it made

1482
01:05:29,400 --> 01:05:32,460
the code a lot more complicated so you

1483
01:05:31,289 --> 01:05:34,490
know it's kind of it's an interesting

1484
01:05:32,460 --> 01:05:39,170
question

1485
01:05:34,489 --> 01:05:44,239
I just wanted all of this last AI api's

1486
01:05:39,170 --> 01:05:45,289
that you are using is it yeah that's a

1487
01:05:44,239 --> 01:05:47,299
great question

1488
01:05:45,289 --> 01:05:48,829
so the faster you go libraries open

1489
01:05:47,300 --> 01:05:52,430
source and let's talk about it a bit

1490
01:05:48,829 --> 01:05:55,068
more generally because you know it's

1491
01:05:52,429 --> 01:05:56,389
like the fact that the fact that we're

1492
01:05:55,068 --> 01:05:59,329
using this library is kind of

1493
01:05:56,389 --> 01:06:02,440
interesting and unusual and it sits on

1494
01:05:59,329 --> 01:06:08,380
top of something called a torch right so

1495
01:06:02,440 --> 01:06:11,050
pi torch is a fairly recent development

1496
01:06:08,380 --> 01:06:13,430
and it's kind of I've noticed all the

1497
01:06:11,050 --> 01:06:16,940
researchers that I respect pretty much

1498
01:06:13,429 --> 01:06:18,949
are now using high torch I found in part

1499
01:06:16,940 --> 01:06:20,510
two of last year's course that a lot of

1500
01:06:18,949 --> 01:06:23,808
the cutting-edge stuff I wanted to teach

1501
01:06:20,510 --> 01:06:26,540
I couldn't do it in chaos and tensorflow

1502
01:06:23,809 --> 01:06:29,030
which is what we used to teach with and

1503
01:06:26,539 --> 01:06:31,699
so I had to switch the course to pay

1504
01:06:29,030 --> 01:06:35,240
torch halfway through part two the

1505
01:06:31,699 --> 01:06:37,219
problem was that PI torch isn't very

1506
01:06:35,239 --> 01:06:38,808
easy to use you have to write your own

1507
01:06:37,219 --> 01:06:40,250
training loop from scratch I basically

1508
01:06:38,809 --> 01:06:41,839
write everything from scratch or the

1509
01:06:40,250 --> 01:06:43,519
stuff you see inside the class they are

1510
01:06:41,838 --> 01:06:47,779
library we would have had to written it

1511
01:06:43,519 --> 01:06:49,309
you know to learn and so it really makes

1512
01:06:47,780 --> 01:06:50,660
it very hard to learn deep learning when

1513
01:06:49,309 --> 01:06:55,069
you have to write hundreds of lines of

1514
01:06:50,659 --> 01:06:57,108
code to do anything so so we decided to

1515
01:06:55,068 --> 01:07:00,949
create a library on top of Pi torch

1516
01:06:57,108 --> 01:07:03,170
because we you know this our mission is

1517
01:07:00,949 --> 01:07:04,308
to teach world class big morning so we

1518
01:07:03,170 --> 01:07:06,369
wanted to show you like here's how you

1519
01:07:04,309 --> 01:07:09,589
can be the best in the world at doing it

1520
01:07:06,369 --> 01:07:12,019
and we found that a lot of the world

1521
01:07:09,588 --> 01:07:13,789
class stuff we needed to show really

1522
01:07:12,019 --> 01:07:16,730
needed PI torch or at least with PI

1523
01:07:13,789 --> 01:07:19,119
torch it was far easier and but then PI

1524
01:07:16,730 --> 01:07:23,389
thought itself just wasn't suitable as a

1525
01:07:19,119 --> 01:07:25,970
first thing to teach with for new for

1526
01:07:23,389 --> 01:07:28,838
new deep learning practitioners so we

1527
01:07:25,969 --> 01:07:31,219
built this library on up of PI torch

1528
01:07:28,838 --> 01:07:32,989
initially heavily influenced by chaos

1529
01:07:31,219 --> 01:07:34,669
which is what we taught last year and

1530
01:07:32,989 --> 01:07:36,529
but then we realized we could actually

1531
01:07:34,670 --> 01:07:38,659
make things much much much easier than

1532
01:07:36,530 --> 01:07:41,059
care us so in care us if you look back

1533
01:07:38,659 --> 01:07:43,489
at last year's course notes you'll find

1534
01:07:41,059 --> 01:07:45,920
that all of the code is two to three

1535
01:07:43,489 --> 01:07:47,598
times longer and there's lots more

1536
01:07:45,920 --> 01:07:48,889
opportunities for

1537
01:07:47,599 --> 01:07:52,338
stakes because there's just a lot of

1538
01:07:48,889 --> 01:07:54,440
things you have to get right so we ended

1539
01:07:52,338 --> 01:07:57,679
up kind of building this this this

1540
01:07:54,440 --> 01:08:00,289
library in order to make it easier to

1541
01:07:57,679 --> 01:08:03,228
get into deep learning but also easier

1542
01:08:00,289 --> 01:08:04,729
to get state-of-the-art results and then

1543
01:08:03,228 --> 01:08:06,199
over the last year as we started

1544
01:08:04,728 --> 01:08:09,848
developing on top of that we started

1545
01:08:06,199 --> 01:08:12,439
discovering that by using this library

1546
01:08:09,849 --> 01:08:14,479
it made us so much more productive that

1547
01:08:12,440 --> 01:08:15,920
we actually started kind of developing

1548
01:08:14,478 --> 01:08:17,778
you state-of-the-art results and new

1549
01:08:15,920 --> 01:08:19,520
methods ourselves and we started

1550
01:08:17,779 --> 01:08:21,080
realizing that there's a whole bunch of

1551
01:08:19,520 --> 01:08:23,839
like papers that have kind of been

1552
01:08:21,079 --> 01:08:26,269
ignored or lost which when you use them

1553
01:08:23,838 --> 01:08:28,460
it could like automate or semi-automated

1554
01:08:26,270 --> 01:08:31,909
stuff like learning read finder that's

1555
01:08:28,460 --> 01:08:33,439
not in any other library so so it kind

1556
01:08:31,908 --> 01:08:36,469
of got to the point where now not only

1557
01:08:33,439 --> 01:08:38,838
is kind of fast AI lets us do things

1558
01:08:36,469 --> 01:08:41,359
easier much easier than any other

1559
01:08:38,838 --> 01:08:44,988
approach but at the same time it

1560
01:08:41,359 --> 01:08:46,338
actually has a lot more kind of

1561
01:08:44,988 --> 01:08:48,858
sophisticated stuff behind the scenes

1562
01:08:46,338 --> 01:08:52,880
than anything else so so it's kind of an

1563
01:08:48,859 --> 01:08:54,980
interesting mix so yeah so we've

1564
01:08:52,880 --> 01:08:57,500
released this library like at this stage

1565
01:08:54,979 --> 01:08:58,789
it's like very early version and so

1566
01:08:57,500 --> 01:09:00,738
through this course by the end of this

1567
01:08:58,789 --> 01:09:02,929
course I hope as a group you know we

1568
01:09:00,738 --> 01:09:04,459
will all a lot of people are already

1569
01:09:02,929 --> 01:09:08,000
helping have developed it into something

1570
01:09:04,460 --> 01:09:12,289
that's you know really pretty stable and

1571
01:09:08,000 --> 01:09:16,488
rock-solid and yeah anybody can then can

1572
01:09:12,289 --> 01:09:18,408
use it to build your own models under an

1573
01:09:16,488 --> 01:09:23,988
open-source license as you can see it's

1574
01:09:18,408 --> 01:09:26,899
available on github behind the scenes

1575
01:09:23,988 --> 01:09:29,689
it's it's creating play torch models and

1576
01:09:26,899 --> 01:09:33,649
so apply torch models can then be

1577
01:09:29,689 --> 01:09:35,448
exported into various different formats

1578
01:09:33,649 --> 01:09:36,710
having said that like a lot of folks

1579
01:09:35,448 --> 01:09:38,838
like issue if you want to do something

1580
01:09:36,710 --> 01:09:40,539
on a mobile phone for example you're

1581
01:09:38,838 --> 01:09:44,838
probably going to need to use tensorflow

1582
01:09:40,539 --> 01:09:46,158
and so later on in this course we're

1583
01:09:44,838 --> 01:09:47,448
going to show like how some of the

1584
01:09:46,158 --> 01:09:50,210
things that we're doing in the past AI

1585
01:09:47,448 --> 01:09:51,798
library you can do in chaos and cancel

1586
01:09:50,210 --> 01:09:53,088
flow so you can going to get a sense of

1587
01:09:51,798 --> 01:09:56,500
what the different libraries look like

1588
01:09:53,088 --> 01:09:59,960
and generally speaking the simple stuff

1589
01:09:56,500 --> 01:10:00,859
is like it'll take you a small number of

1590
01:09:59,960 --> 01:10:02,689
days

1591
01:10:00,859 --> 01:10:04,789
to learn to do it and care us in

1592
01:10:02,689 --> 01:10:08,089
tensorflow versus fast AI and high torch

1593
01:10:04,789 --> 01:10:09,738
and the more complex stuff often this

1594
01:10:08,090 --> 01:10:12,170
won't be possible so that like if you

1595
01:10:09,738 --> 01:10:16,658
needed to be intensive flow you're just

1596
01:10:12,170 --> 01:10:16,658
kind of simplify it off in a little bit

1597
01:10:17,560 --> 01:10:24,830
but you know I think the more important

1598
01:10:19,850 --> 01:10:26,810
thing to realize is every year the kind

1599
01:10:24,829 --> 01:10:28,460
of the libraries that are available and

1600
01:10:26,810 --> 01:10:30,139
which ones are the best totally changes

1601
01:10:28,460 --> 01:10:31,189
so like the main thing I hope that you

1602
01:10:30,139 --> 01:10:33,109
get out of this course is an

1603
01:10:31,189 --> 01:10:34,879
understanding of the concepts like

1604
01:10:33,109 --> 01:10:36,349
here's how you find a learning rate

1605
01:10:34,880 --> 01:10:38,029
here's why differential learning rates

1606
01:10:36,350 --> 01:10:40,850
are important is they do learn where the

1607
01:10:38,029 --> 01:10:42,408
kneeling you know here's what stochastic

1608
01:10:40,850 --> 01:10:45,619
gradient a second's restarts does so on

1609
01:10:42,408 --> 01:10:47,948
and so forth because you know by the

1610
01:10:45,618 --> 01:10:52,399
time we do this course again next year

1611
01:10:47,948 --> 01:10:55,579
you know the library situations and the

1612
01:10:52,399 --> 01:11:05,960
difference the king that's a question of

1613
01:10:55,579 --> 01:11:08,658
that I was wondering if you've had an

1614
01:11:05,960 --> 01:11:10,908
opinion on pyro which is ubers new

1615
01:11:08,658 --> 01:11:11,960
release I haven't looked at it no I'm

1616
01:11:10,908 --> 01:11:14,210
very interested in probabilistic

1617
01:11:11,960 --> 01:11:15,859
programming and it's really cool that's

1618
01:11:14,210 --> 01:11:17,090
built on top of paper so one of the

1619
01:11:15,859 --> 01:11:19,069
things we'll learn about in this course

1620
01:11:17,090 --> 01:11:20,810
is we'll see that PI torch is much more

1621
01:11:19,069 --> 01:11:24,189
than just a deep learning library it

1622
01:11:20,810 --> 01:11:28,369
actually lets us write arbitrary

1623
01:11:24,189 --> 01:11:29,719
gpu-accelerated algorithms from scratch

1624
01:11:28,368 --> 01:11:31,368
which we're actually going to do and

1625
01:11:29,719 --> 01:11:33,260
pyro is a great example of what people

1626
01:11:31,368 --> 01:11:38,198
are now doing with might watch outside

1627
01:11:33,260 --> 01:11:40,190
of the deep level great ok let's take a

1628
01:11:38,198 --> 01:11:52,519
eight-minute break and we'll come back

1629
01:11:40,189 --> 01:11:56,118
at 7:55 so ninety nine point six five

1630
01:11:52,520 --> 01:11:59,060
percent accuracy what does that mean so

1631
01:11:56,118 --> 01:12:01,130
in classification when we do

1632
01:11:59,060 --> 01:12:04,130
classification and machine learning the

1633
01:12:01,130 --> 01:12:05,750
really simple way to look at the result

1634
01:12:04,130 --> 01:12:07,850
of a classification is what's called the

1635
01:12:05,750 --> 01:12:09,529
confusion matrix this is not just deep

1636
01:12:07,850 --> 01:12:12,110
learning but in any kind of classifier

1637
01:12:09,529 --> 01:12:14,689
machine learning where we say okay what

1638
01:12:12,109 --> 01:12:17,960
was the actual truth there were

1639
01:12:14,689 --> 01:12:20,448
thousand cats and a thousand dogs out of

1640
01:12:17,960 --> 01:12:22,969
the thousand actual cats how many did we

1641
01:12:20,448 --> 01:12:25,219
predict were cats this is obviously in

1642
01:12:22,969 --> 01:12:27,319
the validation step this is the images

1643
01:12:25,219 --> 01:12:28,069
that we didn't use to train with it

1644
01:12:27,319 --> 01:12:29,988
turns out there were nine hundred

1645
01:12:28,069 --> 01:12:32,210
ninety-eight cats that we actually

1646
01:12:29,988 --> 01:12:32,659
predicted as cats and two that we got

1647
01:12:32,210 --> 01:12:35,180
wrong

1648
01:12:32,659 --> 01:12:36,439
okay and then for dogs there were nine

1649
01:12:35,180 --> 01:12:38,150
hundred ninety-five that we predicted

1650
01:12:36,439 --> 01:12:41,210
were dogs and then five that we got

1651
01:12:38,149 --> 01:12:42,979
wrong and so often these confusion

1652
01:12:41,210 --> 01:12:44,689
matrices can be helpful particularly if

1653
01:12:42,979 --> 01:12:46,218
you've got like four or five classes

1654
01:12:44,689 --> 01:12:48,198
you're trying to predict to see like

1655
01:12:46,219 --> 01:12:49,880
which group you having the most trouble

1656
01:12:48,198 --> 01:12:52,909
with and you can see it uses color

1657
01:12:49,880 --> 01:12:55,069
coding to tell you you know to highlight

1658
01:12:52,909 --> 01:12:57,979
the large the large bits you've got to

1659
01:12:55,069 --> 01:13:01,069
hope that the diagonal is the

1660
01:12:57,979 --> 01:13:02,509
highlighted section so now that we've

1661
01:13:01,069 --> 01:13:04,698
retrained the model it can be quite

1662
01:13:02,510 --> 01:13:07,130
helpful now that's better to actually

1663
01:13:04,698 --> 01:13:09,469
look back and see like okay which ones

1664
01:13:07,130 --> 01:13:12,380
in particular were incorrect and we can

1665
01:13:09,469 --> 01:13:15,260
see here there were actually only two

1666
01:13:12,380 --> 01:13:16,579
incorrect cats it prints out four by

1667
01:13:15,260 --> 01:13:19,190
default so you can actually see these

1668
01:13:16,579 --> 01:13:21,319
two actually less than 0.5 so they

1669
01:13:19,189 --> 01:13:23,899
weren't they weren't wrong okay so it's

1670
01:13:21,319 --> 01:13:26,079
actually these two were wrong cats and

1671
01:13:23,899 --> 01:13:28,579
this one isn't obviously a cat at all

1672
01:13:26,079 --> 01:13:31,369
this one is but it looks like it's got a

1673
01:13:28,579 --> 01:13:34,189
lot of weird artifacts and you can't see

1674
01:13:31,369 --> 01:13:36,469
its eyeballs at all so and then here are

1675
01:13:34,189 --> 01:13:38,659
the how many dogs where they're all

1676
01:13:36,469 --> 01:13:40,609
wrong there were five wrong dogs here

1677
01:13:38,659 --> 01:13:44,119
are four of them that's not obviously a

1678
01:13:40,609 --> 01:13:46,399
dog that looks like a mistake that looks

1679
01:13:44,119 --> 01:13:47,869
like a mistake that one I guess doesn't

1680
01:13:46,399 --> 01:13:51,349
have enough information that I guess

1681
01:13:47,869 --> 01:13:53,988
it's a mistake so so we've done a pretty

1682
01:13:51,350 --> 01:13:57,890
good job here of creating a good

1683
01:13:53,988 --> 01:13:59,149
classifier I would based on entering a

1684
01:13:57,890 --> 01:14:00,860
lot of capital competitions and

1685
01:13:59,149 --> 01:14:02,869
comparing results I've done two various

1686
01:14:00,859 --> 01:14:05,210
research papers I can tell you it's a

1687
01:14:02,869 --> 01:14:06,319
state of the art classifier it's it's

1688
01:14:05,210 --> 01:14:08,090
right up there with the best in the

1689
01:14:06,319 --> 01:14:09,889
world we're going to make it a little

1690
01:14:08,090 --> 01:14:11,989
bit better in a moment but here in the

1691
01:14:09,890 --> 01:14:14,270
basic steps right so if you want to

1692
01:14:11,988 --> 01:14:16,279
create a world-class image classifier

1693
01:14:14,270 --> 01:14:18,320
the steps that we just went through was

1694
01:14:16,279 --> 01:14:21,050
that we started our week's term data

1695
01:14:18,319 --> 01:14:22,789
augmentation on by saying oil transforms

1696
01:14:21,050 --> 01:14:24,380
equals and you either say sidon or

1697
01:14:22,789 --> 01:14:27,679
top-down depending on what you're doing

1698
01:14:24,380 --> 01:14:28,550
start with pre compute equals true find

1699
01:14:27,679 --> 01:14:31,010
a decent learning

1700
01:14:28,550 --> 01:14:33,079
eight we then train just like it one or

1701
01:14:31,010 --> 01:14:34,820
two epochs which that takes a few

1702
01:14:33,079 --> 01:14:37,670
seconds as we got through compute equals

1703
01:14:34,819 --> 01:14:40,429
true then we turn off pre compute which

1704
01:14:37,670 --> 01:14:43,220
allows us to use data augmentation to do

1705
01:14:40,430 --> 01:14:45,710
another two or three epochs generally

1706
01:14:43,220 --> 01:14:48,260
with cycle length equals one then I

1707
01:14:45,710 --> 01:14:50,359
unfreeze all them as I then set the

1708
01:14:48,260 --> 01:14:52,640
earlier layers to be like either

1709
01:14:50,359 --> 01:14:54,829
somewhere between a 3 times 2 10 times

1710
01:14:52,640 --> 01:15:01,760
mobile learning rate in the previous so

1711
01:14:54,829 --> 01:15:03,229
in this case I did 10 times right so

1712
01:15:01,760 --> 01:15:04,250
it's like this was my learning rate that

1713
01:15:03,229 --> 01:15:06,139
I found from the learning rate finer

1714
01:15:04,250 --> 01:15:08,770
than I went 10 times smaller and then 10

1715
01:15:06,140 --> 01:15:11,090
times smaller as a rule of thumb like

1716
01:15:08,770 --> 01:15:13,640
knowing that you're starting with a pre

1717
01:15:11,090 --> 01:15:15,050
trained imagenet model if you know if

1718
01:15:13,640 --> 01:15:17,030
you can see that the things that you're

1719
01:15:15,050 --> 01:15:18,739
now trying to classify a pretty similar

1720
01:15:17,029 --> 01:15:21,229
the kinds of things in imagenet ie

1721
01:15:18,739 --> 01:15:21,920
pictures of normal objects in normal

1722
01:15:21,229 --> 01:15:24,349
environments

1723
01:15:21,920 --> 01:15:26,149
you probably want about a 10x difference

1724
01:15:24,350 --> 01:15:27,920
because you want those earlier layers

1725
01:15:26,149 --> 01:15:30,409
like you think that the earlier layers

1726
01:15:27,920 --> 01:15:31,699
are probably very good already but also

1727
01:15:30,409 --> 01:15:34,369
if you're doing something like satellite

1728
01:15:31,699 --> 01:15:36,079
imagery or medical imaging which is not

1729
01:15:34,369 --> 01:15:37,939
at all like image net then you probably

1730
01:15:36,079 --> 01:15:39,439
want to be training those earlier layers

1731
01:15:37,939 --> 01:15:42,379
a lot more so you might have like oh

1732
01:15:39,439 --> 01:15:45,769
just a 3/8 difference all right so

1733
01:15:42,380 --> 01:15:52,220
that's like one change that I make is to

1734
01:15:45,770 --> 01:15:56,150
try to make it out of 10x or 3x yes so

1735
01:15:52,220 --> 01:15:59,480
then after unfreezing you can now call

1736
01:15:56,149 --> 01:16:01,489
LR find again but at Nike didn't in this

1737
01:15:59,479 --> 01:16:03,229
case but like once you've unfrozen all

1738
01:16:01,489 --> 01:16:05,389
the layers you've turned on differential

1739
01:16:03,229 --> 01:16:10,309
learning rates you can then call a lot

1740
01:16:05,390 --> 01:16:12,050
of fine again right and so you can then

1741
01:16:10,310 --> 01:16:13,970
check like oh does it still look like

1742
01:16:12,050 --> 01:16:16,460
the same point I had last time is about

1743
01:16:13,970 --> 01:16:20,150
right something to note is that if you

1744
01:16:16,460 --> 01:16:21,949
call LR find having set differential

1745
01:16:20,149 --> 01:16:23,779
learning rates the thing that's actually

1746
01:16:21,949 --> 01:16:25,819
going to print out is the learning rate

1747
01:16:23,779 --> 01:16:27,199
of the last layers right because you've

1748
01:16:25,819 --> 01:16:28,689
got three different learning rates so

1749
01:16:27,199 --> 01:16:31,130
it's actually showing you the last layer

1750
01:16:28,689 --> 01:16:33,289
so then yeah then I trained the full

1751
01:16:31,130 --> 01:16:35,180
network with cycle more equals two and

1752
01:16:33,289 --> 01:16:38,479
it'll either it starts with the fitting

1753
01:16:35,180 --> 01:16:40,579
or I run out of time right so like let

1754
01:16:38,479 --> 01:16:41,639
me show you all right so let's do this

1755
01:16:40,579 --> 01:16:43,019
again

1756
01:16:41,640 --> 01:16:45,510
a totally different data set so this

1757
01:16:43,020 --> 01:16:47,100
morning I noticed that some of you on

1758
01:16:45,510 --> 01:16:49,860
the forums were playing around with this

1759
01:16:47,100 --> 01:16:53,180
playground Kegel competition very

1760
01:16:49,859 --> 01:16:56,489
similar called dog breed identification

1761
01:16:53,180 --> 01:16:59,909
so the dog breed identification cat will

1762
01:16:56,489 --> 01:17:01,500
challenge is one where you don't

1763
01:16:59,909 --> 01:17:02,760
actually have to decide which ones are

1764
01:17:01,500 --> 01:17:05,189
cats and which ones the dogs they're all

1765
01:17:02,760 --> 01:17:07,380
dogs but you have to decide what kind of

1766
01:17:05,189 --> 01:17:10,019
dog it is but there are 120 different

1767
01:17:07,380 --> 01:17:14,029
breeds of dogs okay

1768
01:17:10,020 --> 01:17:18,600
so you know obviously this could be like

1769
01:17:14,029 --> 01:17:20,159
different types of cells and pathology

1770
01:17:18,600 --> 01:17:23,600
slides it could be different kinds of

1771
01:17:20,159 --> 01:17:25,829
cancers in CT scans it could be

1772
01:17:23,600 --> 01:17:28,050
different kinds of icebergs and

1773
01:17:25,829 --> 01:17:29,460
satellite images whatever right as long

1774
01:17:28,050 --> 01:17:33,390
as you've got some kind of labeled

1775
01:17:29,460 --> 01:17:34,640
images so I want to show you what I did

1776
01:17:33,390 --> 01:17:38,610
this morning so it took me about an hour

1777
01:17:34,640 --> 01:17:42,150
basically to go in to end from something

1778
01:17:38,609 --> 01:17:43,769
I'd never seen before so I downloaded

1779
01:17:42,149 --> 01:17:45,420
the data from kaggle and I'll show you

1780
01:17:43,770 --> 01:17:47,190
how to do that shortly but the short

1781
01:17:45,420 --> 01:17:49,680
answer is there's something called cable

1782
01:17:47,189 --> 01:17:51,869
CLI which is a github project you can

1783
01:17:49,680 --> 01:17:54,150
search for and if you read the docs to

1784
01:17:51,869 --> 01:17:55,680
basically run cagey download provide the

1785
01:17:54,149 --> 01:17:57,689
competition name and it will grab all

1786
01:17:55,680 --> 01:18:01,380
the data for you to your crystal or

1787
01:17:57,689 --> 01:18:07,529
Amazon or whatever instance I put in my

1788
01:18:01,380 --> 01:18:12,060
data folder and I then went LS and I saw

1789
01:18:07,529 --> 01:18:14,880
that it's a little bit different to our

1790
01:18:12,060 --> 01:18:17,670
previous data set it's not that there's

1791
01:18:14,880 --> 01:18:20,130
a train folder which has a separate

1792
01:18:17,670 --> 01:18:22,680
folder for each kind of dog but instead

1793
01:18:20,130 --> 01:18:25,590
of tonette there was a CSV file and the

1794
01:18:22,680 --> 01:18:27,990
CSV file I read it in with pandas so

1795
01:18:25,590 --> 01:18:30,210
pandas is the thing we use in python to

1796
01:18:27,989 --> 01:18:33,090
do structured data analysis like csv

1797
01:18:30,210 --> 01:18:36,029
files so he picked pandas we called pd

1798
01:18:33,090 --> 01:18:38,699
that's pretty much universal PDR htsb

1799
01:18:36,029 --> 01:18:39,840
reads in the csv file we can then take a

1800
01:18:38,699 --> 01:18:41,220
look at it and you can see that

1801
01:18:39,840 --> 01:18:45,000
basically it had like some kind of

1802
01:18:41,220 --> 01:18:46,980
identifier and then the debris right so

1803
01:18:45,000 --> 01:18:49,859
this is like a different way this is the

1804
01:18:46,979 --> 01:18:51,659
second main way that people kind of give

1805
01:18:49,859 --> 01:18:53,159
you image labels one is to put different

1806
01:18:51,659 --> 01:18:55,349
images into different folders

1807
01:18:53,159 --> 01:18:57,840
the second is generally to give you as

1808
01:18:55,349 --> 01:18:59,819
some kind of file like a CSV file to

1809
01:18:57,840 --> 01:19:05,369
tell you here's the image name and

1810
01:18:59,819 --> 01:19:08,158
here's the label okay so what I then did

1811
01:19:05,368 --> 01:19:10,018
was I used pandas again to create a

1812
01:19:08,158 --> 01:19:13,288
pivot table which basically groups it up

1813
01:19:10,019 --> 01:19:15,719
just to see how many of each breed there

1814
01:19:13,288 --> 01:19:18,840
were and I sorted them and so I saw okay

1815
01:19:15,719 --> 01:19:21,448
they've got like about a hundred some of

1816
01:19:18,840 --> 01:19:23,489
the more common breeds and some of the

1817
01:19:21,448 --> 01:19:27,029
less common breeds they've got like 60

1818
01:19:23,488 --> 01:19:28,678
or so okay altogether there are 120 rows

1819
01:19:27,029 --> 01:19:32,398
and I've been 120 different breeds

1820
01:19:28,679 --> 01:19:35,878
represented okay so I'm going to go

1821
01:19:32,399 --> 01:19:37,889
through the steps right so enable data

1822
01:19:35,878 --> 01:19:39,328
augmentation so to enable data

1823
01:19:37,889 --> 01:19:42,029
augmentation when we call this

1824
01:19:39,328 --> 01:19:44,219
transforms from model you just pass in

1825
01:19:42,029 --> 01:19:46,108
and all transformers in this case I

1826
01:19:44,219 --> 01:19:48,359
chose side on again these are pictures

1827
01:19:46,109 --> 01:19:53,039
of dots and stuff so this side on photos

1828
01:19:48,359 --> 01:19:55,769
I we're talking about maqsuum as more

1829
01:19:53,038 --> 01:19:58,139
detail later but maximum basically says

1830
01:19:55,769 --> 01:20:01,769
when you do the data augmentation we

1831
01:19:58,139 --> 01:20:04,979
like zoom into it by up to one point one

1832
01:20:01,769 --> 01:20:06,748
times okay so but randomly between one

1833
01:20:04,979 --> 01:20:08,999
the original image size and one point

1834
01:20:06,748 --> 01:20:11,069
one points so it's not always cropping

1835
01:20:08,998 --> 01:20:12,389
out in the middle or an edge but it

1836
01:20:11,069 --> 01:20:16,438
could be cropping out a smaller part

1837
01:20:12,389 --> 01:20:18,769
okay so having done that the key step

1838
01:20:16,439 --> 01:20:21,539
now is to graphically going from paths

1839
01:20:18,769 --> 01:20:23,820
so previously we went from paths and

1840
01:20:21,538 --> 01:20:25,679
that tells it that the the names of the

1841
01:20:23,819 --> 01:20:29,608
folders are the names of the labels we

1842
01:20:25,679 --> 01:20:32,128
go from CSV and we pass in the CSV file

1843
01:20:29,609 --> 01:20:34,800
that contains the letters so we're

1844
01:20:32,128 --> 01:20:37,050
passing in the path that contains all of

1845
01:20:34,800 --> 01:20:40,139
the data the name of the folder that

1846
01:20:37,050 --> 01:20:43,859
contains the training data the CSV that

1847
01:20:40,139 --> 01:20:45,300
contains the labels we need to also tell

1848
01:20:43,859 --> 01:20:47,099
it where the test set is if you want to

1849
01:20:45,300 --> 01:20:53,010
submit to cattle later talk more about

1850
01:20:47,099 --> 01:20:55,229
that next week now this time the

1851
01:20:53,010 --> 01:20:57,449
previous data set we had had actually

1852
01:20:55,229 --> 01:20:59,789
separated a validation set out into a

1853
01:20:57,448 --> 01:21:02,458
separate folder right but in this case

1854
01:20:59,788 --> 01:21:05,998
you'll see that there is not a separate

1855
01:21:02,458 --> 01:21:07,738
folder called validation right so we

1856
01:21:05,998 --> 01:21:09,130
want to be able to track how good our

1857
01:21:07,738 --> 01:21:10,569
performance is low

1858
01:21:09,130 --> 01:21:12,489
so we're going to have to separate some

1859
01:21:10,569 --> 01:21:15,309
of the images out to put it into a

1860
01:21:12,489 --> 01:21:18,340
validation set okay so I do that at

1861
01:21:15,310 --> 01:21:22,960
random and so up here you can see how it

1862
01:21:18,340 --> 01:21:25,840
basically opened up the CSV file turned

1863
01:21:22,960 --> 01:21:28,180
it into a list of rows and then taken

1864
01:21:25,840 --> 01:21:30,789
the length of that minus one because

1865
01:21:28,180 --> 01:21:33,250
there's a header at the top right and so

1866
01:21:30,789 --> 01:21:35,319
that's the number of rows in the CSV

1867
01:21:33,250 --> 01:21:37,899
file which must be the number of images

1868
01:21:35,319 --> 01:21:40,840
that we have and then this is a fast AI

1869
01:21:37,899 --> 01:21:42,309
thing get cross-validation indexes now

1870
01:21:40,840 --> 01:21:44,590
we'll talk about cross-validation later

1871
01:21:42,310 --> 01:21:48,090
but basically if you call this and pass

1872
01:21:44,590 --> 01:21:52,480
in a number it's going to return to you

1873
01:21:48,090 --> 01:21:54,789
by default a random twenty percent of

1874
01:21:52,479 --> 01:21:57,009
the rows who uses your validation set

1875
01:21:54,789 --> 01:21:58,930
and you can pass in parameters to get

1876
01:21:57,010 --> 01:22:00,430
different amounts right so this is now

1877
01:21:58,930 --> 01:22:03,610
going to grab twenty percent of the data

1878
01:22:00,430 --> 01:22:05,890
and say all right this is the this is

1879
01:22:03,609 --> 01:22:07,689
the indexes the numbers of the files

1880
01:22:05,890 --> 01:22:08,079
which we're going to use as a validation

1881
01:22:07,689 --> 01:22:12,789
set

1882
01:22:08,079 --> 01:22:14,529
okay so now that we've got that in fact

1883
01:22:12,789 --> 01:22:19,979
let's kind of run this so you can see

1884
01:22:14,529 --> 01:22:23,590
what that looks like so well indexes is

1885
01:22:19,979 --> 01:22:26,178
just a big bunch of numbers okay and so

1886
01:22:23,590 --> 01:22:31,909
an is

1887
01:22:26,179 --> 01:22:33,800
10,000 right and so we have about twenty

1888
01:22:31,908 --> 01:22:34,809
percent of those is going to be in a

1889
01:22:33,800 --> 01:22:44,929
validation set

1890
01:22:34,810 --> 01:22:46,820
so when we call from CSV we can pass in

1891
01:22:44,929 --> 01:22:48,890
a parameter which is talent which

1892
01:22:46,819 --> 01:22:52,849
indexes to treat us a validation set and

1893
01:22:48,890 --> 01:22:54,739
so that's passed in those indexes one

1894
01:22:52,850 --> 01:23:03,590
thing that's a little bit tricky here is

1895
01:22:54,738 --> 01:23:05,959
that the file names actually have I

1896
01:23:03,590 --> 01:23:07,909
checked they actually have a dot jpg on

1897
01:23:05,960 --> 01:23:11,480
the end and these obviously don't have a

1898
01:23:07,908 --> 01:23:13,849
dot jpg so you can pass in when you call

1899
01:23:11,479 --> 01:23:15,769
from CSV you can pass in a suffix it

1900
01:23:13,850 --> 01:23:17,390
says that the labels don't actually

1901
01:23:15,770 --> 01:23:22,820
contain the full file names you need to

1902
01:23:17,390 --> 01:23:24,710
add this to them okay so that's

1903
01:23:22,819 --> 01:23:28,429
basically all I need to do to set up my

1904
01:23:24,710 --> 01:23:31,819
data and as a lot of Europe noticed

1905
01:23:28,429 --> 01:23:33,890
during the week inside that data object

1906
01:23:31,819 --> 01:23:35,689
you can actually get access to the data

1907
01:23:33,890 --> 01:23:38,929
set like what the training data set by

1908
01:23:35,689 --> 01:23:41,059
same train yes and inside train des is a

1909
01:23:38,929 --> 01:23:41,510
whole bunch of things including the file

1910
01:23:41,060 --> 01:23:43,580
names

1911
01:23:41,510 --> 01:23:45,140
okay so train desktop file names

1912
01:23:43,579 --> 01:23:47,000
contains all of the file names of

1913
01:23:45,140 --> 01:23:48,800
everything in the training set and so

1914
01:23:47,000 --> 01:23:50,630
here's like one file name

1915
01:23:48,800 --> 01:23:54,140
okay so here's an example of one file

1916
01:23:50,630 --> 01:23:56,690
name so I can now go ahead and open that

1917
01:23:54,140 --> 01:23:58,039
file and take a look at it that's the

1918
01:23:56,689 --> 01:23:59,779
next thing I did was to try and

1919
01:23:58,039 --> 01:24:02,600
understand what my file my dataset looks

1920
01:23:59,779 --> 01:24:03,439
like and it found an adorable puppy so

1921
01:24:02,600 --> 01:24:06,380
that was very nice

1922
01:24:03,439 --> 01:24:08,149
so feeling good about this I also want

1923
01:24:06,380 --> 01:24:09,890
to know like how big of these files

1924
01:24:08,149 --> 01:24:12,289
right like how big are the images

1925
01:24:09,890 --> 01:24:13,909
because that's a key issue if they're

1926
01:24:12,289 --> 01:24:15,560
huge and then I have to think really

1927
01:24:13,908 --> 01:24:17,719
carefully about how to deal with huge

1928
01:24:15,560 --> 01:24:19,489
images that's really challenging if

1929
01:24:17,719 --> 01:24:22,579
they're tiny well that's also

1930
01:24:19,488 --> 01:24:25,968
challenging most of imagenet models are

1931
01:24:22,579 --> 01:24:28,908
trained on either 224 by 224 or 299 by

1932
01:24:25,969 --> 01:24:31,369
299 images so anytime you have images in

1933
01:24:28,908 --> 01:24:32,539
that kind of range that's that's really

1934
01:24:31,369 --> 01:24:34,488
hopeful you're probably not going to

1935
01:24:32,539 --> 01:24:36,350
have to do too much different in this

1936
01:24:34,488 --> 01:24:38,368
case the first image I looked at was

1937
01:24:36,350 --> 01:24:41,280
about the right size so I'm thinking of

1938
01:24:38,368 --> 01:24:42,988
pretty hopeful so what I did then is I

1939
01:24:41,279 --> 01:24:44,219
created a dictionary comprehension now

1940
01:24:42,988 --> 01:24:45,598
if you don't know about list

1941
01:24:44,219 --> 01:24:48,840
comprehensions and dictionary

1942
01:24:45,599 --> 01:24:51,179
comprehensions in Python go study them

1943
01:24:48,840 --> 01:24:54,000
they're the most useful thing super

1944
01:24:51,179 --> 01:24:55,260
handy you can see the basic idea here is

1945
01:24:54,000 --> 01:24:57,510
that are going through all of the files

1946
01:24:55,260 --> 01:25:01,770
and then putting a dictionary that map's

1947
01:24:57,510 --> 01:25:05,880
the name of the file to the size of that

1948
01:25:01,770 --> 01:25:07,500
file again this is a handy little Python

1949
01:25:05,880 --> 01:25:08,639
feature which I'll let you think learn

1950
01:25:07,500 --> 01:25:10,649
about during the week if you don't know

1951
01:25:08,639 --> 01:25:12,719
about it which is zip and using a

1952
01:25:10,649 --> 01:25:15,259
special star notation is never to take

1953
01:25:12,719 --> 01:25:19,920
this dictionary and turn it into the

1954
01:25:15,260 --> 01:25:22,079
rows and the columns and so I can now

1955
01:25:19,920 --> 01:25:26,219
turn those into num pay arrays and like

1956
01:25:22,079 --> 01:25:28,469
okay here are the first five rows sizes

1957
01:25:26,219 --> 01:25:30,329
for each of my images and then

1958
01:25:28,469 --> 01:25:32,130
matplotlib is something you want to be

1959
01:25:30,329 --> 01:25:33,479
very familiar with if you do any kind of

1960
01:25:32,130 --> 01:25:36,239
data science or machine learning in

1961
01:25:33,479 --> 01:25:39,779
python matplotlib we always refer to as

1962
01:25:36,238 --> 01:25:43,439
PLT as if this is a histogram and so I

1963
01:25:39,779 --> 01:25:45,420
got a histogram of the how high how many

1964
01:25:43,439 --> 01:25:46,738
rows there are in each image so you can

1965
01:25:45,420 --> 01:25:49,020
see here I'm kind of getting a sense

1966
01:25:46,738 --> 01:25:50,189
before I start doing any modeling I kind

1967
01:25:49,020 --> 01:25:52,170
of need to know what I'm modeling with

1968
01:25:50,189 --> 01:25:54,689
and I can see some of the images are

1969
01:25:52,170 --> 01:25:57,560
going to be like 2500 3000 pixels high

1970
01:25:54,689 --> 01:26:00,269
but most of them seem to be around 500

1971
01:25:57,560 --> 01:26:03,409
so given it so few of them were bigger

1972
01:26:00,270 --> 01:26:05,940
than a thousand I use standard numpy

1973
01:26:03,408 --> 01:26:08,460
slicing to just grab those at a smaller

1974
01:26:05,939 --> 01:26:10,408
than a thousand and histogram that just

1975
01:26:08,460 --> 01:26:12,118
to zoom in a little bit and I can see

1976
01:26:10,408 --> 01:26:15,089
here all right it looks like yet the

1977
01:26:12,118 --> 01:26:17,969
vast majority are around 500 and so this

1978
01:26:15,090 --> 01:26:19,319
actually also prints out the histogram

1979
01:26:17,969 --> 01:26:21,090
so I can actually go through and I can

1980
01:26:19,319 --> 01:26:23,849
see here for four thousand five hundred

1981
01:26:21,090 --> 01:26:28,440
of them are about 450 okay so I get

1982
01:26:23,849 --> 01:26:32,219
about that seems about anywhere so

1983
01:26:28,439 --> 01:26:36,799
generally how many images should we get

1984
01:26:32,219 --> 01:26:36,800
in the validation set is always a 20%

1985
01:26:38,679 --> 01:26:43,690
so the size of the validation set like

1986
01:26:44,408 --> 01:26:49,848
using 20% is fine unless you kind of

1987
01:26:48,019 --> 01:26:53,530
feeling like my data is my data sets

1988
01:26:49,849 --> 01:26:57,949
really small I'm not sure that's enough

1989
01:26:53,529 --> 01:26:59,538
you know like if you've got basically

1990
01:26:57,948 --> 01:27:01,488
think of it this way if you train like

1991
01:26:59,538 --> 01:27:02,958
the same model multiple times and you're

1992
01:27:01,488 --> 01:27:04,848
getting very different validation set

1993
01:27:02,958 --> 01:27:08,408
results and your validation sets kind of

1994
01:27:04,849 --> 01:27:10,369
small but smaller than a thousand or so

1995
01:27:08,408 --> 01:27:13,069
then it's going to be quite hard to

1996
01:27:10,368 --> 01:27:14,958
interpret how well you're doing now this

1997
01:27:13,069 --> 01:27:16,609
is particularly true like if you're like

1998
01:27:14,958 --> 01:27:19,670
if you care about the third decimal

1999
01:27:16,609 --> 01:27:21,348
place of accuracy and you've got like a

2000
01:27:19,670 --> 01:27:22,849
thousand things in your validation set

2001
01:27:21,349 --> 01:27:25,489
then you bring about like a single image

2002
01:27:22,849 --> 01:27:28,519
changing class is changing you know it's

2003
01:27:25,488 --> 01:27:31,399
what you're looking at so it's it really

2004
01:27:28,519 --> 01:27:35,389
depends on my cow accurate you have much

2005
01:27:31,399 --> 01:27:37,969
difference you care about I would say in

2006
01:27:35,389 --> 01:27:39,019
general like at the point where you care

2007
01:27:37,969 --> 01:27:41,090
about difference between like out of

2008
01:27:39,019 --> 01:27:44,510
0.01 and 0.02 like the second decimal

2009
01:27:41,090 --> 01:27:48,229
place you want that to represent like 10

2010
01:27:44,510 --> 01:27:51,619
or 20 roads you know like changing the

2011
01:27:48,229 --> 01:27:53,440
class of that 10 or 20 rows then that's

2012
01:27:51,618 --> 01:27:57,738
something you can be pretty confident of

2013
01:27:53,439 --> 01:28:00,228
so like most of the time you know give

2014
01:27:57,738 --> 01:28:04,098
them the data sizes we normally have 20

2015
01:28:00,229 --> 01:28:07,208
percent seems to work fine but yeah it's

2016
01:28:04,099 --> 01:28:09,260
it's it's kind of a it depends a lot on

2017
01:28:07,208 --> 01:28:12,979
specifically what you're doing and what

2018
01:28:09,260 --> 01:28:15,019
you care about and it's not it's not a

2019
01:28:12,979 --> 01:28:17,059
deep learning specific question either

2020
01:28:15,019 --> 01:28:17,989
you know so those who are interested in

2021
01:28:17,059 --> 01:28:19,639
this kind of thing we're going to look

2022
01:28:17,988 --> 01:28:23,208
into it a lot more detail in our machine

2023
01:28:19,639 --> 01:28:28,458
learning course which will also be

2024
01:28:23,208 --> 01:28:29,868
available online ok so I did the same

2025
01:28:28,458 --> 01:28:31,698
thing for the columns just to make sure

2026
01:28:29,868 --> 01:28:33,408
that these aren't like super wide and

2027
01:28:31,698 --> 01:28:35,029
I've got similar results and checked in

2028
01:28:33,408 --> 01:28:37,629
and again found they're kind of like 4

2029
01:28:35,029 --> 01:28:39,828
or 500 seem to be about the average size

2030
01:28:37,630 --> 01:28:41,239
so based on all of that I kind of

2031
01:28:39,828 --> 01:28:43,279
thought ok this looks like a pretty

2032
01:28:41,238 --> 01:28:44,928
normal kind of image data set that I can

2033
01:28:43,279 --> 01:28:46,969
probably use pretty normal kinds of

2034
01:28:44,929 --> 01:28:48,319
models on I was also particularly

2035
01:28:46,969 --> 01:28:48,630
encouraged to see that when I looked at

2036
01:28:48,319 --> 01:28:51,059
the

2037
01:28:48,630 --> 01:28:53,039
that the dog like takes up most of the

2038
01:28:51,060 --> 01:28:56,220
frame right so I'm not too worried about

2039
01:28:53,039 --> 01:28:57,840
like cropping problems you know if the

2040
01:28:56,220 --> 01:29:00,480
if the dog was just like a tiny little

2041
01:28:57,840 --> 01:29:02,909
piece of one little corner that I'd be

2042
01:29:00,479 --> 01:29:04,709
thinking about doing different you know

2043
01:29:02,909 --> 01:29:07,229
maybe zooming in a lot more or something

2044
01:29:04,710 --> 01:29:09,149
like a medical imaging that happens a

2045
01:29:07,229 --> 01:29:11,129
lot like often the tumor or the cell

2046
01:29:09,149 --> 01:29:14,069
whatever is like one tiny piece and

2047
01:29:11,130 --> 01:29:15,779
there's much more complex so yeah based

2048
01:29:14,069 --> 01:29:18,389
on all that and this morning I kind of

2049
01:29:15,779 --> 01:29:24,630
thought like okay this looks pretty

2050
01:29:18,390 --> 01:29:26,190
standard so I I went ahead and created a

2051
01:29:24,630 --> 01:29:28,140
little function called get data that

2052
01:29:26,189 --> 01:29:31,889
basically had my normal two lines of

2053
01:29:28,140 --> 01:29:35,520
code in it but I made it so I could

2054
01:29:31,890 --> 01:29:37,350
passed in a size and a batch size the

2055
01:29:35,520 --> 01:29:38,820
reason for this is that when I start

2056
01:29:37,350 --> 01:29:41,010
working with new data set I want

2057
01:29:38,819 --> 01:29:43,349
everything to go super fast and so if I

2058
01:29:41,010 --> 01:29:46,380
use small images it's going to go super

2059
01:29:43,350 --> 01:29:49,350
fast so I actually started out with size

2060
01:29:46,380 --> 01:29:51,390
equals 64 just to create some super

2061
01:29:49,350 --> 01:29:54,840
small images that just go like a second

2062
01:29:51,390 --> 01:29:57,420
to run through and see how it later on I

2063
01:29:54,840 --> 01:29:59,310
started using some big images and some

2064
01:29:57,420 --> 01:30:01,140
and some also some bigger architectures

2065
01:29:59,310 --> 01:30:02,970
at which point I started running out of

2066
01:30:01,140 --> 01:30:05,910
GPU memory so I started getting these

2067
01:30:02,970 --> 01:30:08,220
errors saying CUDA out of memory error

2068
01:30:05,909 --> 01:30:10,109
when you get a CUDA out of memory error

2069
01:30:08,220 --> 01:30:13,770
the first thing you need to do is to go

2070
01:30:10,109 --> 01:30:15,449
kernel restart once you get a code an

2071
01:30:13,770 --> 01:30:17,130
out of memory error on your GPU you

2072
01:30:15,449 --> 01:30:18,779
can't really recover from it right

2073
01:30:17,130 --> 01:30:22,079
doesn't matter what you do you know you

2074
01:30:18,779 --> 01:30:24,119
have to go restart and once I've

2075
01:30:22,079 --> 01:30:26,430
restarted I then just changed my batch

2076
01:30:24,119 --> 01:30:30,779
size to something smaller so when you

2077
01:30:26,430 --> 01:30:34,800
call create your data object you can

2078
01:30:30,779 --> 01:30:37,800
pass in a batch size parameter okay and

2079
01:30:34,800 --> 01:30:39,239
like i normally use 64 until i hit

2080
01:30:37,800 --> 01:30:40,980
something that says out of memory and

2081
01:30:39,239 --> 01:30:42,389
then i'll just have it and if i still

2082
01:30:40,979 --> 01:30:45,929
get out of memory I was hobbit again

2083
01:30:42,390 --> 01:30:47,910
okay so that's where I created this to

2084
01:30:45,930 --> 01:30:50,310
allow me to like start making my size as

2085
01:30:47,909 --> 01:30:51,449
bigger as I looked into it more and you

2086
01:30:50,310 --> 01:30:55,289
know as I started running out of memory

2087
01:30:51,449 --> 01:30:57,809
to decrease my batch size so at this

2088
01:30:55,289 --> 01:30:59,399
point you know I went through this a

2089
01:30:57,810 --> 01:31:01,350
couple of iterations but I basically

2090
01:30:59,399 --> 01:31:02,319
found everything was working fine so

2091
01:31:01,350 --> 01:31:07,900
once it's working fine

2092
01:31:02,319 --> 01:31:10,000
set size 2 to 24 and I created my you

2093
01:31:07,899 --> 01:31:11,679
know pre-compute equals true first time

2094
01:31:10,000 --> 01:31:13,810
I did that it took a minute to create

2095
01:31:11,680 --> 01:31:15,700
the precomputed activations and then it

2096
01:31:13,810 --> 01:31:17,230
ran through this in about 4 or 5 seconds

2097
01:31:15,699 --> 01:31:18,460
and you can see I was getting

2098
01:31:17,229 --> 01:31:21,669
eighty-three percent accuracy

2099
01:31:18,460 --> 01:31:24,069
now remember accuracy means it's it's

2100
01:31:21,670 --> 01:31:26,170
exactly right and so it's predicting out

2101
01:31:24,069 --> 01:31:27,819
of a hundred and twenty categories it's

2102
01:31:26,170 --> 01:31:30,850
predicting exactly right so when you see

2103
01:31:27,819 --> 01:31:33,489
something with two classes is you know

2104
01:31:30,850 --> 01:31:36,010
80% accurate versus something with 120

2105
01:31:33,489 --> 01:31:39,130
classes is 80% accurate they're very

2106
01:31:36,010 --> 01:31:41,320
different levels you know so when I saw

2107
01:31:39,130 --> 01:31:43,930
like eighty-three percent accuracy with

2108
01:31:41,319 --> 01:31:45,460
just a pre computed classify and OData

2109
01:31:43,930 --> 01:31:46,690
augmentation though I'm freezing

2110
01:31:45,460 --> 01:31:49,779
anything else

2111
01:31:46,689 --> 01:31:54,099
across 120 classes of the oh this looks

2112
01:31:49,779 --> 01:31:56,380
good right so um then I just kind of

2113
01:31:54,100 --> 01:32:01,060
kept going throughout at all standard

2114
01:31:56,380 --> 01:32:06,670
process right so then I turn precompute

2115
01:32:01,060 --> 01:32:10,090
off okay and cycle length equals one and

2116
01:32:06,670 --> 01:32:15,399
I started doing a few more cycles few

2117
01:32:10,090 --> 01:32:19,500
more epochs so remember an epoch is one

2118
01:32:15,399 --> 01:32:22,479
pass through the data and a cycle is

2119
01:32:19,500 --> 01:32:24,909
however many epochs you said is in a

2120
01:32:22,479 --> 01:32:26,529
cycle it's one it's the learning rate

2121
01:32:24,909 --> 01:32:29,289
going from the top that you asked for

2122
01:32:26,529 --> 01:32:31,989
all the way down so since here cycle

2123
01:32:29,289 --> 01:32:35,670
length equals one a cycle in an epoch at

2124
01:32:31,989 --> 01:32:39,489
the same okay so I did I tried a few

2125
01:32:35,670 --> 01:32:41,140
epochs I did actually do the learning

2126
01:32:39,489 --> 01:32:43,719
rate finder and I found one in a two

2127
01:32:41,140 --> 01:32:46,690
again looked fine it often looks fine

2128
01:32:43,720 --> 01:32:48,460
and I found it kind of kept improving so

2129
01:32:46,689 --> 01:32:54,219
I tried five epochs and I found my

2130
01:32:48,460 --> 01:32:56,980
accuracy getting better so then I saved

2131
01:32:54,220 --> 01:32:58,480
that and I tried something which we

2132
01:32:56,979 --> 01:33:01,829
haven't looked at before but it's kind

2133
01:32:58,479 --> 01:33:05,799
of cool if you train something on a

2134
01:33:01,829 --> 01:33:10,059
smaller size you can then actually call

2135
01:33:05,800 --> 01:33:12,369
learned set data and pass in a larger

2136
01:33:10,060 --> 01:33:14,710
size data set and that's gonna take your

2137
01:33:12,369 --> 01:33:16,389
model however it's trained so far and

2138
01:33:14,710 --> 01:33:20,439
it's going to let you can

2139
01:33:16,389 --> 01:33:22,599
in you to train on on larger images and

2140
01:33:20,439 --> 01:33:25,659
I tell you something amazing

2141
01:33:22,599 --> 01:33:26,918
this actually is another way you can get

2142
01:33:25,658 --> 01:33:29,078
state-of-the-art results and I've never

2143
01:33:26,918 --> 01:33:30,880
seen this written in any paper or

2144
01:33:29,078 --> 01:33:34,268
discussed anywhere as far as I know this

2145
01:33:30,880 --> 01:33:36,248
is a new insight basically I've got a

2146
01:33:34,269 --> 01:33:38,349
pre trained model which in this case

2147
01:33:36,248 --> 01:33:41,618
I've trained a few epochs with the size

2148
01:33:38,349 --> 01:33:43,748
of 224 by 224 and I'm now going to do a

2149
01:33:41,618 --> 01:33:47,408
few more air pops with the size of 299

2150
01:33:43,748 --> 01:33:49,090
by 299 now I've gotten very little data

2151
01:33:47,408 --> 01:33:52,208
cut out by deep learning standards only

2152
01:33:49,090 --> 01:33:55,179
about 10,000 images right so with a 224

2153
01:33:52,208 --> 01:33:57,880
by 224 I kind of built this these final

2154
01:33:55,179 --> 01:34:01,328
layers to try to find things that work

2155
01:33:57,880 --> 01:34:05,918
well to 24 but to 24 but I go to 299 by

2156
01:34:01,328 --> 01:34:07,238
299 I basically if I over fit before I'm

2157
01:34:05,918 --> 01:34:08,948
definitely not going to over fit now

2158
01:34:07,238 --> 01:34:10,288
might have changed the size of my images

2159
01:34:08,948 --> 01:34:13,268
they're kind of like totally different

2160
01:34:10,288 --> 01:34:14,859
but like conceptually they're still

2161
01:34:13,269 --> 01:34:16,929
picked the same kinds of pictures are

2162
01:34:14,859 --> 01:34:19,089
the same kinds of things so I found this

2163
01:34:16,929 --> 01:34:21,219
trick of like starting training on small

2164
01:34:19,090 --> 01:34:22,809
images for a few a box and then

2165
01:34:21,219 --> 01:34:25,208
switching to bigger images and

2166
01:34:22,809 --> 01:34:28,708
continuing training is an amazingly

2167
01:34:25,208 --> 01:34:32,590
effective way to avoid overfitting and

2168
01:34:28,708 --> 01:34:34,569
it's like it's so easy and so obvious I

2169
01:34:32,590 --> 01:34:36,069
don't understand why it's never been

2170
01:34:34,569 --> 01:34:37,478
written about before maybe it's in some

2171
01:34:36,069 --> 01:34:44,259
paper somewhere and I haven't found it

2172
01:34:37,479 --> 01:34:46,300
but it's I haven't seen it would it be

2173
01:34:44,260 --> 01:34:49,719
possible to do the same thing on using

2174
01:34:46,300 --> 01:34:53,708
let's take a resort our disposal to feed

2175
01:34:49,719 --> 01:34:56,010
a different size yeah I think so like as

2176
01:34:53,708 --> 01:34:57,760
long as you use one of these more modern

2177
01:34:56,010 --> 01:35:00,340
architectures what we call fully

2178
01:34:57,760 --> 01:35:02,979
convolutional architectures which means

2179
01:35:00,340 --> 01:35:04,389
not vgg and you'll see we don't use vgg

2180
01:35:02,979 --> 01:35:06,208
in this course because it doesn't have

2181
01:35:04,389 --> 01:35:07,840
this property but most of the

2182
01:35:06,208 --> 01:35:09,550
architectures developed in the last

2183
01:35:07,840 --> 01:35:13,319
couple of years can handle pretty much

2184
01:35:09,550 --> 01:35:18,189
arbitrary sizes yeah be worth trying

2185
01:35:13,319 --> 01:35:20,109
yeah I think it ought to work okay so I

2186
01:35:18,189 --> 01:35:21,280
call get data again remember get data is

2187
01:35:20,109 --> 01:35:22,929
the just a little function that I

2188
01:35:21,279 --> 01:35:24,668
created back up here right get data is

2189
01:35:22,929 --> 01:35:27,208
just this little function that's oh I

2190
01:35:24,668 --> 01:35:31,898
just passed a different size to it and

2191
01:35:27,208 --> 01:35:33,668
so I call freeze just to make sure that

2192
01:35:31,899 --> 01:35:35,709
but everything so the last layer is

2193
01:35:33,668 --> 01:35:37,809
frozen I mean it actually already was at

2194
01:35:35,708 --> 01:35:40,050
its point that really doing a thing

2195
01:35:37,809 --> 01:35:40,050
and

2196
01:35:42,149 --> 01:35:46,589
you can see now with free compute off

2197
01:35:44,670 --> 01:35:48,510
I've now got that data augmentation

2198
01:35:46,590 --> 01:35:51,750
working so I kind of run a few more a

2199
01:35:48,510 --> 01:35:54,420
pox and what I notice here is that the

2200
01:35:51,750 --> 01:35:56,340
loss to my training set and the loss of

2201
01:35:54,420 --> 01:35:58,619
my validation set my validation set loss

2202
01:35:56,340 --> 01:36:00,929
is a lot lower than my training set this

2203
01:35:58,619 --> 01:36:02,880
is still just training the last layer so

2204
01:36:00,929 --> 01:36:05,279
what this is telling me is I'm under

2205
01:36:02,880 --> 01:36:07,469
fitting right and so from under fitting

2206
01:36:05,279 --> 01:36:08,849
it means this cycle length equals one is

2207
01:36:07,469 --> 01:36:10,800
too short it means it's like finding

2208
01:36:08,850 --> 01:36:12,329
something better popped with popping out

2209
01:36:10,800 --> 01:36:15,750
and it's like never getting a chance to

2210
01:36:12,329 --> 01:36:18,539
zoom in properly so then I'd set cycle

2211
01:36:15,750 --> 01:36:21,270
mod equals two to give it more time so

2212
01:36:18,539 --> 01:36:23,010
like the first time is one epoch the

2213
01:36:21,270 --> 01:36:26,070
second one is two epochs

2214
01:36:23,010 --> 01:36:28,350
the third one is for epochs and you can

2215
01:36:26,069 --> 01:36:30,750
see now the validation train and

2216
01:36:28,350 --> 01:36:32,130
training are about the same okay so

2217
01:36:30,750 --> 01:36:34,800
that's kind of thinking yeah this is

2218
01:36:32,130 --> 01:36:36,560
this is about the right track and so

2219
01:36:34,800 --> 01:36:38,489
then I tried using test time

2220
01:36:36,560 --> 01:36:40,679
augmentation to see if that gets any

2221
01:36:38,488 --> 01:36:44,009
better still didn't actually help a hell

2222
01:36:40,679 --> 01:36:46,020
of a lot just a tiny bit and just kind

2223
01:36:44,010 --> 01:36:48,630
of at this point I think here this is

2224
01:36:46,020 --> 01:36:50,550
nearly done so I just did it like you

2225
01:36:48,630 --> 01:36:53,190
know one more cycle of two to see if it

2226
01:36:50,550 --> 01:36:55,679
got any better and it did get a little

2227
01:36:53,189 --> 01:36:57,599
bit better and then I'm like okay that

2228
01:36:55,679 --> 01:37:02,539
looks pretty good

2229
01:36:57,600 --> 01:37:05,520
I've got a validation set lost 0.199

2230
01:37:02,539 --> 01:37:08,010
and so your Lotus here actually you

2231
01:37:05,520 --> 01:37:09,630
haven't tried unfreezing the reason why

2232
01:37:08,010 --> 01:37:11,369
I was going to try to unfreezing and

2233
01:37:09,630 --> 01:37:13,890
training more it didn't get any better

2234
01:37:11,369 --> 01:37:16,469
and so the reason for this clearly is

2235
01:37:13,890 --> 01:37:19,230
that this data set is so similar the

2236
01:37:16,469 --> 01:37:20,789
image net that the training that

2237
01:37:19,229 --> 01:37:23,579
convolutional layers actually doesn't

2238
01:37:20,789 --> 01:37:25,229
help in the slightest and actually when

2239
01:37:23,579 --> 01:37:27,979
I loaded up into it it turns out that

2240
01:37:25,229 --> 01:37:30,750
this competition is actually using a

2241
01:37:27,979 --> 01:37:33,359
subset of improve image net so that's

2242
01:37:30,750 --> 01:37:36,109
okay so that if we check this out point

2243
01:37:33,359 --> 01:37:38,549
one nine nine against the leaderboard

2244
01:37:36,109 --> 01:37:40,619
this is only a playground competition so

2245
01:37:38,550 --> 01:37:44,779
it's not like the best of here but you

2246
01:37:40,619 --> 01:37:48,109
know it's still interesting it gets us

2247
01:37:44,779 --> 01:37:52,050
somewhere around ten thrillers okay and

2248
01:37:48,109 --> 01:37:53,420
in fact we're competing against I

2249
01:37:52,050 --> 01:37:55,340
noticed other

2250
01:37:53,420 --> 01:37:58,880
the first AI student this is a first AI

2251
01:37:55,340 --> 01:38:00,890
student these people up here I know they

2252
01:37:58,880 --> 01:38:02,060
actually posted that they cheated they

2253
01:38:00,890 --> 01:38:07,429
actually went you downloaded the

2254
01:38:02,060 --> 01:38:08,630
original images and train to that so and

2255
01:38:07,429 --> 01:38:10,489
this is why this is a playground

2256
01:38:08,630 --> 01:38:12,289
competition they call it it's not it's

2257
01:38:10,488 --> 01:38:14,269
not real right you know it's just to

2258
01:38:12,289 --> 01:38:18,109
allow us to try things out but you can

2259
01:38:14,270 --> 01:38:20,270
basically see out of two hundred and

2260
01:38:18,109 --> 01:38:23,299
something people where you know we're

2261
01:38:20,270 --> 01:38:25,429
getting some very good results without

2262
01:38:23,300 --> 01:38:26,659
doing anything remotely interesting or

2263
01:38:25,429 --> 01:38:27,739
clever and we haven't even used the

2264
01:38:26,659 --> 01:38:29,389
whole data set you're going to use to

2265
01:38:27,738 --> 01:38:31,698
eighty percent of it like to get a

2266
01:38:29,390 --> 01:38:33,679
better result I would go back and remove

2267
01:38:31,698 --> 01:38:36,109
that validation set and just rerun the

2268
01:38:33,679 --> 01:38:37,100
same steps and then submit that exact

2269
01:38:36,109 --> 01:38:48,649
let's just use it under percent of the

2270
01:38:37,100 --> 01:38:50,480
data I have three questions the first

2271
01:38:48,649 --> 01:38:53,779
one is like that class in this case is

2272
01:38:50,479 --> 01:38:56,928
very it's not balanced

2273
01:38:53,779 --> 01:38:58,519
instead unbalanced like it's not totally

2274
01:38:56,929 --> 01:39:02,420
balanced but it's not bad right it's

2275
01:38:58,520 --> 01:39:04,670
like between sixty and a hundred like

2276
01:39:02,420 --> 01:39:05,869
it's it's it's it's not unbalanced

2277
01:39:04,670 --> 01:39:06,079
enough that I would give it a second

2278
01:39:05,869 --> 01:39:14,599
thought

2279
01:39:06,079 --> 01:39:16,069
okay yeah let's get to that later in

2280
01:39:14,600 --> 01:39:18,710
this course and don't let me forget

2281
01:39:16,069 --> 01:39:20,090
right the short answer is that there was

2282
01:39:18,710 --> 01:39:21,980
a recent list the paper came out about

2283
01:39:20,090 --> 01:39:23,390
two or three weeks ago on this and it

2284
01:39:21,979 --> 01:39:25,639
said the best way to deal with very

2285
01:39:23,390 --> 01:39:35,600
unbalanced data sets is to basically

2286
01:39:25,640 --> 01:39:38,660
make copies of the rare cases yeah my

2287
01:39:35,600 --> 01:39:40,310
second question is I want to pin down a

2288
01:39:38,659 --> 01:39:44,389
difference between creation he read was

2289
01:39:40,310 --> 01:39:47,179
and so you have these two options right

2290
01:39:44,390 --> 01:39:49,130
so when you beginning I did an

2291
01:39:47,179 --> 01:39:52,369
optimization use that pre computed it

2292
01:39:49,130 --> 01:39:54,289
was true by not using layers right right

2293
01:39:52,369 --> 01:39:55,670
so it's not only they frozen their pre

2294
01:39:54,289 --> 01:39:59,079
computed so the data augmentation

2295
01:39:55,670 --> 01:39:59,079
doesn't do anything at that point

2296
01:39:59,409 --> 01:40:06,050
right before you outcries everything

2297
01:40:03,579 --> 01:40:08,130
what as examples you and IV only you

2298
01:40:06,050 --> 01:40:10,650
only on freeze

2299
01:40:08,130 --> 01:40:12,630
so we're going to learn more about the

2300
01:40:10,649 --> 01:40:14,458
details as we look into the the math and

2301
01:40:12,630 --> 01:40:16,979
stuff in coming lessons but basically

2302
01:40:14,458 --> 01:40:20,550
what happened was we started with a pre

2303
01:40:16,979 --> 01:40:23,400
trained network right which was kind of

2304
01:40:20,550 --> 01:40:27,538
finding activations that had these kind

2305
01:40:23,399 --> 01:40:29,399
of rich features and we were adding then

2306
01:40:27,538 --> 01:40:33,658
we add a couple of layers on the end of

2307
01:40:29,399 --> 01:40:36,658
it which which start out random and so

2308
01:40:33,658 --> 01:40:38,549
with fries equals with with everything

2309
01:40:36,658 --> 01:40:40,828
frozen and indeed with pre compute

2310
01:40:38,550 --> 01:40:42,538
equals true all we're learning is told

2311
01:40:40,828 --> 01:40:45,179
is those couple of layers that we've

2312
01:40:42,538 --> 01:40:47,639
added and so with pre compute equals

2313
01:40:45,179 --> 01:40:49,769
true we actually pretty calculate like

2314
01:40:47,639 --> 01:40:51,239
how much does this image have something

2315
01:40:49,769 --> 01:40:53,489
that looks like this a ball one looks

2316
01:40:51,238 --> 01:40:55,078
like this face and so forth and

2317
01:40:53,488 --> 01:40:56,728
therefore data augmentation doesn't do

2318
01:40:55,078 --> 01:40:59,429
anything with pre compute equals true

2319
01:40:56,729 --> 01:41:01,079
because you know we're actually showing

2320
01:40:59,429 --> 01:41:03,510
exactly the same activations each time

2321
01:41:01,078 --> 01:41:06,359
we can then set pre compute equals false

2322
01:41:03,510 --> 01:41:09,208
which means it's still only training

2323
01:41:06,359 --> 01:41:11,698
those last two layers that we added it's

2324
01:41:09,208 --> 01:41:12,929
still frozen but data augmentations now

2325
01:41:11,698 --> 01:41:14,428
working because it's actually going

2326
01:41:12,929 --> 01:41:17,248
through and recalculating all of the

2327
01:41:14,429 --> 01:41:19,949
activations from scratch and then

2328
01:41:17,248 --> 01:41:21,359
finally when we unfreeze that's actually

2329
01:41:19,948 --> 01:41:23,089
saying okay now you can go ahead and

2330
01:41:21,359 --> 01:41:28,518
change all of these earlier

2331
01:41:23,090 --> 01:41:28,519
convolutional filters so well you just

2332
01:41:28,998 --> 01:41:33,748
so the only reason to have pre compute

2333
01:41:31,679 --> 01:41:35,670
equals true is it's just much faster so

2334
01:41:33,748 --> 01:41:38,248
it's like it is it's about you know ten

2335
01:41:35,670 --> 01:41:39,359
or more times faster so particularly if

2336
01:41:38,248 --> 01:41:42,599
you're working with like quite a large

2337
01:41:39,359 --> 01:41:45,719
data set you know it can save quite a

2338
01:41:42,599 --> 01:41:49,498
bit of time but it's never there's no

2339
01:41:45,719 --> 01:41:51,239
like companies like accuracy reason ever

2340
01:41:49,498 --> 01:41:53,038
to use pre computed calls true it's just

2341
01:41:51,238 --> 01:41:55,589
a it's just a shortcut it's also like

2342
01:41:53,038 --> 01:41:57,658
quite handy if you're like throwing

2343
01:41:55,590 --> 01:42:01,699
together a quick model you know it can

2344
01:41:57,658 --> 01:42:03,618
take a few seconds to create

2345
01:42:01,698 --> 01:42:07,279
my last question which I think you

2346
01:42:03,618 --> 01:42:11,078
answer is I don't like your suggestions

2347
01:42:07,279 --> 01:42:15,228
to build a model you have this aged yeah

2348
01:42:11,078 --> 01:42:19,759
what if would you like we just wanted

2349
01:42:15,229 --> 01:42:24,170
one initial setting without these like

2350
01:42:19,760 --> 01:42:25,849
checking after each I mean if you want

2351
01:42:24,170 --> 01:42:27,559
it like if your question is like is

2352
01:42:25,849 --> 01:42:29,719
there some shorter version of this

2353
01:42:27,559 --> 01:42:33,820
that's like a bit quicker and easier I

2354
01:42:29,719 --> 01:42:33,819
could like to lead a few things here

2355
01:42:39,158 --> 01:42:43,819
okay I think this is a kind of a minimal

2356
01:42:41,988 --> 01:42:45,768
version to get you a very good result

2357
01:42:43,819 --> 01:42:47,448
which is like don't worry about pre

2358
01:42:45,769 --> 01:42:49,550
compute equals true because that's just

2359
01:42:47,448 --> 01:42:52,549
saving a little bit of time you know so

2360
01:42:49,550 --> 01:42:56,029
so I still suggest use LR find at the

2361
01:42:52,550 --> 01:42:57,650
start to find a good learning rate by

2362
01:42:56,029 --> 01:42:59,059
default everything is frozen from the

2363
01:42:57,649 --> 01:43:01,158
start so then you can just go ahead and

2364
01:42:59,059 --> 01:43:05,208
run two or three epochs or cyclic

2365
01:43:01,158 --> 01:43:07,038
Nichols one unfreeze and then train the

2366
01:43:05,208 --> 01:43:09,078
rest of the network with differential

2367
01:43:07,038 --> 01:43:13,279
learning rates so it's basically three

2368
01:43:09,078 --> 01:43:15,408
steps learning rate finder trained

2369
01:43:13,279 --> 01:43:18,559
frozen network with cycle methods one

2370
01:43:15,408 --> 01:43:20,478
and then trained unfrozen network with

2371
01:43:18,559 --> 01:43:22,969
differential learning rates and cycle

2372
01:43:20,479 --> 01:43:26,929
molecules too so like that's something

2373
01:43:22,969 --> 01:43:30,828
you could turn into I guess five or six

2374
01:43:26,929 --> 01:43:34,219
lines of code at all I think it's a

2375
01:43:30,828 --> 01:43:36,978
question provide your own mix book by

2376
01:43:34,219 --> 01:43:39,408
reusing the batch size does the only at

2377
01:43:36,979 --> 01:43:42,139
better speed of training yeah pretty

2378
01:43:39,408 --> 01:43:43,429
much so each batch and again we're going

2379
01:43:42,139 --> 01:43:45,739
to see like all this stuff about

2380
01:43:43,429 --> 01:43:47,328
precomputing batch sizes we dig into the

2381
01:43:45,738 --> 01:43:49,129
details of the algorithms it's going to

2382
01:43:47,328 --> 01:43:53,058
make a lot more sense intuitively but

2383
01:43:49,130 --> 01:43:56,150
basically if you're showing it less

2384
01:43:53,059 --> 01:43:58,519
images each time then it's calculating

2385
01:43:56,149 --> 01:44:00,288
the gradient with less images which

2386
01:43:58,519 --> 01:44:02,179
means it's less accurate which means

2387
01:44:00,288 --> 01:44:04,788
like knowing which direction to go and

2388
01:44:02,179 --> 01:44:06,590
how far to go in that direction is less

2389
01:44:04,788 --> 01:44:07,158
accurate so as you make the batch size

2390
01:44:06,590 --> 01:44:09,319
smaller

2391
01:44:07,158 --> 01:44:11,429
you're basically making it kind of more

2392
01:44:09,319 --> 01:44:18,139
volatile it's

2393
01:44:11,429 --> 01:44:20,158
kind of like it kind of impacts the

2394
01:44:18,139 --> 01:44:22,260
optimal learning rate that you would

2395
01:44:20,158 --> 01:44:24,388
need to use but in practice where only

2396
01:44:22,260 --> 01:44:26,909
you know I generally find only dividing

2397
01:44:24,389 --> 01:44:28,199
with the batch size by like 2 or 4 it

2398
01:44:26,908 --> 01:44:30,868
doesn't seem to change things very much

2399
01:44:28,198 --> 01:44:33,328
should I reveals the learning rate of

2400
01:44:30,868 --> 01:44:34,978
quality me if you if you change the

2401
01:44:33,328 --> 01:44:36,719
batch size by much you can rerun the

2402
01:44:34,979 --> 01:44:38,760
learning rate finder to see if it's

2403
01:44:36,719 --> 01:44:40,078
changed if I match but it it I was in

2404
01:44:38,760 --> 01:44:42,179
for only generally looking at like a

2405
01:44:40,078 --> 01:44:43,498
power of 10 it probably is not going to

2406
01:44:42,179 --> 01:44:47,149
change the it's not that you can't

2407
01:44:43,498 --> 01:44:47,149
because plus back there

2408
01:44:48,090 --> 01:44:51,779
this is sort of a conceptual basic

2409
01:44:50,429 --> 01:44:53,699
questions we're going back to the

2410
01:44:51,779 --> 01:44:55,679
previous night where you should put in

2411
01:44:53,698 --> 01:44:57,839
the thought behind sorry yeah this is

2412
01:44:55,679 --> 01:44:59,940
well one for conceptual so a basic

2413
01:44:57,840 --> 01:45:01,440
question we've actually really slide

2414
01:44:59,939 --> 01:45:07,888
where you should what the different

2415
01:45:01,439 --> 01:45:11,428
layers were doing yes from this slide I

2416
01:45:07,889 --> 01:45:12,960
understand right the meaning of sync the

2417
01:45:11,429 --> 01:45:16,289
third column relative to the fourth

2418
01:45:12,960 --> 01:45:19,500
column is that what you're interpreting

2419
01:45:16,289 --> 01:45:23,010
what the layer is doing based on what

2420
01:45:19,500 --> 01:45:24,510
the image is actually yeah so we're

2421
01:45:23,010 --> 01:45:26,550
going to look at this in more detail so

2422
01:45:24,510 --> 01:45:28,920
these these gray ones basically say this

2423
01:45:26,550 --> 01:45:31,170
is kind of what the filter looks like so

2424
01:45:28,920 --> 01:45:32,730
on the first layer you can see exactly

2425
01:45:31,170 --> 01:45:34,920
what the filter looks like because the

2426
01:45:32,729 --> 01:45:36,448
input to it of pixels right so you can

2427
01:45:34,920 --> 01:45:38,940
absolutely say and remember we looked at

2428
01:45:36,448 --> 01:45:41,039
what a convolutional kernel was like was

2429
01:45:38,939 --> 01:45:43,019
that three by three thing so this look

2430
01:45:41,039 --> 01:45:44,429
like there's seven by seven kernels you

2431
01:45:43,020 --> 01:45:47,369
can say this is actually what it looks

2432
01:45:44,429 --> 01:45:50,449
like but later on it's combined you know

2433
01:45:47,368 --> 01:45:52,408
the the the input to it are themselves

2434
01:45:50,448 --> 01:45:54,178
activations which are combinations of

2435
01:45:52,408 --> 01:45:56,819
activations relation to activations so

2436
01:45:54,179 --> 01:45:58,710
you can't draw it but there's clever

2437
01:45:56,819 --> 01:46:00,569
technique that I learned focus created

2438
01:45:58,710 --> 01:46:02,460
which allowed them to say this is kind

2439
01:46:00,569 --> 01:46:04,769
of what the filters tended to look like

2440
01:46:02,460 --> 01:46:06,569
on average alright so this is kind of

2441
01:46:04,770 --> 01:46:10,110
what the photos look like and then here

2442
01:46:06,569 --> 01:46:14,368
is specific examples of patches of image

2443
01:46:10,109 --> 01:46:16,229
which activated that filter highly so

2444
01:46:14,368 --> 01:46:17,848
yet the pictures are the ones that I

2445
01:46:16,229 --> 01:46:21,928
kind of find more useful because it

2446
01:46:17,849 --> 01:46:35,069
tells you this kernel is kind of a mini

2447
01:46:21,929 --> 01:46:40,289
cycle we all find right how do we know

2448
01:46:35,069 --> 01:46:41,789
that's it well we'll come back well we

2449
01:46:40,289 --> 01:46:45,210
may come back to that if not in this

2450
01:46:41,789 --> 01:46:48,029
part in the next part that probably a

2451
01:46:45,210 --> 01:46:49,859
part two actually because this paper

2452
01:46:48,029 --> 01:46:50,908
this paper uses to create these things

2453
01:46:49,859 --> 01:46:53,308
this paper uses something called a

2454
01:46:50,908 --> 01:46:54,719
deconvolution which I'm pretty sure we

2455
01:46:53,309 --> 01:46:57,420
won't do in this part but we will do it

2456
01:46:54,719 --> 01:47:00,090
in part two so if you're interested

2457
01:46:57,420 --> 01:47:01,859
check out the paper it's it's in the

2458
01:47:00,090 --> 01:47:06,060
notebook has a link to it xyler in

2459
01:47:01,859 --> 01:47:10,159
Fergus it's a very clever technique and

2460
01:47:06,060 --> 01:47:10,160
not terribly intuitive

2461
01:47:10,880 --> 01:47:20,630
um right so so you mentioned that it was

2462
01:47:18,590 --> 01:47:22,340
good that the dog took up the full

2463
01:47:20,630 --> 01:47:24,409
picture and it would have been a problem

2464
01:47:22,340 --> 01:47:27,260
if it was kind of like off in one of the

2465
01:47:24,408 --> 01:47:29,599
corners in really tiny well what would

2466
01:47:27,260 --> 01:47:34,039
you what would you technique have been

2467
01:47:29,600 --> 01:47:36,260
to try to make that work something that

2468
01:47:34,039 --> 01:47:38,448
we'll learn about in part two but

2469
01:47:36,260 --> 01:47:41,539
basically there's a technique that

2470
01:47:38,448 --> 01:47:43,579
allows you to to kind of figure out

2471
01:47:41,539 --> 01:47:45,529
roughly which parts of an image and most

2472
01:47:43,579 --> 01:47:47,809
likely to have the interesting things in

2473
01:47:45,529 --> 01:47:50,059
them and then you can like crop out

2474
01:47:47,810 --> 01:47:52,250
those bits if you're interested in

2475
01:47:50,060 --> 01:47:55,730
learning about it we did cover it

2476
01:47:52,250 --> 01:47:58,250
briefly in lesson seven of part one but

2477
01:47:55,729 --> 01:48:01,609
I'm going to actually do it properly in

2478
01:47:58,250 --> 01:48:06,469
part two of this course because I didn't

2479
01:48:01,609 --> 01:48:07,849
really cover it thoroughly enough maybe

2480
01:48:06,469 --> 01:48:10,189
we'll find time to have a quick look at

2481
01:48:07,850 --> 01:48:11,300
it but we'll see I know your Nets

2482
01:48:10,189 --> 01:48:13,669
written some of the code that we need

2483
01:48:11,300 --> 01:48:15,699
already

2484
01:48:13,670 --> 01:48:15,699
ah

2485
01:48:16,719 --> 01:48:24,359
so once I have something like this

2486
01:48:18,810 --> 01:48:28,390
notebook that's basically working I can

2487
01:48:24,359 --> 01:48:32,139
immediately make it better by doing two

2488
01:48:28,390 --> 01:48:34,150
things assuming that the size image I

2489
01:48:32,140 --> 01:48:35,980
was using is smaller than the average

2490
01:48:34,149 --> 01:48:38,829
size of the image that we've been given

2491
01:48:35,979 --> 01:48:40,329
I can increase the size and as I showed

2492
01:48:38,829 --> 01:48:42,670
before with the dog breeds you can

2493
01:48:40,329 --> 01:48:45,519
actually increase it during training the

2494
01:48:42,670 --> 01:48:48,279
other thing I can do is to create is to

2495
01:48:45,520 --> 01:48:49,840
use a better architecture now an

2496
01:48:48,279 --> 01:48:51,880
architect we're going to talk a lot in

2497
01:48:49,840 --> 01:48:58,300
this course about architectures but

2498
01:48:51,880 --> 01:49:00,579
basically there are different ways of

2499
01:48:58,300 --> 01:49:02,230
putting together like what size

2500
01:49:00,579 --> 01:49:05,729
convolutional filters and how are they

2501
01:49:02,229 --> 01:49:07,809
connected to each other and so forth and

2502
01:49:05,729 --> 01:49:09,759
different architectures have different

2503
01:49:07,810 --> 01:49:11,650
like numbers of layers and sizes of

2504
01:49:09,760 --> 01:49:16,780
kernels and number of filters and so

2505
01:49:11,649 --> 01:49:19,049
forth and so there are some the one that

2506
01:49:16,779 --> 01:49:21,729
we've been using ResNet 34 is a great

2507
01:49:19,050 --> 01:49:23,770
starting point and often a good

2508
01:49:21,729 --> 01:49:25,479
finishing point because it's like it's

2509
01:49:23,770 --> 01:49:27,040
pretty it doesn't have too many

2510
01:49:25,479 --> 01:49:28,988
parameters often it works pretty well

2511
01:49:27,039 --> 01:49:32,560
with small amounts of data as we've seen

2512
01:49:28,988 --> 01:49:34,359
and so forth but there's actually an

2513
01:49:32,560 --> 01:49:37,630
architecture that I really like called

2514
01:49:34,359 --> 01:49:39,849
not res net but res next which was

2515
01:49:37,630 --> 01:49:44,590
actually the second-place winner in last

2516
01:49:39,850 --> 01:49:46,930
year's image net competition and like

2517
01:49:44,590 --> 01:49:49,300
ResNet you can put a number after the

2518
01:49:46,930 --> 01:49:52,510
res next to say like how big it is and

2519
01:49:49,300 --> 01:49:55,300
like my next step after resume 34 is

2520
01:49:52,510 --> 01:49:58,119
always res next 50 now you'll find res

2521
01:49:55,300 --> 01:50:01,719
next 50 takes like can take like twice

2522
01:49:58,119 --> 01:50:06,090
as long as ResNet 34 that can take like

2523
01:50:01,719 --> 01:50:08,260
2 to 4 times as much memory as retina 34

2524
01:50:06,090 --> 01:50:10,390
so what I wanted to do was I wanted to

2525
01:50:08,260 --> 01:50:12,909
rerun that previous notebook with res

2526
01:50:10,390 --> 01:50:15,699
next and increasing the image size to

2527
01:50:12,909 --> 01:50:18,219
turn on a node so here I just said

2528
01:50:15,698 --> 01:50:20,439
architecture equals res next 50 size

2529
01:50:18,219 --> 01:50:21,939
equals 299 and then I found that I had

2530
01:50:20,439 --> 01:50:25,000
to take the batch size all the way back

2531
01:50:21,939 --> 01:50:27,849
to 28 to get it to fit my GPU is 11 gig

2532
01:50:25,000 --> 01:50:30,550
if you're using AWS or cresol I think

2533
01:50:27,850 --> 01:50:32,800
they're like 12 gigs they might be

2534
01:50:30,550 --> 01:50:34,659
a bit higher but this is what I found I

2535
01:50:32,800 --> 01:50:36,699
had to do so then I this is literally a

2536
01:50:34,658 --> 01:50:39,308
copy of the previous notebook so you can

2537
01:50:36,698 --> 01:50:41,469
actually go file make a copy right and

2538
01:50:39,309 --> 01:50:45,400
then rerun it with with these different

2539
01:50:41,469 --> 01:50:47,529
parameters and so I deleted some of the

2540
01:50:45,399 --> 01:50:49,868
pros and some of the expiratory stuff to

2541
01:50:47,529 --> 01:50:52,750
see you know basically I said everything

2542
01:50:49,868 --> 01:50:54,549
else is the same all the same steps as

2543
01:50:52,750 --> 01:50:55,988
before there's my in fact you can kind

2544
01:50:54,550 --> 01:50:57,460
of see what this minimum service desk

2545
01:50:55,988 --> 01:50:59,138
looks like I didn't need to worry about

2546
01:50:57,460 --> 01:51:02,408
learning rate finder so I just left it

2547
01:50:59,139 --> 01:51:06,460
as is so transforms data equals loan

2548
01:51:02,408 --> 01:51:08,939
equals bit pre computed false feet with

2549
01:51:06,460 --> 01:51:12,130
cycle integrals one and freeze

2550
01:51:08,939 --> 01:51:14,738
differential learning rates bits and

2551
01:51:12,130 --> 01:51:17,618
more and you can see here I didn't do

2552
01:51:14,738 --> 01:51:19,539
the cycle mop thing because I found like

2553
01:51:17,618 --> 01:51:21,308
now that I'm using a bigger architecture

2554
01:51:19,539 --> 01:51:23,979
it's got more parameters it was

2555
01:51:21,309 --> 01:51:25,840
overfitting pretty quickly so rather

2556
01:51:23,979 --> 01:51:28,000
than like cycle length equals one never

2557
01:51:25,840 --> 01:51:29,920
finding the right spot it actually did

2558
01:51:28,000 --> 01:51:34,599
find the right spot and if I used longer

2559
01:51:29,920 --> 01:51:37,149
cycle legs I found that my validation

2560
01:51:34,599 --> 01:51:40,900
error was higher than my training error

2561
01:51:37,149 --> 01:51:43,868
it was over there so check us out though

2562
01:51:40,899 --> 01:51:47,368
by using these you know three steps

2563
01:51:43,868 --> 01:51:50,380
I got plus TTA 99.75

2564
01:51:47,368 --> 01:51:54,399
so what does that mean that means I have

2565
01:51:50,380 --> 01:51:58,779
one incorrect dog for incorrect cats and

2566
01:51:54,399 --> 01:52:02,049
when we look at the pictures of them my

2567
01:51:58,779 --> 01:52:04,179
incorrect dog has a cat now this one is

2568
01:52:02,050 --> 01:52:07,449
not a either this one is not either so

2569
01:52:04,179 --> 01:52:12,340
I've actually got one mistake and then

2570
01:52:07,448 --> 01:52:15,250
my incorrect dog is teeth right so like

2571
01:52:12,340 --> 01:52:18,639
we're at a point where we're now able to

2572
01:52:15,250 --> 01:52:23,408
train a classifier that's so good that

2573
01:52:18,639 --> 01:52:25,239
it has like basically one's dead right

2574
01:52:23,408 --> 01:52:27,848
and so when people say like we have

2575
01:52:25,238 --> 01:52:29,078
superhuman image performance now this is

2576
01:52:27,849 --> 01:52:31,380
kind of what they're talking about right

2577
01:52:29,078 --> 01:52:34,090
so did you actually when I looked at the

2578
01:52:31,380 --> 01:52:36,400
dog breed one I did this morning I was

2579
01:52:34,090 --> 01:52:39,719
like it was it was getting the dog

2580
01:52:36,399 --> 01:52:39,719
breeds much better than I ever could

2581
01:52:39,849 --> 01:52:43,750
so like hits this this is what we can

2582
01:52:42,130 --> 01:52:44,350
get to if you use a really modern

2583
01:52:43,750 --> 01:52:48,159
architect

2584
01:52:44,350 --> 01:52:51,789
like redneck and this suddenly took out

2585
01:52:48,159 --> 01:52:55,539
a tall way and remember don't like 20

2586
01:52:51,789 --> 01:53:02,640
minutes to Train so that's kind of where

2587
01:52:55,539 --> 01:53:04,119
we're up to so if you want to do

2588
01:53:02,640 --> 01:53:07,720
satellite imagery

2589
01:53:04,119 --> 01:53:09,970
instead right then it's the same thing

2590
01:53:07,720 --> 01:53:11,380
and in fact the the planet satellite

2591
01:53:09,970 --> 01:53:12,610
data sets already oh and Chris or if

2592
01:53:11,380 --> 01:53:17,319
you're using Chris or you can jump

2593
01:53:12,609 --> 01:53:20,019
straight there right and I just went

2594
01:53:17,319 --> 01:53:25,479
into this data stash planet and I can do

2595
01:53:20,020 --> 01:53:28,600
exactly the same thing right I can image

2596
01:53:25,479 --> 01:53:30,009
classifier from CSV right and you can

2597
01:53:28,600 --> 01:53:31,690
see these three lines are actually

2598
01:53:30,010 --> 01:53:34,239
exactly the same as my dog breed lines

2599
01:53:31,689 --> 01:53:37,000
you know how big how many lines are in

2600
01:53:34,239 --> 01:53:38,739
the file grab my validation indexes this

2601
01:53:37,000 --> 01:53:42,220
get data as you can see it's identical

2602
01:53:38,739 --> 01:53:44,800
except I've changed side on to top down

2603
01:53:42,220 --> 01:53:47,020
the satellite images about top down so I

2604
01:53:44,800 --> 01:53:49,779
can fit them vertically and they still

2605
01:53:47,020 --> 01:53:51,460
make sense right and so you can see here

2606
01:53:49,779 --> 01:53:55,239
I'm doing this trick round back to size

2607
01:53:51,460 --> 01:53:57,100
equals 64 and train a little bit first

2608
01:53:55,239 --> 01:53:59,229
learning rate find on right and

2609
01:53:57,100 --> 01:54:01,890
interestingly in this case you can see

2610
01:53:59,229 --> 01:54:03,669
it I want really high learning rates I

2611
01:54:01,890 --> 01:54:05,800
don't know what it is about this

2612
01:54:03,670 --> 01:54:07,810
particular data set this is true but

2613
01:54:05,800 --> 01:54:08,920
it's clearly I can use super high

2614
01:54:07,810 --> 01:54:12,789
learning rate so I use a lot here at a

2615
01:54:08,920 --> 01:54:13,329
point too and so I've trained for a

2616
01:54:12,789 --> 01:54:16,210
while

2617
01:54:13,329 --> 01:54:18,970
differential learning rates right and so

2618
01:54:16,210 --> 01:54:20,980
remember I said like if the data sets

2619
01:54:18,970 --> 01:54:23,500
very different to image net I probably

2620
01:54:20,979 --> 01:54:25,299
want to train those middle layers a lot

2621
01:54:23,500 --> 01:54:27,340
more so I'm using divided by three

2622
01:54:25,300 --> 01:54:28,930
rather than divided by ten all right the

2623
01:54:27,340 --> 01:54:33,430
other than that is the same thing cycle

2624
01:54:28,930 --> 01:54:34,510
Nauticals - all right and then I just

2625
01:54:33,430 --> 01:54:35,980
kind of keep an eye on it so you can

2626
01:54:34,510 --> 01:54:37,989
actually plot the loss if you go and

2627
01:54:35,979 --> 01:54:40,899
learned up shared a plot loss you can

2628
01:54:37,989 --> 01:54:43,149
see here that here's the first cycle is

2629
01:54:40,899 --> 01:54:45,219
the second cycle is the third cycle

2630
01:54:43,149 --> 01:54:46,960
right so you can see it gets better pops

2631
01:54:45,220 --> 01:54:48,220
out gets better pops out if better pops

2632
01:54:46,960 --> 01:54:50,159
out and each time it finds something

2633
01:54:48,220 --> 01:54:53,440
better than the last time

2634
01:54:50,159 --> 01:54:55,359
then set the size up to 128 and just

2635
01:54:53,439 --> 01:54:58,500
repeat exactly the last few steps and

2636
01:54:55,359 --> 01:55:02,170
then set up to 256

2637
01:54:58,500 --> 01:55:05,649
repeat the last two steps and then do

2638
01:55:02,170 --> 01:55:10,300
TTA and if you submit this and this gets

2639
01:55:05,649 --> 01:55:12,819
about 30th place in this competition so

2640
01:55:10,300 --> 01:55:14,590
these basic steps work super well this

2641
01:55:12,819 --> 01:55:19,630
this thing where I went all the way back

2642
01:55:14,590 --> 01:55:21,819
to a size of 64 I wouldn't do that if I

2643
01:55:19,630 --> 01:55:23,590
was doing like dogs and cats or dog

2644
01:55:21,819 --> 01:55:26,500
breeds because like this is so small

2645
01:55:23,590 --> 01:55:29,520
that if if the thing I was working on is

2646
01:55:26,500 --> 01:55:32,469
very similar to imagenet I would kind of

2647
01:55:29,520 --> 01:55:34,929
destroy those imagenet weights like 64

2648
01:55:32,469 --> 01:55:36,460
by 64 is so small but in this case the

2649
01:55:34,929 --> 01:55:38,590
satellite imagery data it's so different

2650
01:55:36,460 --> 01:55:39,460
to imagenet um you know I really found

2651
01:55:38,590 --> 01:55:43,300
that it worked pretty well

2652
01:55:39,460 --> 01:55:45,329
start right back to these tiny images it

2653
01:55:43,300 --> 01:55:48,610
really helped me to avoid overfitting

2654
01:55:45,329 --> 01:55:50,500
and interestingly using this kind of

2655
01:55:48,609 --> 01:55:53,859
approach I actually found that even with

2656
01:55:50,500 --> 01:55:55,929
using only 128 by 128 I was getting like

2657
01:55:53,859 --> 01:55:57,609
much better cackled results than really

2658
01:55:55,929 --> 01:56:00,130
everybody on the leader board

2659
01:55:57,609 --> 01:56:02,710
and when I say 30th place this is a very

2660
01:56:00,130 --> 01:56:05,710
recent competition right and so I find

2661
01:56:02,710 --> 01:56:07,210
like in the last year like a lot of

2662
01:56:05,710 --> 01:56:09,189
people have got a lot better at computer

2663
01:56:07,210 --> 01:56:10,510
vision and so the people in the top 50

2664
01:56:09,189 --> 01:56:13,059
in this competition were generally

2665
01:56:10,510 --> 01:56:15,640
ensemble in dozens of models lots of

2666
01:56:13,060 --> 01:56:18,340
people on a team lots of pre-processing

2667
01:56:15,640 --> 01:56:21,250
specific satellite data and so forth so

2668
01:56:18,340 --> 01:56:22,750
like to be able to get xxx using this

2669
01:56:21,250 --> 01:56:24,929
totally standard technique is pretty

2670
01:56:22,750 --> 01:56:24,929
cool

2671
01:56:24,960 --> 01:56:29,619
alright so now that we've got to this

2672
01:56:28,119 --> 01:56:32,649
point right we've got through two

2673
01:56:29,619 --> 01:56:35,260
lessons if you're still here then

2674
01:56:32,649 --> 01:56:37,689
hopefully you're thinking okay this is

2675
01:56:35,260 --> 01:56:41,170
actually pretty useful I want to do more

2676
01:56:37,689 --> 01:56:43,719
in which case Kressel might not be where

2677
01:56:41,170 --> 01:56:45,908
you want to stay the issues with Kressel

2678
01:56:43,719 --> 01:56:47,679
I mean it's it's it's pretty handy it's

2679
01:56:45,908 --> 01:56:50,019
pretty cheap and something we haven't

2680
01:56:47,679 --> 01:56:52,210
talked about much is paper space is

2681
01:56:50,020 --> 01:56:53,409
another great choice by the way paper

2682
01:56:52,210 --> 01:56:55,779
space are short they're going to be

2683
01:56:53,408 --> 01:56:57,158
releasing kress or like instant Drupal

2684
01:56:55,779 --> 01:56:59,738
notebooks unfortunately they're not

2685
01:56:57,158 --> 01:57:01,809
ready quite yet but they do have an

2686
01:56:59,738 --> 01:57:04,718
ability to basically they have the best

2687
01:57:01,810 --> 01:57:08,110
price performance relationship right now

2688
01:57:04,719 --> 01:57:10,539
and they you can SSH into them and use

2689
01:57:08,109 --> 01:57:11,559
them so they're also a great choice and

2690
01:57:10,539 --> 01:57:13,869
probably by the time this

2691
01:57:11,560 --> 01:57:15,940
the MOOC will probably have a separate

2692
01:57:13,869 --> 01:57:18,099
lesson showing you how to set up set up

2693
01:57:15,939 --> 01:57:20,799
paper space because there they're likely

2694
01:57:18,100 --> 01:57:21,880
to be a great option but at some point

2695
01:57:20,800 --> 01:57:26,260
you're probably going to want to look at

2696
01:57:21,880 --> 01:57:30,460
AWS a couple of reasons why the first is

2697
01:57:26,260 --> 01:57:33,369
as you all know by now amazon have been

2698
01:57:30,460 --> 01:57:35,980
kind enough to donate about $200,000

2699
01:57:33,369 --> 01:57:37,840
worth of compute time to this course so

2700
01:57:35,979 --> 01:57:40,719
I want to say thank you very much to

2701
01:57:37,840 --> 01:57:42,190
Amazon we've all been given credit so

2702
01:57:40,720 --> 01:57:46,420
everybody this year so thanks very much

2703
01:57:42,189 --> 01:57:48,009
hey don't worry we're so sorry you're

2704
01:57:46,420 --> 01:57:50,440
sure in the MOOC we didn't get it for

2705
01:57:48,010 --> 01:57:53,170
you but everybody here is like AWS

2706
01:57:50,439 --> 01:57:55,329
credits for everybody so um but you can

2707
01:57:53,170 --> 01:57:57,130
get even if you're not here in person

2708
01:57:55,329 --> 01:58:00,130
you can get AWS credits from lots of

2709
01:57:57,130 --> 01:58:02,050
places github has a student pack Google

2710
01:58:00,130 --> 01:58:05,199
for github student pack that's like 150

2711
01:58:02,050 --> 01:58:08,320
bucks worth of credits AWS educate can

2712
01:58:05,199 --> 01:58:09,369
get credits these our office students so

2713
01:58:08,319 --> 01:58:13,049
there's lots of places you can get

2714
01:58:09,369 --> 01:58:15,250
started on AWS pretty much everybody

2715
01:58:13,050 --> 01:58:18,659
everybody a lot of the people that you

2716
01:58:15,250 --> 01:58:21,699
might work with will be using AWS

2717
01:58:18,659 --> 01:58:25,599
because it's like super flexible right

2718
01:58:21,699 --> 01:58:28,949
now AWS has the fastest available GPUs

2719
01:58:25,600 --> 01:58:31,090
you can get in the cloud they're p3s

2720
01:58:28,949 --> 01:58:32,859
they're kind of expensive at three bucks

2721
01:58:31,090 --> 01:58:34,630
an hour but if you've got like a model

2722
01:58:32,859 --> 01:58:35,829
where you've done all the steps before

2723
01:58:34,630 --> 01:58:38,470
you're thinking this is looking pretty

2724
01:58:35,829 --> 01:58:41,800
good you know for 6 bucks you could get

2725
01:58:38,470 --> 01:58:45,880
a p3 for 2 hours and run turbo speed

2726
01:58:41,800 --> 01:58:48,130
right um we didn't start with AWS

2727
01:58:45,880 --> 01:58:49,989
because well hey it's like twice as

2728
01:58:48,130 --> 01:58:54,310
expensive as Chris Hall for the cheapest

2729
01:58:49,989 --> 01:58:57,189
GPU and being a Texan setup right but I

2730
01:58:54,310 --> 01:58:59,650
wanted to kind of go through and show

2731
01:58:57,189 --> 01:59:01,059
you how to get your AWS setup and so

2732
01:58:59,649 --> 01:59:03,789
we're going to be going slightly over

2733
01:59:01,060 --> 01:59:05,350
time to do that but I want to show you a

2734
01:59:03,789 --> 01:59:07,300
very quick place I feel prettier if you

2735
01:59:05,350 --> 01:59:10,270
have to but I want to show you very

2736
01:59:07,300 --> 01:59:14,050
quickly how you can get your AWS setup

2737
01:59:10,270 --> 01:59:17,140
right from scratch so basically you have

2738
01:59:14,050 --> 01:59:19,360
to go to consult on AWS but amazon.com

2739
01:59:17,140 --> 01:59:22,300
and it'll take you to the console right

2740
01:59:19,359 --> 01:59:23,118
and so you can follow along on the video

2741
01:59:22,300 --> 01:59:27,079
with this

2742
01:59:23,118 --> 01:59:29,658
quickly from here you have to go to AC -

2743
01:59:27,079 --> 01:59:33,618
this is where you set up your instances

2744
01:59:29,658 --> 01:59:35,689
and so from ec2 you need to do what's

2745
01:59:33,618 --> 01:59:36,920
called launching an instance so

2746
01:59:35,689 --> 01:59:38,960
launching an instance means you're

2747
01:59:36,920 --> 01:59:41,239
basically creating a computer right now

2748
01:59:38,960 --> 01:59:44,149
creating a computer on Amazon so I say

2749
01:59:41,238 --> 01:59:47,479
launch instance and what we've done is

2750
01:59:44,149 --> 01:59:49,939
we've created a fast AI it's got an amo

2751
01:59:47,479 --> 01:59:51,709
and ami is like a template for how your

2752
01:59:49,939 --> 01:59:54,259
computer's going to begin so if you've

2753
01:59:51,710 --> 01:59:56,719
got a community a Mis and type in fast

2754
01:59:54,260 --> 02:00:00,560
AI you'll see that there's one there

2755
01:59:56,719 --> 02:00:04,090
called fast AI part 1 version 2 for the

2756
02:00:00,560 --> 02:00:06,409
p2 ok so I'm going to select that and

2757
02:00:04,090 --> 02:00:08,900
then we need to say what kind of

2758
02:00:06,408 --> 02:00:13,219
computer do you want and so I can say I

2759
02:00:08,899 --> 02:00:16,670
want a GPU compute computer and then I

2760
02:00:13,219 --> 02:00:19,340
can say I want a p2 x large this is the

2761
02:00:16,670 --> 02:00:21,260
cheapest reasonably effective for deep

2762
02:00:19,340 --> 02:00:25,520
learning instance type they have and

2763
02:00:21,260 --> 02:00:29,510
then I can say launch and then I can say

2764
02:00:25,520 --> 02:00:33,530
launch and so at this point they asked

2765
02:00:29,510 --> 02:00:35,630
you to choose a key pair right now

2766
02:00:33,529 --> 02:00:39,698
if you don't have a key pair you have to

2767
02:00:35,630 --> 02:00:44,480
create one right so to create a key pair

2768
02:00:39,698 --> 02:00:47,118
you need to open your terminal if you

2769
02:00:44,479 --> 02:00:48,829
don't have a terminal if you've got a

2770
02:00:47,118 --> 02:00:50,809
Mac or Linux box you've definitely got

2771
02:00:48,829 --> 02:00:53,929
one if you've got Windows hopefully

2772
02:00:50,810 --> 02:00:56,270
you've got Ubuntu if you don't already

2773
02:00:53,929 --> 02:01:01,819
have Ubuntu setup you can go to the

2774
02:00:56,270 --> 02:01:04,280
Windows Store and click on Ubuntu right

2775
02:01:01,819 --> 02:01:08,229
we'll get it from the Windows Store so

2776
02:01:04,279 --> 02:01:12,319
from there you basically go SSH

2777
02:01:08,229 --> 02:01:15,019
- caged in and that will create like a

2778
02:01:12,319 --> 02:01:16,969
special password for your computer to be

2779
02:01:15,020 --> 02:01:19,489
able to log in to Amazon and then you

2780
02:01:16,969 --> 02:01:22,250
just hit enter three times okay and

2781
02:01:19,488 --> 02:01:24,559
that's going to create for you your key

2782
02:01:22,250 --> 02:01:26,658
you can use to get into Amazon alright

2783
02:01:24,560 --> 02:01:28,280
so then what I do is I copy that key

2784
02:01:26,658 --> 02:01:30,609
somewhere that I know where it is so

2785
02:01:28,279 --> 02:01:33,729
it'll be in the dot SSH folder

2786
02:01:30,609 --> 02:01:39,489
it's called IDRs a dub and so I'm going

2787
02:01:33,729 --> 02:01:41,169
to copy it to my hard drive so if you're

2788
02:01:39,489 --> 02:01:42,550
in a macro and Linux it'll already be in

2789
02:01:41,170 --> 02:01:49,359
an easy to find place it'll be in your

2790
02:01:42,550 --> 02:01:53,529
SSH folder that in documents so from

2791
02:01:49,359 --> 02:01:55,299
there back in AWS you have to tell it

2792
02:01:53,529 --> 02:01:59,259
that you've created this key so you can

2793
02:01:55,300 --> 02:02:02,020
go to key pairs and you say import key

2794
02:01:59,260 --> 02:02:06,489
pair and you just browse to that file

2795
02:02:02,020 --> 02:02:11,020
that you just created there it is

2796
02:02:06,489 --> 02:02:13,420
I say import okay so if you've ever used

2797
02:02:11,020 --> 02:02:14,980
SSH before you've already got the key

2798
02:02:13,420 --> 02:02:17,260
pair you don't have to do those depths

2799
02:02:14,979 --> 02:02:18,909
if you've used AWS before you've already

2800
02:02:17,260 --> 02:02:20,770
imported it you don't have to do that

2801
02:02:18,909 --> 02:02:24,430
step maybe haven't done any of those

2802
02:02:20,770 --> 02:02:29,640
things you have to do both steps so now

2803
02:02:24,430 --> 02:02:29,640
I can go ahead and launch my instance

2804
02:02:30,510 --> 02:02:41,010
community I am eyes search last day I

2805
02:02:35,520 --> 02:02:43,810
select launch and so now it asks me

2806
02:02:41,010 --> 02:02:48,750
what's where's your key pair and I can

2807
02:02:43,810 --> 02:02:48,750
choose that one that I just grabbed okay

2808
02:02:48,989 --> 02:02:55,389
so this is going to go ahead and create

2809
02:02:51,399 --> 02:02:56,679
a new computer for me to log into and

2810
02:02:55,390 --> 02:02:58,869
you can see here it says the following

2811
02:02:56,680 --> 02:03:02,770
have been initiated and so if I click on

2812
02:02:58,869 --> 02:03:05,949
that it'll show me this new computer

2813
02:03:02,770 --> 02:03:08,520
that I've created okay so it'll be able

2814
02:03:05,949 --> 02:03:11,789
to log into it

2815
02:03:08,520 --> 02:03:14,100
I need to know its IP address so here it

2816
02:03:11,789 --> 02:03:17,460
is the IP address there okay so I can

2817
02:03:14,100 --> 02:03:20,700
copy that and that's the IP address of

2818
02:03:17,460 --> 02:03:23,310
my computer so to get to this computer I

2819
02:03:20,699 --> 02:03:25,109
need to SSH to it so SSH into a computer

2820
02:03:23,310 --> 02:03:26,310
means connecting to that computer so

2821
02:03:25,109 --> 02:03:29,960
that it's like you're typing in that

2822
02:03:26,310 --> 02:03:33,690
computer so I type SSH and they username

2823
02:03:29,960 --> 02:03:36,420
for this instance is always Ubuntu right

2824
02:03:33,689 --> 02:03:38,909
and then I can paste in that IP address

2825
02:03:36,420 --> 02:03:41,730
and then there's one more thing I have

2826
02:03:38,909 --> 02:03:44,340
to do which is I have to connect up the

2827
02:03:41,729 --> 02:03:46,739
jupiter notebook on that instance to the

2828
02:03:44,340 --> 02:03:48,930
jupiter notebook on my machine and so to

2829
02:03:46,739 --> 02:03:50,969
do that there's just a particular flag

2830
02:03:48,930 --> 02:03:52,440
that i said okay we can talk about it on

2831
02:03:50,970 --> 02:03:55,220
the forums as to exactly what it does

2832
02:03:52,439 --> 02:03:59,969
but you just type - l.a today date

2833
02:03:55,220 --> 02:04:01,470
localhost 8 8 8 8 ok so like once you've

2834
02:03:59,970 --> 02:04:03,449
done it once you can like save that as

2835
02:04:01,470 --> 02:04:05,250
an alias and type in the same thing

2836
02:04:03,449 --> 02:04:07,590
every time

2837
02:04:05,250 --> 02:04:09,510
so we can check here we can see it says

2838
02:04:07,590 --> 02:04:12,569
that it's running so we should be able

2839
02:04:09,510 --> 02:04:14,699
to now hit enter first time ever which

2840
02:04:12,569 --> 02:04:19,829
sit reconnect to it it does checks this

2841
02:04:14,699 --> 02:04:27,420
is okay I'll say yes and then that goes

2842
02:04:19,829 --> 02:04:29,699
ahead and SSH is in so this ami is all

2843
02:04:27,420 --> 02:04:31,380
set up for you alright so you'll find

2844
02:04:29,699 --> 02:04:32,880
that the very first time you log in it

2845
02:04:31,380 --> 02:04:34,079
takes a few extra seconds because it

2846
02:04:32,880 --> 02:04:36,600
just kind of is getting everything set

2847
02:04:34,079 --> 02:04:38,369
up but once it's logged in you'll see

2848
02:04:36,600 --> 02:04:41,820
there that there's a directory called

2849
02:04:38,369 --> 02:04:45,119
fast AI and the fast AI directory

2850
02:04:41,819 --> 02:04:48,809
contains our fast AI repo that contains

2851
02:04:45,119 --> 02:04:51,689
all the notebooks or the code etc so I

2852
02:04:48,810 --> 02:04:53,640
can just go CD faster all right first

2853
02:04:51,689 --> 02:04:55,139
thing you do when you get in is to make

2854
02:04:53,640 --> 02:04:59,880
sure it's updated so you just go git

2855
02:04:55,140 --> 02:05:01,800
pull right and that updates to make sure

2856
02:04:59,880 --> 02:05:05,190
that your repo is the same as the most

2857
02:05:01,800 --> 02:05:05,610
recent video and so as you can see there

2858
02:05:05,189 --> 02:05:06,839
we go

2859
02:05:05,609 --> 02:05:08,519
let's make sure it's got all the most

2860
02:05:06,840 --> 02:05:11,970
recent code the second thing you should

2861
02:05:08,520 --> 02:05:13,800
do is type Condor and update you can

2862
02:05:11,970 --> 02:05:15,690
just do this maybe once a month or so

2863
02:05:13,800 --> 02:05:17,760
and that makes sure that the libraries

2864
02:05:15,689 --> 02:05:19,259
there are all the most recent libraries

2865
02:05:17,760 --> 02:05:20,440
I'm not going to run that so it takes a

2866
02:05:19,260 --> 02:05:22,449
couple of minutes okay

2867
02:05:20,439 --> 02:05:26,649
and then the last step is to type

2868
02:05:22,449 --> 02:05:29,800
particular notebook okay

2869
02:05:26,649 --> 02:05:32,529
so this is going to go ahead and launch

2870
02:05:29,800 --> 02:05:35,199
the triplet notebook server on this

2871
02:05:32,529 --> 02:05:37,539
machine again the first time I do it the

2872
02:05:35,199 --> 02:05:40,449
first time you do everything on AWS

2873
02:05:37,539 --> 02:05:42,189
it just takes like a minute or two and

2874
02:05:40,449 --> 02:05:44,380
then once you've done it in the future

2875
02:05:42,189 --> 02:05:47,529
we just as fast as running it locally

2876
02:05:44,380 --> 02:05:49,900
basically okay so you can see it's going

2877
02:05:47,529 --> 02:05:51,460
ahead and firing out the notebook and so

2878
02:05:49,899 --> 02:05:53,739
what's going to happen is that because

2879
02:05:51,460 --> 02:05:56,890
when we SSH into it we said to both

2880
02:05:53,739 --> 02:05:58,809
connect our notebook port to the remote

2881
02:05:56,890 --> 02:06:00,880
notebook port we're just going to be

2882
02:05:58,810 --> 02:06:03,130
able to use this locally so I see he

2883
02:06:00,880 --> 02:06:06,279
says here copy paste this URL so I'm

2884
02:06:03,130 --> 02:06:11,980
going to grab that URL and I'm going to

2885
02:06:06,279 --> 02:06:15,729
paste it into my browser and that's it

2886
02:06:11,979 --> 02:06:17,919
okay so this notebook is now actually

2887
02:06:15,729 --> 02:06:21,129
not running on my machine it's actually

2888
02:06:17,920 --> 02:06:23,230
running on AWS okay using the AWS GPU

2889
02:06:21,130 --> 02:06:25,869
it's got a lot of memory it's not the

2890
02:06:23,229 --> 02:06:28,479
fastest around but it's not terrible

2891
02:06:25,869 --> 02:06:30,069
you can always fire up a p3 if you want

2892
02:06:28,479 --> 02:06:33,579
something that's super fast this is

2893
02:06:30,069 --> 02:06:36,009
costing me ninety cents a minute okay so

2894
02:06:33,579 --> 02:06:38,399
when you're finished please don't forget

2895
02:06:36,010 --> 02:06:43,030
to shut it down right so to shut it down

2896
02:06:38,399 --> 02:06:48,849
you can right-click on it and say

2897
02:06:43,029 --> 02:06:51,670
instance date stop okay we've got five

2898
02:06:48,850 --> 02:06:53,380
hundred bucks of credit assuming that

2899
02:06:51,670 --> 02:06:55,750
you put your code down in the

2900
02:06:53,380 --> 02:06:57,250
spreadsheet one thing I forgot to do the

2901
02:06:55,750 --> 02:07:02,170
first time I showed you this by the way

2902
02:06:57,250 --> 02:07:03,430
I said make sure you choose a p2 the

2903
02:07:02,170 --> 02:07:05,529
second time I went through I didn't

2904
02:07:03,430 --> 02:07:09,369
choose p2 by mistake so just don't

2905
02:07:05,529 --> 02:07:11,378
forget choose gpq compute P - do you

2906
02:07:09,369 --> 02:07:15,658
have a question

2907
02:07:11,378 --> 02:07:20,378
my Bernice it's an hour thank you

2908
02:07:15,658 --> 02:07:21,788
90 cents an hour it also costs like I

2909
02:07:20,378 --> 02:07:24,248
don't know three or four bucks a month

2910
02:07:21,788 --> 02:07:26,108
for the storage as well thanks for

2911
02:07:24,248 --> 02:07:29,309
checking that all right see you next

2912
02:07:26,109 --> 02:07:29,309
week sorry we're a bit over

