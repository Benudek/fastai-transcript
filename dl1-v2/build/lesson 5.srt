1
00:00:00,030 --> 00:00:11,638
welcome back so we had a busy lesson

2
00:00:05,278 --> 00:00:14,160
last week and I was really thrilled to

3
00:00:11,638 --> 00:00:18,149
see actually one of our masters students

4
00:00:14,160 --> 00:00:23,310
here at USF actually actually took what

5
00:00:18,149 --> 00:00:25,018
we learned took what we learned with

6
00:00:23,309 --> 00:00:28,198
structured deep learning and turned it

7
00:00:25,018 --> 00:00:30,750
into a blog post which as I suspected

8
00:00:28,199 --> 00:00:33,380
has been incredibly popular because it's

9
00:00:30,750 --> 00:00:35,488
just something people didn't know about

10
00:00:33,380 --> 00:00:38,190
and so it actually ended up getting

11
00:00:35,488 --> 00:00:40,259
picked up by the towards data science

12
00:00:38,189 --> 00:00:41,789
publication which I quite liked actually

13
00:00:40,259 --> 00:00:43,619
if you're interested in keeping up with

14
00:00:41,789 --> 00:00:46,500
what's going on a data science it's

15
00:00:43,619 --> 00:00:50,159
quite good medium publication and so

16
00:00:46,500 --> 00:00:53,128
Karen talked about structured deep

17
00:00:50,159 --> 00:00:55,319
learning and basically introduced you

18
00:00:53,128 --> 00:00:58,649
know the the the basic ideas that we

19
00:00:55,320 --> 00:01:01,170
learned about last week and it got

20
00:00:58,649 --> 00:01:02,698
picked up quite quite widely one of the

21
00:01:01,170 --> 00:01:04,290
one of the things I was pleased to say

22
00:01:02,698 --> 00:01:06,179
actually sebastian ruder who actually

23
00:01:04,290 --> 00:01:09,240
mentioned in last week's class as being

24
00:01:06,180 --> 00:01:11,159
one of my favorite researchers tweeted

25
00:01:09,239 --> 00:01:12,780
it and then somebody from stitch fix

26
00:01:11,159 --> 00:01:16,590
said oh yeah we've actually been doing

27
00:01:12,780 --> 00:01:18,599
that for ages which is kind of cute I I

28
00:01:16,590 --> 00:01:20,130
kind of know that this is happening in

29
00:01:18,599 --> 00:01:21,449
industry a lot and I've been telling

30
00:01:20,129 --> 00:01:23,069
people this is happening in industry a

31
00:01:21,450 --> 00:01:24,900
lot but nobody's been talking about it

32
00:01:23,069 --> 00:01:26,339
and now the Karen's kind of published a

33
00:01:24,900 --> 00:01:28,140
blog saying hey check out this cool

34
00:01:26,340 --> 00:01:30,978
thing and they all stitch fixes like

35
00:01:28,140 --> 00:01:34,978
yeah we're doing that already so so

36
00:01:30,978 --> 00:01:38,129
that's been great great to see and I

37
00:01:34,978 --> 00:01:39,989
think there's still a lot more that can

38
00:01:38,129 --> 00:01:42,209
be dug into with this structured people

39
00:01:39,989 --> 00:01:44,269
learning stuff you know to build on top

40
00:01:42,209 --> 00:01:46,109
of Karen's post would be that maybe like

41
00:01:44,269 --> 00:01:48,449
experiment with some different datasets

42
00:01:46,109 --> 00:01:50,728
maybe find some old careful competitions

43
00:01:48,450 --> 00:01:53,490
and see like there's some competitions

44
00:01:50,728 --> 00:01:55,259
that you could now win with this or some

45
00:01:53,489 --> 00:01:58,129
which doesn't work for would be equally

46
00:01:55,259 --> 00:01:59,700
interesting and also like just

47
00:01:58,129 --> 00:02:01,709
experimenting a bit with different

48
00:01:59,700 --> 00:02:05,430
amounts of dropout different layer sizes

49
00:02:01,709 --> 00:02:07,438
you know because nobody much is written

50
00:02:05,430 --> 00:02:09,000
about this I don't think there's been

51
00:02:07,438 --> 00:02:10,788
any blog posts about this before that

52
00:02:09,000 --> 00:02:13,969
I've seen anywhere

53
00:02:10,788 --> 00:02:16,369
there's a lot of unexplored territory so

54
00:02:13,969 --> 00:02:17,870
I think there's a lot we could we could

55
00:02:16,370 --> 00:02:19,759
build on top of here and there's

56
00:02:17,870 --> 00:02:21,950
definitely a lot of interest as well one

57
00:02:19,759 --> 00:02:25,310
person on Twitter saying this is what

58
00:02:21,949 --> 00:02:27,669
I've been looking for for ages another

59
00:02:25,310 --> 00:02:31,189
thing which I was pleased to see is

60
00:02:27,669 --> 00:02:33,619
Nikki or who we saw his cricket versus

61
00:02:31,189 --> 00:02:36,939
baseball predictor as well as his a

62
00:02:33,620 --> 00:02:40,819
currency predictor after less than one

63
00:02:36,939 --> 00:02:43,489
went on to download something a bit

64
00:02:40,818 --> 00:02:46,458
bigger which was to download a couple of

65
00:02:43,489 --> 00:02:48,739
hundred of images of actors and he

66
00:02:46,459 --> 00:02:50,598
manually went through and checked which

67
00:02:48,739 --> 00:02:52,069
well I think first of all he like used

68
00:02:50,598 --> 00:02:53,780
Google to try and find ones with glasses

69
00:02:52,068 --> 00:02:55,009
and ones without then he manually went

70
00:02:53,780 --> 00:02:57,140
through and checked that that they put

71
00:02:55,009 --> 00:03:00,139
in the right spot and this was a good

72
00:02:57,139 --> 00:03:01,969
example of one where vanilla ResNet

73
00:03:00,139 --> 00:03:06,018
didn't do so well with just the last

74
00:03:01,969 --> 00:03:07,669
layer and so what Nikhil did was he went

75
00:03:06,019 --> 00:03:09,140
through and tried on freezing the layers

76
00:03:07,669 --> 00:03:11,708
and using differential learning rates

77
00:03:09,139 --> 00:03:14,419
and got up to a hundred percent accuracy

78
00:03:11,709 --> 00:03:15,889
and the thing I like about these things

79
00:03:14,419 --> 00:03:18,649
that Nikhil was doing is the way he's

80
00:03:15,889 --> 00:03:20,689
he's not downloading a kegel data set

81
00:03:18,650 --> 00:03:22,280
he's like deciding on a problem that

82
00:03:20,689 --> 00:03:24,590
he's going to try and solve he's going

83
00:03:22,280 --> 00:03:26,719
from scratch from google and he's

84
00:03:24,590 --> 00:03:28,280
actually got a link here even to the

85
00:03:26,719 --> 00:03:30,409
suggested way to help you download

86
00:03:28,280 --> 00:03:33,680
images from Google so I think this is

87
00:03:30,409 --> 00:03:36,289
great and actually gave a talk just this

88
00:03:33,680 --> 00:03:38,870
afternoon at singularity University to

89
00:03:36,289 --> 00:03:40,909
an executive team of one of the world's

90
00:03:38,870 --> 00:03:43,989
largest telecommunications companies and

91
00:03:40,909 --> 00:03:46,430
actually show them this post because the

92
00:03:43,989 --> 00:03:47,900
folks there were telling me that that

93
00:03:46,430 --> 00:03:50,180
all the vendors that come to them and

94
00:03:47,900 --> 00:03:52,010
tell them they need like millions of

95
00:03:50,180 --> 00:03:53,629
images and huge data centers will have

96
00:03:52,009 --> 00:03:56,719
hardware and you know they have to buy a

97
00:03:53,629 --> 00:03:58,909
special software that only these vendors

98
00:03:56,719 --> 00:04:00,229
can provide and I said like actually

99
00:03:58,909 --> 00:04:01,759
this person has been doing it of course

100
00:04:00,229 --> 00:04:04,129
for three weeks now and look at what

101
00:04:01,759 --> 00:04:06,259
he's just done with a computer that cost

102
00:04:04,129 --> 00:04:08,989
him 60 cents an hour and they were like

103
00:04:06,259 --> 00:04:11,090
they were so happy to hear that like

104
00:04:08,989 --> 00:04:13,759
okay they're you know this actually is

105
00:04:11,090 --> 00:04:15,289
in the reach of normal people I'm

106
00:04:13,759 --> 00:04:15,979
assuming Nikhil is a normal person I

107
00:04:15,289 --> 00:04:20,029
haven't actually

108
00:04:15,979 --> 00:04:23,509
and if your proudly abnormal Nicole I

109
00:04:20,029 --> 00:04:26,089
apologize I actually went and actually

110
00:04:23,509 --> 00:04:27,620
had a look at his cricket classifier and

111
00:04:26,089 --> 00:04:29,899
I was really pleased to see that his

112
00:04:27,620 --> 00:04:32,300
code actually is the exact same code

113
00:04:29,899 --> 00:04:33,500
that were used in Lesson one I was

114
00:04:32,300 --> 00:04:35,840
hoping that would be the case you know

115
00:04:33,500 --> 00:04:38,689
the only thing he changed was the number

116
00:04:35,839 --> 00:04:40,250
of epochs I guess so this idea that we

117
00:04:38,689 --> 00:04:41,660
can take those four lines of code and

118
00:04:40,250 --> 00:04:44,899
reuse it to do other things that's

119
00:04:41,660 --> 00:04:46,729
definitely turned out to be true and so

120
00:04:44,899 --> 00:04:49,159
these are good things to show like it

121
00:04:46,728 --> 00:04:51,319
yeah your organization if you're

122
00:04:49,160 --> 00:04:52,729
anything like the executives of this big

123
00:04:51,319 --> 00:04:55,219
company I spoke to today there'll be a

124
00:04:52,728 --> 00:04:58,129
certain amount of like not to surprise

125
00:04:55,220 --> 00:04:59,900
but almost like pushback like if this

126
00:04:58,129 --> 00:05:00,949
was true somebody does it all that

127
00:04:59,899 --> 00:05:02,810
message she said if this was true

128
00:05:00,949 --> 00:05:04,819
somebody would have told us so like why

129
00:05:02,810 --> 00:05:05,930
isn't everybody doing this already so

130
00:05:04,819 --> 00:05:08,149
we'd like it I think you might have to

131
00:05:05,930 --> 00:05:09,978
actually show them you know maybe you

132
00:05:08,149 --> 00:05:11,299
can build your own there's some internal

133
00:05:09,978 --> 00:05:14,569
data you've got at work or something

134
00:05:11,300 --> 00:05:20,569
like here it is you know didn't cost me

135
00:05:14,569 --> 00:05:21,589
anything it's all finished fiddly or

136
00:05:20,569 --> 00:05:23,029
badly I don't know how to pronounce his

137
00:05:21,589 --> 00:05:26,149
name correctly has done another very

138
00:05:23,029 --> 00:05:28,699
nice post on just an introductory post

139
00:05:26,149 --> 00:05:30,620
on how we train neural networks and I've

140
00:05:28,699 --> 00:05:33,079
wanted to point this one out as being

141
00:05:30,620 --> 00:05:34,519
like I think this is one of the

142
00:05:33,079 --> 00:05:36,349
participants in this course who has got

143
00:05:34,519 --> 00:05:37,909
a particular knack for technical

144
00:05:36,350 --> 00:05:40,340
communication and I think we can all

145
00:05:37,910 --> 00:05:43,280
learn from you know from his post about

146
00:05:40,339 --> 00:05:46,310
about good technical writing what I

147
00:05:43,279 --> 00:05:48,109
really like particularly is that he he

148
00:05:46,310 --> 00:05:50,240
assumes almost nothing like he has a

149
00:05:48,110 --> 00:05:52,220
kind of a very chatty tone and describes

150
00:05:50,240 --> 00:05:54,410
everything but he also assumes that the

151
00:05:52,220 --> 00:05:55,700
reader is intelligent but you know so

152
00:05:54,410 --> 00:05:57,560
like he's not afraid to kind of say

153
00:05:55,699 --> 00:05:59,538
here's a paper or here's an equation or

154
00:05:57,560 --> 00:06:00,829
or whatever but then he's going to go

155
00:05:59,538 --> 00:06:03,889
through and tell you exactly what that

156
00:06:00,829 --> 00:06:06,490
equation means so it's kind of like this

157
00:06:03,889 --> 00:06:08,449
nice mix of like writing for

158
00:06:06,490 --> 00:06:11,449
respectfully for an intelligent audience

159
00:06:08,449 --> 00:06:16,879
but also not assuming any particular

160
00:06:11,449 --> 00:06:18,860
background knowledge so then I made the

161
00:06:16,879 --> 00:06:21,978
mistake earlier this week of posting a

162
00:06:18,860 --> 00:06:24,199
picture of my first placing on the

163
00:06:21,978 --> 00:06:26,779
Carroll seedlings competition at which

164
00:06:24,199 --> 00:06:28,509
point five other fast AI students posted

165
00:06:26,779 --> 00:06:31,719
their pictures of them pass

166
00:06:28,509 --> 00:06:33,639
over the next few days so this is the

167
00:06:31,720 --> 00:06:35,140
current leaderboard for the cattle plant

168
00:06:33,639 --> 00:06:38,079
seedlings competition I believe the

169
00:06:35,139 --> 00:06:42,219
product top six are all fast AI students

170
00:06:38,079 --> 00:06:45,759
or in the worst of those teachers and so

171
00:06:42,220 --> 00:06:48,370
I think this is like a really Oh James

172
00:06:45,759 --> 00:06:52,149
is just passed he was first this is a

173
00:06:48,370 --> 00:06:55,629
really good example of like what you can

174
00:06:52,149 --> 00:06:58,539
do but this is trying to think it was

175
00:06:55,629 --> 00:07:02,800
like a small number of thousands of

176
00:06:58,540 --> 00:07:04,480
images and most of the images were only

177
00:07:02,800 --> 00:07:09,819
were less than a hundred pixels by a

178
00:07:04,480 --> 00:07:11,710
hundred pixels and yet week you know I

179
00:07:09,819 --> 00:07:13,060
bet my approach was basically to say

180
00:07:11,709 --> 00:07:15,129
let's just run through the notebook we

181
00:07:13,060 --> 00:07:19,269
have pretty much default took the I

182
00:07:15,129 --> 00:07:21,310
don't know an hour and I'm I think the

183
00:07:19,269 --> 00:07:23,169
other students doing a little bit more

184
00:07:21,310 --> 00:07:25,750
than that but not a lot more and

185
00:07:23,170 --> 00:07:28,720
basically what this is saying is yeah

186
00:07:25,750 --> 00:07:30,730
these these techniques work pretty

187
00:07:28,720 --> 00:07:33,450
reliably to a point where people that

188
00:07:30,730 --> 00:07:37,410
aren't using the fast I know libraries

189
00:07:33,449 --> 00:07:40,000
you know literally really struggling

190
00:07:37,410 --> 00:07:41,620
let's just pick off these are first aid

191
00:07:40,000 --> 00:07:43,870
a students might have to go down quite a

192
00:07:41,620 --> 00:07:48,519
way so I thought that was very

193
00:07:43,870 --> 00:07:52,329
interesting and really really cool so

194
00:07:48,519 --> 00:07:55,509
today we are going to start what I would

195
00:07:52,329 --> 00:07:58,300
kind of call like the second half of

196
00:07:55,509 --> 00:08:01,349
this course so the first half of this

197
00:07:58,300 --> 00:08:04,030
course is being like getting through

198
00:08:01,350 --> 00:08:07,660
like these are the applications that we

199
00:08:04,029 --> 00:08:10,839
can use this for here's kind of the code

200
00:08:07,660 --> 00:08:13,590
you have to write here's a fairly high

201
00:08:10,839 --> 00:08:17,829
level ish description of what it's doing

202
00:08:13,589 --> 00:08:19,509
and we're kind of we're kind of done for

203
00:08:17,829 --> 00:08:21,669
that bit and what we're now going to do

204
00:08:19,509 --> 00:08:23,319
is go in reverse we're going to go back

205
00:08:21,670 --> 00:08:25,569
over all of those exact same things

206
00:08:23,319 --> 00:08:27,550
again but this time we're going to dig

207
00:08:25,569 --> 00:08:29,019
into the detail of every one and we're

208
00:08:27,550 --> 00:08:30,550
going to look inside the source code of

209
00:08:29,019 --> 00:08:35,590
the first idea library to see what it's

210
00:08:30,550 --> 00:08:38,049
doing and try to replicate that so in a

211
00:08:35,590 --> 00:08:40,710
students like there's not going to be a

212
00:08:38,049 --> 00:08:40,709
lot more

213
00:08:41,500 --> 00:08:45,370
best practices to show you like I've

214
00:08:43,629 --> 00:08:47,830
kind of shown you the best best

215
00:08:45,370 --> 00:08:50,019
practices I know but I feel like for us

216
00:08:47,830 --> 00:08:52,540
to now build on top of those to debug

217
00:08:50,019 --> 00:08:54,250
those models to come back to part two

218
00:08:52,539 --> 00:08:56,379
where we're going to kind of try out

219
00:08:54,250 --> 00:08:58,779
some new things you know it really helps

220
00:08:56,379 --> 00:09:02,830
to understand what's going on behind the

221
00:08:58,779 --> 00:09:06,069
scenes okay so the goal here today is

222
00:09:02,830 --> 00:09:09,930
we're going to try and create a pretty

223
00:09:06,070 --> 00:09:09,930
effective collaborative filtering model

224
00:09:09,960 --> 00:09:15,389
almost entirely from scratch so we'll

225
00:09:13,149 --> 00:09:17,559
use the kind of we'll use PI torch as a

226
00:09:15,389 --> 00:09:19,330
automatic differentiation tool and

227
00:09:17,559 --> 00:09:21,339
there's a GPU programming tool and not

228
00:09:19,330 --> 00:09:23,190
very much else we'll try not to use its

229
00:09:21,340 --> 00:09:26,170
neural net features we'll try not to use

230
00:09:23,190 --> 00:09:30,640
fast AI library anymore than necessary

231
00:09:26,169 --> 00:09:31,990
so that's the goal so let's go back and

232
00:09:30,639 --> 00:09:33,699
you know we only very quickly know

233
00:09:31,990 --> 00:09:35,799
collaborative filtering last time so

234
00:09:33,700 --> 00:09:37,570
let's let's go back and have a look at

235
00:09:35,799 --> 00:09:40,659
collaborative filtering and so we're

236
00:09:37,570 --> 00:09:47,230
going to look at this movie lens data

237
00:09:40,659 --> 00:09:49,750
set so the movie lens data set basically

238
00:09:47,230 --> 00:09:51,850
is a list of ratings it's got a bunch of

239
00:09:49,750 --> 00:09:54,549
different users that are represented by

240
00:09:51,850 --> 00:09:57,850
some ID and a bunch of movies that are

241
00:09:54,549 --> 00:09:58,899
represented by some ID and rating it

242
00:09:57,850 --> 00:10:00,310
also has a timestamp

243
00:09:58,899 --> 00:10:02,769
I haven't actually ever tried to use

244
00:10:00,309 --> 00:10:07,449
this I guess this is just like what what

245
00:10:02,769 --> 00:10:09,069
time did that person read that movie so

246
00:10:07,450 --> 00:10:13,000
that's all we're going to use for

247
00:10:09,070 --> 00:10:16,330
modelling is three columns user ID movie

248
00:10:13,000 --> 00:10:18,700
ID and rating and so thinking of that in

249
00:10:16,330 --> 00:10:20,379
kind of structured data terms user ID

250
00:10:18,700 --> 00:10:23,770
and movie ID would be categorical

251
00:10:20,379 --> 00:10:28,409
variables we have two of them and rating

252
00:10:23,769 --> 00:10:30,669
would be a with the independent variable

253
00:10:28,409 --> 00:10:32,769
we're not going to use this for modeling

254
00:10:30,669 --> 00:10:35,079
but we can use it for looking at stuff

255
00:10:32,769 --> 00:10:37,990
later we can grab a list of the names of

256
00:10:35,080 --> 00:10:40,839
the movies as well and reproduce this

257
00:10:37,990 --> 00:10:42,159
genre information I haven't tried to be

258
00:10:40,839 --> 00:10:44,740
interested if during the week anybody

259
00:10:42,159 --> 00:10:46,088
tries it and finds it helpful my guess

260
00:10:44,740 --> 00:10:50,709
is you might not find it helpful

261
00:10:46,089 --> 00:10:55,180
we'll see so in order to kind of look at

262
00:10:50,708 --> 00:10:57,879
this better I just grabbed

263
00:10:55,179 --> 00:10:59,859
the users that have watched the most

264
00:10:57,879 --> 00:11:03,820
movies and the movies that have been the

265
00:10:59,860 --> 00:11:05,620
most watched and made a crosstab of it

266
00:11:03,820 --> 00:11:08,500
right so this is exactly the same data

267
00:11:05,620 --> 00:11:11,399
but it's a subset and now rather than

268
00:11:08,500 --> 00:11:16,960
being user movie rating we've got user

269
00:11:11,399 --> 00:11:19,120
movie rating and so some users haven't

270
00:11:16,960 --> 00:11:23,650
watched some of these movies that's why

271
00:11:19,120 --> 00:11:27,820
some of these okay then I copied that

272
00:11:23,649 --> 00:11:31,539
into Excel and you'll see there's a

273
00:11:27,820 --> 00:11:32,950
thing called collab your XLS if you

274
00:11:31,539 --> 00:11:37,839
don't see it there now I'll make sure I

275
00:11:32,950 --> 00:11:43,060
put it there back tomorrow and here is

276
00:11:37,840 --> 00:11:46,000
where I've copied that table okay so as

277
00:11:43,059 --> 00:11:47,739
I go through this like setup of the

278
00:11:46,000 --> 00:11:50,889
problem and kind of how its described

279
00:11:47,740 --> 00:11:54,279
and stuff if you're ever feeling lost

280
00:11:50,889 --> 00:11:56,949
feel free to ask either directly or

281
00:11:54,279 --> 00:11:59,199
through the forum if you ask through the

282
00:11:56,950 --> 00:12:02,050
forum and somebody answers there I want

283
00:11:59,200 --> 00:12:03,400
you to answer it here but if somebody

284
00:12:02,049 --> 00:12:07,449
else asks a question you would like

285
00:12:03,399 --> 00:12:09,789
answered of course just like it and your

286
00:12:07,450 --> 00:12:11,920
network keep an eye out for that because

287
00:12:09,789 --> 00:12:13,329
kind of that's we're digging in to the

288
00:12:11,919 --> 00:12:14,889
details of what's going on behind the

289
00:12:13,330 --> 00:12:16,590
scenes it's kind of important that at

290
00:12:14,889 --> 00:12:26,740
each stage you feel like okay I can see

291
00:12:16,590 --> 00:12:29,860
what's going on okay so we can actually

292
00:12:26,740 --> 00:12:32,529
not going to build a neural net to start

293
00:12:29,860 --> 00:12:36,909
with instead we're going to do something

294
00:12:32,529 --> 00:12:38,049
called a matrix factorization the reason

295
00:12:36,909 --> 00:12:39,969
we're not going to build a neural net to

296
00:12:38,049 --> 00:12:43,000
start with is that it so happens there's

297
00:12:39,970 --> 00:12:44,649
a really really simple kind of way of

298
00:12:43,000 --> 00:12:47,440
solving these kinds of problems which

299
00:12:44,649 --> 00:12:50,289
I'm going to show you and so if I scroll

300
00:12:47,440 --> 00:12:53,560
down I've basically what I've got here

301
00:12:50,289 --> 00:12:56,019
is the same the same thing but this time

302
00:12:53,559 --> 00:12:57,219
these are my predictions rather than my

303
00:12:56,019 --> 00:12:59,679
actuals and I'm going to show you how I

304
00:12:57,220 --> 00:13:03,450
created these predictions okay so here

305
00:12:59,679 --> 00:13:07,789
my actuals right here my predictions and

306
00:13:03,450 --> 00:13:11,959
then down here we have

307
00:13:07,789 --> 00:13:15,289
our score which is the sum of the

308
00:13:11,958 --> 00:13:18,469
different squared average square root

309
00:13:15,289 --> 00:13:22,099
okay so this is I are MSE down here okay

310
00:13:18,470 --> 00:13:25,759
so on average we're randomly initialized

311
00:13:22,100 --> 00:13:28,009
model is out by 2.8 so let me show you

312
00:13:25,759 --> 00:13:30,708
what this model is and I'm going to show

313
00:13:28,009 --> 00:13:35,778
you by saying how do we guess how much

314
00:13:30,708 --> 00:13:39,198
user ID number 14 likes movie ID number

315
00:13:35,778 --> 00:13:41,559
27 and the prediction here this is just

316
00:13:39,198 --> 00:13:46,669
at this stage this is still random is

317
00:13:41,559 --> 00:13:50,989
0.9 1 so how we calculate 0.9 1 and the

318
00:13:46,669 --> 00:13:55,789
answer is we're taking it as this vector

319
00:13:50,990 --> 00:13:59,600
here dot product with this vector here

320
00:13:55,789 --> 00:14:03,319
so dot product means 0.71 times 0.1 9

321
00:13:59,600 --> 00:14:05,990
plus 0.8 1 times point 6 3 plus point 7

322
00:14:03,320 --> 00:14:08,240
volt plus point 3 1 and so forth and in

323
00:14:05,990 --> 00:14:09,560
you know linear algebra speak because

324
00:14:08,240 --> 00:14:11,959
one of them is a column and one of them

325
00:14:09,559 --> 00:14:13,549
is a row this is the same as a matrix

326
00:14:11,958 --> 00:14:17,028
product so you can see here I've used

327
00:14:13,549 --> 00:14:21,379
the Excel fashion matrix multiplier and

328
00:14:17,028 --> 00:14:28,088
that's my prediction having said that if

329
00:14:21,379 --> 00:14:30,649
the original rating doesn't exist at all

330
00:14:28,089 --> 00:14:33,050
then I'm just going to set this to 0

331
00:14:30,649 --> 00:14:34,220
right because like there's no error in

332
00:14:33,049 --> 00:14:37,099
predicting something that hasn't

333
00:14:34,220 --> 00:14:38,360
happened okay so what I'm going to do is

334
00:14:37,100 --> 00:14:40,819
I'm basically going to say alright

335
00:14:38,360 --> 00:14:43,519
everyone of my right rate my predictions

336
00:14:40,818 --> 00:14:45,559
is not going to be a neural net it's

337
00:14:43,519 --> 00:14:48,879
going to be a single matrix

338
00:14:45,559 --> 00:14:51,669
multiplication all right now the matrix

339
00:14:48,879 --> 00:14:54,828
multiplication that it's doing is

340
00:14:51,669 --> 00:15:02,120
basically in practice is between like

341
00:14:54,828 --> 00:15:04,429
this matrix and this matrix right so

342
00:15:02,120 --> 00:15:11,240
each one of these is a single part of

343
00:15:04,429 --> 00:15:14,059
that so I randomly initialize these

344
00:15:11,240 --> 00:15:16,669
these are just random numbers that I've

345
00:15:14,059 --> 00:15:20,000
just pasted in here so I've basically

346
00:15:16,669 --> 00:15:21,569
started off with two random matrices and

347
00:15:20,000 --> 00:15:24,450
I've said let's

348
00:15:21,570 --> 00:15:27,990
assume for the time being that every

349
00:15:24,450 --> 00:15:31,980
rating can be represented as the matrix

350
00:15:27,990 --> 00:15:36,200
product of those two so then in Excel

351
00:15:31,980 --> 00:15:36,200
you can actually do a gradient descent

352
00:15:36,679 --> 00:15:41,699
you have to go to your options to the

353
00:15:39,389 --> 00:15:43,230
add-ins section and check the box to say

354
00:15:41,700 --> 00:15:44,670
turn it on and once you do you'll see

355
00:15:43,230 --> 00:15:47,850
there's something there called solver

356
00:15:44,669 --> 00:15:51,179
and if I go solver it says okay what's

357
00:15:47,850 --> 00:15:54,019
your objective function and you just

358
00:15:51,179 --> 00:15:56,250
choose the cell so in this case we chose

359
00:15:54,019 --> 00:15:59,789
the cell that contains that repeats

360
00:15:56,250 --> 00:16:02,909
grade error and then it says okay what

361
00:15:59,789 --> 00:16:05,459
do you want to change and you can see

362
00:16:02,909 --> 00:16:07,199
here we've selected this matrix and this

363
00:16:05,460 --> 00:16:09,690
matrix and so it's going to do a

364
00:16:07,200 --> 00:16:12,120
gradient descent for us by changing

365
00:16:09,690 --> 00:16:16,410
these matrices to try and in this case

366
00:16:12,120 --> 00:16:20,519
minimize this min minimize this Excel so

367
00:16:16,409 --> 00:16:23,339
right GRG nonlinear is a gradient just

368
00:16:20,519 --> 00:16:26,759
yet so I'll say solve and you'll see it

369
00:16:23,340 --> 00:16:29,430
starts at 2.8 and then down here you'll

370
00:16:26,759 --> 00:16:30,840
see that numbers drain down it's not

371
00:16:29,429 --> 00:16:32,099
actually showing us what it's doing but

372
00:16:30,840 --> 00:16:37,230
we can see that the numbers going down

373
00:16:32,100 --> 00:16:39,360
so this has kind of got a near or nettie

374
00:16:37,230 --> 00:16:41,009
feel to it in that we're doing like a

375
00:16:39,360 --> 00:16:43,340
matrix product and we're doing a

376
00:16:41,009 --> 00:16:46,769
gradient descent but we don't have a

377
00:16:43,340 --> 00:16:49,710
nonlinear layer and we don't have a

378
00:16:46,769 --> 00:16:51,629
second linear layer on top of that so we

379
00:16:49,710 --> 00:16:53,490
don't get to call this deep learning so

380
00:16:51,629 --> 00:16:55,080
things where people do like deep

381
00:16:53,490 --> 00:16:58,379
learning each things where they have

382
00:16:55,080 --> 00:17:00,150
kind of matrix products and gradient

383
00:16:58,379 --> 00:17:01,830
descents but it's not deep people tend

384
00:17:00,149 --> 00:17:05,160
to just call that shallow learning okay

385
00:17:01,830 --> 00:17:06,120
so we're doing this chattering yeah all

386
00:17:05,160 --> 00:17:07,949
right so I'm just going to go ahead and

387
00:17:06,119 --> 00:17:12,809
press escape to stop it because I'm sick

388
00:17:07,949 --> 00:17:17,189
of waiting and so you can see we've now

389
00:17:12,809 --> 00:17:22,588
got down to the 0.39 all right so for

390
00:17:17,189 --> 00:17:24,779
example it guessed that movie 72 for

391
00:17:22,588 --> 00:17:30,359
sorry movie 27 for user seventy two

392
00:17:24,779 --> 00:17:32,670
would get 4.4 for rating 2772 and

393
00:17:30,359 --> 00:17:34,349
actually got a four ready so you can see

394
00:17:32,670 --> 00:17:36,650
like it's it's it's doing something

395
00:17:34,349 --> 00:17:40,679
quite useful

396
00:17:36,650 --> 00:17:42,890
so why is it doing something quite

397
00:17:40,680 --> 00:17:45,120
useful I mean something to note here is

398
00:17:42,890 --> 00:17:48,170
the number of things we're trying to

399
00:17:45,119 --> 00:17:51,299
predict here is there's 225 of them

400
00:17:48,170 --> 00:17:54,210
right and the number of things we're

401
00:17:51,299 --> 00:17:56,039
using to predict is that times two so

402
00:17:54,210 --> 00:17:57,809
hundred and fifty of them so it's not

403
00:17:56,039 --> 00:18:00,240
like we can just exactly fit we actually

404
00:17:57,809 --> 00:18:03,419
have to do some kind of machine learning

405
00:18:00,240 --> 00:18:06,950
here so basically what this is saying is

406
00:18:03,420 --> 00:18:11,190
that there does seem to be some way of

407
00:18:06,950 --> 00:18:12,299
making predictions in this way and so

408
00:18:11,190 --> 00:18:14,580
for those of you that have done some

409
00:18:12,299 --> 00:18:17,309
linear algebra and this is actually a

410
00:18:14,579 --> 00:18:19,220
matrix decomposition normally in linear

411
00:18:17,309 --> 00:18:22,139
algebra you would do this using a

412
00:18:19,220 --> 00:18:23,130
analytical technique or using some

413
00:18:22,140 --> 00:18:25,410
techniques that are specifically

414
00:18:23,130 --> 00:18:27,810
designed for this purpose but the nice

415
00:18:25,410 --> 00:18:29,430
thing is that we can use gradient

416
00:18:27,809 --> 00:18:32,490
descent to solve pretty much everything

417
00:18:29,430 --> 00:18:33,900
including this I don't like to so much

418
00:18:32,490 --> 00:18:35,130
think of it from a linear algebra point

419
00:18:33,900 --> 00:18:36,960
of view though I like to think of it

420
00:18:35,130 --> 00:18:39,810
from an insured point of view which is

421
00:18:36,960 --> 00:18:45,170
this let's say movie sorry let's say

422
00:18:39,809 --> 00:18:51,720
movie id 27 is Lord of the Rings part 1

423
00:18:45,170 --> 00:18:53,190
and let's say move and so let's say

424
00:18:51,720 --> 00:18:56,430
we're trying to make that prediction for

425
00:18:53,190 --> 00:18:59,360
user 2072 are they going to like Lord of

426
00:18:56,430 --> 00:19:02,970
the Rings part 1 and so conceptually

427
00:18:59,359 --> 00:19:03,839
that particular movie maybe there's like

428
00:19:02,970 --> 00:19:07,559
this 4

429
00:19:03,839 --> 00:19:09,480
so there's 5 numbers here and we could

430
00:19:07,559 --> 00:19:11,250
say like well what if the first one was

431
00:19:09,480 --> 00:19:15,960
like how much is it sci-fi and fantasy

432
00:19:11,250 --> 00:19:17,759
and the second one is like how recent a

433
00:19:15,960 --> 00:19:19,230
movie and how much special effects is

434
00:19:17,759 --> 00:19:21,390
there you know and the one at the top

435
00:19:19,230 --> 00:19:23,309
might be like how dialogue-driven is it

436
00:19:21,390 --> 00:19:25,230
right like let's say those kind of five

437
00:19:23,309 --> 00:19:27,569
these five numbers represented

438
00:19:25,230 --> 00:19:30,509
particular things about the movie and so

439
00:19:27,569 --> 00:19:32,730
if that was the case then we could have

440
00:19:30,509 --> 00:19:34,769
the same five numbers for the user

441
00:19:32,730 --> 00:19:37,380
saying like ok how much does the use of

442
00:19:34,769 --> 00:19:42,509
like sci-fi fantasy how much does the

443
00:19:37,380 --> 00:19:45,810
user like modern modern CGI driven

444
00:19:42,509 --> 00:19:48,119
movies how much does this give us a like

445
00:19:45,809 --> 00:19:48,720
dialogue different movies and so if you

446
00:19:48,119 --> 00:19:51,689
then took that

447
00:19:48,720 --> 00:19:53,730
cross-product you would expect to have a

448
00:19:51,690 --> 00:19:57,029
good model right would expect to have a

449
00:19:53,730 --> 00:19:58,589
reasonable reading now the problem is we

450
00:19:57,029 --> 00:20:00,839
don't have this information for each

451
00:19:58,589 --> 00:20:02,939
user we don't have the information for

452
00:20:00,839 --> 00:20:06,689
each movie so we're just going to like

453
00:20:02,940 --> 00:20:08,279
assume that this is a reasonable kind of

454
00:20:06,690 --> 00:20:10,019
way of thinking about this system and

455
00:20:08,279 --> 00:20:13,950
let's unless stochastic gradient descent

456
00:20:10,019 --> 00:20:16,200
try and find these models right so so in

457
00:20:13,950 --> 00:20:19,080
other words these these factors we call

458
00:20:16,200 --> 00:20:20,548
these things factors these factors and

459
00:20:19,079 --> 00:20:22,529
we call them factors because you can

460
00:20:20,548 --> 00:20:24,480
multiply them together to create this

461
00:20:22,529 --> 00:20:26,730
not they're factors and how many

462
00:20:24,480 --> 00:20:29,490
addresses these factors we call them

463
00:20:26,730 --> 00:20:33,450
latent factors because they're not

464
00:20:29,490 --> 00:20:36,029
actually this is not actually a vector

465
00:20:33,450 --> 00:20:38,240
that we've like named and understood and

466
00:20:36,029 --> 00:20:41,940
like entered in manually we've kind of

467
00:20:38,240 --> 00:20:43,650
assumed that we can think of movie

468
00:20:41,940 --> 00:20:46,740
ratings this way we've assumed that we

469
00:20:43,650 --> 00:20:49,169
can think of them as a dot product of

470
00:20:46,740 --> 00:20:51,720
some particular features about a movie

471
00:20:49,169 --> 00:20:53,520
and some particular features of to look

472
00:20:51,720 --> 00:20:55,589
what users like those kinds of movies

473
00:20:53,519 --> 00:20:58,679
right and then we've used gradient

474
00:20:55,589 --> 00:21:03,359
descent to just say okay try and find

475
00:20:58,679 --> 00:21:05,280
some numbers that work so that's that's

476
00:21:03,359 --> 00:21:09,089
basically the technique right and it's

477
00:21:05,279 --> 00:21:10,500
kind of the end and the entirety is in

478
00:21:09,089 --> 00:21:12,538
this printing right so that is

479
00:21:10,500 --> 00:21:14,509
collaborative filtering using what we

480
00:21:12,538 --> 00:21:17,339
call probabilistic matrix factorization

481
00:21:14,509 --> 00:21:19,169
and as you can see the whole thing is

482
00:21:17,339 --> 00:21:21,538
easy to do in an excel spreadsheet and

483
00:21:19,169 --> 00:21:23,429
the entirety of it really is this single

484
00:21:21,538 --> 00:21:26,490
thing which is a single matrix

485
00:21:23,429 --> 00:21:32,490
multiplication plus randomly

486
00:21:26,490 --> 00:21:38,250
initializing if it would be better to

487
00:21:32,490 --> 00:21:39,690
cap this to 0 and 5 maybe yes yeah and

488
00:21:38,250 --> 00:21:41,038
we're gonna do that later right there's

489
00:21:39,690 --> 00:21:44,159
a whole lot of stuff we can do to

490
00:21:41,038 --> 00:21:46,379
improve this this is like our simple as

491
00:21:44,159 --> 00:21:47,490
possible starting point all right so so

492
00:21:46,380 --> 00:21:50,850
what we're going to do now is we're

493
00:21:47,490 --> 00:21:54,750
going to try and implement this in

494
00:21:50,849 --> 00:21:56,939
Python and run it on the whole data set

495
00:21:54,750 --> 00:22:00,359
another question is how do you figure

496
00:21:56,940 --> 00:22:02,539
out how many you know how it's clear how

497
00:22:00,359 --> 00:22:05,839
long are the matrix

498
00:22:02,539 --> 00:22:09,230
five yeah yeah so something to think

499
00:22:05,839 --> 00:22:13,009
about given that this is like movie 49

500
00:22:09,230 --> 00:22:16,279
right and we're looking at a rating for

501
00:22:13,009 --> 00:22:21,289
movie 49 think about this this is

502
00:22:16,279 --> 00:22:24,410
actually at embedding matrix and so this

503
00:22:21,289 --> 00:22:26,779
length is actually the size of the

504
00:22:24,410 --> 00:22:29,060
embedding matrix I'm not saying this is

505
00:22:26,779 --> 00:22:31,519
an analogy I'm saying it literally this

506
00:22:29,059 --> 00:22:34,450
is literally an embedding mattress we

507
00:22:31,519 --> 00:22:37,730
could have a one hot encoding where 72

508
00:22:34,450 --> 00:22:39,920
where a one is in the 72nd position and

509
00:22:37,730 --> 00:22:43,220
so we'd like to look it up and it would

510
00:22:39,920 --> 00:22:45,920
return this list of five numbers so the

511
00:22:43,220 --> 00:22:47,329
question is actually how do we decide on

512
00:22:45,920 --> 00:22:50,210
the dimensionality of our embedding

513
00:22:47,329 --> 00:22:53,779
vectors and the answer to that question

514
00:22:50,210 --> 00:22:57,250
is we have no idea we have to try a few

515
00:22:53,779 --> 00:23:01,009
things and see what was the underlying

516
00:22:57,250 --> 00:23:04,279
concept is you need to pick an embedding

517
00:23:01,009 --> 00:23:06,769
dimensionality which is enough to

518
00:23:04,279 --> 00:23:11,059
reflect the kind of true complexity of

519
00:23:06,769 --> 00:23:14,150
this causal system but not so big that

520
00:23:11,059 --> 00:23:16,309
you have too many parameters that it

521
00:23:14,150 --> 00:23:20,870
could take forever to Tehran or even

522
00:23:16,309 --> 00:23:23,779
with regularization in my overfit so

523
00:23:20,869 --> 00:23:27,349
what does it mean when the factor is

524
00:23:23,779 --> 00:23:30,049
negative that the factor being negative

525
00:23:27,349 --> 00:23:32,299
in the movie case would mean like this

526
00:23:30,049 --> 00:23:34,609
is not dialogue-driven in fact it's like

527
00:23:32,299 --> 00:23:37,579
the opposite dialogue here is terrible a

528
00:23:34,609 --> 00:23:41,539
negative for the user would be like I

529
00:23:37,579 --> 00:23:44,029
actually dislike modern CGI movies so

530
00:23:41,539 --> 00:23:47,480
it's not from zero to whatever it's the

531
00:23:44,029 --> 00:23:49,970
range of score it'd be negative this is

532
00:23:47,480 --> 00:23:51,910
a range of score even like no net Maxim

533
00:23:49,970 --> 00:23:54,259
no there's no constraints at all here

534
00:23:51,910 --> 00:24:02,360
these are just standard embedding

535
00:23:54,259 --> 00:24:05,119
matrices questions so first question is

536
00:24:02,359 --> 00:24:07,159
why do what why can we trust this

537
00:24:05,119 --> 00:24:09,169
embeddings because like if you take a

538
00:24:07,160 --> 00:24:11,840
number six it can be expressed as 1 into

539
00:24:09,170 --> 00:24:14,900
6 or like 6 into 1 or 22 3 &amp; 3 into 2

540
00:24:11,839 --> 00:24:17,629
all so you're saying like we could like

541
00:24:14,900 --> 00:24:19,340
reorder these higher

542
00:24:17,630 --> 00:24:21,410
hardly the value itself might be

543
00:24:19,339 --> 00:24:23,689
different as long as the product is

544
00:24:21,410 --> 00:24:26,180
something well but you see we're using

545
00:24:23,690 --> 00:24:29,990
gradient descent to find the best

546
00:24:26,180 --> 00:24:33,380
numbers so like once we've found a good

547
00:24:29,990 --> 00:24:35,180
minimum the idea is like yeah there are

548
00:24:33,380 --> 00:24:39,620
other numbers but they don't give you as

549
00:24:35,180 --> 00:24:40,789
good an objective value and of course we

550
00:24:39,619 --> 00:24:42,799
should be checking that on a validation

551
00:24:40,789 --> 00:24:44,750
set really which we'll be doing in the

552
00:24:42,799 --> 00:24:47,419
Python version okay and the second

553
00:24:44,750 --> 00:24:50,240
question is when we have a new movie or

554
00:24:47,420 --> 00:24:51,830
a new user to be a 30 trainer model that

555
00:24:50,240 --> 00:24:53,950
is a really good question and there

556
00:24:51,829 --> 00:24:56,839
isn't a straightforward answer to that

557
00:24:53,950 --> 00:24:58,880
time permitting will come back but

558
00:24:56,839 --> 00:25:01,970
basically you would need to have like a

559
00:24:58,880 --> 00:25:05,900
kind of a new user model or a new movie

560
00:25:01,970 --> 00:25:08,000
model that you would use initially and

561
00:25:05,900 --> 00:25:11,000
then over time yes you would then have

562
00:25:08,000 --> 00:25:12,440
to retrain the model so like I don't

563
00:25:11,000 --> 00:25:13,609
know if they still do it but Netflix

564
00:25:12,440 --> 00:25:15,890
used to have this thing that when you

565
00:25:13,609 --> 00:25:17,500
were first on boarded on Netflix it

566
00:25:15,890 --> 00:25:19,640
would say like what movies do you like

567
00:25:17,500 --> 00:25:21,650
and you'd have to go through and let's

568
00:25:19,640 --> 00:25:29,570
say a bunch of movies you like and it

569
00:25:21,650 --> 00:25:34,670
would then my train is moral just find

570
00:25:29,569 --> 00:25:39,980
the nearest movie yeah you could use

571
00:25:34,670 --> 00:25:43,220
nearest neighbors for sure but the thing

572
00:25:39,980 --> 00:25:47,210
is initially at least in this case we

573
00:25:43,220 --> 00:25:48,410
have no columns to describe a movie so

574
00:25:47,210 --> 00:25:51,559
if you had something about like the

575
00:25:48,410 --> 00:25:53,060
movies genre release date who was in it

576
00:25:51,559 --> 00:25:55,609
or something you could have some kind of

577
00:25:53,059 --> 00:25:58,009
non collaborative filtering model and

578
00:25:55,609 --> 00:26:00,109
that's kind of what I meant a new movie

579
00:25:58,009 --> 00:26:08,990
model you have to have some some kind of

580
00:26:00,109 --> 00:26:10,459
predictors okay so a lot of this is

581
00:26:08,990 --> 00:26:12,289
going to look familiar and and the way

582
00:26:10,460 --> 00:26:14,029
I'm going to do this is again it's kind

583
00:26:12,289 --> 00:26:17,659
of this top-down approach we're going to

584
00:26:14,029 --> 00:26:20,089
start using a few features of Pi torch

585
00:26:17,660 --> 00:26:20,720
and fast AI and gradually we're going to

586
00:26:20,089 --> 00:26:24,049
redo it

587
00:26:20,720 --> 00:26:26,799
a few times in a few different ways kind

588
00:26:24,049 --> 00:26:29,269
of doing a little bit deeper each time

589
00:26:26,799 --> 00:26:30,049
um regardless we do need a validation

590
00:26:29,269 --> 00:26:31,779
set

591
00:26:30,049 --> 00:26:34,849
so we can use our standard

592
00:26:31,779 --> 00:26:39,859
cross-validation indexes approach to

593
00:26:34,849 --> 00:26:41,119
grab a random set of ID's this is

594
00:26:39,859 --> 00:26:43,459
something called weight decay which

595
00:26:41,119 --> 00:26:45,169
we'll talk about later in the course for

596
00:26:43,460 --> 00:26:47,049
those of you that have done some machine

597
00:26:45,170 --> 00:26:50,210
learning it's l2 regularization

598
00:26:47,049 --> 00:26:52,639
basically and this is where we choose

599
00:26:50,210 --> 00:26:54,819
how big a embedding matrix do we want

600
00:26:52,640 --> 00:26:57,350
okay

601
00:26:54,819 --> 00:27:02,450
so again you know here's where we get

602
00:26:57,349 --> 00:27:06,730
our model data object from CSV passing

603
00:27:02,450 --> 00:27:10,400
in that ratings file which remember

604
00:27:06,730 --> 00:27:12,799
looks like that okay so you'll see like

605
00:27:10,400 --> 00:27:17,360
stuff tends to look pretty familiar

606
00:27:12,799 --> 00:27:22,849
after a while and then you just have to

607
00:27:17,359 --> 00:27:24,469
pass in the what are your rows

608
00:27:22,849 --> 00:27:25,849
effectively what are your columns

609
00:27:24,470 --> 00:27:28,400
effectively and what are your values

610
00:27:25,849 --> 00:27:31,099
effectively alright so any any

611
00:27:28,400 --> 00:27:32,630
collaborative filtering recommendation

612
00:27:31,099 --> 00:27:35,779
system approach there's basically a

613
00:27:32,630 --> 00:27:39,200
concept of like you know a user and an

614
00:27:35,779 --> 00:27:40,660
item now they might not be users and

615
00:27:39,200 --> 00:27:43,279
items like if you're doing the

616
00:27:40,660 --> 00:27:45,769
Ecuadorian groceries competition there

617
00:27:43,279 --> 00:27:47,089
are stores and items and you're trying

618
00:27:45,769 --> 00:27:53,529
to predict how many things are you going

619
00:27:47,089 --> 00:27:55,699
to sell at this store of this type but

620
00:27:53,529 --> 00:27:57,139
generally speaking just this idea of

621
00:27:55,700 --> 00:27:59,240
like you've got a couple of kind of high

622
00:27:57,140 --> 00:28:00,470
cardinality categorical variables and

623
00:27:59,240 --> 00:28:02,450
something that you're measuring and

624
00:28:00,470 --> 00:28:05,960
you're kind of conceptualizing and

625
00:28:02,450 --> 00:28:08,120
saying okay we could predict the rating

626
00:28:05,960 --> 00:28:12,230
we can predict the value by doing this

627
00:28:08,119 --> 00:28:13,699
this dot for that interestingly this is

628
00:28:12,230 --> 00:28:17,450
kind of relevant to that that last

629
00:28:13,700 --> 00:28:19,610
question or suggestion an identical way

630
00:28:17,450 --> 00:28:23,170
to think about this what I've expressed

631
00:28:19,609 --> 00:28:26,809
this is to say when we're deciding

632
00:28:23,170 --> 00:28:30,529
whether user 72 will like movie

633
00:28:26,809 --> 00:28:36,230
twenty-seven it's basically saying which

634
00:28:30,529 --> 00:28:40,490
other users liked movies that 72 liked

635
00:28:36,230 --> 00:28:43,579
and which other movies were liked by

636
00:28:40,490 --> 00:28:46,880
people like you

637
00:28:43,579 --> 00:28:49,699
user 72 it turns out that these are

638
00:28:46,880 --> 00:28:50,870
basically two ways of saying the exact

639
00:28:49,700 --> 00:28:53,539
same thing so basically what

640
00:28:50,869 --> 00:28:56,239
collaborative filtering is doing you

641
00:28:53,539 --> 00:28:59,509
know kind of conceptually is to say okay

642
00:28:56,240 --> 00:29:01,940
this movie and this user which other

643
00:28:59,509 --> 00:29:04,519
movies are similar to it in terms of

644
00:29:01,940 --> 00:29:07,279
like similar people enjoyed them and

645
00:29:04,519 --> 00:29:08,960
which people are similar to this person

646
00:29:07,279 --> 00:29:11,049
based on people that like the same kind

647
00:29:08,960 --> 00:29:14,480
of movies so that's kind of the

648
00:29:11,049 --> 00:29:16,250
underlying structure at any time there's

649
00:29:14,480 --> 00:29:17,480
an underlying structure like this that

650
00:29:16,250 --> 00:29:22,730
kind of collaborative filtering approach

651
00:29:17,480 --> 00:29:25,099
is likely to be useful okay so so you

652
00:29:22,730 --> 00:29:26,660
yeah so there's basically two parts the

653
00:29:25,099 --> 00:29:28,459
two bits of your thing that you're

654
00:29:26,660 --> 00:29:31,910
factoring and then the the value of the

655
00:29:28,460 --> 00:29:34,309
dependent variable so as per usual we

656
00:29:31,910 --> 00:29:36,380
can take our model data and ask for a

657
00:29:34,309 --> 00:29:39,529
learner from it and we need to tell it

658
00:29:36,380 --> 00:29:42,350
what size of any matrix to use how many

659
00:29:39,529 --> 00:29:45,289
sorry what validation set index is to

660
00:29:42,349 --> 00:29:47,779
use what batch size to use and what

661
00:29:45,289 --> 00:29:51,379
optimizer to use and we're going to be

662
00:29:47,779 --> 00:29:54,649
talking more about optimizes surely we

663
00:29:51,380 --> 00:29:57,530
want to Adam today Adam next week or the

664
00:29:54,650 --> 00:30:00,860
week after and then we can go ahead and

665
00:29:57,529 --> 00:30:03,349
say fit alright and it all looks pretty

666
00:30:00,859 --> 00:30:05,659
similar interest is usually

667
00:30:03,349 --> 00:30:08,029
interestingly I only had to do three

668
00:30:05,660 --> 00:30:08,660
pops like this kind of model seem to

669
00:30:08,029 --> 00:30:11,450
Train

670
00:30:08,660 --> 00:30:13,250
super quickly you can use the learning

671
00:30:11,450 --> 00:30:15,890
rate finder as per usual all the stuff

672
00:30:13,250 --> 00:30:17,809
you're familiar with will work fine and

673
00:30:15,890 --> 00:30:21,080
that was it so this talk you know about

674
00:30:17,809 --> 00:30:22,940
two seconds the Train there's no free

675
00:30:21,079 --> 00:30:27,199
trained anything's here this is from

676
00:30:22,940 --> 00:30:29,840
random from scratch okay so this is our

677
00:30:27,200 --> 00:30:31,730
validation set and we can compare it we

678
00:30:29,839 --> 00:30:33,379
have this is a mean squared error not a

679
00:30:31,730 --> 00:30:37,039
root mean squared error so we can take a

680
00:30:33,380 --> 00:30:39,260
square root so with that last time I ran

681
00:30:37,039 --> 00:30:42,170
it was point seven seven six and that's

682
00:30:39,259 --> 00:30:45,259
0.88 and there's some benchmarks

683
00:30:42,170 --> 00:30:46,700
available for this data set and when I

684
00:30:45,259 --> 00:30:48,470
scrolled through and found the bench the

685
00:30:46,700 --> 00:30:51,019
best benchmark I could find here from

686
00:30:48,470 --> 00:30:54,170
this recommendation system specific

687
00:30:51,019 --> 00:30:58,879
library they had point nine one so we've

688
00:30:54,170 --> 00:31:03,960
got a better loss in two seconds

689
00:30:58,880 --> 00:31:05,610
already so that's good so that's

690
00:31:03,960 --> 00:31:09,299
basically how you can do collaborative

691
00:31:05,609 --> 00:31:12,479
filtering with the faster I library

692
00:31:09,299 --> 00:31:14,399
without thinking too much but so now

693
00:31:12,480 --> 00:31:16,289
we're going to dig in and try and

694
00:31:14,400 --> 00:31:17,790
rebuild that we'll try and get to the

695
00:31:16,289 --> 00:31:21,000
point that we're getting something

696
00:31:17,789 --> 00:31:25,019
around 0.7 seven point seven eight from

697
00:31:21,000 --> 00:31:26,039
scratch but if you want to do this

698
00:31:25,019 --> 00:31:27,799
yourself at home

699
00:31:26,039 --> 00:31:30,450
you know without worry about the detail

700
00:31:27,799 --> 00:31:33,930
that's you know those three lines of

701
00:31:30,450 --> 00:31:35,519
code here's what you need okay so we can

702
00:31:33,930 --> 00:31:39,000
get the predictions in the usual way and

703
00:31:35,519 --> 00:31:40,769
you know we could for example plot SNS

704
00:31:39,000 --> 00:31:42,660
is Seabourn see one's a really great

705
00:31:40,769 --> 00:31:45,119
flooding library it sits on top of

706
00:31:42,660 --> 00:31:47,430
matplotlib it actually leverages

707
00:31:45,119 --> 00:31:48,959
matplotlib so anything you learn about

708
00:31:47,430 --> 00:31:50,310
matplotlib will help you with SIBO and

709
00:31:48,960 --> 00:31:54,230
it's got a few like nice little plots

710
00:31:50,309 --> 00:31:58,740
like this joint plot here is I'm doing

711
00:31:54,230 --> 00:32:00,120
predictions against against actuals so

712
00:31:58,740 --> 00:32:02,339
these are my actual season my

713
00:32:00,119 --> 00:32:04,619
predictions and you can kind of see the

714
00:32:02,339 --> 00:32:06,149
the shape here is that as we predict

715
00:32:04,619 --> 00:32:07,799
higher numbers they actually are higher

716
00:32:06,150 --> 00:32:10,620
numbers and you can also see the

717
00:32:07,799 --> 00:32:12,960
histogram of the predictions and a

718
00:32:10,619 --> 00:32:14,369
histogram of the ashes so that's kind of

719
00:32:12,960 --> 00:32:18,420
floating that is to show you another

720
00:32:14,369 --> 00:32:22,259
interesting visualization would you

721
00:32:18,420 --> 00:32:25,110
please explain the n factors why it's

722
00:32:22,259 --> 00:32:29,400
set to 50 it's set to 50 because I tried

723
00:32:25,109 --> 00:32:31,909
a few things in the world it's the

724
00:32:29,400 --> 00:32:34,890
dimensionality of the embedding images

725
00:32:31,910 --> 00:32:39,860
or to think for it another way it's like

726
00:32:34,890 --> 00:32:39,860
how you know rather than five it's fit

727
00:32:41,750 --> 00:32:50,220
Jeremy I have a question about suppose

728
00:32:45,779 --> 00:32:52,670
that your recommendation system is more

729
00:32:50,220 --> 00:32:56,610
implicit so you have zeros or ones

730
00:32:52,670 --> 00:32:59,730
instead of just actual numbers right so

731
00:32:56,609 --> 00:33:01,759
basically we would then need to use a

732
00:32:59,730 --> 00:33:05,519
classifier instead of regresa

733
00:33:01,759 --> 00:33:06,960
I have to sample the negative or

734
00:33:05,519 --> 00:33:09,509
something like that so if you don't have

735
00:33:06,960 --> 00:33:11,279
it which is up once let's say like just

736
00:33:09,509 --> 00:33:13,890
kind of implicit feedback

737
00:33:11,279 --> 00:33:15,869
oh I'm not sure we'll get to that one in

738
00:33:13,890 --> 00:33:16,980
this class but what I will say is like

739
00:33:15,869 --> 00:33:20,729
in the case that you just doing

740
00:33:16,980 --> 00:33:21,599
classification rather than regression we

741
00:33:20,730 --> 00:33:23,039
haven't actually built that in the

742
00:33:21,599 --> 00:33:24,569
library yet maybe somebody this week

743
00:33:23,039 --> 00:33:26,339
that wants to try adding it it would

744
00:33:24,569 --> 00:33:28,159
only be a small number of lines of code

745
00:33:26,339 --> 00:33:30,720
you basically have to change the

746
00:33:28,160 --> 00:33:34,080
activation function to be a sigmoid and

747
00:33:30,720 --> 00:33:37,039
you would have to change the criterion

748
00:33:34,079 --> 00:33:41,669
or the loss function to be cross-entropy

749
00:33:37,039 --> 00:33:44,849
rather than rmse and that will give you

750
00:33:41,670 --> 00:33:45,990
a classifier rather than a regresar how

751
00:33:44,849 --> 00:33:47,549
those are the only things you'll have to

752
00:33:45,990 --> 00:33:49,319
change so hopefully somebody this week

753
00:33:47,549 --> 00:33:50,940
won't take up that challenge and by the

754
00:33:49,319 --> 00:33:57,029
time we come back next week we've all

755
00:33:50,940 --> 00:33:59,430
have that working ok so I said that

756
00:33:57,029 --> 00:34:02,579
we're basically doing a dot product

757
00:33:59,430 --> 00:34:04,799
right or you know a dot product is kind

758
00:34:02,579 --> 00:34:07,949
of the vector version I guess of this

759
00:34:04,799 --> 00:34:10,409
matrix product so we're basically doing

760
00:34:07,950 --> 00:34:14,130
each of these things times each of these

761
00:34:10,409 --> 00:34:16,110
things and then add it together so let's

762
00:34:14,130 --> 00:34:20,309
just have a look at how we do that in

763
00:34:16,110 --> 00:34:22,110
Python so we can create a tensor in pi

764
00:34:20,309 --> 00:34:25,079
torch just using this little capital T

765
00:34:22,110 --> 00:34:26,610
thing you can just say that's the first

766
00:34:25,079 --> 00:34:29,699
day I version the full version is torch

767
00:34:26,610 --> 00:34:31,289
dot from I'm pie or something but I've

768
00:34:29,699 --> 00:34:34,408
got to set up so you can possibly pass

769
00:34:31,289 --> 00:34:37,679
in even a list of lists so this is going

770
00:34:34,409 --> 00:34:39,929
to create a torch tensor with one two

771
00:34:37,679 --> 00:34:43,440
three four and then here's a torch

772
00:34:39,929 --> 00:34:45,240
tensor with two to ten ten ok so here

773
00:34:43,440 --> 00:34:48,840
are two more chances

774
00:34:45,239 --> 00:34:51,389
I didn't say doc CUDA so they're not on

775
00:34:48,840 --> 00:34:55,559
the GPU they're sitting on the CPU just

776
00:34:51,389 --> 00:34:57,509
FYI we can multiply them together right

777
00:34:55,559 --> 00:35:01,469
and so anytime you have a mathematical

778
00:34:57,510 --> 00:35:04,820
operator between tensors in numpy or

779
00:35:01,469 --> 00:35:06,329
pipe torch it will do element wise

780
00:35:04,820 --> 00:35:07,920
assuming that they're the same

781
00:35:06,329 --> 00:35:11,009
dimensionality which they are they're

782
00:35:07,920 --> 00:35:15,960
both to about two okay and so here we've

783
00:35:11,010 --> 00:35:19,590
got 2 by 2 is 4 3 by 10 is 30 and so

784
00:35:15,960 --> 00:35:21,539
forth ok so there's a a times B so if

785
00:35:19,590 --> 00:35:25,610
you think about basically what we want

786
00:35:21,539 --> 00:35:25,610
to do here is we want to take

787
00:35:29,119 --> 00:35:40,139
ok so I've got 1 times 2 is 2 2 times 2

788
00:35:35,940 --> 00:35:43,860
is 4 2 plus 4 is 6 and so that is

789
00:35:40,139 --> 00:35:48,150
actually the dot product between 1 2 &amp; 2

790
00:35:43,860 --> 00:35:51,780
4 and then here we've got 3 by 10 is 34

791
00:35:48,150 --> 00:35:56,519
by 40 sorry 4 by 10 is 40 30 and 40 and

792
00:35:51,780 --> 00:35:58,380
70 so in other words a times B dot some

793
00:35:56,519 --> 00:36:00,239
along the first dimension

794
00:35:58,380 --> 00:36:03,180
so that's summing up the columns in

795
00:36:00,239 --> 00:36:07,559
other words across a row okay this thing

796
00:36:03,179 --> 00:36:11,399
here is doing the dot product of each of

797
00:36:07,559 --> 00:36:13,110
these rows with each of these rows so it

798
00:36:11,400 --> 00:36:16,079
makes sense and obviously we could do

799
00:36:13,110 --> 00:36:17,640
that with you know some kind of matrix

800
00:36:16,079 --> 00:36:20,150
multiplication approach but I'm trying

801
00:36:17,639 --> 00:36:24,059
to really do things with this little

802
00:36:20,150 --> 00:36:25,220
special case stuff as possible ok so

803
00:36:24,059 --> 00:36:28,079
that's what we're going to use for our

804
00:36:25,219 --> 00:36:31,679
dot products from now on so basically

805
00:36:28,079 --> 00:36:34,259
all we need to do now is remember we

806
00:36:31,679 --> 00:36:36,929
have the data we have is not in that

807
00:36:34,260 --> 00:36:39,270
crosstab format so in excel we've got it

808
00:36:36,929 --> 00:36:42,359
in this crosstab format but we've got it

809
00:36:39,269 --> 00:36:44,639
here in this listed format here's our

810
00:36:42,360 --> 00:36:46,650
movie rating user movie revenue so

811
00:36:44,639 --> 00:36:49,319
conceptually we want to be like looking

812
00:36:46,650 --> 00:36:51,389
up this user into our embedding matrix

813
00:36:49,320 --> 00:36:53,880
to find their 50 factors looking up that

814
00:36:51,389 --> 00:36:56,940
movie to find their 50 factors and then

815
00:36:53,880 --> 00:37:05,130
take the dot product of those two 50

816
00:36:56,940 --> 00:37:08,519
long vectors so let's do that to do it

817
00:37:05,130 --> 00:37:12,210
we're going to build a layer our own

818
00:37:08,519 --> 00:37:15,630
custom neural net layer that's not right

819
00:37:12,210 --> 00:37:17,369
so the the the more generic vocabulary

820
00:37:15,630 --> 00:37:21,119
we call this is we're going to build a

821
00:37:17,369 --> 00:37:24,179
high torch module okay so a PI torch

822
00:37:21,119 --> 00:37:26,369
module is a very specific thing it's

823
00:37:24,179 --> 00:37:27,869
something that you can use as a layer

824
00:37:26,369 --> 00:37:29,909
and a neural net once you've created

825
00:37:27,869 --> 00:37:32,549
your own height watch module you can

826
00:37:29,909 --> 00:37:35,609
throw it into a mirror on it and a

827
00:37:32,550 --> 00:37:36,720
module works by assuming we've already

828
00:37:35,610 --> 00:37:39,059
got once a cordon

829
00:37:36,719 --> 00:37:40,618
model you can pass in some things in

830
00:37:39,059 --> 00:37:43,980
parentheses and it will calculate it

831
00:37:40,619 --> 00:37:48,329
right so assuming that we already have a

832
00:37:43,980 --> 00:37:52,949
modular product we can instantiate it

833
00:37:48,329 --> 00:37:54,900
like so to create our product object and

834
00:37:52,949 --> 00:37:57,960
we can basically now treat that like a

835
00:37:54,900 --> 00:38:00,809
function right but the thing is it's not

836
00:37:57,960 --> 00:38:02,639
just a function because we'll be able to

837
00:38:00,809 --> 00:38:06,358
do things like take derivatives of it

838
00:38:02,639 --> 00:38:08,368
stack them up together into a big stack

839
00:38:06,358 --> 00:38:11,400
of neural network layers blah blah blah

840
00:38:08,369 --> 00:38:14,730
so it's basically a function that we can

841
00:38:11,400 --> 00:38:18,510
kind of compose very conveniently so

842
00:38:14,730 --> 00:38:20,070
here how do we define a module which as

843
00:38:18,510 --> 00:38:23,270
you can see here returns a dot product

844
00:38:20,070 --> 00:38:26,550
well we have to create a Python class

845
00:38:23,269 --> 00:38:28,829
and so if you haven't done - oo before

846
00:38:26,550 --> 00:38:31,410
you're going to have to learn because

847
00:38:28,829 --> 00:38:33,690
all my torch modules are written in

848
00:38:31,409 --> 00:38:35,759
Python oo and that's one of the things I

849
00:38:33,690 --> 00:38:38,400
really like about PI torch is that it

850
00:38:35,760 --> 00:38:40,740
doesn't reinvent totally new ways of

851
00:38:38,400 --> 00:38:43,440
doing things by tensorflow does all the

852
00:38:40,739 --> 00:38:46,049
time in pi torch that you know really

853
00:38:43,440 --> 00:38:48,659
tend to use pythonic ways to do things

854
00:38:46,050 --> 00:38:51,060
so in this case how do you create you

855
00:38:48,659 --> 00:38:55,769
know some kind of new behavior you

856
00:38:51,059 --> 00:38:59,789
create a Python plus it's so Jeremy

857
00:38:55,769 --> 00:39:01,230
suppose that you have a lot of data not

858
00:38:59,789 --> 00:39:03,570
just a little bit of data you can have

859
00:39:01,230 --> 00:39:07,019
in memory will you be able to use fossae

860
00:39:03,570 --> 00:39:08,460
I to solve glory filtering yes

861
00:39:07,019 --> 00:39:15,059
absolutely

862
00:39:08,460 --> 00:39:16,530
it's it uses mini-batch stochastic

863
00:39:15,059 --> 00:39:26,820
gradient descent which does have a batch

864
00:39:16,530 --> 00:39:30,030
at a time the this particular version is

865
00:39:26,820 --> 00:39:33,559
going to create a panda's data frame and

866
00:39:30,030 --> 00:39:37,560
panda's data frame has to live in memory

867
00:39:33,559 --> 00:39:40,920
having said that you can get easily 512

868
00:39:37,559 --> 00:39:42,900
gig you know instances on Amazon so like

869
00:39:40,920 --> 00:39:46,200
if you had a CSV that was bigger than

870
00:39:42,900 --> 00:39:48,510
512 gig you know that would be

871
00:39:46,199 --> 00:39:50,098
impressive if that did happen I guess

872
00:39:48,510 --> 00:39:52,769
you would have to instead

873
00:39:50,099 --> 00:39:54,210
save that as a be calls array and create

874
00:39:52,768 --> 00:39:56,338
a slightly different version that reads

875
00:39:54,210 --> 00:40:00,358
from a because array just streaming in

876
00:39:56,338 --> 00:40:03,778
or maybe from a desk data frame which

877
00:40:00,358 --> 00:40:06,778
also so it would be easy to do I don't

878
00:40:03,778 --> 00:40:09,028
think I've seen real-world situations

879
00:40:06,778 --> 00:40:11,130
where you have 512 gigabyte

880
00:40:09,028 --> 00:40:18,028
collaborative filtering matrices but

881
00:40:11,130 --> 00:40:20,068
yeah we can do it okay now this is PI

882
00:40:18,028 --> 00:40:23,099
torch specific this next bit is that

883
00:40:20,068 --> 00:40:26,278
when you define like the actual work to

884
00:40:23,099 --> 00:40:29,190
be done which is here return user times

885
00:40:26,278 --> 00:40:32,639
movie dot some you have to put it in a

886
00:40:29,190 --> 00:40:34,470
special method called forward okay and

887
00:40:32,639 --> 00:40:37,108
this is this idea that like it's very

888
00:40:34,469 --> 00:40:39,088
likely you're on that right in a neural

889
00:40:37,108 --> 00:40:43,018
net the thing where you calculate the

890
00:40:39,088 --> 00:40:45,210
next set of activations is called the

891
00:40:43,018 --> 00:40:48,179
the forward pass and so that's doing a

892
00:40:45,210 --> 00:40:50,039
forward calculation the gradients is

893
00:40:48,179 --> 00:40:51,989
called the backward calculation we don't

894
00:40:50,039 --> 00:40:53,970
have to do that because PI torch

895
00:40:51,989 --> 00:40:56,849
calculates that automatically so we just

896
00:40:53,969 --> 00:40:59,459
have to define forward so we create a

897
00:40:56,849 --> 00:41:01,528
new class we define forward and here we

898
00:40:59,460 --> 00:41:05,460
write in our definition of dot product

899
00:41:01,528 --> 00:41:08,130
ok so that's it so now that we've

900
00:41:05,460 --> 00:41:11,670
created this class definition we can

901
00:41:08,130 --> 00:41:13,979
instantiate our model right and we can

902
00:41:11,670 --> 00:41:16,528
call our model and get back the numbers

903
00:41:13,978 --> 00:41:20,639
be expected okay so that's it that's how

904
00:41:16,528 --> 00:41:23,389
we create a custom PI torch layer and if

905
00:41:20,639 --> 00:41:25,978
you compare that to like any other

906
00:41:23,389 --> 00:41:28,348
library around pretty much this is way

907
00:41:25,978 --> 00:41:32,248
easier basically I guess because we're

908
00:41:28,349 --> 00:41:34,499
leveraging what's already in person so

909
00:41:32,248 --> 00:41:38,038
let's go ahead and now create a more

910
00:41:34,498 --> 00:41:40,259
complex module and we're going to

911
00:41:38,039 --> 00:41:43,319
basically do the same thing we've got to

912
00:41:40,259 --> 00:41:46,289
have a forward again we're going to have

913
00:41:43,318 --> 00:41:47,699
our users x movies dot sum but we're

914
00:41:46,289 --> 00:41:49,670
going to do one more thing before hand

915
00:41:47,699 --> 00:41:52,618
which is we're going to create two

916
00:41:49,670 --> 00:41:54,930
embedding matrices and then we're going

917
00:41:52,619 --> 00:41:57,509
to look up our users and our movies in

918
00:41:54,929 --> 00:42:01,409
those inventing matrices so let's go

919
00:41:57,509 --> 00:42:03,099
through and and do that so the first

920
00:42:01,409 --> 00:42:07,690
thing to realize is

921
00:42:03,099 --> 00:42:11,109
that the uses the user IDs and the movie

922
00:42:07,690 --> 00:42:12,548
IDs may not be contiguous you know like

923
00:42:11,108 --> 00:42:16,748
they're maybe they start at a million

924
00:42:12,548 --> 00:42:21,278
and go to a million in 1000 so right so

925
00:42:16,748 --> 00:42:23,439
if we just used those IDs directly to

926
00:42:21,278 --> 00:42:24,880
look up into an embedding matrix we

927
00:42:23,440 --> 00:42:27,369
would have to create an embedding matrix

928
00:42:24,880 --> 00:42:29,470
of size 1 million 1000 right which we

929
00:42:27,369 --> 00:42:33,880
don't want to do so the first thing I do

930
00:42:29,469 --> 00:42:37,929
is to get a list of the unique user IDs

931
00:42:33,880 --> 00:42:42,640
and then I create a mapping from every

932
00:42:37,929 --> 00:42:44,469
user ID to a contiguous integer this

933
00:42:42,639 --> 00:42:48,759
thing I've done here where I've created

934
00:42:44,469 --> 00:42:51,669
a dictionary which maps from every

935
00:42:48,759 --> 00:42:52,568
unique thing to a unique index is well

936
00:42:51,670 --> 00:42:54,309
worth studying

937
00:42:52,568 --> 00:42:56,170
during the week because like it's is

938
00:42:54,309 --> 00:42:58,930
super super handy it's something you

939
00:42:56,170 --> 00:43:00,789
very very often have to do in all kinds

940
00:42:58,929 --> 00:43:02,018
of machine learning all right and so I

941
00:43:00,789 --> 00:43:03,640
won't go through it here it's easy

942
00:43:02,018 --> 00:43:06,728
enough to figure out if you can't figure

943
00:43:03,639 --> 00:43:10,150
it out just ask on the forum anyway so

944
00:43:06,728 --> 00:43:14,858
once we've got the mapping from user to

945
00:43:10,150 --> 00:43:18,759
a contiguous index we then can say let's

946
00:43:14,858 --> 00:43:22,119
now replace the user ID column with that

947
00:43:18,759 --> 00:43:25,210
contiguous index right so pandas dot

948
00:43:22,119 --> 00:43:27,729
apply applies an arbitrary function and

949
00:43:25,210 --> 00:43:29,920
python lambda is how you create an

950
00:43:27,728 --> 00:43:32,558
anonymous function on the fly and this

951
00:43:29,920 --> 00:43:36,039
anonymous function simply returns the NS

952
00:43:32,559 --> 00:43:38,380
through the same thing for movies and so

953
00:43:36,039 --> 00:43:41,319
after that we now have the same ratings

954
00:43:38,380 --> 00:43:43,690
table we had before but our IDs have

955
00:43:41,318 --> 00:43:45,518
been mapped to contiguous integers

956
00:43:43,690 --> 00:43:49,239
therefore they're things that we can

957
00:43:45,518 --> 00:43:51,578
look up into an embedding matrix so

958
00:43:49,239 --> 00:43:54,818
let's get the count of our users in our

959
00:43:51,579 --> 00:43:59,430
movies and let's now go ahead and try

960
00:43:54,818 --> 00:44:00,608
and create our Python version of this

961
00:43:59,429 --> 00:44:05,498
okay

962
00:44:00,608 --> 00:44:10,598
so earlier on when we created our

963
00:44:05,498 --> 00:44:14,528
simplest possible PI torch module

964
00:44:10,599 --> 00:44:16,910
there was no like state we didn't need a

965
00:44:14,528 --> 00:44:18,650
constructor because we weren't like

966
00:44:16,909 --> 00:44:19,849
saying how many users are there or how

967
00:44:18,650 --> 00:44:21,760
many movies are there or how many

968
00:44:19,849 --> 00:44:25,719
factors do we want or whatever right

969
00:44:21,760 --> 00:44:28,850
anytime we want to do something like

970
00:44:25,719 --> 00:44:32,449
this where we're passing in and saying

971
00:44:28,849 --> 00:44:34,730
we want to construct our module with

972
00:44:32,449 --> 00:44:37,849
this number of users and this number of

973
00:44:34,730 --> 00:44:40,039
movies then we need a constructor for

974
00:44:37,849 --> 00:44:44,000
our class and you create a constructor

975
00:44:40,039 --> 00:44:45,650
in Python by defining a dunder init

976
00:44:44,000 --> 00:44:47,840
underscore underscore init underscore

977
00:44:45,650 --> 00:44:50,750
underscore yet special name

978
00:44:47,840 --> 00:44:53,360
so this just creates a constructor and

979
00:44:50,750 --> 00:44:54,710
if you haven't done over before you

980
00:44:53,360 --> 00:44:57,890
wanted to do some study during the week

981
00:44:54,710 --> 00:44:59,119
but it's pretty simple idea this is just

982
00:44:57,889 --> 00:45:01,699
the thing that when we create this

983
00:44:59,119 --> 00:45:04,839
object this is what gets wrong okay

984
00:45:01,699 --> 00:45:07,339
again special python thing when you

985
00:45:04,840 --> 00:45:09,559
create your own constructor you have to

986
00:45:07,340 --> 00:45:11,090
call the parent class constructor and if

987
00:45:09,559 --> 00:45:13,909
you want to have all of the cool

988
00:45:11,090 --> 00:45:17,269
behavior of a PI porch module you get

989
00:45:13,909 --> 00:45:20,029
that by inheriting from an end module

990
00:45:17,269 --> 00:45:22,400
neural net module okay so basically by

991
00:45:20,030 --> 00:45:25,280
inheriting here and calling the

992
00:45:22,400 --> 00:45:28,579
superclass constructor we now have a

993
00:45:25,280 --> 00:45:30,190
fully functioning PI torch layer okay so

994
00:45:28,579 --> 00:45:32,989
now we have to give it some behavior and

995
00:45:30,190 --> 00:45:35,900
so we give us some behavior by storing

996
00:45:32,989 --> 00:45:38,139
some things in it all right so here

997
00:45:35,900 --> 00:45:41,869
we're going to create something called

998
00:45:38,139 --> 00:45:45,049
self dot you users and that is going to

999
00:45:41,869 --> 00:45:47,779
be an embedding layer a number of rows

1000
00:45:45,050 --> 00:45:51,890
is an user's number of columns is in

1001
00:45:47,780 --> 00:45:55,280
factors so that is exactly this right

1002
00:45:51,889 --> 00:45:58,879
the number of rows is M users number of

1003
00:45:55,280 --> 00:46:01,880
columns is inventors and then we'll have

1004
00:45:58,880 --> 00:46:04,160
to do the same thing for movies okay so

1005
00:46:01,880 --> 00:46:09,910
that's going to go ahead and create

1006
00:46:04,159 --> 00:46:12,920
these two randomly initialized arrays

1007
00:46:09,909 --> 00:46:14,989
however when you randomly initialize

1008
00:46:12,920 --> 00:46:17,750
over an array it's important to randomly

1009
00:46:14,989 --> 00:46:20,599
initialize it to a reasonable set of

1010
00:46:17,750 --> 00:46:22,400
numbers like a reasonable scale right if

1011
00:46:20,599 --> 00:46:24,769
we randomly initialize them from like

1012
00:46:22,400 --> 00:46:26,180
naught to a million then we would start

1013
00:46:24,769 --> 00:46:29,210
out and you know these things would

1014
00:46:26,179 --> 00:46:30,349
start out being like you know billions

1015
00:46:29,210 --> 00:46:32,300
and billions of

1016
00:46:30,349 --> 00:46:35,659
writing and that's going to be very hard

1017
00:46:32,300 --> 00:46:38,060
to do gradient descent on so I just kind

1018
00:46:35,659 --> 00:46:41,869
of manually figured here like okay about

1019
00:46:38,059 --> 00:46:43,699
what size numbers are going to give me

1020
00:46:41,869 --> 00:46:44,929
about the right readiness and so we

1021
00:46:43,699 --> 00:46:48,199
don't we know we did ratings between

1022
00:46:44,929 --> 00:46:51,019
about normal five so if we start out

1023
00:46:48,199 --> 00:46:52,969
with stuff between about naught and 0.05

1024
00:46:51,019 --> 00:46:56,420
then we're going to get ratings of about

1025
00:46:52,969 --> 00:46:58,969
the right level you can easily enough

1026
00:46:56,420 --> 00:47:02,829
like that calculate that in in neural

1027
00:46:58,969 --> 00:47:05,329
nets there are standard algorithms for

1028
00:47:02,829 --> 00:47:08,920
basically doing doing that calculation

1029
00:47:05,329 --> 00:47:12,170
and the basic the key algorithm is

1030
00:47:08,920 --> 00:47:19,099
something called initialization from

1031
00:47:12,170 --> 00:47:25,639
climbing her and the basic idea is that

1032
00:47:19,099 --> 00:47:29,349
you take the yeah you basically set the

1033
00:47:25,639 --> 00:47:29,349
weights equal to a normal distribution

1034
00:47:32,320 --> 00:47:37,309
with a standard deviation which is

1035
00:47:35,150 --> 00:47:40,990
basically inversely proportional to the

1036
00:47:37,309 --> 00:47:48,949
number of things in the previous layer

1037
00:47:40,989 --> 00:47:51,619
and so in our previous layer so in this

1038
00:47:48,949 --> 00:47:54,859
case we basically if you basically take

1039
00:47:51,619 --> 00:47:58,639
that nor to 0.05 and multiply it by the

1040
00:47:54,860 --> 00:48:00,849
fact that you've got 40 things with a 40

1041
00:47:58,639 --> 00:48:03,799
or 50 things coming out of it

1042
00:48:00,849 --> 00:48:04,940
50 50 things coming out of it and then

1043
00:48:03,800 --> 00:48:09,500
you're going to get something about the

1044
00:48:04,940 --> 00:48:13,159
right size pi torch has already has like

1045
00:48:09,500 --> 00:48:15,440
her initialization class they're like we

1046
00:48:13,159 --> 00:48:16,969
don't in normally in real life have to

1047
00:48:15,440 --> 00:48:19,309
think about this we can just call the

1048
00:48:16,969 --> 00:48:21,230
existing initialization functions but

1049
00:48:19,309 --> 00:48:24,949
we're trying to do this all like from

1050
00:48:21,230 --> 00:48:28,429
scratch here okay without any special

1051
00:48:24,949 --> 00:48:31,699
stuff going on so there's quite a bit of

1052
00:48:28,429 --> 00:48:34,969
pi torch notation here so self dot u

1053
00:48:31,699 --> 00:48:39,489
we've already set to an instance of the

1054
00:48:34,969 --> 00:48:42,949
embedding class it has a dot weight

1055
00:48:39,489 --> 00:48:44,748
attribute which contains the actual the

1056
00:48:42,949 --> 00:48:49,968
actual embed images

1057
00:48:44,748 --> 00:48:53,988
so that contains this the actual

1058
00:48:49,969 --> 00:48:57,679
embedding matrix is not a tensor it's a

1059
00:48:53,989 --> 00:48:59,568
variable a variable is exactly the same

1060
00:48:57,679 --> 00:49:02,478
as a tensor in other words it supports

1061
00:48:59,568 --> 00:49:05,599
the exact same operations as a tensor

1062
00:49:02,478 --> 00:49:08,328
but it also does automatic

1063
00:49:05,599 --> 00:49:13,670
differentiation that's all a variable is

1064
00:49:08,329 --> 00:49:16,339
basically to pull the tensor out of a

1065
00:49:13,670 --> 00:49:19,729
variable you get its data attribute okay

1066
00:49:16,338 --> 00:49:22,639
so this is so this is now the tensor of

1067
00:49:19,728 --> 00:49:25,338
the weight matrix of the self dot you're

1068
00:49:22,639 --> 00:49:27,708
inventing and then something that's

1069
00:49:25,338 --> 00:49:31,038
really handy to know is that all of the

1070
00:49:27,708 --> 00:49:32,558
tensor functions in pi torch you can

1071
00:49:31,039 --> 00:49:35,900
stick an underscore at the end and that

1072
00:49:32,559 --> 00:49:38,839
means do it in place all right so this

1073
00:49:35,900 --> 00:49:41,959
is say create a random uniform random

1074
00:49:38,838 --> 00:49:45,318
number of an appropriate size for this

1075
00:49:41,958 --> 00:49:49,518
tensor and don't return it but actually

1076
00:49:45,318 --> 00:49:51,469
fill in that matrix unless okay so

1077
00:49:49,518 --> 00:49:53,748
that's a super handy thing to know about

1078
00:49:51,469 --> 00:49:57,099
I mean it wouldn't be rocket science

1079
00:49:53,748 --> 00:49:57,098
otherwise we would have to have gone

1080
00:49:59,670 --> 00:50:05,269
[Music]

1081
00:50:02,798 --> 00:50:07,608
okay here's the non in-place version

1082
00:50:05,268 --> 00:50:15,018
that's what saves us some typing saves

1083
00:50:07,608 --> 00:50:18,228
us some screen noise that's all okay so

1084
00:50:15,018 --> 00:50:21,498
now we've got our randomly initialized

1085
00:50:18,228 --> 00:50:24,768
embedding weight matrices and so now the

1086
00:50:21,498 --> 00:50:27,348
forward I'm actually going to use the

1087
00:50:24,768 --> 00:50:31,068
same columnar model data that we used

1088
00:50:27,349 --> 00:50:32,959
for Russman and so it's actually going

1089
00:50:31,068 --> 00:50:35,838
to be passed both categorical variables

1090
00:50:32,958 --> 00:50:38,538
and continuous variables and in this

1091
00:50:35,838 --> 00:50:41,420
case there are no continuous variables

1092
00:50:38,539 --> 00:50:43,400
so I'm just going to grab the 0th column

1093
00:50:41,420 --> 00:50:45,829
out of the categorical variables and

1094
00:50:43,400 --> 00:50:48,229
call it users and the first column and

1095
00:50:45,829 --> 00:50:50,929
call it movies okay so I'm just kind of

1096
00:50:48,228 --> 00:50:52,399
too lazy to create my own I've lots to

1097
00:50:50,929 --> 00:50:53,660
do about too lazy out that we do have a

1098
00:50:52,400 --> 00:50:55,489
special class with this but I'm trying

1099
00:50:53,659 --> 00:50:57,808
to avoid creating a special class so

1100
00:50:55,489 --> 00:51:01,139
just going to leverage this columnar

1101
00:50:57,809 --> 00:51:04,430
model data plus okay so we can basically

1102
00:51:01,139 --> 00:51:07,048
grab our user and movies mini-batches

1103
00:51:04,429 --> 00:51:08,728
right and remember this is not a single

1104
00:51:07,048 --> 00:51:13,679
user in a single movie this is going to

1105
00:51:08,728 --> 00:51:15,929
be a whole mini batch of them we can now

1106
00:51:13,679 --> 00:51:18,899
look up that mini batch of users in our

1107
00:51:15,929 --> 00:51:21,688
embedding matrix U and the movies in are

1108
00:51:18,900 --> 00:51:23,639
embedding matrix okay so this is like

1109
00:51:21,688 --> 00:51:27,048
exactly the same is just doing an array

1110
00:51:23,639 --> 00:51:29,338
lookup to grab the user ID numbered

1111
00:51:27,048 --> 00:51:31,318
value but we're doing that a whole mini

1112
00:51:29,338 --> 00:51:33,478
batch at a time right and so it's

1113
00:51:31,318 --> 00:51:34,708
because PI torch can do a whole mini

1114
00:51:33,478 --> 00:51:36,838
batch at a time with pretty much

1115
00:51:34,708 --> 00:51:39,208
everything that we can get really easy

1116
00:51:36,838 --> 00:51:41,489
speed up we don't have to write any

1117
00:51:39,208 --> 00:51:43,919
loops on the whole to do everything

1118
00:51:41,489 --> 00:51:45,509
through our mini batch and in fact if

1119
00:51:43,920 --> 00:51:48,088
you do ever loop through your mini batch

1120
00:51:45,509 --> 00:51:50,489
manually you don't get GPU acceleration

1121
00:51:48,088 --> 00:51:52,619
that's really important to know right so

1122
00:51:50,489 --> 00:51:54,630
you never want to loop have a for loop

1123
00:51:52,619 --> 00:51:56,640
going through your mini batch you always

1124
00:51:54,630 --> 00:51:59,338
want to do things in this kind of like

1125
00:51:56,639 --> 00:52:01,618
whole mini batch at a time but pretty

1126
00:51:59,338 --> 00:52:03,268
much everything imply torch does things

1127
00:52:01,619 --> 00:52:05,119
are holding events at a time so you

1128
00:52:03,268 --> 00:52:08,038
shouldn't have to worry about it

1129
00:52:05,119 --> 00:52:14,849
and then here's our product just like

1130
00:52:08,039 --> 00:52:18,539
before right so having to find that I'm

1131
00:52:14,849 --> 00:52:22,079
now going to go ahead and say alright my

1132
00:52:18,539 --> 00:52:24,630
X values is everything except the rating

1133
00:52:22,079 --> 00:52:27,298
and the timestamp in my writings table

1134
00:52:24,630 --> 00:52:30,838
my Y is my rating and then I can just

1135
00:52:27,298 --> 00:52:33,929
say okay let's grab a model data from a

1136
00:52:30,838 --> 00:52:36,989
data frame using that X and that Y and

1137
00:52:33,929 --> 00:52:43,048
here is our list of categorical

1138
00:52:36,989 --> 00:52:47,519
variables okay and then so let's now

1139
00:52:43,048 --> 00:52:49,548
instantiate that PI torch object alright

1140
00:52:47,518 --> 00:52:52,228
so we've now created that from scratch

1141
00:52:49,548 --> 00:52:55,469
and then the next thing we need to do is

1142
00:52:52,228 --> 00:53:00,149
to create an optimizer so this is part

1143
00:52:55,469 --> 00:53:02,639
of pi torch the only fast AI thing here

1144
00:53:00,150 --> 00:53:05,160
is this line right because that's like I

1145
00:53:02,639 --> 00:53:07,498
don't think showing you how to build

1146
00:53:05,159 --> 00:53:09,509
data sets and data load is interesting

1147
00:53:07,498 --> 00:53:11,399
enough really we might do that in part

1148
00:53:09,509 --> 00:53:12,838
two of the course and

1149
00:53:11,400 --> 00:53:14,430
it's actually so straightforward like a

1150
00:53:12,838 --> 00:53:17,219
lot of you are already doing it on the

1151
00:53:14,429 --> 00:53:19,048
forums so I'm not going to show you that

1152
00:53:17,219 --> 00:53:21,959
in this part but if you're interested

1153
00:53:19,048 --> 00:53:24,380
feel free to talk on the forums about it

1154
00:53:21,960 --> 00:53:26,670
but I'm just going to basically take the

1155
00:53:24,380 --> 00:53:27,930
thing that feeds us data is a given

1156
00:53:26,670 --> 00:53:29,700
particularly cuz these things are so

1157
00:53:27,929 --> 00:53:31,379
flexible right you know if you've got

1158
00:53:29,699 --> 00:53:34,588
stuff enough data frame you can just use

1159
00:53:31,380 --> 00:53:35,940
this you don't have to rewrite it so

1160
00:53:34,588 --> 00:53:39,230
that's the only fast AI thing we're

1161
00:53:35,940 --> 00:53:42,420
using so this is a PI torch thing and so

1162
00:53:39,230 --> 00:53:44,639
optiom is the thing and pi torch that

1163
00:53:42,420 --> 00:53:48,358
gives us an optimizer will be learning

1164
00:53:44,639 --> 00:53:49,858
about that very shortly so it's actually

1165
00:53:48,358 --> 00:53:54,900
the thing that's going to update our

1166
00:53:49,858 --> 00:53:58,798
weights pi torch calls them the

1167
00:53:54,900 --> 00:54:01,079
parameters of the model so earlier on we

1168
00:53:58,798 --> 00:54:04,829
set model equals embedding dot blah blah

1169
00:54:01,079 --> 00:54:07,859
right and because embedding dot derives

1170
00:54:04,829 --> 00:54:10,230
from NN module we get all of the pi

1171
00:54:07,858 --> 00:54:12,358
torch module behavior and one of the

1172
00:54:10,230 --> 00:54:16,889
things we got for free is the ability to

1173
00:54:12,358 --> 00:54:19,528
say got parameters so that's pretty

1174
00:54:16,889 --> 00:54:21,230
that's pretty any right that's the thing

1175
00:54:19,528 --> 00:54:24,838
that basically is going to automatically

1176
00:54:21,230 --> 00:54:27,358
give us a list of all of the weights in

1177
00:54:24,838 --> 00:54:29,058
our model that have to be updated and so

1178
00:54:27,358 --> 00:54:31,380
that's what gets passed to the optimizer

1179
00:54:29,059 --> 00:54:34,650
we also passed the optimized at the

1180
00:54:31,380 --> 00:54:37,079
learning rate the weight decay which

1181
00:54:34,650 --> 00:54:42,119
we'll talk about later and momentum that

1182
00:54:37,079 --> 00:54:43,950
we'll talk about later okay one other

1183
00:54:42,119 --> 00:54:45,809
thing that I'm not going to do right now

1184
00:54:43,949 --> 00:54:48,239
but we will do later is to write a

1185
00:54:45,809 --> 00:54:51,170
training loop so the training loop is a

1186
00:54:48,239 --> 00:54:54,778
thing that lives for each mini batch and

1187
00:54:51,170 --> 00:54:58,230
updates the weight to subtract the

1188
00:54:54,778 --> 00:55:01,079
gradient times the moment there's a

1189
00:54:58,230 --> 00:55:07,740
function in fast AI which is the

1190
00:55:01,079 --> 00:55:12,510
training loop and it's it's pretty

1191
00:55:07,739 --> 00:55:16,048
simple here it is right for a POC in

1192
00:55:12,510 --> 00:55:18,359
epochs this is just the thing that shows

1193
00:55:16,048 --> 00:55:21,769
a progress bar so ignore this for X

1194
00:55:18,358 --> 00:55:25,098
comma Y in my training data loader

1195
00:55:21,769 --> 00:55:25,099
calculate the loss

1196
00:55:25,489 --> 00:55:32,069
print out the lots you know in a

1197
00:55:28,860 --> 00:55:37,980
progress bar call any callbacks you have

1198
00:55:32,070 --> 00:55:39,960
and at the end call the call the metrics

1199
00:55:37,980 --> 00:55:41,670
on the validation alright so there's

1200
00:55:39,960 --> 00:55:46,110
there's just eh

1201
00:55:41,670 --> 00:55:50,760
Apoc go through each mini batch and do

1202
00:55:46,110 --> 00:55:52,050
one step of optimizer step is basically

1203
00:55:50,760 --> 00:55:54,060
going to take advantage of this

1204
00:55:52,050 --> 00:55:58,289
optimizer but we'll be writing that from

1205
00:55:54,059 --> 00:56:00,659
scratch shortly so this is notice we're

1206
00:55:58,289 --> 00:56:04,590
not using a learner okay we're just

1207
00:56:00,659 --> 00:56:05,969
using a hi book module so this this fit

1208
00:56:04,590 --> 00:56:09,000
thing although it's passed to a part of

1209
00:56:05,969 --> 00:56:10,919
fast AI it's like lower down the layers

1210
00:56:09,000 --> 00:56:14,699
of abstraction now this is the thing

1211
00:56:10,920 --> 00:56:19,159
that takes a regular high torch model so

1212
00:56:14,699 --> 00:56:21,509
if you ever want to like skip as much

1213
00:56:19,159 --> 00:56:23,609
faster eye stuff as possible like you've

1214
00:56:21,510 --> 00:56:24,990
got some high torch model you've got

1215
00:56:23,610 --> 00:56:27,300
some code on the internet you basically

1216
00:56:24,989 --> 00:56:29,279
want to run it that you don't want to

1217
00:56:27,300 --> 00:56:30,960
write your own training loop then this

1218
00:56:29,280 --> 00:56:33,210
is this is what you want to do you want

1219
00:56:30,960 --> 00:56:35,849
to call fast a high speed version and so

1220
00:56:33,210 --> 00:56:37,920
what you'll find is like the library is

1221
00:56:35,849 --> 00:56:41,250
designed so that you can kind of dig in

1222
00:56:37,920 --> 00:56:43,039
at any layer abstraction you like right

1223
00:56:41,250 --> 00:56:46,039
and so at this layer of abstraction

1224
00:56:43,039 --> 00:56:47,699
you're not going to get things like

1225
00:56:46,039 --> 00:56:49,980
stochastic gradient descent with

1226
00:56:47,699 --> 00:56:51,779
restarts you're not going to get like

1227
00:56:49,980 --> 00:56:53,400
differential learning rates like all

1228
00:56:51,780 --> 00:56:55,050
that stuff that's in the learner like

1229
00:56:53,400 --> 00:56:57,539
you could do it but you'd have to write

1230
00:56:55,050 --> 00:56:59,490
it all about by hand yourself alright

1231
00:56:57,539 --> 00:57:02,099
and that's the downside of kind of going

1232
00:56:59,489 --> 00:57:04,559
down to this level of abstraction the

1233
00:57:02,099 --> 00:57:06,539
upside is that as you saw the code for

1234
00:57:04,559 --> 00:57:08,369
this is very simple it's just a simple

1235
00:57:06,539 --> 00:57:11,130
training loop it takes a standard 5

1236
00:57:08,369 --> 00:57:13,710
torch model so this is like this is a

1237
00:57:11,130 --> 00:57:15,720
good thing for us to use here we can we

1238
00:57:13,710 --> 00:57:19,099
just call it and it looks exactly like

1239
00:57:15,719 --> 00:57:22,889
what we used to see all right we got our

1240
00:57:19,099 --> 00:57:29,460
validation and training loss for the 3 e

1241
00:57:22,889 --> 00:57:34,049
bus now you'll notice that we wanted

1242
00:57:29,460 --> 00:57:37,019
something around 0.76 so we're not there

1243
00:57:34,050 --> 00:57:38,640
so in other words the the the default

1244
00:57:37,019 --> 00:57:41,369
fast AI collaborative dory

1245
00:57:38,639 --> 00:57:44,538
rhythm is doing something smarter than

1246
00:57:41,369 --> 00:57:46,920
this so we're going to try and do that

1247
00:57:44,539 --> 00:57:49,019
one thing that we can do since we're

1248
00:57:46,920 --> 00:57:50,940
calling our you know this lower level

1249
00:57:49,018 --> 00:57:52,409
fifth function there's no learning rate

1250
00:57:50,940 --> 00:57:54,088
and kneeling we could do our own

1251
00:57:52,409 --> 00:57:55,828
learning rate annealing so you can hear

1252
00:57:54,088 --> 00:57:57,960
it see here there's a first day I

1253
00:57:55,829 --> 00:57:59,789
function called set learning rates you

1254
00:57:57,960 --> 00:58:02,159
can pass in a standard height watch

1255
00:57:59,789 --> 00:58:05,250
optimizer and pass in your new learning

1256
00:58:02,159 --> 00:58:08,489
rate and then call fit again and so this

1257
00:58:05,250 --> 00:58:10,170
is how we can let manually do a learning

1258
00:58:08,489 --> 00:58:13,739
rate schedule and so you can see we've

1259
00:58:10,170 --> 00:58:18,930
got a little bit better 1.13 where you

1260
00:58:13,739 --> 00:58:20,729
still got a long way to go okay so I

1261
00:58:18,929 --> 00:58:24,328
think what we might do is we might have

1262
00:58:20,730 --> 00:58:27,630
a seven minute break and then we're

1263
00:58:24,329 --> 00:58:38,490
going to come back and try and improve

1264
00:58:27,630 --> 00:58:39,809
this core of it for those who are

1265
00:58:38,489 --> 00:58:42,348
interested somebody was asking me the

1266
00:58:39,809 --> 00:58:45,778
break for a kind of a quick walkthrough

1267
00:58:42,349 --> 00:58:47,400
so this is totally optional but if you

1268
00:58:45,778 --> 00:58:54,960
go into the first day I library there's

1269
00:58:47,400 --> 00:58:57,119
a model py file and that's where fit is

1270
00:58:54,960 --> 00:59:01,048
which we're just looking at which goes

1271
00:58:57,119 --> 00:59:03,568
through each epoch in epochs and then

1272
00:59:01,048 --> 00:59:07,469
goes through each x and y in the mini

1273
00:59:03,568 --> 00:59:15,389
batch and then it calls this step

1274
00:59:07,469 --> 00:59:17,038
function so the step function is here

1275
00:59:15,389 --> 00:59:19,409
and you can see the key thing is it

1276
00:59:17,039 --> 00:59:20,940
calculates the output from the model the

1277
00:59:19,409 --> 00:59:26,250
models for M right and so if you

1278
00:59:20,940 --> 00:59:28,500
remember our dot product we didn't

1279
00:59:26,250 --> 00:59:30,719
actually call model dot forward we just

1280
00:59:28,500 --> 00:59:35,059
called model parentheses and that's

1281
00:59:30,719 --> 00:59:37,139
because the N n dot module automatically

1282
00:59:35,059 --> 00:59:39,569
you know when you call it as if it's a

1283
00:59:37,139 --> 00:59:42,058
function it passes it along to forward

1284
00:59:39,568 --> 00:59:44,099
okay so that's that's what that's doing

1285
00:59:42,059 --> 00:59:45,960
there right and then the rest of this

1286
00:59:44,099 --> 00:59:49,680
world will learn about shortly which is

1287
00:59:45,960 --> 00:59:50,550
basically doing the the loss function

1288
00:59:49,679 --> 00:59:53,789
and

1289
00:59:50,550 --> 00:59:55,740
the backward pass okay so for those who

1290
00:59:53,789 --> 00:59:57,480
are interested that's that's kind of

1291
00:59:55,739 --> 00:59:59,009
gets you a bit of a sense of how the

1292
00:59:57,480 --> 01:00:02,880
cone it's structured if you want to look

1293
00:59:59,010 --> 01:00:06,450
at it and as I say like the the faster I

1294
01:00:02,880 --> 01:00:09,780
code is designed to both be world-class

1295
01:00:06,449 --> 01:00:13,469
performance but also pretty easy to read

1296
01:00:09,780 --> 01:00:15,210
so like feel free like take a look at it

1297
01:00:13,469 --> 01:00:18,000
and if you want to know what's going on

1298
01:00:15,210 --> 01:00:19,230
just ask on the forums and if you you

1299
01:00:18,000 --> 01:00:25,349
know if you think is anything that could

1300
01:00:19,230 --> 01:00:26,550
be clearer let us know because yeah the

1301
01:00:25,349 --> 01:00:31,829
code is definitely now we're going to be

1302
01:00:26,550 --> 01:00:33,539
digging into the code or in law okay so

1303
01:00:31,829 --> 01:00:36,119
let's try and improve this a little bit

1304
01:00:33,539 --> 01:00:40,139
and let's start off by improving it in

1305
01:00:36,119 --> 01:00:43,529
Excel so you might have noticed here

1306
01:00:40,139 --> 01:00:44,789
that we've kind of got the idea that use

1307
01:00:43,530 --> 01:00:48,210
a 72

1308
01:00:44,789 --> 01:00:50,969
you know like sci-fi modern movies with

1309
01:00:48,210 --> 01:00:53,760
special effects you know whatever and

1310
01:00:50,969 --> 01:00:57,149
movie number 27 is sci-fi and that

1311
01:00:53,760 --> 01:01:00,240
special effects so much dialogue but

1312
01:00:57,150 --> 01:01:05,400
we're missing an important case which is

1313
01:01:00,239 --> 01:01:07,919
like user 72 is pretty enthusiastic on

1314
01:01:05,400 --> 01:01:10,710
the hall and on average rates things

1315
01:01:07,920 --> 01:01:14,420
higher and Highland you know and movie

1316
01:01:10,710 --> 01:01:17,039
27 you know it's just a popular movie

1317
01:01:14,420 --> 01:01:20,039
you know it's just on average its higher

1318
01:01:17,039 --> 01:01:24,300
so what would really like is to add a

1319
01:01:20,039 --> 01:01:26,699
constant for the user and a constant for

1320
01:01:24,300 --> 01:01:29,640
the movie and remember in neural network

1321
01:01:26,699 --> 01:01:32,279
terms we call that a bias that's what we

1322
01:01:29,639 --> 01:01:35,069
want to add a bias so we could easily do

1323
01:01:32,280 --> 01:01:37,380
that and if we go into the bias tab here

1324
01:01:35,070 --> 01:01:41,430
we've got the same data as before and

1325
01:01:37,380 --> 01:01:44,420
we've got the same latent factors as

1326
01:01:41,429 --> 01:01:48,419
before and I've just got one extra row

1327
01:01:44,420 --> 01:01:51,780
here and one extra column here and you

1328
01:01:48,420 --> 01:01:53,460
won't be surprised here that we now take

1329
01:01:51,780 --> 01:01:58,400
these same matrix multiplication as

1330
01:01:53,460 --> 01:02:01,750
before and we add in that and we add in

1331
01:01:58,400 --> 01:02:05,559
that okay so that's

1332
01:02:01,750 --> 01:02:07,510
bias so other than that we've got

1333
01:02:05,559 --> 01:02:11,200
exactly the same loss function over here

1334
01:02:07,510 --> 01:02:14,650
and so just like before we can now go

1335
01:02:11,199 --> 01:02:18,519
ahead and solve that and now our

1336
01:02:14,650 --> 01:02:20,980
changing variables include the bias and

1337
01:02:18,519 --> 01:02:23,139
we can say solve and if we leave that

1338
01:02:20,980 --> 01:02:27,630
for a little while it will come to a

1339
01:02:23,139 --> 01:02:30,578
better result than we had before

1340
01:02:27,630 --> 01:02:32,579
okay so that's the first thing we're

1341
01:02:30,579 --> 01:02:37,900
going to do to improve our model and

1342
01:02:32,579 --> 01:02:41,559
there's really very little show just to

1343
01:02:37,900 --> 01:02:43,690
make the code a bit shorter I have to

1344
01:02:41,559 --> 01:02:46,900
find a function called get embedding

1345
01:02:43,690 --> 01:02:48,730
which takes a number of inputs and a

1346
01:02:46,900 --> 01:02:50,470
number of factors so the number of rows

1347
01:02:48,730 --> 01:02:52,449
and the embedding matrix Nomos they're

1348
01:02:50,469 --> 01:02:57,189
both medications creates the embedding

1349
01:02:52,449 --> 01:02:59,019
and then randomly initializes it I don't

1350
01:02:57,190 --> 01:03:00,940
know why I'm doing negative to positive

1351
01:02:59,019 --> 01:03:02,230
here and it zeroed last time honestly it

1352
01:03:00,940 --> 01:03:05,349
doesn't matter much as long as it's in

1353
01:03:02,230 --> 01:03:09,400
the right ballpark and then we return

1354
01:03:05,349 --> 01:03:11,950
that initialized emitting so now we need

1355
01:03:09,400 --> 01:03:14,710
not just our users by factors which are

1356
01:03:11,949 --> 01:03:16,750
Chuck into you our movies by factors

1357
01:03:14,710 --> 01:03:20,440
which I've shocked into M but we also

1358
01:03:16,750 --> 01:03:23,528
need users by one which will put into UV

1359
01:03:20,440 --> 01:03:25,900
user bias and movies by one which will

1360
01:03:23,528 --> 01:03:28,480
put into the movie bias okay so this is

1361
01:03:25,900 --> 01:03:29,889
just doing a list comprehension going

1362
01:03:28,480 --> 01:03:31,599
through each of the tuples create an

1363
01:03:29,889 --> 01:03:34,629
embedding for each of them and putting

1364
01:03:31,599 --> 01:03:40,510
them into these things okay so now our

1365
01:03:34,630 --> 01:03:44,260
forward is exactly the same as before u

1366
01:03:40,510 --> 01:03:45,520
times M sub I mean this is actually a

1367
01:03:44,260 --> 01:03:50,289
little confusing because we're doing it

1368
01:03:45,519 --> 01:03:51,750
into two steps maybe they make it a bit

1369
01:03:50,289 --> 01:03:58,380
easier let's pull this out

1370
01:03:51,750 --> 01:03:58,380
put it up here put this in parentheses

1371
01:03:58,440 --> 01:04:02,769
okay so maybe that looks a little bit

1372
01:04:00,460 --> 01:04:04,389
more familiar all right you times n dot

1373
01:04:02,769 --> 01:04:07,150
some that's the same dot product and

1374
01:04:04,389 --> 01:04:08,650
then here it is going to add in our user

1375
01:04:07,150 --> 01:04:14,260
pious and

1376
01:04:08,650 --> 01:04:17,829
our movie bus dot squeeze is the PI

1377
01:04:14,260 --> 01:04:21,370
torch thing that adds an additional unit

1378
01:04:17,829 --> 01:04:23,340
axis that's not going to make any sense

1379
01:04:21,369 --> 01:04:25,719
if you haven't done broadcasting before

1380
01:04:23,340 --> 01:04:27,640
I'm not going to do a broadcasting in

1381
01:04:25,719 --> 01:04:28,029
this course because we've already done

1382
01:04:27,639 --> 01:04:29,379
it

1383
01:04:28,030 --> 01:04:31,750
and we're doing it in the machine

1384
01:04:29,380 --> 01:04:34,720
learning course but basically in in

1385
01:04:31,750 --> 01:04:36,760
short broadcasting is what happens when

1386
01:04:34,719 --> 01:04:40,689
you do something like this where um is a

1387
01:04:36,760 --> 01:04:44,740
matrix you be self-taught you the users

1388
01:04:40,690 --> 01:04:46,900
is a is a vector how do you add a vector

1389
01:04:44,739 --> 01:04:51,219
to a matrix and basically what it does

1390
01:04:46,900 --> 01:04:53,800
is it duplicates the vector so that it

1391
01:04:51,219 --> 01:04:55,629
makes it the same size as the matrix and

1392
01:04:53,800 --> 01:04:58,000
the particular way whether it duplicates

1393
01:04:55,630 --> 01:05:00,550
it across columns or down rows or how it

1394
01:04:58,000 --> 01:05:03,300
does it is called broadcasting the

1395
01:05:00,550 --> 01:05:05,380
broadcasting rules are the same as numpy

1396
01:05:03,300 --> 01:05:07,360
Pytor didn't actually used to support

1397
01:05:05,380 --> 01:05:09,730
broadcasting so I was actually the guy

1398
01:05:07,360 --> 01:05:11,920
who first added broadcasting to PI torch

1399
01:05:09,730 --> 01:05:13,719
using an ugly hack and then the pipe or

1400
01:05:11,920 --> 01:05:17,110
authors did an awesome job of supporting

1401
01:05:13,719 --> 01:05:19,149
it actually inside the language so now

1402
01:05:17,110 --> 01:05:22,180
you can use the same broadcasting

1403
01:05:19,150 --> 01:05:24,639
operations in five torches non-player if

1404
01:05:22,179 --> 01:05:27,399
you haven't dealt with this before it's

1405
01:05:24,639 --> 01:05:29,469
really important to learn it because

1406
01:05:27,400 --> 01:05:31,450
like it's it's kind of the most

1407
01:05:29,469 --> 01:05:34,179
important fundamental way to do

1408
01:05:31,449 --> 01:05:35,980
computations quickly in the high-end

1409
01:05:34,179 --> 01:05:37,389
paid warship it's the thing that lets

1410
01:05:35,980 --> 01:05:38,889
you not have to do loops

1411
01:05:37,389 --> 01:05:40,509
how could you imagine here if I had to

1412
01:05:38,889 --> 01:05:43,929
look through every row of this matrix

1413
01:05:40,510 --> 01:05:46,870
and add each did you know this vector to

1414
01:05:43,929 --> 01:05:50,589
every row it would be slow the you know

1415
01:05:46,869 --> 01:05:52,299
a lot more code and the idea of

1416
01:05:50,590 --> 01:05:54,610
broadcasting it actually goes all the

1417
01:05:52,300 --> 01:05:56,890
way back to APL which was a language

1418
01:05:54,610 --> 01:05:59,769
designed in the 50s by an extraordinary

1419
01:05:56,889 --> 01:06:02,109
guy called Ken Iverson yeah APL was

1420
01:05:59,769 --> 01:06:05,110
originally designed or written out as a

1421
01:06:02,110 --> 01:06:08,170
new type of mathematical notation he has

1422
01:06:05,110 --> 01:06:10,329
this great essay called notation as a

1423
01:06:08,170 --> 01:06:12,159
tool for thought and the idea was that

1424
01:06:10,329 --> 01:06:15,219
like really good notation could actually

1425
01:06:12,159 --> 01:06:17,559
make you think of better things and part

1426
01:06:15,219 --> 01:06:20,109
of that notation is this idea of

1427
01:06:17,559 --> 01:06:21,429
broadcasting I'm incredibly enthusiastic

1428
01:06:20,110 --> 01:06:25,630
about it

1429
01:06:21,429 --> 01:06:30,879
and we're gonna use it plenty so either

1430
01:06:25,630 --> 01:06:33,880
watch the machine learning lesson or you

1431
01:06:30,880 --> 01:06:36,220
know google numpy broadcasting for

1432
01:06:33,880 --> 01:06:37,990
information anyway

1433
01:06:36,219 --> 01:06:40,859
so basically it works reasonably

1434
01:06:37,989 --> 01:06:46,919
intuitively we can add on we can add the

1435
01:06:40,860 --> 01:06:49,630
vectors to the matrix all right

1436
01:06:46,920 --> 01:06:53,619
having done that we're now going to do

1437
01:06:49,630 --> 01:06:56,220
one more trick which is I think it was

1438
01:06:53,619 --> 01:06:59,889
your net asked earlier about could we

1439
01:06:56,219 --> 01:07:05,289
squish the ratings to be between one and

1440
01:06:59,889 --> 01:07:09,099
five and the answer is we could right

1441
01:07:05,289 --> 01:07:12,840
and specifically what we could do is we

1442
01:07:09,099 --> 01:07:14,110
could put it through a sigmoid function

1443
01:07:12,840 --> 01:07:19,360
all right

1444
01:07:14,110 --> 01:07:24,900
so remind you a sigmoid function looks

1445
01:07:19,360 --> 01:07:27,400
like that right and this is that's one

1446
01:07:24,900 --> 01:07:29,500
okay we could put it through a secret

1447
01:07:27,400 --> 01:07:31,599
function so we could take like four

1448
01:07:29,500 --> 01:07:33,519
point nine six and put it through a

1449
01:07:31,599 --> 01:07:35,170
sigmoid function and like that you know

1450
01:07:33,519 --> 01:07:37,449
that's kind of high so it kind of be

1451
01:07:35,170 --> 01:07:41,170
over here somewhere right

1452
01:07:37,449 --> 01:07:44,799
and then we could multiply that sigmoid

1453
01:07:41,170 --> 01:07:46,000
like the result of that by five for

1454
01:07:44,800 --> 01:07:47,019
example all right

1455
01:07:46,000 --> 01:07:49,360
and in this case we want it to be

1456
01:07:47,019 --> 01:07:51,610
between one and five right so maybe we

1457
01:07:49,360 --> 01:07:56,500
would multiply it by four and add one

1458
01:07:51,610 --> 01:08:00,099
instance that's the basic idea and so

1459
01:07:56,500 --> 01:08:01,630
here is that trick we take the result so

1460
01:08:00,099 --> 01:08:03,940
the result is basically the the thing

1461
01:08:01,630 --> 01:08:05,769
that comes straight out of the dot

1462
01:08:03,940 --> 01:08:08,710
product plus the addition of the biases

1463
01:08:05,769 --> 01:08:13,960
and put it through a sigmoid function

1464
01:08:08,710 --> 01:08:16,449
now in pi torch basically all of the

1465
01:08:13,960 --> 01:08:19,149
functions you can do to tensors are

1466
01:08:16,449 --> 01:08:21,429
available inside this thing called

1467
01:08:19,149 --> 01:08:23,979
capital F and this is like totally

1468
01:08:21,430 --> 01:08:26,470
standard in pi torch it's actually

1469
01:08:23,979 --> 01:08:28,059
called torch and or functional but

1470
01:08:26,470 --> 01:08:30,579
everybody including all of the pipe

1471
01:08:28,060 --> 01:08:33,659
torch Doc's import torch start and end

1472
01:08:30,579 --> 01:08:36,809
are functional as capital F all right so

1473
01:08:33,658 --> 01:08:38,750
capital F dot sigmoid means a function

1474
01:08:36,810 --> 01:08:42,989
called sigmoid that is coming from

1475
01:08:38,750 --> 01:08:44,729
tortures functional module right and so

1476
01:08:42,988 --> 01:08:46,798
that's going to apply a sigmoid function

1477
01:08:44,729 --> 01:08:48,388
for the result so I squish them all

1478
01:08:46,798 --> 01:08:50,278
between zero and one using that nice

1479
01:08:48,389 --> 01:08:54,989
little shape and then I can multiply

1480
01:08:50,279 --> 01:08:57,150
that by five minus one plus four right

1481
01:08:54,988 --> 01:08:58,408
and then add on one and that's gonna

1482
01:08:57,149 --> 01:09:03,119
give me plumbing between one and five

1483
01:08:58,408 --> 01:09:04,979
okay so like there's no need to do this

1484
01:09:03,119 --> 01:09:08,429
I could comment it out and it'll still

1485
01:09:04,979 --> 01:09:09,988
work right but now it has to come up

1486
01:09:08,429 --> 01:09:13,289
with a set of calculations that are

1487
01:09:09,988 --> 01:09:15,269
always between one and five right where

1488
01:09:13,289 --> 01:09:16,710
else if I leave this in then it's like

1489
01:09:15,270 --> 01:09:17,880
makes it really easy it's basically like

1490
01:09:16,710 --> 01:09:20,100
oh if you think this is a really good

1491
01:09:17,880 --> 01:09:22,920
movie just calculate a really high

1492
01:09:20,100 --> 01:09:24,569
number it's a really crappy movies low

1493
01:09:22,920 --> 01:09:26,338
number and I'll make sure it's in the

1494
01:09:24,569 --> 01:09:28,440
right regions so even though this is a

1495
01:09:26,338 --> 01:09:30,689
neural network it's still a good example

1496
01:09:28,439 --> 01:09:33,210
of this kind of like if you're doing any

1497
01:09:30,689 --> 01:09:35,399
kind of parameter fitting try and make

1498
01:09:33,210 --> 01:09:37,259
it so that the thing that you want your

1499
01:09:35,399 --> 01:09:40,619
function to return it's like it's easy

1500
01:09:37,259 --> 01:09:44,759
for it to return that okay so that's why

1501
01:09:40,619 --> 01:09:48,088
we do that that function squishing so we

1502
01:09:44,759 --> 01:09:49,710
call this embedding dot bias so we can

1503
01:09:48,088 --> 01:09:53,189
create that in the same way as before

1504
01:09:49,710 --> 01:09:54,929
you'll see here I'm calling dr. to put

1505
01:09:53,189 --> 01:09:56,789
it on the GPU because we're not using

1506
01:09:54,929 --> 01:09:58,199
any learner stuff normally it'll all

1507
01:09:56,789 --> 01:10:01,350
happen for you but we have to manually

1508
01:09:58,198 --> 01:10:03,658
say put it on the GPU this is the same

1509
01:10:01,350 --> 01:10:05,579
as before create our optimizer fit

1510
01:10:03,658 --> 01:10:08,129
exactly the same as before and these

1511
01:10:05,579 --> 01:10:11,519
numbers are looking good all right and

1512
01:10:08,130 --> 01:10:13,199
again we'll do a little change to our

1513
01:10:11,520 --> 01:10:16,050
learning rate learning rate schedule and

1514
01:10:13,198 --> 01:10:23,638
we're down to 0.8 so we're actually

1515
01:10:16,050 --> 01:10:31,590
pretty close pretty close so that's the

1516
01:10:23,639 --> 01:10:35,810
key steps and this is how this is how

1517
01:10:31,590 --> 01:10:38,279
most collaborative filtering is done and

1518
01:10:35,810 --> 01:10:42,389
unit reminded me of an important point

1519
01:10:38,279 --> 01:10:46,139
which is that this is not strictly

1520
01:10:42,389 --> 01:10:46,800
speaking a matrix factorization because

1521
01:10:46,139 --> 01:10:49,289
strictly

1522
01:10:46,800 --> 01:10:54,119
a matrix factorization would take that

1523
01:10:49,289 --> 01:11:03,149
matrix by that matrix to create this

1524
01:10:54,119 --> 01:11:08,609
matrix and remembering anywhere that

1525
01:11:03,149 --> 01:11:11,729
this is empty like here or here we're

1526
01:11:08,609 --> 01:11:14,449
putting in a zero right we're saying if

1527
01:11:11,729 --> 01:11:18,089
the original was empty put in a zero

1528
01:11:14,449 --> 01:11:20,279
right now normally you can't do that

1529
01:11:18,090 --> 01:11:22,289
with normal matrix factorization normal

1530
01:11:20,279 --> 01:11:25,019
matrix factorization it creates the

1531
01:11:22,289 --> 01:11:27,449
whole matrix and so it was a real

1532
01:11:25,020 --> 01:11:29,580
problem actually when people used to try

1533
01:11:27,449 --> 01:11:31,949
and use traditional linear algebra for

1534
01:11:29,579 --> 01:11:33,449
this because when you have these sparse

1535
01:11:31,949 --> 01:11:36,779
matrices like in practice

1536
01:11:33,449 --> 01:11:38,579
this matrix is not doesn't have many

1537
01:11:36,779 --> 01:11:40,500
gaps because we picked the users that

1538
01:11:38,579 --> 01:11:42,119
watch the most movies and the movies

1539
01:11:40,500 --> 01:11:43,649
that are the most watched but if you

1540
01:11:42,119 --> 01:11:46,199
look at the whole matrix it's it's

1541
01:11:43,649 --> 01:11:49,979
mainly empty and so traditional

1542
01:11:46,199 --> 01:11:51,750
techniques treated empty is zero and so

1543
01:11:49,979 --> 01:11:54,119
like you basically have to predict a

1544
01:11:51,750 --> 01:11:55,680
zero as if the fact that I haven't

1545
01:11:54,119 --> 01:11:58,949
watched a movie means I don't like the

1546
01:11:55,680 --> 01:12:01,350
movie that's gives terrible answers so

1547
01:11:58,949 --> 01:12:05,579
this probabilistic matrix factorization

1548
01:12:01,350 --> 01:12:08,640
approach takes advantage of the fact

1549
01:12:05,579 --> 01:12:11,220
that our data structure actually looks

1550
01:12:08,640 --> 01:12:13,770
like this rather than that cross tab

1551
01:12:11,220 --> 01:12:16,110
right and so it's only calculating the

1552
01:12:13,770 --> 01:12:18,240
loss for the user ID movie ID

1553
01:12:16,109 --> 01:12:20,159
combinations that actually appear that's

1554
01:12:18,239 --> 01:12:22,859
its if you like use red a1 movie I think

1555
01:12:20,159 --> 01:12:25,529
102 9 should be 3 it's actually three

1556
01:12:22,859 --> 01:12:27,299
and a half sauce is 0.5 like there's

1557
01:12:25,529 --> 01:12:30,960
nothing here that's ever going to

1558
01:12:27,300 --> 01:12:32,850
calculate a prediction or a loss for a

1559
01:12:30,960 --> 01:12:36,029
user movie combination that doesn't

1560
01:12:32,850 --> 01:12:38,130
appear in this table by definition the

1561
01:12:36,029 --> 01:12:45,389
only stuff that we can appear in a mini

1562
01:12:38,130 --> 01:12:46,619
batch is what's in this table okay and

1563
01:12:45,390 --> 01:12:48,000
like a lot of this happened

1564
01:12:46,619 --> 01:12:50,760
interestingly enough actually in the

1565
01:12:48,000 --> 01:12:54,180
Netflix price so before the Netflix

1566
01:12:50,760 --> 01:12:56,340
prize came along there's probabilistic

1567
01:12:54,180 --> 01:12:59,130
matrix factorization it had actually

1568
01:12:56,340 --> 01:13:00,640
already been invented but nobody noticed

1569
01:12:59,130 --> 01:13:02,739
all right and then in the

1570
01:13:00,640 --> 01:13:04,539
first year of the Netflix price someone

1571
01:13:02,739 --> 01:13:06,699
wrote this like really really famous

1572
01:13:04,539 --> 01:13:08,710
blog post where they basically said like

1573
01:13:06,699 --> 01:13:10,989
hey check this out

1574
01:13:08,710 --> 01:13:12,640
incredibly simple technique works

1575
01:13:10,989 --> 01:13:17,260
incredibly well when suddenly all the

1576
01:13:12,640 --> 01:13:19,180
net fix leaderboard entries and so

1577
01:13:17,260 --> 01:13:21,489
that's quite a few years ago now and

1578
01:13:19,180 --> 01:13:23,920
this is like now every collaborative

1579
01:13:21,489 --> 01:13:25,179
filtering approach does this not every

1580
01:13:23,920 --> 01:13:27,640
collaborative filtering approach adds

1581
01:13:25,180 --> 01:13:30,940
this sigmoid thing by the way it's not

1582
01:13:27,640 --> 01:13:32,800
like rocket science this is this is not

1583
01:13:30,939 --> 01:13:33,849
like the NLP thing we saw last week

1584
01:13:32,800 --> 01:13:35,770
which is like hey this is a new

1585
01:13:33,850 --> 01:13:37,660
state-of-the-art like this is you know

1586
01:13:35,770 --> 01:13:39,640
not particularly uncommon but there are

1587
01:13:37,659 --> 01:13:41,619
still people that don't do this it

1588
01:13:39,640 --> 01:13:45,280
definitely helps a lot I have to have

1589
01:13:41,619 --> 01:13:47,800
this and so actually you know what we

1590
01:13:45,279 --> 01:13:50,079
could do is maybe now's a good time to

1591
01:13:47,800 --> 01:13:56,590
have a look at the definition of this

1592
01:13:50,079 --> 01:14:00,369
right so the column data module contains

1593
01:13:56,590 --> 01:14:03,640
all these definitions and we can now

1594
01:14:00,369 --> 01:14:07,210
compare this to the thing we originally

1595
01:14:03,640 --> 01:14:09,550
used which was whatever came out of

1596
01:14:07,210 --> 01:14:16,060
collaborative data set all right so

1597
01:14:09,550 --> 01:14:20,050
let's go to collab filter data set here

1598
01:14:16,060 --> 01:14:22,660
it is and we called get learner all

1599
01:14:20,050 --> 01:14:27,329
right so we can go down to get Elena and

1600
01:14:22,659 --> 01:14:29,710
that created a collab filter learner

1601
01:14:27,329 --> 01:14:32,739
passing in the model from get model is

1602
01:14:29,710 --> 01:14:37,119
get model so created an embedding bias

1603
01:14:32,739 --> 01:14:40,300
and so here is embedding drop bias and

1604
01:14:37,119 --> 01:14:42,460
you can see here here it is like it's

1605
01:14:40,300 --> 01:14:44,920
the same thing there's the embedding for

1606
01:14:42,460 --> 01:14:49,000
each of the things here's our forward

1607
01:14:44,920 --> 01:14:52,750
that does the you times I dot some plus

1608
01:14:49,000 --> 01:14:55,810
plus sigmoid so in fact we have just

1609
01:14:52,750 --> 01:15:00,340
actually rebuilt what's in the past our

1610
01:14:55,810 --> 01:15:03,070
library literally okay it's a little

1611
01:15:00,340 --> 01:15:04,960
shorter and easier because we're taking

1612
01:15:03,069 --> 01:15:08,849
advantage of the fact that there's a

1613
01:15:04,960 --> 01:15:11,409
special collaborative filtering data set

1614
01:15:08,850 --> 01:15:12,610
so we can actually we're getting past in

1615
01:15:11,409 --> 01:15:13,889
the users and the items and we don't

1616
01:15:12,609 --> 01:15:16,799
have to pull them out of cat

1617
01:15:13,889 --> 01:15:18,420
Kant's but other than that this is

1618
01:15:16,800 --> 01:15:20,670
exactly the same so hopefully you can

1619
01:15:18,420 --> 01:15:22,710
see like the faster you have ivory is

1620
01:15:20,670 --> 01:15:24,630
not some inscrutable code containing

1621
01:15:22,710 --> 01:15:26,880
concepts you can never understand we've

1622
01:15:24,630 --> 01:15:31,319
actually just built up this entire thing

1623
01:15:26,880 --> 01:15:38,429
from scratch ourselves and so why did we

1624
01:15:31,319 --> 01:15:40,319
get 0.76 rather than 0.8 you know I I

1625
01:15:38,429 --> 01:15:41,940
think it's simply because we used

1626
01:15:40,319 --> 01:15:44,460
stochastic gradient descent with

1627
01:15:41,939 --> 01:15:46,229
restarts or the cycle multiplier and an

1628
01:15:44,460 --> 01:15:51,689
atom optimizer you know like a few

1629
01:15:46,229 --> 01:15:54,709
little training chase some I'm looking

1630
01:15:51,689 --> 01:15:58,158
at this and thinking that is we could

1631
01:15:54,710 --> 01:16:00,929
totally improve this small but maybe

1632
01:15:58,158 --> 01:16:02,819
looking at the date and doing some

1633
01:16:00,929 --> 01:16:05,639
tricks with the date because this this

1634
01:16:02,819 --> 01:16:08,219
is kind of a just a regular kind of

1635
01:16:05,639 --> 01:16:10,560
smaller no way yeah you can add more

1636
01:16:08,219 --> 01:16:13,260
features yeah exactly exactly so like

1637
01:16:10,560 --> 01:16:15,770
now that you've seen this you could now

1638
01:16:13,260 --> 01:16:18,179
you know even if you didn't have

1639
01:16:15,770 --> 01:16:19,619
embedding dot bias in a notebook that

1640
01:16:18,179 --> 01:16:21,600
you've written yourself through some

1641
01:16:19,618 --> 01:16:23,488
other model that's in fast AI you could

1642
01:16:21,600 --> 01:16:25,889
look at it in faster and be like oh that

1643
01:16:23,488 --> 01:16:28,319
does most of the things that I'd want to

1644
01:16:25,889 --> 01:16:29,850
do but it doesn't deal with time and so

1645
01:16:28,319 --> 01:16:33,000
you could just go oh okay let's grab it

1646
01:16:29,850 --> 01:16:36,449
copy it you know pop it into my notebook

1647
01:16:33,000 --> 01:16:38,969
and let's create you know the better

1648
01:16:36,448 --> 01:16:41,669
version all right and then you can start

1649
01:16:38,969 --> 01:16:46,829
playing that and you can now create your

1650
01:16:41,670 --> 01:16:49,618
own model class from the open source

1651
01:16:46,829 --> 01:16:50,850
code here and so yeah your that's

1652
01:16:49,618 --> 01:16:52,408
mentioning a couple things we could do

1653
01:16:50,850 --> 01:16:55,560
we could try and incorporate in time

1654
01:16:52,408 --> 01:16:57,960
stamp so we could assume that maybe well

1655
01:16:55,560 --> 01:17:00,389
maybe there's just like some for a

1656
01:16:57,960 --> 01:17:03,590
particular user over time users tend to

1657
01:17:00,389 --> 01:17:06,690
get more or less positive about movies

1658
01:17:03,590 --> 01:17:08,940
also remember there was the list of

1659
01:17:06,689 --> 01:17:12,689
genres for each movie maybe we could

1660
01:17:08,939 --> 01:17:13,799
incorporate that so one problem is it's

1661
01:17:12,689 --> 01:17:17,579
a little bit difficult to incorporate

1662
01:17:13,800 --> 01:17:19,650
that stuff into this embedding bias

1663
01:17:17,579 --> 01:17:21,899
model because it's kind of it's pretty

1664
01:17:19,649 --> 01:17:25,368
custom right so what we're going to do

1665
01:17:21,899 --> 01:17:27,109
next is we're going to try to create a

1666
01:17:25,368 --> 01:17:33,159
neural net version

1667
01:17:27,109 --> 01:17:36,769
of this hey so the basic idea here is

1668
01:17:33,159 --> 01:17:38,239
we're going to take exactly the same

1669
01:17:36,770 --> 01:17:43,450
thing as we had before here's our list

1670
01:17:38,239 --> 01:17:46,130
of users right and here is Erin Bates

1671
01:17:43,449 --> 01:17:51,019
alright and here's our list of movies

1672
01:17:46,130 --> 01:17:52,840
and here is our embedded right and so as

1673
01:17:51,020 --> 01:17:55,460
you can see I've just kind of transposed

1674
01:17:52,840 --> 01:17:58,880
the movie ones so that so that they're

1675
01:17:55,460 --> 01:18:02,960
all in the same orientation and here is

1676
01:17:58,880 --> 01:18:05,300
our user movie rating but D cross tab

1677
01:18:02,960 --> 01:18:10,369
okay so in the original format so each

1678
01:18:05,300 --> 01:18:13,539
row is a user movie rating okay so the

1679
01:18:10,369 --> 01:18:19,010
first thing I do is I need to replace

1680
01:18:13,539 --> 01:18:22,180
user 14 with that users contiguous in

1681
01:18:19,010 --> 01:18:25,430
this right and so I can do that in Excel

1682
01:18:22,180 --> 01:18:27,710
using this match that basically says

1683
01:18:25,430 --> 01:18:31,880
what you know how far down this list do

1684
01:18:27,710 --> 01:18:34,039
you have to go and it said user 14 was

1685
01:18:31,880 --> 01:18:36,260
the first thing in that list okay

1686
01:18:34,039 --> 01:18:39,979
user 29 was the second thing in that

1687
01:18:36,260 --> 01:18:43,220
list so forth okay so this is the same

1688
01:18:39,979 --> 01:18:45,169
as that thing that we did in our Python

1689
01:18:43,220 --> 01:18:49,909
code where we basically created a

1690
01:18:45,170 --> 01:18:52,180
dictionary to master so now we can for

1691
01:18:49,909 --> 01:18:54,849
this particular user movie rating

1692
01:18:52,180 --> 01:18:58,640
combination we can look up the

1693
01:18:54,850 --> 01:19:01,400
appropriate embedding right and so you

1694
01:18:58,640 --> 01:19:01,700
can see here what it's doing is it's

1695
01:19:01,399 --> 01:19:05,119
saying

1696
01:19:01,699 --> 01:19:08,599
all right let's basically offset from

1697
01:19:05,119 --> 01:19:10,130
the start of this list and the number of

1698
01:19:08,600 --> 01:19:12,200
rows we're going to go down is equal to

1699
01:19:10,130 --> 01:19:14,810
the user index and the number of columns

1700
01:19:12,199 --> 01:19:16,880
we're going to go across is one two

1701
01:19:14,810 --> 01:19:18,590
three four or five okay and so you can

1702
01:19:16,880 --> 01:19:19,850
see what it does is it creates point one

1703
01:19:18,590 --> 01:19:22,579
nine point six three point three one

1704
01:19:19,850 --> 01:19:25,910
here it is point one nine point okay so

1705
01:19:22,579 --> 01:19:29,239
so this is literally modern embedding

1706
01:19:25,909 --> 01:19:34,309
this but remember this is exactly the

1707
01:19:29,239 --> 01:19:37,819
same as doing a one hot encoding right

1708
01:19:34,310 --> 01:19:40,420
because if instead this was a vector

1709
01:19:37,819 --> 01:19:44,149
containing one zero zero zero zero

1710
01:19:40,420 --> 01:19:46,699
right and we multiplied that by this

1711
01:19:44,149 --> 01:19:51,799
matrix then the only row it's going to

1712
01:19:46,699 --> 01:19:53,420
return would be the first one okay so so

1713
01:19:51,800 --> 01:19:56,210
it's really useful to remember that

1714
01:19:53,420 --> 01:19:59,300
embedding actually just is a matrix

1715
01:19:56,210 --> 01:20:02,270
product the only reason it exists the

1716
01:19:59,300 --> 01:20:04,550
only reason it exists is because this is

1717
01:20:02,270 --> 01:20:07,280
an optimization you know this let's pipe

1718
01:20:04,550 --> 01:20:10,460
or to know like okay this is just a

1719
01:20:07,279 --> 01:20:13,369
matrix multiply but I guarantee you that

1720
01:20:10,460 --> 01:20:15,319
you know this thing is one hard encoded

1721
01:20:13,369 --> 01:20:16,849
therefore you don't have to actually do

1722
01:20:15,319 --> 01:20:19,279
the matrix multiply you can just do a

1723
01:20:16,850 --> 01:20:21,940
directory of that okay so that's

1724
01:20:19,279 --> 01:20:26,420
literally all an embedding is is it is a

1725
01:20:21,939 --> 01:20:28,460
computational performance thing for a

1726
01:20:26,420 --> 01:20:31,340
particular kind of matrix multiplier all

1727
01:20:28,460 --> 01:20:34,550
right so that looks up that uses user

1728
01:20:31,340 --> 01:20:38,420
and then we can look up that users movie

1729
01:20:34,550 --> 01:20:41,060
all right so here is movie ID movie ID

1730
01:20:38,420 --> 01:20:43,550
four one seven which apparently is

1731
01:20:41,060 --> 01:20:45,320
indexed number fourteen here it is here

1732
01:20:43,550 --> 01:20:47,900
so it should have been point seven five

1733
01:20:45,319 --> 01:20:49,460
point four seven yes it is point seven

1734
01:20:47,899 --> 01:20:52,250
five point plus it okay

1735
01:20:49,460 --> 01:20:55,039
so we've now got the user embedding and

1736
01:20:52,250 --> 01:21:02,479
the movie embedding and rather than

1737
01:20:55,039 --> 01:21:06,369
doing a dot product of those two okay

1738
01:21:02,479 --> 01:21:09,339
which is what we do normally instead

1739
01:21:06,369 --> 01:21:15,309
what if we concatenate the two together

1740
01:21:09,340 --> 01:21:19,819
into a single vector of length 10 and

1741
01:21:15,310 --> 01:21:24,620
then feed that into a neural net okay

1742
01:21:19,819 --> 01:21:27,920
and so anytime we've got you know a

1743
01:21:24,619 --> 01:21:30,109
tensor of import activations or in this

1744
01:21:27,920 --> 01:21:31,819
case a tensor of actually this is a

1745
01:21:30,109 --> 01:21:34,130
tensor of output activations this is

1746
01:21:31,819 --> 01:21:35,960
coming out of an embedding layer we can

1747
01:21:34,130 --> 01:21:40,119
chuck it in a neural net because neural

1748
01:21:35,960 --> 01:21:43,189
Nets we now know can calculate anything

1749
01:21:40,119 --> 01:21:46,939
okay including hopefully collaborative

1750
01:21:43,189 --> 01:21:51,598
filtering so let's try that so here is

1751
01:21:46,939 --> 01:21:54,719
our embedding net so

1752
01:21:51,599 --> 01:22:01,319
this time I have not bothered to create

1753
01:21:54,719 --> 01:22:05,939
a separate bias because instead the

1754
01:22:01,319 --> 01:22:10,049
linear layer in pi torch already has a

1755
01:22:05,939 --> 01:22:17,928
bias in it all right so when we go NN

1756
01:22:10,050 --> 01:22:17,929
Linea right that's kind of draw this out

1757
01:22:18,050 --> 01:22:27,650
so we've got our U matrix right and this

1758
01:22:24,060 --> 01:22:31,289
is the number of users and this is the

1759
01:22:27,649 --> 01:22:36,299
number of factors right and we've got

1760
01:22:31,289 --> 01:22:39,210
our M matrix that so here's our number

1761
01:22:36,300 --> 01:22:42,960
of movies and here's our again number of

1762
01:22:39,210 --> 01:22:50,099
factors okay and so remember we look up

1763
01:22:42,960 --> 01:22:53,789
a single user we look up a single movie

1764
01:22:50,099 --> 01:22:57,239
and let's grab them and concatenate them

1765
01:22:53,789 --> 01:23:00,479
together okay so here's like the user

1766
01:22:57,238 --> 01:23:05,000
part here's the movie part and then

1767
01:23:00,479 --> 01:23:08,009
let's put that through a matrix product

1768
01:23:05,000 --> 01:23:09,510
right so that number of rows here is

1769
01:23:08,010 --> 01:23:12,090
going to have to be the number of users

1770
01:23:09,510 --> 01:23:14,880
plus the number of movies right because

1771
01:23:12,090 --> 01:23:18,000
that's how long that is and then the

1772
01:23:14,880 --> 01:23:23,250
number of columns can be anything we

1773
01:23:18,000 --> 01:23:25,020
want because we're going to take that so

1774
01:23:23,250 --> 01:23:27,689
in this case we're going to pick 10

1775
01:23:25,020 --> 01:23:30,300
apparently so it's pick 10 and then

1776
01:23:27,689 --> 01:23:34,939
we're going to stick that through a rail

1777
01:23:30,300 --> 01:23:37,349
you and then stick that through another

1778
01:23:34,939 --> 01:23:40,589
matrix which obviously needs to be of

1779
01:23:37,349 --> 01:23:43,889
size 10 here and then the number of

1780
01:23:40,590 --> 01:23:51,029
columns is a size 1 because we want to

1781
01:23:43,889 --> 01:23:54,868
predict a single rating okay and so

1782
01:23:51,029 --> 01:23:58,889
that's our kind of flow chart of what's

1783
01:23:54,868 --> 01:24:00,988
going on right it is a standard I'm

1784
01:23:58,889 --> 01:24:02,730
called a one hidden layer neural net it

1785
01:24:00,988 --> 01:24:05,189
depends how you think of it like there's

1786
01:24:02,729 --> 01:24:07,919
kind of an embedding layer but because

1787
01:24:05,189 --> 01:24:09,359
is linear and this is linear the two

1788
01:24:07,920 --> 01:24:11,190
together is really one linear layer

1789
01:24:09,359 --> 01:24:14,099
right this just a computational

1790
01:24:11,189 --> 01:24:15,899
convenience so it's really got one

1791
01:24:14,100 --> 01:24:21,270
hidden layer because it's got one layer

1792
01:24:15,899 --> 01:24:26,519
before this nonlinear activation so in

1793
01:24:21,270 --> 01:24:27,690
order to create a linear layer with some

1794
01:24:26,520 --> 01:24:31,290
number of rows and some number of

1795
01:24:27,689 --> 01:24:34,349
columns you just go in and on in the

1796
01:24:31,289 --> 01:24:37,229
machine learning class this week we

1797
01:24:34,350 --> 01:24:39,300
learnt how to create a linear layer from

1798
01:24:37,229 --> 01:24:41,759
scratch by creating our own weight

1799
01:24:39,300 --> 01:24:43,739
matrix and our own biases so if you want

1800
01:24:41,760 --> 01:24:46,020
to check that out you couldn't do so

1801
01:24:43,738 --> 01:24:51,059
there right but it's the same basic

1802
01:24:46,020 --> 01:24:52,680
technique we've already seen so we

1803
01:24:51,060 --> 01:24:55,140
create our embeddings we create our two

1804
01:24:52,680 --> 01:24:57,750
linear layers that's all the stuff that

1805
01:24:55,140 --> 01:24:59,369
we need to start with you know really if

1806
01:24:57,750 --> 01:25:01,770
I wanted to make this more general I

1807
01:24:59,369 --> 01:25:08,000
would have had another parameter here

1808
01:25:01,770 --> 01:25:11,160
called like num hidden you know equals

1809
01:25:08,000 --> 01:25:14,250
equals 10 and then this would be a

1810
01:25:11,159 --> 01:25:16,529
parameter and then you could like more

1811
01:25:14,250 --> 01:25:18,569
easily play around with different

1812
01:25:16,529 --> 01:25:20,698
numbers of activations so when we say

1813
01:25:18,569 --> 01:25:21,809
like okay in this layer I'm going to

1814
01:25:20,698 --> 01:25:25,139
create a layer with this many

1815
01:25:21,810 --> 01:25:28,469
activations all I mean assuming it's a

1816
01:25:25,140 --> 01:25:30,690
fully connected layer is my linear layer

1817
01:25:28,469 --> 01:25:32,819
has how many columns in its weight

1818
01:25:30,689 --> 01:25:36,599
matrix that's how many activations it

1819
01:25:32,819 --> 01:25:38,549
creates all right so we grab our users

1820
01:25:36,600 --> 01:25:40,800
and movies we put them through our

1821
01:25:38,550 --> 01:25:41,670
embedding matrix and then we concatenate

1822
01:25:40,800 --> 01:25:44,640
them together

1823
01:25:41,670 --> 01:25:47,100
okay so torch cat concatenate them

1824
01:25:44,640 --> 01:25:48,900
together on the first dimension so in

1825
01:25:47,100 --> 01:25:52,320
other words we concatenate the columns

1826
01:25:48,899 --> 01:25:55,969
together to create longer rows okay so

1827
01:25:52,319 --> 01:25:55,969
that's concatenating on dimension one

1828
01:25:56,300 --> 01:26:01,579
drop out we'll come back to her in a

1829
01:25:58,529 --> 01:26:04,469
moment we've got that briefly

1830
01:26:01,579 --> 01:26:07,979
so then having done that we'll put it

1831
01:26:04,469 --> 01:26:10,050
through that linear layer we had we'll

1832
01:26:07,979 --> 01:26:14,159
do our value and you'll notice that

1833
01:26:10,050 --> 01:26:15,810
value is again inside our capital F and

1834
01:26:14,159 --> 01:26:16,198
end up optional right it's just a

1835
01:26:15,810 --> 01:26:18,930
function

1836
01:26:16,198 --> 01:26:20,549
so remember activation function

1837
01:26:18,930 --> 01:26:23,159
are basically things that take one

1838
01:26:20,550 --> 01:26:26,070
activation in and spit one activation

1839
01:26:23,159 --> 01:26:28,220
out in this case taking something that

1840
01:26:26,069 --> 01:26:30,659
can have negatives or positives and

1841
01:26:28,220 --> 01:26:34,949
truncate the negatives to zero that's

1842
01:26:30,659 --> 01:26:39,539
what well you does and then here's a

1843
01:26:34,949 --> 01:26:42,630
sigmoid so that's that that is now a

1844
01:26:39,539 --> 01:26:44,369
genuine neural network I don't know if I

1845
01:26:42,630 --> 01:26:45,690
get to call it deep it's only got one

1846
01:26:44,369 --> 01:26:48,029
hidden layer but it's definitely a

1847
01:26:45,689 --> 01:26:50,099
neural network all right and so we can

1848
01:26:48,029 --> 01:26:52,889
now construct it we can put it on the

1849
01:26:50,100 --> 01:26:56,730
GPU you can create an optimizer for it

1850
01:26:52,890 --> 01:26:57,660
and we can fit it now you'll notice

1851
01:26:56,729 --> 01:27:00,809
there's one other thing I've been

1852
01:26:57,659 --> 01:27:03,029
passing to fit which is what loss

1853
01:27:00,810 --> 01:27:05,280
function are we trying to minimize okay

1854
01:27:03,029 --> 01:27:08,130
this is the mean squared error loss and

1855
01:27:05,279 --> 01:27:12,269
again it's inside F okay pretty much all

1856
01:27:08,130 --> 01:27:14,609
the functions are inside it okay so one

1857
01:27:12,270 --> 01:27:16,590
of the things that you have to pass fit

1858
01:27:14,609 --> 01:27:20,339
is something saying like how do you

1859
01:27:16,590 --> 01:27:23,550
score it's what counts as good or bad so

1860
01:27:20,340 --> 01:27:25,860
it should I mean now that we have a real

1861
01:27:23,550 --> 01:27:27,840
neural net do we have to use the same

1862
01:27:25,859 --> 01:27:30,479
number of embeddings for users and

1863
01:27:27,840 --> 01:27:31,800
that's a great question you don't know

1864
01:27:30,479 --> 01:27:34,049
absolutely right

1865
01:27:31,800 --> 01:27:38,100
you don't and so like we've got a lot of

1866
01:27:34,050 --> 01:27:45,420
benefits here right because if we you

1867
01:27:38,100 --> 01:27:48,600
know think about you know we're grabbing

1868
01:27:45,420 --> 01:27:50,489
a user embedding or concatenating it

1869
01:27:48,600 --> 01:27:54,810
with a movie embedding which maybe is

1870
01:27:50,489 --> 01:27:57,960
like some different size but then also

1871
01:27:54,810 --> 01:27:59,820
perhaps we looked up the genre of the

1872
01:27:57,960 --> 01:28:02,489
movie and like you know there's actually

1873
01:27:59,819 --> 01:28:06,239
a embedding matrix of like number of

1874
01:28:02,489 --> 01:28:07,829
genres by I don't know three or

1875
01:28:06,239 --> 01:28:09,750
something and so like we could then

1876
01:28:07,829 --> 01:28:12,269
concatenate like a genre embedding and

1877
01:28:09,750 --> 01:28:14,880
then maybe the timestamp is in here as a

1878
01:28:12,270 --> 01:28:18,570
continuous number right and so then that

1879
01:28:14,880 --> 01:28:24,420
whole thing we can then feed into you

1880
01:28:18,569 --> 01:28:28,439
know and you're on it all right and then

1881
01:28:24,420 --> 01:28:30,750
at the end remember a final

1882
01:28:28,439 --> 01:28:32,488
non-linearity was a sigmoid right so we

1883
01:28:30,750 --> 01:28:34,618
can now recognize that

1884
01:28:32,488 --> 01:28:36,569
thing we did where we did sigmoid x max

1885
01:28:34,618 --> 01:28:40,788
reading vote - min reading + blah blah

1886
01:28:36,569 --> 01:28:43,828
blah is actually just another nonlinear

1887
01:28:40,788 --> 01:28:46,590
activation function alright remember in

1888
01:28:43,828 --> 01:28:48,349
our last layer we use generally

1889
01:28:46,590 --> 01:28:51,269
different kinds of activation functions

1890
01:28:48,349 --> 01:28:52,949
so as we said we don't need any

1891
01:28:51,269 --> 01:28:59,309
activation function at all right we

1892
01:28:52,948 --> 01:29:01,229
could just do that right but by not

1893
01:28:59,309 --> 01:29:03,029
having any nonlinear activation function

1894
01:29:01,229 --> 01:29:06,299
we're just making it harder so that's

1895
01:29:03,029 --> 01:29:10,229
why we put the sigmoid in there as well

1896
01:29:06,300 --> 01:29:14,010
okay so we can then fit it in the usual

1897
01:29:10,229 --> 01:29:15,598
way and there we go

1898
01:29:14,010 --> 01:29:20,460
you know interestingly we actually got a

1899
01:29:15,599 --> 01:29:23,519
better score than we did with our this

1900
01:29:20,460 --> 01:29:24,899
model so I'll be interesting to try

1901
01:29:23,519 --> 01:29:26,429
training this with stochastic gradient

1902
01:29:24,899 --> 01:29:29,368
descent with restarts and see if it's

1903
01:29:26,429 --> 01:29:30,960
actually better you know maybe you can

1904
01:29:29,368 --> 01:29:32,908
play around with the number of hidden

1905
01:29:30,960 --> 01:29:35,698
layers and the drop out and whatever

1906
01:29:32,908 --> 01:29:44,808
else and see if you can come up with you

1907
01:29:35,698 --> 01:29:50,788
know get a better answer than point

1908
01:29:44,809 --> 01:29:53,788
seven six ish okay so so general so this

1909
01:29:50,788 --> 01:29:55,109
is like if you were going deep into

1910
01:29:53,788 --> 01:29:55,618
collaborative filtering at your

1911
01:29:55,109 --> 01:29:57,389
workplace

1912
01:29:55,618 --> 01:29:59,339
whatever this wouldn't be a bad way to

1913
01:29:57,389 --> 01:30:01,498
go I could like I'd start out with like

1914
01:29:59,340 --> 01:30:04,319
oh okay here's like a flat footed

1915
01:30:01,498 --> 01:30:06,269
dataset 30 in first day I get learner

1916
01:30:04,319 --> 01:30:07,979
there's you know not much I can send it

1917
01:30:06,269 --> 01:30:10,920
basically number of factors is about the

1918
01:30:07,979 --> 01:30:13,589
only thing that I pass in I can learn

1919
01:30:10,920 --> 01:30:15,420
for a while maybe try a few different

1920
01:30:13,590 --> 01:30:18,510
approaches and then you're like okay

1921
01:30:15,420 --> 01:30:22,019
there's like that's how I go if I use

1922
01:30:18,510 --> 01:30:23,699
the defaults okay how do I make it

1923
01:30:22,019 --> 01:30:24,960
better and then I'd be like dig into the

1924
01:30:23,698 --> 01:30:27,058
code and seeing like okay well what if

1925
01:30:24,960 --> 01:30:30,868
Jeremy actually do here this is actually

1926
01:30:27,059 --> 01:30:32,820
what I want you know and so one of the

1927
01:30:30,868 --> 01:30:36,149
nice things about the neural net

1928
01:30:32,819 --> 01:30:38,788
approach is that you know as unit

1929
01:30:36,149 --> 01:30:43,348
mentioned we can have different numbers

1930
01:30:38,788 --> 01:30:45,929
of embeddings we can choose how many

1931
01:30:43,349 --> 01:30:51,000
hidden and we can also choose

1932
01:30:45,929 --> 01:30:52,710
drop now right so so what we're actually

1933
01:30:51,000 --> 01:30:57,619
doing is we haven't just got real you

1934
01:30:52,710 --> 01:30:57,619
that we're also going like okay let's

1935
01:31:00,770 --> 01:31:08,489
let's delete a few things at random

1936
01:31:05,179 --> 01:31:12,569
alright let's drop out so in this case

1937
01:31:08,488 --> 01:31:16,529
we were deleting after the first linear

1938
01:31:12,569 --> 01:31:18,509
layer 75% of them all right and then

1939
01:31:16,529 --> 01:31:19,649
after the second one in like 75% of them

1940
01:31:18,510 --> 01:31:22,829
so we can add a whole lot of

1941
01:31:19,649 --> 01:31:24,389
regularization yes so you know this it

1942
01:31:22,829 --> 01:31:28,319
kind of feels like the this this

1943
01:31:24,390 --> 01:31:29,880
embedding net you know you could you

1944
01:31:28,319 --> 01:31:31,710
could change this again we could like

1945
01:31:29,880 --> 01:31:37,529
have it so that we can pass into the

1946
01:31:31,710 --> 01:31:39,210
constructor well if you're gonna make it

1947
01:31:37,529 --> 01:31:41,179
look as much as possible like what we

1948
01:31:39,210 --> 01:31:48,630
had before we could surpass him peace

1949
01:31:41,179 --> 01:31:51,469
peace equals 0.75 oh I'm not sure this

1950
01:31:48,630 --> 01:31:53,520
is the best API but it's not terrible

1951
01:31:51,469 --> 01:31:55,230
probably what since we've only got

1952
01:31:53,520 --> 01:32:08,790
exactly two layers we could say p1

1953
01:31:55,229 --> 01:32:18,629
equals 0.75 v p2 v and so then this will

1954
01:32:08,789 --> 01:32:20,908
be P 1 this will be Peter you know where

1955
01:32:18,630 --> 01:32:24,719
we go and like if you wanted to go

1956
01:32:20,908 --> 01:32:29,609
further you could make it look more like

1957
01:32:24,719 --> 01:32:31,469
our structured data learner you could

1958
01:32:29,609 --> 01:32:34,619
actually have a thing this number of

1959
01:32:31,469 --> 01:32:37,590
hidden you know maybe you could make a

1960
01:32:34,619 --> 01:32:40,260
list and so then rather than creating

1961
01:32:37,590 --> 01:32:42,779
exactly one hidden layer and one output

1962
01:32:40,260 --> 01:32:45,300
layer this could be a little loop that

1963
01:32:42,779 --> 01:32:47,009
creates and hidden miners each one of

1964
01:32:45,300 --> 01:32:48,449
the size you want so like this is all

1965
01:32:47,010 --> 01:32:52,230
stuff you can play with during the

1966
01:32:48,448 --> 01:32:53,789
hearing the week if you want to and I

1967
01:32:52,229 --> 01:32:55,678
feel like if you've got like a much

1968
01:32:53,789 --> 01:32:57,988
smaller collaborative children data set

1969
01:32:55,679 --> 01:32:59,529
you know maybe you need like more

1970
01:32:57,988 --> 01:33:02,500
regularization or whatever

1971
01:32:59,529 --> 01:33:04,929
it's a much bigger one maybe more layers

1972
01:33:02,500 --> 01:33:08,710
would help I don't know you know III

1973
01:33:04,930 --> 01:33:10,150
haven't seen much discussion of this

1974
01:33:08,710 --> 01:33:12,039
kind of neural network approach to

1975
01:33:10,149 --> 01:33:13,869
collaborative filtering but I'm not a

1976
01:33:12,039 --> 01:33:15,880
collaborative filtering expert so maybe

1977
01:33:13,869 --> 01:33:22,510
it's maybe it's around but that'd be

1978
01:33:15,880 --> 01:33:27,000
interesting thing to try so the next

1979
01:33:22,510 --> 01:33:29,890
thing I wanted to do was to talk about

1980
01:33:27,000 --> 01:33:34,869
the training loop so what's actually

1981
01:33:29,890 --> 01:33:37,920
happening inside the training loop so at

1982
01:33:34,869 --> 01:33:42,220
the moment we're basically passing off

1983
01:33:37,920 --> 01:33:46,210
the actual updating of the weights to PI

1984
01:33:42,220 --> 01:33:49,329
torches optimizer but what I'm going to

1985
01:33:46,210 --> 01:33:52,119
do is like understand what that

1986
01:33:49,329 --> 01:33:54,159
optimizer is is actually good and we're

1987
01:33:52,119 --> 01:33:58,449
also I also want to understand what this

1988
01:33:54,159 --> 01:34:03,220
Momentum's him he's doing so you'll find

1989
01:33:58,449 --> 01:34:06,460
we have a spreadsheet called grab disk

1990
01:34:03,220 --> 01:34:08,110
gradient descent and it's kind of

1991
01:34:06,460 --> 01:34:11,199
designed to be read left to right sorry

1992
01:34:08,109 --> 01:34:14,589
right to left worksheet was so the

1993
01:34:11,199 --> 01:34:16,029
rightmost worksheet is some data right

1994
01:34:14,590 --> 01:34:18,520
and we're going to implement gradient

1995
01:34:16,029 --> 01:34:20,559
descent in Excel because obviously

1996
01:34:18,520 --> 01:34:22,360
everybody wants to do deep learning in

1997
01:34:20,560 --> 01:34:23,770
it Selman we've done collaborative

1998
01:34:22,359 --> 01:34:27,009
filtering in Excel we've done

1999
01:34:23,770 --> 01:34:30,040
convolutions in Excel so now we need SJD

2000
01:34:27,010 --> 01:34:30,430
in Excel so we can replace - once and

2001
01:34:30,039 --> 01:34:33,369
for all

2002
01:34:30,430 --> 01:34:37,270
okay so let's start by creating some

2003
01:34:33,369 --> 01:34:40,300
data right and so here's you know here's

2004
01:34:37,270 --> 01:34:45,330
some independent you know I've got one

2005
01:34:40,300 --> 01:34:48,400
column of X's you know and one column of

2006
01:34:45,329 --> 01:34:50,439
wise and these are actually directly

2007
01:34:48,399 --> 01:34:54,460
linearly related so this is this is

2008
01:34:50,439 --> 01:35:03,519
random right and this one here is equal

2009
01:34:54,460 --> 01:35:09,159
to x times 2 plus 30 ok so let's try and

2010
01:35:03,520 --> 01:35:13,010
use Excel to take that data and try and

2011
01:35:09,159 --> 01:35:18,289
learn those parameters

2012
01:35:13,010 --> 01:35:21,829
okay that's going to be able so let's

2013
01:35:18,289 --> 01:35:23,630
start with the most basic version of SGD

2014
01:35:21,829 --> 01:35:25,489
and so the first thing I'm going to do

2015
01:35:23,630 --> 01:35:27,409
is I'm going to run a macro so you can

2016
01:35:25,489 --> 01:35:31,099
see what this looks like so I'll hit run

2017
01:35:27,409 --> 01:35:32,180
and it does five eight bucks under

2018
01:35:31,100 --> 01:35:36,829
another five eight bucks

2019
01:35:32,180 --> 01:35:38,300
- another five eight bucks okay so the

2020
01:35:36,829 --> 01:35:39,769
first one was pretty terrible it's hard

2021
01:35:38,300 --> 01:35:45,650
to see so I'll just delete that first

2022
01:35:39,770 --> 01:35:47,450
one get better scaling alright so you

2023
01:35:45,649 --> 01:35:50,210
can see it actually it's pretty

2024
01:35:47,449 --> 01:35:53,059
constantly improving the loss all right

2025
01:35:50,210 --> 01:35:57,939
this is the loss per pot all right so

2026
01:35:53,060 --> 01:36:03,740
how do we do that so let's reset it so

2027
01:35:57,939 --> 01:36:06,500
here is my X's and my y's and what I do

2028
01:36:03,739 --> 01:36:10,039
is I start out by assuming some

2029
01:36:06,500 --> 01:36:13,819
intercept and some slope right so this

2030
01:36:10,039 --> 01:36:15,949
is my randomly initialized weights so I

2031
01:36:13,819 --> 01:36:18,439
have randomly initialized them both to

2032
01:36:15,949 --> 01:36:21,529
one you could pick a different random

2033
01:36:18,439 --> 01:36:24,669
number if you like but I promise that I

2034
01:36:21,529 --> 01:36:27,849
randomly picked the number one twice

2035
01:36:24,670 --> 01:36:27,850
there you go

2036
01:36:27,880 --> 01:36:34,430
it was a random number between one and

2037
01:36:30,319 --> 01:36:36,049
one so here is my intercept and slope

2038
01:36:34,430 --> 01:36:37,940
I'm just going to copy them over here

2039
01:36:36,050 --> 01:36:41,390
right so you can literally see this is

2040
01:36:37,939 --> 01:36:44,359
just equal see one here is equals c2

2041
01:36:41,390 --> 01:36:47,240
okay so I'm gonna start with my very

2042
01:36:44,359 --> 01:36:51,710
first row of data x equals 14 y equals

2043
01:36:47,239 --> 01:36:53,029
58 and my goal is to come up after I

2044
01:36:51,710 --> 01:36:54,859
look at this piece of data I want to

2045
01:36:53,029 --> 01:36:59,449
come up with a slightly better intercept

2046
01:36:54,859 --> 01:37:02,949
and a slightly better slope okay so to

2047
01:36:59,449 --> 01:37:08,029
do that I need to first of all basically

2048
01:37:02,949 --> 01:37:10,460
figure out which direction is is down in

2049
01:37:08,029 --> 01:37:13,219
other words if I make my intercept a

2050
01:37:10,460 --> 01:37:15,050
little bit higher or a little bit lower

2051
01:37:13,220 --> 01:37:17,449
would it make my error a little bit

2052
01:37:15,050 --> 01:37:20,329
better or a little bit worse so let's

2053
01:37:17,449 --> 01:37:21,649
start out by calculating the error so to

2054
01:37:20,329 --> 01:37:24,409
calculate the error the first thing we

2055
01:37:21,649 --> 01:37:26,538
need is a prediction so the prediction

2056
01:37:24,409 --> 01:37:33,019
is equal to the interest

2057
01:37:26,538 --> 01:37:36,708
at plus x times so that is our zero

2058
01:37:33,019 --> 01:37:40,128
hidden layer neural network okay

2059
01:37:36,708 --> 01:37:43,969
and so here is our era it's equal to our

2060
01:37:40,128 --> 01:37:45,769
prediction - our actual squared so we

2061
01:37:43,969 --> 01:37:47,958
could like play around with this I don't

2062
01:37:45,769 --> 01:37:49,760
want my error to be 18-49 I'd like it to

2063
01:37:47,958 --> 01:37:54,529
be lower so what if we set the

2064
01:37:49,760 --> 01:37:57,048
intercepts to one point one 18-49 goes

2065
01:37:54,529 --> 01:38:00,318
to 1840 okay so a higher intercept would

2066
01:37:57,048 --> 01:38:05,599
be better okay what about the slope to

2067
01:38:00,319 --> 01:38:07,399
increase that it goes from 1849 to 1730

2068
01:38:05,599 --> 01:38:09,969
okay a higher slope would be better as

2069
01:38:07,399 --> 01:38:14,659
well not surprising because we know

2070
01:38:09,969 --> 01:38:18,128
actually that there should be 30 into so

2071
01:38:14,658 --> 01:38:20,988
one way to figure that out

2072
01:38:18,128 --> 01:38:23,059
you know encode and this protein is to

2073
01:38:20,988 --> 01:38:24,558
do literally what I just did is to add a

2074
01:38:23,059 --> 01:38:26,418
little bit to the intercept and the

2075
01:38:24,559 --> 01:38:27,889
slope and see what happens and that's

2076
01:38:26,418 --> 01:38:30,769
called finding the derivative through

2077
01:38:27,889 --> 01:38:34,038
finite differencing right and so let's

2078
01:38:30,769 --> 01:38:40,969
go ahead and do that so here is the

2079
01:38:34,038 --> 01:38:43,878
value of my error if I add 0.01 to my

2080
01:38:40,969 --> 01:38:46,219
intercept all right so it's c4 plus 0.01

2081
01:38:43,878 --> 01:38:48,529
and then I just put that into my Lydian

2082
01:38:46,219 --> 01:38:51,229
function and then I subtract my actual

2083
01:38:48,529 --> 01:38:53,389
all squared all right and so that causes

2084
01:38:51,229 --> 01:38:59,088
my arrow to go down a bit

2085
01:38:53,389 --> 01:39:01,548
that's our increasing my is that

2086
01:38:59,088 --> 01:39:03,349
increasing will see for increasing the

2087
01:39:01,548 --> 01:39:05,809
intercept a little bit has caused my

2088
01:39:03,349 --> 01:39:07,399
arrow to go down so what's the

2089
01:39:05,809 --> 01:39:09,559
derivative well the derivative is equal

2090
01:39:07,399 --> 01:39:11,749
to how much the dependent variable

2091
01:39:09,559 --> 01:39:13,998
changed by divided by how much the

2092
01:39:11,748 --> 01:39:16,088
independent variable changed by all

2093
01:39:13,998 --> 01:39:18,469
right and so there it is right our

2094
01:39:16,088 --> 01:39:21,349
dependent variable changed by that -

2095
01:39:18,469 --> 01:39:24,048
that right and our independent variable

2096
01:39:21,349 --> 01:39:29,088
we changed by 0.01 so there is the

2097
01:39:24,048 --> 01:39:30,529
estimated value of the error dB so

2098
01:39:29,088 --> 01:39:32,899
remember when people talking about

2099
01:39:30,529 --> 01:39:34,608
derivatives right this is this is all

2100
01:39:32,899 --> 01:39:37,209
they're doing is they're saying what's

2101
01:39:34,609 --> 01:39:39,570
this value but as we make this number

2102
01:39:37,208 --> 01:39:43,529
smaller and smaller and smaller

2103
01:39:39,569 --> 01:39:45,899
and smaller as it as limits to zero

2104
01:39:43,529 --> 01:39:47,759
I'm not mad enough to think in terms of

2105
01:39:45,899 --> 01:39:49,409
like derivatives and integrals and stuff

2106
01:39:47,760 --> 01:39:51,539
like that so whatever I think about this

2107
01:39:49,409 --> 01:39:54,689
I always think about you know an actual

2108
01:39:51,539 --> 01:39:56,430
like plus 0.01 and divided by 0.01

2109
01:39:54,689 --> 01:39:58,379
because like I just find that easier

2110
01:39:56,430 --> 01:40:00,539
just like I'd ever think about

2111
01:39:58,380 --> 01:40:02,220
probability density functions I always

2112
01:40:00,539 --> 01:40:03,239
think about actual probabilities of that

2113
01:40:02,220 --> 01:40:06,990
toss a coin

2114
01:40:03,239 --> 01:40:08,909
something happens three times so I

2115
01:40:06,989 --> 01:40:10,469
always think like remember it's it's

2116
01:40:08,909 --> 01:40:14,220
totally fair to do this because a

2117
01:40:10,470 --> 01:40:15,960
computer is discrete it's not continuous

2118
01:40:14,220 --> 01:40:18,390
like a computer can't do anything

2119
01:40:15,960 --> 01:40:21,090
infinitely small anyway right so it's

2120
01:40:18,390 --> 01:40:23,460
actually got to be calculating things at

2121
01:40:21,090 --> 01:40:26,970
some level of precision right and our

2122
01:40:23,460 --> 01:40:29,489
brains kind of need that as well so this

2123
01:40:26,970 --> 01:40:31,409
is like my version of Jeffery Clinton's

2124
01:40:29,489 --> 01:40:33,659
like to visualize things in more than

2125
01:40:31,409 --> 01:40:34,769
two dimensions you just like say 12

2126
01:40:33,659 --> 01:40:36,779
dimensions really quickly well

2127
01:40:34,770 --> 01:40:38,850
visualizing in two dimensions this is my

2128
01:40:36,779 --> 01:40:41,969
equivalent you know to to think about

2129
01:40:38,850 --> 01:40:44,789
derivatives just think about division

2130
01:40:41,970 --> 01:40:47,640
and like although all the mathematicians

2131
01:40:44,789 --> 01:40:49,949
say no you can't do that you actually

2132
01:40:47,640 --> 01:40:51,810
can like if you think of DX dy is being

2133
01:40:49,949 --> 01:40:55,289
literally you know change in X over

2134
01:40:51,810 --> 01:40:58,080
changing Y like the division actually

2135
01:40:55,289 --> 01:41:01,710
like the calculations do work like all

2136
01:40:58,079 --> 01:41:05,250
the time so okay so let's do the same

2137
01:41:01,710 --> 01:41:07,680
thing now with changing my slope by a

2138
01:41:05,250 --> 01:41:10,079
little bit and so here's the same thing

2139
01:41:07,680 --> 01:41:13,619
right and so you can see both of these

2140
01:41:10,079 --> 01:41:16,649
are negative okay so that's saying if I

2141
01:41:13,619 --> 01:41:20,010
increase my intercept my loss goes down

2142
01:41:16,649 --> 01:41:24,259
if I increase my slope my loss goes down

2143
01:41:20,010 --> 01:41:29,010
right and so my derivative of my error

2144
01:41:24,260 --> 01:41:31,380
with respect to my slope is is actually

2145
01:41:29,010 --> 01:41:36,239
pretty high and that's not surprising

2146
01:41:31,380 --> 01:41:37,920
because it's actually you know the

2147
01:41:36,239 --> 01:41:42,019
constant term is just being added where

2148
01:41:37,920 --> 01:41:42,020
else as slope is being multiplied by 40

2149
01:41:42,890 --> 01:41:49,950
okay now find that differencing is all

2150
01:41:48,600 --> 01:41:52,039
very well and good but there's a big

2151
01:41:49,949 --> 01:41:53,170
problem with finite difference seeing in

2152
01:41:52,039 --> 01:41:56,050
Hyden

2153
01:41:53,170 --> 01:42:00,219
no spaces and the problem is this right

2154
01:41:56,050 --> 01:42:02,770
and this is like you don't need to learn

2155
01:42:00,219 --> 01:42:04,448
how to calculate derivatives or

2156
01:42:02,770 --> 01:42:06,969
integrals but you need to learn how to

2157
01:42:04,448 --> 01:42:12,159
think about them spatially right and so

2158
01:42:06,969 --> 01:42:13,630
remember we have some vector very high

2159
01:42:12,159 --> 01:42:18,069
dimensional vector it's got like a

2160
01:42:13,630 --> 01:42:22,090
million items in it right and it's going

2161
01:42:18,069 --> 01:42:24,219
through some weight matrix right of size

2162
01:42:22,090 --> 01:42:26,890
like 1 million by size a hundred

2163
01:42:24,219 --> 01:42:28,719
thousand or whatever and it's spitting

2164
01:42:26,890 --> 01:42:33,579
out something of size I hundred thousand

2165
01:42:28,719 --> 01:42:36,640
and so you need to realize like there

2166
01:42:33,579 --> 01:42:38,380
isn't like a gradient yeah but it's like

2167
01:42:36,640 --> 01:42:43,000
for every one of these things in this

2168
01:42:38,380 --> 01:42:45,850
vector right there's a gradient in every

2169
01:42:43,000 --> 01:42:50,679
direction you know in every part of the

2170
01:42:45,850 --> 01:42:52,719
output right so it actually has not a

2171
01:42:50,679 --> 01:42:57,179
single gradient number not even a

2172
01:42:52,719 --> 01:43:03,100
gradient vector but a gradient matrix

2173
01:42:57,179 --> 01:43:04,989
right and so this this is a lot to

2174
01:43:03,100 --> 01:43:07,989
calculate right I would literally have

2175
01:43:04,988 --> 01:43:09,789
to like add a little bit to this and see

2176
01:43:07,988 --> 01:43:11,319
what happens to all of these add a

2177
01:43:09,789 --> 01:43:14,738
little bit to this see what happens to

2178
01:43:11,319 --> 01:43:18,479
all of these right to fill in one column

2179
01:43:14,738 --> 01:43:21,399
of this at a time so that's going to be

2180
01:43:18,479 --> 01:43:23,019
horrendously slow like that so that's

2181
01:43:21,399 --> 01:43:24,250
why like if you're ever thinking like

2182
01:43:23,020 --> 01:43:26,860
how we can just do this with finite

2183
01:43:24,250 --> 01:43:28,479
differencing just remember like okay we

2184
01:43:26,859 --> 01:43:32,529
we're dealing in the with these very

2185
01:43:28,479 --> 01:43:37,869
high dimensional vectors where you know

2186
01:43:32,529 --> 01:43:40,329
this this kind of matrix calculus like

2187
01:43:37,869 --> 01:43:42,880
all the concepts are identical but when

2188
01:43:40,329 --> 01:43:44,649
you actually draw it out like this you

2189
01:43:42,880 --> 01:43:47,199
suddenly realize like okay for each

2190
01:43:44,649 --> 01:43:49,059
number I could change there's a whole

2191
01:43:47,198 --> 01:43:50,738
bunch of numbers that impacts and I have

2192
01:43:49,060 --> 01:43:54,310
this whole matrix of things to compute

2193
01:43:50,738 --> 01:43:56,738
right and so your gradient calculations

2194
01:43:54,310 --> 01:43:59,260
can take up a lot of memory and they can

2195
01:43:56,738 --> 01:44:05,049
take up a lot of time so we want to find

2196
01:43:59,260 --> 01:44:07,000
some way to do this more quickly okay

2197
01:44:05,050 --> 01:44:10,180
and it's definitely well worth like

2198
01:44:07,000 --> 01:44:14,199
spending time kind of studying these

2199
01:44:10,180 --> 01:44:16,570
ideas of like you know the idea of like

2200
01:44:14,199 --> 01:44:24,639
the gradients like look up things like

2201
01:44:16,569 --> 01:44:27,179
Jacobian and Hessian they're the things

2202
01:44:24,640 --> 01:44:29,020
that you want to search for just that

2203
01:44:27,180 --> 01:44:32,560
unfortunately people normally write

2204
01:44:29,020 --> 01:44:35,860
about them with you know lots of great

2205
01:44:32,560 --> 01:44:39,760
letters and bla bla bla right but there

2206
01:44:35,859 --> 01:44:41,710
are some there are some nice you know

2207
01:44:39,760 --> 01:44:43,060
intuitive explanations out there and

2208
01:44:41,710 --> 01:44:44,560
hopefully you can share them on the

2209
01:44:43,060 --> 01:44:47,830
forum if you find them because this is

2210
01:44:44,560 --> 01:44:51,960
stuff you really need to really need to

2211
01:44:47,829 --> 01:44:54,489
understand in here you know because

2212
01:44:51,960 --> 01:44:56,380
you're trying to train something and

2213
01:44:54,489 --> 01:44:58,149
it's not working properly and like later

2214
01:44:56,380 --> 01:45:00,640
on we'll learn how to like look inside

2215
01:44:58,149 --> 01:45:02,049
hi torch to like actually get the values

2216
01:45:00,640 --> 01:45:04,240
of the gradients and you need to know

2217
01:45:02,050 --> 01:45:06,430
like okay well how would I like what the

2218
01:45:04,239 --> 01:45:08,590
gradients you know what would I consider

2219
01:45:06,430 --> 01:45:10,840
unusual like you know these are the

2220
01:45:08,590 --> 01:45:13,480
things that turn you into a really

2221
01:45:10,840 --> 01:45:15,579
awesome deep learning practitioner is

2222
01:45:13,479 --> 01:45:17,379
when you can like debug your problems by

2223
01:45:15,579 --> 01:45:19,720
like grabbing the gradients and doing

2224
01:45:17,380 --> 01:45:21,400
histograms of them and like knowing you

2225
01:45:19,720 --> 01:45:23,530
know that you could like plot that all

2226
01:45:21,399 --> 01:45:28,269
each layer my average gradients getting

2227
01:45:23,529 --> 01:45:30,639
worse or you know bigger okay so the

2228
01:45:28,270 --> 01:45:34,270
trick to doing this more quickly is to

2229
01:45:30,640 --> 01:45:37,660
do it analytically rather than through

2230
01:45:34,270 --> 01:45:40,150
finite differencing and so analytically

2231
01:45:37,659 --> 01:45:41,500
is basically there is a list you

2232
01:45:40,149 --> 01:45:42,969
probably all learned it at high school

2233
01:45:41,500 --> 01:45:46,600
there is a literally a list of rules

2234
01:45:42,970 --> 01:45:48,789
that for every mathematical function

2235
01:45:46,600 --> 01:45:53,050
there's a like this is the derivative of

2236
01:45:48,789 --> 01:46:00,189
that function so you probably remember a

2237
01:45:53,050 --> 01:46:02,920
few of them for example x squared - it's

2238
01:46:00,189 --> 01:46:07,299
alright and so we actually have here an

2239
01:46:02,920 --> 01:46:10,720
x squared so here is our two x right now

2240
01:46:07,300 --> 01:46:15,250
the one that I actually want you to know

2241
01:46:10,720 --> 01:46:17,550
is not any of the individual rules but I

2242
01:46:15,250 --> 01:46:19,050
want you to know the chain rule right

2243
01:46:17,550 --> 01:46:22,449
which

2244
01:46:19,050 --> 01:46:25,630
you've got some function of some

2245
01:46:22,449 --> 01:46:26,380
function of something why is this

2246
01:46:25,630 --> 01:46:28,600
important

2247
01:46:26,380 --> 01:46:33,670
I don't know that's a linear layer

2248
01:46:28,600 --> 01:46:37,920
that's a rally right and then we can

2249
01:46:33,670 --> 01:46:42,159
kind of keep going backwards map etc

2250
01:46:37,920 --> 01:46:43,180
right a neural net is just a function of

2251
01:46:42,159 --> 01:46:45,099
a function of a function of a function

2252
01:46:43,180 --> 01:46:49,600
where the innermost is you know it's

2253
01:46:45,100 --> 01:47:00,940
basically linear rally your linear rally

2254
01:46:49,600 --> 01:47:02,170
your dot linear sigmoid or soft mass all

2255
01:47:00,939 --> 01:47:03,369
right and so it's a function of a

2256
01:47:02,170 --> 01:47:06,940
function of a function and so therefore

2257
01:47:03,369 --> 01:47:10,960
to calculate the derivative of the

2258
01:47:06,939 --> 01:47:12,099
weights in your model the loss of your

2259
01:47:10,960 --> 01:47:13,329
model with respect to the weights of

2260
01:47:12,100 --> 01:47:15,910
your model you're going to need to use

2261
01:47:13,329 --> 01:47:17,859
the chain rule and specifically whatever

2262
01:47:15,909 --> 01:47:19,329
layer it is that you're up to like I

2263
01:47:17,859 --> 01:47:23,019
want to calculate the derivative here

2264
01:47:19,329 --> 01:47:24,279
and got a need to use all of these all

2265
01:47:23,020 --> 01:47:25,690
of these ones because that's all that's

2266
01:47:24,279 --> 01:47:28,149
that's the function that's being applied

2267
01:47:25,689 --> 01:47:30,129
right and that's why they call this back

2268
01:47:28,149 --> 01:47:36,309
propagation because the value of the

2269
01:47:30,130 --> 01:47:39,159
derivative of that is equal to that

2270
01:47:36,310 --> 01:47:43,810
derivative now basically you can do it

2271
01:47:39,159 --> 01:47:44,710
like this you can say let's call you is

2272
01:47:43,810 --> 01:47:47,020
this right

2273
01:47:44,710 --> 01:47:51,840
let's call that you all right then it's

2274
01:47:47,020 --> 01:47:56,440
simply equal to the derivative of that

2275
01:47:51,840 --> 01:47:59,170
times derivative of that right

2276
01:47:56,439 --> 01:48:00,879
you just multiply them together and so

2277
01:47:59,170 --> 01:48:03,909
that's what back propagation is like

2278
01:48:00,880 --> 01:48:06,720
it's not that back propagation is a new

2279
01:48:03,909 --> 01:48:10,239
thing for you to learn it's not a new

2280
01:48:06,720 --> 01:48:12,659
algorithm it is literally take the

2281
01:48:10,239 --> 01:48:16,269
derivative of every one of your layers

2282
01:48:12,659 --> 01:48:18,909
and multiply them all together so like

2283
01:48:16,270 --> 01:48:23,050
it doesn't deserve a new name right

2284
01:48:18,909 --> 01:48:24,970
apply the chain rule to my layers does

2285
01:48:23,050 --> 01:48:27,789
not deserve a new name but it gets one

2286
01:48:24,970 --> 01:48:29,920
because us neural networks folk really

2287
01:48:27,789 --> 01:48:31,750
need to seem as clever as possible it's

2288
01:48:29,920 --> 01:48:32,319
really important that everybody else

2289
01:48:31,750 --> 01:48:34,929
thinks

2290
01:48:32,319 --> 01:48:37,719
we are way outside of their capabilities

2291
01:48:34,929 --> 01:48:39,429
so the fact that you're here means that

2292
01:48:37,719 --> 01:48:41,980
we've failed because you guys somehow

2293
01:48:39,429 --> 01:48:43,539
think that you're capable right so

2294
01:48:41,979 --> 01:48:45,279
remember it's really important when you

2295
01:48:43,539 --> 01:48:48,010
talk to other people that you say

2296
01:48:45,279 --> 01:48:51,039
backpropagation and rectified linear

2297
01:48:48,010 --> 01:48:54,400
unit rather than like multiply the

2298
01:48:51,039 --> 01:48:57,519
layers gradients or replace negatives

2299
01:48:54,399 --> 01:49:00,388
with zeros okay so so here we go

2300
01:48:57,520 --> 01:49:03,159
so here is so I've just gone ahead and

2301
01:49:00,389 --> 01:49:05,078
grabbed the derivative unfortunately

2302
01:49:03,158 --> 01:49:07,839
there is no automatic differentiation in

2303
01:49:05,078 --> 01:49:09,609
Excel yet so I did the alternative which

2304
01:49:07,840 --> 01:49:12,190
is to paste the formula into Wolfram

2305
01:49:09,609 --> 01:49:14,019
Alpha and got back the derivative so

2306
01:49:12,189 --> 01:49:16,569
there's the first derivative and there's

2307
01:49:14,020 --> 01:49:19,510
the second derivative analytically we

2308
01:49:16,569 --> 01:49:21,908
only have one layer in this infinite

2309
01:49:19,510 --> 01:49:23,860
finally small neural network so we don't

2310
01:49:21,908 --> 01:49:25,479
have to worry about the chain rule and

2311
01:49:23,859 --> 01:49:27,098
we should see that this analytical

2312
01:49:25,479 --> 01:49:28,598
derivative is pretty close to our

2313
01:49:27,099 --> 01:49:31,750
estimated derivative from the find out

2314
01:49:28,599 --> 01:49:33,880
differencing and indeed it is right and

2315
01:49:31,750 --> 01:49:35,649
we should see that these ones are pretty

2316
01:49:33,880 --> 01:49:38,789
similar as well and indeed they are

2317
01:49:35,649 --> 01:49:41,948
right and if you're you know back when I

2318
01:49:38,789 --> 01:49:45,130
implemented my own neural Nets 20 years

2319
01:49:41,948 --> 01:49:46,960
ago I you know had to actually calculate

2320
01:49:45,130 --> 01:49:48,550
the derivatives and so I always would

2321
01:49:46,960 --> 01:49:50,289
write like had something that would

2322
01:49:48,550 --> 01:49:52,090
check the derivatives using finite

2323
01:49:50,289 --> 01:49:53,469
difference in and so for those poor

2324
01:49:52,090 --> 01:49:55,420
people that they'd have to write these

2325
01:49:53,469 --> 01:49:57,609
things by hand you'll still see that

2326
01:49:55,420 --> 01:49:59,710
they have like a finite differencing

2327
01:49:57,609 --> 01:50:02,649
checkout so if you ever do have to

2328
01:49:59,710 --> 01:50:05,469
implement a derivative by hand please

2329
01:50:02,649 --> 01:50:07,149
make sure that you have a finite

2330
01:50:05,469 --> 01:50:10,719
differencing checker so that you can

2331
01:50:07,149 --> 01:50:13,479
test it alright so there's no

2332
01:50:10,719 --> 01:50:17,139
derivatives so we know that if we

2333
01:50:13,479 --> 01:50:19,868
increase B then we're going to get a

2334
01:50:17,139 --> 01:50:22,239
slightly better loss so let's increase B

2335
01:50:19,868 --> 01:50:24,670
by a bit how much should we increase it

2336
01:50:22,238 --> 01:50:26,259
by well we'll increase it by some more

2337
01:50:24,670 --> 01:50:28,060
for this so the motor-pod we're going to

2338
01:50:26,260 --> 01:50:29,770
choose is called a learning rate and so

2339
01:50:28,060 --> 01:50:37,300
here's our learning rate so here's one

2340
01:50:29,770 --> 01:50:39,570
enoch 4 ok so our new value is equal to

2341
01:50:37,300 --> 01:50:45,219
whatever it was before

2342
01:50:39,569 --> 01:50:47,738
- our derivative times our learning rate

2343
01:50:45,219 --> 01:50:52,239
okay so we've gone from one to one point

2344
01:50:47,738 --> 01:50:56,348
or one and then a we've done the same

2345
01:50:52,238 --> 01:51:00,609
thing so it's gone from one to one point

2346
01:50:56,349 --> 01:51:02,529
one two so this is a special kind of

2347
01:51:00,609 --> 01:51:04,929
mini batch it's a mini batch of size one

2348
01:51:02,529 --> 01:51:09,099
okay so we call this online grading does

2349
01:51:04,929 --> 01:51:11,050
it just means mini batch of size one so

2350
01:51:09,099 --> 01:51:11,800
then we can go into the next one next is

2351
01:51:11,050 --> 01:51:15,760
86

2352
01:51:11,800 --> 01:51:17,590
why is 202 right this is my intercept

2353
01:51:15,760 --> 01:51:22,329
and slope copied across from the last

2354
01:51:17,590 --> 01:51:26,170
row okay so here's my new wire

2355
01:51:22,328 --> 01:51:29,939
prediction here's my new era here are my

2356
01:51:26,170 --> 01:51:32,559
derivatives here are my new a and B okay

2357
01:51:29,939 --> 01:51:37,899
so we keep doing that for every mini

2358
01:51:32,559 --> 01:51:41,139
batch of one until eventually we run out

2359
01:51:37,899 --> 01:51:43,328
at the end of the new pocket okay and so

2360
01:51:41,139 --> 01:51:48,849
then at the end of an epoch we would

2361
01:51:43,328 --> 01:51:53,578
grab our intercept and slope and paste

2362
01:51:48,849 --> 01:51:56,440
them back over here as our new values

2363
01:51:53,578 --> 01:51:59,049
there we are and we can now continue

2364
01:51:56,439 --> 01:52:02,078
again all right so we're now starting

2365
01:51:59,050 --> 01:52:06,070
with pops today's either in the wrong

2366
01:52:02,078 --> 01:52:10,420
spot it should be pasted special

2367
01:52:06,069 --> 01:52:12,460
transpose values all right okay

2368
01:52:10,420 --> 01:52:13,809
so there's a new intercept there's any

2369
01:52:12,460 --> 01:52:15,670
slope possibly I got that the wrong way

2370
01:52:13,809 --> 01:52:19,570
around but anyway you get the idea and

2371
01:52:15,670 --> 01:52:22,989
then we continue okay so I recorded the

2372
01:52:19,569 --> 01:52:29,799
world's tiniest macro which literally

2373
01:52:22,988 --> 01:52:32,069
just copies the final slope and puts it

2374
01:52:29,800 --> 01:52:36,909
into the new slope copies the final

2375
01:52:32,069 --> 01:52:40,988
intercept put the new intercept and does

2376
01:52:36,908 --> 01:52:42,488
that five times and after each time it

2377
01:52:40,988 --> 01:52:45,549
grabs the root mean squared error and

2378
01:52:42,488 --> 01:52:48,399
pastes it into the next spare area and

2379
01:52:45,550 --> 01:52:49,690
that is attached to this Run button and

2380
01:52:48,399 --> 01:52:53,768
so that's going to go ahead and do that

2381
01:52:49,689 --> 01:52:57,848
five times okay so that's stochastic

2382
01:52:53,769 --> 01:52:58,989
gradient descent and if so so to turn

2383
01:52:57,849 --> 01:53:02,560
this into a CNN

2384
01:52:58,989 --> 01:53:05,170
all right you would just replace this

2385
01:53:02,560 --> 01:53:08,490
error function right and therefore this

2386
01:53:05,170 --> 01:53:12,220
prediction with the output of that

2387
01:53:08,489 --> 01:53:15,340
convolutional example spreadsheet okay

2388
01:53:12,220 --> 01:53:19,750
and that then would be in CNN being

2389
01:53:15,340 --> 01:53:24,369
trained with with SGD okay

2390
01:53:19,750 --> 01:53:30,609
now the problem is that you'll see when

2391
01:53:24,369 --> 01:53:32,739
I run this it's kind of going very

2392
01:53:30,609 --> 01:53:35,409
slowly right we know that we need to get

2393
01:53:32,739 --> 01:53:37,569
to a slope of two and an intercept of

2394
01:53:35,409 --> 01:53:40,180
thirty and you can kind of see it this

2395
01:53:37,569 --> 01:53:49,210
rate it's going to take a very long time

2396
01:53:40,180 --> 01:53:51,850
right and specifically it's like it

2397
01:53:49,210 --> 01:53:53,770
keeps going the same direction so it's

2398
01:53:51,850 --> 01:53:56,829
like come on take a hint that's a good

2399
01:53:53,770 --> 01:53:58,240
direction so they come on take a hint

2400
01:53:56,829 --> 01:54:01,260
that's a good direction please keep

2401
01:53:58,239 --> 01:54:05,500
doing that but more is called momentum

2402
01:54:01,260 --> 01:54:11,350
right so on our next spreadsheet we're

2403
01:54:05,500 --> 01:54:14,890
going to implement momentum okay so what

2404
01:54:11,350 --> 01:54:16,510
momentum does is the same thing and what

2405
01:54:14,890 --> 01:54:19,720
to simplify this spreadsheet I've

2406
01:54:16,510 --> 01:54:21,430
removed the finite difference cause okay

2407
01:54:19,720 --> 01:54:22,900
other than that this is just the same

2408
01:54:21,430 --> 01:54:28,810
right so it's true what our X is our

2409
01:54:22,899 --> 01:54:32,639
wise A's and B's predictions our error

2410
01:54:28,810 --> 01:54:39,340
is now over here okay and here's our

2411
01:54:32,640 --> 01:54:45,730
derivatives okay our new calculation for

2412
01:54:39,340 --> 01:54:49,659
this particular row our new calculation

2413
01:54:45,729 --> 01:54:56,639
here for our new a term just like before

2414
01:54:49,659 --> 01:54:59,109
is is equal to whatever a was before -

2415
01:54:56,640 --> 01:55:01,560
okay now this time I'm not taking the

2416
01:54:59,109 --> 01:55:03,849
derivative but I'm - income other number

2417
01:55:01,560 --> 01:55:08,470
times the loan rate so what's this other

2418
01:55:03,850 --> 01:55:13,020
number okay so this other number is

2419
01:55:08,470 --> 01:55:22,920
equal to the derivative

2420
01:55:13,020 --> 01:55:26,800
times what's this k 1.02 plus 0.98 times

2421
01:55:22,920 --> 01:55:31,239
the thing just above it okay so this is

2422
01:55:26,800 --> 01:55:32,650
a linear interpolation between this rows

2423
01:55:31,239 --> 01:55:35,829
derivative for this mini-batches

2424
01:55:32,649 --> 01:55:39,219
derivative and whatever direction we

2425
01:55:35,829 --> 01:55:41,739
went last time right so in other words

2426
01:55:39,220 --> 01:55:44,650
keep going the same direction as you

2427
01:55:41,739 --> 01:55:48,939
were before right then update it a

2428
01:55:44,649 --> 01:55:51,460
little bit right and so in our rich in

2429
01:55:48,939 --> 01:55:55,599
our Python just before we had a momentum

2430
01:55:51,460 --> 01:56:01,090
of 0.9 okay so you can see what tends to

2431
01:55:55,600 --> 01:56:03,280
happen is that our negative kind of gets

2432
01:56:01,090 --> 01:56:08,079
more and more negative right all the way

2433
01:56:03,279 --> 01:56:11,979
up to like 2,000 where else with our

2434
01:56:08,079 --> 01:56:14,199
standard SGD approach a derivatives are

2435
01:56:11,979 --> 01:56:16,419
kind of all over the place right

2436
01:56:14,199 --> 01:56:18,789
sometimes there's 700 something negative

2437
01:56:16,420 --> 01:56:21,039
7 positive 100 you know so this is

2438
01:56:18,789 --> 01:56:24,100
basically saying like yeah if you've

2439
01:56:21,039 --> 01:56:26,500
been going down for quite a while keep

2440
01:56:24,100 --> 01:56:28,300
doing that until finally here it's like

2441
01:56:26,500 --> 01:56:29,680
okay that's that seems to be far enough

2442
01:56:28,300 --> 01:56:32,140
so that's being less and less and less

2443
01:56:29,680 --> 01:56:34,090
negative all right mister we start being

2444
01:56:32,140 --> 01:56:35,680
positive again so you can kind of see

2445
01:56:34,090 --> 01:56:37,630
why it's called momentum it's like once

2446
01:56:35,680 --> 01:56:40,240
you start traveling in a particular

2447
01:56:37,630 --> 01:56:41,949
direction for a particular weight you're

2448
01:56:40,239 --> 01:56:44,739
kind of the wheel start spinning and

2449
01:56:41,949 --> 01:56:46,809
then once the gradient turns around the

2450
01:56:44,739 --> 01:56:48,189
other way it's like Oh slow down we've

2451
01:56:46,810 --> 01:56:51,700
got this kind of event um and then

2452
01:56:48,189 --> 01:56:57,339
finally turn back around right so when

2453
01:56:51,699 --> 01:56:59,439
we do it this way all right we can do

2454
01:56:57,340 --> 01:57:04,300
exactly the same thing right and after

2455
01:56:59,439 --> 01:57:07,719
five iterations we're at 89 where else

2456
01:57:04,300 --> 01:57:11,949
before after five iterations we're at

2457
01:57:07,720 --> 01:57:20,199
104 right and after a few more let's go

2458
01:57:11,949 --> 01:57:22,619
maybe 15 okay so get this 102 for us

2459
01:57:20,199 --> 01:57:22,619
here

2460
01:57:26,159 --> 01:57:31,659
it's going right so it's it's it's a bit

2461
01:57:30,250 --> 01:57:35,819
better it's not hips better you can

2462
01:57:31,659 --> 01:57:38,710
still see like these numbers they're not

2463
01:57:35,819 --> 01:57:40,000
zipping along right but it's definitely

2464
01:57:38,710 --> 01:57:41,680
an improvement and it also gives us

2465
01:57:40,000 --> 01:57:43,510
something else to tune which is nice

2466
01:57:41,680 --> 01:57:45,909
like so if this is kind of a

2467
01:57:43,510 --> 01:57:48,489
well-behaved error surface right in

2468
01:57:45,909 --> 01:57:50,800
other words like although it might be

2469
01:57:48,489 --> 01:57:53,139
bumpy along the way there's kind of some

2470
01:57:50,800 --> 01:57:55,300
overall direction like imagine you're

2471
01:57:53,140 --> 01:57:57,460
going down a hill right and there's like

2472
01:57:55,300 --> 01:57:59,170
bumps oh alright so the mobile more

2473
01:57:57,460 --> 01:58:01,239
momentum you got going to skipping over

2474
01:57:59,170 --> 01:58:03,329
the tops right so we could say like okay

2475
01:58:01,239 --> 01:58:06,699
let's increase our beater up to 0.98

2476
01:58:03,329 --> 01:58:08,140
right and see if that like allows us to

2477
01:58:06,699 --> 01:58:10,689
train a little faster and whoa look at

2478
01:58:08,140 --> 01:58:12,160
that suddenly what's going to okay so

2479
01:58:10,689 --> 01:58:13,750
one nice thing about things like

2480
01:58:12,159 --> 01:58:16,420
momentum is it's like another parameter

2481
01:58:13,750 --> 01:58:21,670
that you can choose to try and make your

2482
01:58:16,420 --> 01:58:24,250
model train better in practice basically

2483
01:58:21,670 --> 01:58:26,230
everybody does this every like you look

2484
01:58:24,250 --> 01:58:37,930
at any like image net winner or whatever

2485
01:58:26,229 --> 01:58:41,949
they all use momentum okay and so back

2486
01:58:37,930 --> 01:58:44,740
over here when we said here's SGD that

2487
01:58:41,949 --> 01:58:46,960
basically means use the basic tab of our

2488
01:58:44,739 --> 01:58:52,750
Excel spreadsheet but then momentum

2489
01:58:46,960 --> 01:58:58,000
equals 0.9 means add in put a point nine

2490
01:58:52,750 --> 01:59:03,239
over here okay and so that that's kind

2491
01:58:58,000 --> 01:59:03,239
of your like default starting point so

2492
01:59:03,720 --> 01:59:18,220
let's keep going and talk about Adam so

2493
01:59:12,069 --> 01:59:20,769
Adam is something which I actually was

2494
01:59:18,220 --> 01:59:22,720
not right earlier on in this course I

2495
01:59:20,770 --> 01:59:24,370
said we've been using Adam by default we

2496
01:59:22,720 --> 01:59:25,690
actually haven't we've actually been I

2497
01:59:24,369 --> 01:59:29,019
noticed our we've actually been using

2498
01:59:25,689 --> 01:59:34,979
SGD with momentum by default and the

2499
01:59:29,020 --> 01:59:36,690
reason is that Adam has had

2500
01:59:34,979 --> 01:59:38,488
much faster as you'll see it's much much

2501
01:59:36,689 --> 01:59:39,779
faster to learn with but there's been

2502
01:59:38,488 --> 01:59:41,699
some problems which is people who

2503
01:59:39,779 --> 01:59:44,519
haven't been getting quite as good like

2504
01:59:41,699 --> 01:59:46,439
final answers with Adam as they have

2505
01:59:44,520 --> 01:59:48,180
with std with momentum and that's why

2506
01:59:46,439 --> 01:59:50,669
you'll see like all the you know image

2507
01:59:48,180 --> 01:59:54,239
net winning solutions and so forth and

2508
01:59:50,670 --> 01:59:56,880
all the academic papers always use SGD

2509
01:59:54,238 --> 01:59:58,799
with momentum and I'll Adam seems to be

2510
01:59:56,880 --> 02:00:00,329
a particular problem in NLP people

2511
01:59:58,800 --> 02:00:06,210
really haven't got Adam working at all

2512
02:00:00,329 --> 02:00:08,640
well the good news is this was I built

2513
02:00:06,210 --> 02:00:13,020
it looks like this was solved two weeks

2514
02:00:08,640 --> 02:00:14,730
ago it basically it turned out that the

2515
02:00:13,020 --> 02:00:17,310
way people were dealing with a

2516
02:00:14,729 --> 02:00:20,549
combination of weight decay in Adam had

2517
02:00:17,310 --> 02:00:22,200
a nasty kind of bargainer basically and

2518
02:00:20,550 --> 02:00:24,750
that's that's kind of carried through to

2519
02:00:22,199 --> 02:00:27,210
every single library and one of our

2520
02:00:24,750 --> 02:00:31,170
students and then Sahara has actually

2521
02:00:27,210 --> 02:00:33,060
just completed a prototype of adding is

2522
02:00:31,170 --> 02:00:36,390
this new version of Adam has called Adam

2523
02:00:33,060 --> 02:00:40,039
W into fast AI and he's confirmed that

2524
02:00:36,390 --> 02:00:43,710
he's getting much faster both the faster

2525
02:00:40,039 --> 02:00:47,279
performance and also the better accuracy

2526
02:00:43,710 --> 02:00:49,560
so hopefully we'll have this Adam W in

2527
02:00:47,279 --> 02:00:51,238
faster ideally before next week we'll

2528
02:00:49,560 --> 02:00:53,660
see how we go very very soon

2529
02:00:51,238 --> 02:00:58,829
so so it is worth telling you about

2530
02:00:53,659 --> 02:01:02,909
about Adam so let's talk about it it's

2531
02:00:58,829 --> 02:01:04,559
actually incredibly simple but again you

2532
02:01:02,909 --> 02:01:06,599
know make sure you make it sound really

2533
02:01:04,560 --> 02:01:09,690
complicated when you tell people so that

2534
02:01:06,600 --> 02:01:13,920
you can so here's the same spreadsheet

2535
02:01:09,689 --> 02:01:16,469
again right and here's our randomly

2536
02:01:13,920 --> 02:01:19,020
selected a and B again somehow it's

2537
02:01:16,470 --> 02:01:21,810
still one here's a prediction here's our

2538
02:01:19,020 --> 02:01:25,230
derivatives okay so now how we count

2539
02:01:21,810 --> 02:01:28,289
letting on you hey you could immediately

2540
02:01:25,229 --> 02:01:31,979
see it's looking pretty hopeful because

2541
02:01:28,289 --> 02:01:34,109
even by like row ten we're like we're

2542
02:01:31,979 --> 02:01:38,789
seeing the numbers move a lot more right

2543
02:01:34,109 --> 02:01:42,960
so this is looking pretty encouraging so

2544
02:01:38,789 --> 02:01:48,939
how are we calculating this it's equal

2545
02:01:42,960 --> 02:01:49,989
to our previous value with B minus j h

2546
02:01:48,939 --> 02:01:56,738
we're gonna have to find out what that

2547
02:01:49,988 --> 02:01:59,079
is times our learning rate divided by

2548
02:01:56,738 --> 02:02:00,399
the square root of LH okay so I'm gonna

2549
02:01:59,079 --> 02:02:02,409
have to dig it and see what's going on

2550
02:02:00,399 --> 02:02:05,649
one thing to notice here is that my

2551
02:02:02,409 --> 02:02:08,609
learning rate is way higher than it used

2552
02:02:05,649 --> 02:02:12,729
to be but then we're dividing it by this

2553
02:02:08,609 --> 02:02:14,380
big number okay so let's start out by

2554
02:02:12,729 --> 02:02:19,299
looking and seeing what this day-out

2555
02:02:14,380 --> 02:02:23,079
thing is okay

2556
02:02:19,300 --> 02:02:25,690
j8 is identical to what we had before j8

2557
02:02:23,079 --> 02:02:28,750
is equal to the linear interpolation of

2558
02:02:25,689 --> 02:02:33,549
the derivative and the previous

2559
02:02:28,750 --> 02:02:36,550
direction okay so that was easy so one

2560
02:02:33,550 --> 02:02:39,300
part of atom is to use momentum in the

2561
02:02:36,550 --> 02:02:41,680
way we just defined it

2562
02:02:39,300 --> 02:02:45,640
okay the second piece was to divide by

2563
02:02:41,680 --> 02:02:48,610
square root L 8 what is that square root

2564
02:02:45,640 --> 02:02:52,920
L 8 okay is another linear interpolation

2565
02:02:48,609 --> 02:02:54,488
of something and something else and

2566
02:02:52,920 --> 02:03:00,609
specifically it's a linear interpolation

2567
02:02:54,488 --> 02:03:03,869
of F 8 squared okay it's a linear

2568
02:03:00,609 --> 02:03:07,210
interpolation of the derivative squared

2569
02:03:03,869 --> 02:03:09,939
along with the derivative squared last

2570
02:03:07,210 --> 02:03:13,210
time okay so in other words we've got

2571
02:03:09,939 --> 02:03:19,210
two pieces of momentum going on here

2572
02:03:13,210 --> 02:03:22,420
one is calculating the momentum version

2573
02:03:19,210 --> 02:03:24,310
of the gradient the other is calculating

2574
02:03:22,420 --> 02:03:28,779
the momentum version of the gradient

2575
02:03:24,310 --> 02:03:31,900
squared and we often refer to this idea

2576
02:03:28,779 --> 02:03:34,090
as a exponentially weighted moving

2577
02:03:31,899 --> 02:03:36,099
average in other words it's basically

2578
02:03:34,090 --> 02:03:37,630
equal to the average of this one and the

2579
02:03:36,100 --> 02:03:39,900
last one in the last one in the last one

2580
02:03:37,630 --> 02:03:42,130
that we're like multiplicatively

2581
02:03:39,899 --> 02:03:44,349
decreasing the previous ones right

2582
02:03:42,130 --> 02:03:47,829
because we're multiplying it by 0.9

2583
02:03:44,350 --> 02:03:53,370
times what 999 and so you actually see

2584
02:03:47,829 --> 02:03:53,369
that for instance in the faster I code

2585
02:04:00,180 --> 02:04:10,380
if you look at fish we don't just

2586
02:04:04,779 --> 02:04:13,000
calculate the average loss right because

2587
02:04:10,380 --> 02:04:14,980
what I actually want we certainly don't

2588
02:04:13,000 --> 02:04:16,210
just report the loss for every mini

2589
02:04:14,979 --> 02:04:20,019
match because that just bounces around

2590
02:04:16,210 --> 02:04:22,359
so much so instead I say average loss is

2591
02:04:20,020 --> 02:04:27,699
equal to whatever the average loss was

2592
02:04:22,359 --> 02:04:31,899
last time times 0.98 plus the loss this

2593
02:04:27,698 --> 02:04:34,869
time times 0.02 right so in other words

2594
02:04:31,899 --> 02:04:36,488
the faster you library the thing that

2595
02:04:34,869 --> 02:04:38,890
it's actually when you do like the

2596
02:04:36,488 --> 02:04:40,929
learning rate finder or plot loss it's

2597
02:04:38,890 --> 02:04:44,260
actually showing you the exponentially

2598
02:04:40,930 --> 02:04:47,289
weighted moving average of the loss okay

2599
02:04:44,260 --> 02:04:49,780
so it's like a really handy concept it

2600
02:04:47,289 --> 02:04:52,390
appears quite a lot right the other in

2601
02:04:49,779 --> 02:04:55,779
handy concept know about is this idea of

2602
02:04:52,390 --> 02:04:58,119
like you've got two numbers one of them

2603
02:04:55,779 --> 02:05:00,429
is multiplied by some value the other is

2604
02:04:58,119 --> 02:05:02,769
multiplied by one minus that value so

2605
02:05:00,430 --> 02:05:05,550
this is a linear interpolation of two

2606
02:05:02,770 --> 02:05:09,190
values you'll see it all the time and

2607
02:05:05,550 --> 02:05:11,949
for some reason deep learning people

2608
02:05:09,189 --> 02:05:13,869
nearly always use the value alpha when

2609
02:05:11,948 --> 02:05:15,279
they do this so like keep an eye out if

2610
02:05:13,869 --> 02:05:17,920
you're reading a paper or something and

2611
02:05:15,279 --> 02:05:22,840
you see like alpha times bla bla bla bla

2612
02:05:17,920 --> 02:05:25,270
bla plus one minus alpha times some

2613
02:05:22,840 --> 02:05:28,180
other bla bla bla bla right immediately

2614
02:05:25,270 --> 02:05:31,239
like when people read papers none of us

2615
02:05:28,180 --> 02:05:33,730
like read every thing in the equation we

2616
02:05:31,238 --> 02:05:35,889
look at it we go oh linear interpolation

2617
02:05:33,729 --> 02:05:37,659
right and I said something I was just

2618
02:05:35,890 --> 02:05:39,670
talking to Rachel about yesterday is

2619
02:05:37,659 --> 02:05:42,189
like whether we could start trying to

2620
02:05:39,670 --> 02:05:44,079
find like a a new way of writing papers

2621
02:05:42,189 --> 02:05:45,639
where we literally refactor them right

2622
02:05:44,079 --> 02:05:50,619
like it'd be so much better to have

2623
02:05:45,640 --> 02:05:52,810
written like linear interpolate bla bla

2624
02:05:50,619 --> 02:05:55,090
bla bla bla right because then you don't

2625
02:05:52,810 --> 02:05:57,489
have to have that pattern recognition

2626
02:05:55,090 --> 02:05:59,440
right but until we convince the world to

2627
02:05:57,488 --> 02:06:00,819
change how they write papers this is

2628
02:05:59,439 --> 02:06:03,279
what you have to do is you have to look

2629
02:06:00,819 --> 02:06:06,849
you know know what to look for right and

2630
02:06:03,279 --> 02:06:09,189
once you do suddenly the huge page with

2631
02:06:06,850 --> 02:06:11,020
formulas

2632
02:06:09,189 --> 02:06:13,659
that at all like you often notice like

2633
02:06:11,020 --> 02:06:15,580
for example the two things in here like

2634
02:06:13,659 --> 02:06:17,260
they might be totally identical but this

2635
02:06:15,579 --> 02:06:19,659
might be a time T and this might be at

2636
02:06:17,260 --> 02:06:21,730
like time t minus y or something right

2637
02:06:19,659 --> 02:06:24,399
like it's very often these big ugly

2638
02:06:21,729 --> 02:06:26,759
formulas turn out to be really really

2639
02:06:24,399 --> 02:06:30,339
simple if only they had ripped out them

2640
02:06:26,760 --> 02:06:34,060
okay so what are we doing with this

2641
02:06:30,340 --> 02:06:37,539
gradient squared so what we were doing

2642
02:06:34,060 --> 02:06:40,239
with the gradient squared is we were

2643
02:06:37,539 --> 02:06:42,369
taking the square root and then we were

2644
02:06:40,238 --> 02:06:45,609
adjusting the learning rate by dividing

2645
02:06:42,369 --> 02:06:49,319
the learning rate by that okay so

2646
02:06:45,609 --> 02:06:53,500
gradient squared is always positive

2647
02:06:49,319 --> 02:06:55,299
right and we're taking the exponentially

2648
02:06:53,500 --> 02:06:57,399
waiting move moving average of a bunch

2649
02:06:55,300 --> 02:06:58,750
of things that are always positive and

2650
02:06:57,399 --> 02:07:01,389
then we're taking the square root of

2651
02:06:58,750 --> 02:07:04,060
that right so when is this number going

2652
02:07:01,390 --> 02:07:07,150
to be high it's going to be particularly

2653
02:07:04,060 --> 02:07:09,550
high if there's like one big you know if

2654
02:07:07,149 --> 02:07:11,349
the gradients got a lot of variation

2655
02:07:09,550 --> 02:07:14,980
that's oh there's a high variance of

2656
02:07:11,350 --> 02:07:16,840
gradient then this G squared thing is

2657
02:07:14,979 --> 02:07:20,919
going to be a really high number for us

2658
02:07:16,840 --> 02:07:22,630
if it's like a constant amount right

2659
02:07:20,920 --> 02:07:24,730
it's going to be smaller that cuz when

2660
02:07:22,630 --> 02:07:26,980
you add things that are squared the

2661
02:07:24,729 --> 02:07:28,509
squared slight jump out much bigger for

2662
02:07:26,979 --> 02:07:31,629
us if there wasn't if there wasn't much

2663
02:07:28,510 --> 02:07:34,869
change it's not going to be as big so

2664
02:07:31,630 --> 02:07:38,980
basically this number at the bottom here

2665
02:07:34,869 --> 02:07:41,319
is going to be high if our Brady --nt is

2666
02:07:38,979 --> 02:07:44,289
changing a lot now what do you want to

2667
02:07:41,319 --> 02:07:46,210
do if you've got something which is like

2668
02:07:44,289 --> 02:07:50,920
first negative and then positive and

2669
02:07:46,210 --> 02:07:53,260
then small and then high right well you

2670
02:07:50,920 --> 02:07:54,369
probably want to be more careful right

2671
02:07:53,260 --> 02:07:56,650
you probably don't want to take a big

2672
02:07:54,369 --> 02:07:59,289
step because you can't really trust it

2673
02:07:56,649 --> 02:08:01,569
right so when the when the variance of

2674
02:07:59,289 --> 02:08:03,479
the gradient is high we're going to

2675
02:08:01,569 --> 02:08:06,238
divide our learning rate by a big number

2676
02:08:03,479 --> 02:08:09,759
we also found learning rate is very

2677
02:08:06,238 --> 02:08:11,139
similar kind of size all the time then

2678
02:08:09,760 --> 02:08:13,119
we probably feel pretty good about the

2679
02:08:11,140 --> 02:08:15,520
step so we're dividing it by a small

2680
02:08:13,119 --> 02:08:18,099
amount yeah and so this is called an

2681
02:08:15,520 --> 02:08:19,900
adaptive learning rate yeah and like a

2682
02:08:18,100 --> 02:08:22,240
lot of people have this confusion about

2683
02:08:19,899 --> 02:08:24,699
atom I've seen it on the forum actually

2684
02:08:22,239 --> 02:08:26,319
like there's some kind of adaptive

2685
02:08:24,699 --> 02:08:29,649
learning rate where somehow you like

2686
02:08:26,319 --> 02:08:31,329
setting different learning rates for

2687
02:08:29,649 --> 02:08:33,009
different layers or something it's like

2688
02:08:31,329 --> 02:08:35,559
no not really

2689
02:08:33,010 --> 02:08:37,720
right all we're doing is we're just

2690
02:08:35,560 --> 02:08:39,400
saying like this keep track of the

2691
02:08:37,720 --> 02:08:43,060
average of the squares of the gradients

2692
02:08:39,399 --> 02:08:45,699
and use that to adjust the learning rate

2693
02:08:43,060 --> 02:08:48,550
so there's still one learning rate okay

2694
02:08:45,699 --> 02:08:51,579
in this case it's one okay but

2695
02:08:48,550 --> 02:08:55,029
effectively every parameter at every

2696
02:08:51,579 --> 02:08:57,279
epoch is being kind of like getting a

2697
02:08:55,029 --> 02:08:58,840
bigger jump if the learning rate if the

2698
02:08:57,279 --> 02:09:02,739
gradients been pretty constant for that

2699
02:08:58,840 --> 02:09:04,659
wait and a smaller jump otherwise okay

2700
02:09:02,739 --> 02:09:07,659
and that's Adam that's the entirety of

2701
02:09:04,659 --> 02:09:09,939
Adam in in Excel right so there's now no

2702
02:09:07,659 --> 02:09:11,529
reason at all why you can't train

2703
02:09:09,939 --> 02:09:13,539
imagenet in Excel because you've got

2704
02:09:11,529 --> 02:09:18,659
you've got access to all of the pieces

2705
02:09:13,539 --> 02:09:18,659
you need and so let's try this out run

2706
02:09:19,829 --> 02:09:25,000
okay that's not bad right five and we

2707
02:09:22,779 --> 02:09:28,569
straight up to twenty nine and two right

2708
02:09:25,000 --> 02:09:31,479
so the difference between like you know

2709
02:09:28,569 --> 02:09:32,649
standard SGD and this is is huge and

2710
02:09:31,479 --> 02:09:35,139
basically that you know the key

2711
02:09:32,649 --> 02:09:37,779
difference was that it figured out that

2712
02:09:35,140 --> 02:09:42,100
we need to be you know moving this

2713
02:09:37,779 --> 02:09:45,539
number much faster okay and so and so it

2714
02:09:42,100 --> 02:09:48,370
do and so you can see we've now got like

2715
02:09:45,539 --> 02:09:50,500
two different parameters one is kind of

2716
02:09:48,369 --> 02:09:52,510
momentum for the gradient piece the

2717
02:09:50,500 --> 02:09:56,500
other is the momentum for the gradient

2718
02:09:52,510 --> 02:09:58,030
squared piece and there I think they're

2719
02:09:56,500 --> 02:10:00,670
called like I think there's just a

2720
02:09:58,029 --> 02:10:02,319
couple of the beta I think when you when

2721
02:10:00,670 --> 02:10:03,789
you want to change it in PI tortes is I

2722
02:10:02,319 --> 02:10:11,189
think what beta which is just a couple

2723
02:10:03,789 --> 02:10:15,430
of two numbers you can change Jeremy so

2724
02:10:11,189 --> 02:10:18,939
so you set the yeah I think I understand

2725
02:10:15,430 --> 02:10:21,310
this concept of you know one day when a

2726
02:10:18,939 --> 02:10:24,609
gradient is it goes up and down then

2727
02:10:21,310 --> 02:10:26,770
you're not really sure which direction

2728
02:10:24,609 --> 02:10:27,579
should should go so you should kind of

2729
02:10:26,770 --> 02:10:29,590
slow things down

2730
02:10:27,579 --> 02:10:32,140
therefore you subtract that gradient

2731
02:10:29,590 --> 02:10:35,500
from the learning rate so but how do you

2732
02:10:32,140 --> 02:10:36,480
implement how far do you go I guess

2733
02:10:35,500 --> 02:10:38,609
maybe I miss something

2734
02:10:36,479 --> 02:10:42,899
early on you do you set a number

2735
02:10:38,609 --> 02:10:45,929
somewhere we divide yeah we divide the

2736
02:10:42,899 --> 02:10:48,829
learning rate divided by the square root

2737
02:10:45,930 --> 02:10:50,880
of the moving average gradient squared

2738
02:10:48,829 --> 02:10:55,109
so that's where we use it

2739
02:10:50,880 --> 02:10:57,720
oh I'm sorry can you be a little more

2740
02:10:55,109 --> 02:11:00,420
sure so d2 is the learning rate which is

2741
02:10:57,720 --> 02:11:02,850
one yeah m27

2742
02:11:00,420 --> 02:11:06,480
is our moving average of the squared

2743
02:11:02,850 --> 02:11:12,750
gradients so we just go D 2 divided by

2744
02:11:06,479 --> 02:11:16,949
square root and preserve that's it okay

2745
02:11:12,750 --> 02:11:19,859
thanks I have one question yeah

2746
02:11:16,949 --> 02:11:21,840
so the new method that you just

2747
02:11:19,859 --> 02:11:27,479
mentioned which is in the process of

2748
02:11:21,840 --> 02:11:32,579
getting implemented in yes how different

2749
02:11:27,479 --> 02:11:34,738
is it from here okay let's do that so to

2750
02:11:32,579 --> 02:11:38,220
understand Adam W we have to understand

2751
02:11:34,738 --> 02:11:39,509
wait okay and maybe we'll learn more

2752
02:11:38,220 --> 02:11:40,440
about that later let's see how we go now

2753
02:11:39,510 --> 02:11:46,440
with great okay

2754
02:11:40,439 --> 02:11:48,569
so the idea is that when you have lots

2755
02:11:46,439 --> 02:11:50,279
and lots of parameters like we do with

2756
02:11:48,569 --> 02:11:54,329
you know most of the neural Nets we

2757
02:11:50,279 --> 02:11:56,279
train you very often have like more

2758
02:11:54,329 --> 02:11:57,988
parameters and data points or you know

2759
02:11:56,279 --> 02:12:00,090
like regularization becomes important

2760
02:11:57,988 --> 02:12:03,359
and we've learnt how to avoid

2761
02:12:00,090 --> 02:12:07,199
overfitting by using dropout right which

2762
02:12:03,359 --> 02:12:08,909
randomly deletes some activations in the

2763
02:12:07,199 --> 02:12:12,630
hope that's going to learn some kind of

2764
02:12:08,909 --> 02:12:14,099
more resilient set of weights there's

2765
02:12:12,630 --> 02:12:16,800
another kind of Ritter ization we can

2766
02:12:14,100 --> 02:12:19,140
use called weight decay or l2

2767
02:12:16,800 --> 02:12:21,180
regularization and it's actually comes

2768
02:12:19,140 --> 02:12:23,400
kind of as a kind of classic statistical

2769
02:12:21,180 --> 02:12:26,130
technique and the idea is that we take

2770
02:12:23,399 --> 02:12:29,819
our loss function right so we take out

2771
02:12:26,130 --> 02:12:32,880
like arrow squared loss function and we

2772
02:12:29,819 --> 02:12:35,670
add an additional piece to it let's add

2773
02:12:32,880 --> 02:12:39,270
weight decay right now the additional

2774
02:12:35,670 --> 02:12:44,069
piece we add is to basically add the

2775
02:12:39,270 --> 02:12:49,619
square of the weights so we'd say plus B

2776
02:12:44,069 --> 02:12:55,960
squared plus a squared

2777
02:12:49,618 --> 02:12:58,448
okay that is now wait 2 K or L tree

2778
02:12:55,960 --> 02:13:04,840
regularization and so the idea is that

2779
02:12:58,448 --> 02:13:07,448
now the the loss function wants to keep

2780
02:13:04,840 --> 02:13:10,750
the weight small because increasing the

2781
02:13:07,448 --> 02:13:12,578
weights makes the loss worse and so it's

2782
02:13:10,750 --> 02:13:15,908
only going to increase the weights if

2783
02:13:12,578 --> 02:13:17,920
the loss improves by more than the

2784
02:13:15,908 --> 02:13:19,629
amount of that penalty and in fact to

2785
02:13:17,920 --> 02:13:20,170
make this weight to get to proper weight

2786
02:13:19,630 --> 02:13:24,219
decay

2787
02:13:20,170 --> 02:13:27,190
we then need some multiplier yeah right

2788
02:13:24,219 --> 02:13:31,719
so if you remember back in our here we

2789
02:13:27,189 --> 02:13:33,519
said weight decay equals W d5e neg 4

2790
02:13:31,719 --> 02:13:39,158
okay so to actually use the same way to

2791
02:13:33,520 --> 02:13:41,230
K I would have to multiply by 0.005 all

2792
02:13:39,158 --> 02:13:47,679
right so that's actually now the same

2793
02:13:41,229 --> 02:13:49,658
weight okay so if you have a really high

2794
02:13:47,679 --> 02:13:52,149
weight decay that it's going to set all

2795
02:13:49,658 --> 02:13:55,000
the parameters to zero so it'll never

2796
02:13:52,149 --> 02:13:57,549
over fit right because it can't set any

2797
02:13:55,000 --> 02:13:59,670
parameter to anything and so as you

2798
02:13:57,550 --> 02:14:02,770
gradually decrease the weight decay a

2799
02:13:59,670 --> 02:14:05,289
few more weights can actually be used

2800
02:14:02,770 --> 02:14:07,210
right but the ones that don't help much

2801
02:14:05,289 --> 02:14:11,559
it's still going to leave at zero or

2802
02:14:07,210 --> 02:14:14,469
close to zero right so that's what

2803
02:14:11,559 --> 02:14:16,809
that's what weight decay is is is

2804
02:14:14,469 --> 02:14:21,779
literally to change the loss function to

2805
02:14:16,809 --> 02:14:25,329
a D in this sum of squares of weights

2806
02:14:21,779 --> 02:14:28,809
times some parameter some hyper

2807
02:14:25,328 --> 02:14:31,689
parameter I should say the problem is

2808
02:14:28,809 --> 02:14:35,829
that if you put that into the loss

2809
02:14:31,689 --> 02:14:38,138
function as I have here then it ends up

2810
02:14:35,828 --> 02:14:39,639
in the moving average of gradients and

2811
02:14:38,139 --> 02:14:42,940
the moving average of Squared's of

2812
02:14:39,639 --> 02:14:46,929
gradients for atom right and so

2813
02:14:42,939 --> 02:14:52,029
basically we end up when there's a lot

2814
02:14:46,929 --> 02:14:53,559
of variation we end up decreasing the

2815
02:14:52,029 --> 02:14:55,118
amount of weight decay and if there's

2816
02:14:53,559 --> 02:14:56,920
very little variation we end up

2817
02:14:55,118 --> 02:15:00,609
increasing the amount of weight decay so

2818
02:14:56,920 --> 02:15:02,739
we end up basically saying penalize

2819
02:15:00,609 --> 02:15:03,069
parameters you know weights that are

2820
02:15:02,738 --> 02:15:06,609
really

2821
02:15:03,069 --> 02:15:09,579
hi unless their gradient varies a lot

2822
02:15:06,609 --> 02:15:12,579
which is never what we intended right

2823
02:15:09,579 --> 02:15:15,460
that's just not not the plan at all so

2824
02:15:12,579 --> 02:15:19,750
the trick with Adam W is we basically

2825
02:15:15,460 --> 02:15:22,000
remove weight decay from here so it's

2826
02:15:19,750 --> 02:15:25,449
not in the last function it's not in the

2827
02:15:22,000 --> 02:15:28,479
G not in the G squared and we move it so

2828
02:15:25,449 --> 02:15:31,510
that instead it's it's it's added

2829
02:15:28,479 --> 02:15:32,109
directly to the when we update with the

2830
02:15:31,510 --> 02:15:34,119
learning rate

2831
02:15:32,109 --> 02:15:36,399
it's out of there instead so in other

2832
02:15:34,119 --> 02:15:38,079
words it would be we would put the

2833
02:15:36,399 --> 02:15:40,299
weight decay or I should a gradient of

2834
02:15:38,079 --> 02:15:42,760
the weight decay in here when we

2835
02:15:40,300 --> 02:15:47,409
calculate the new a mu V so it never

2836
02:15:42,760 --> 02:15:50,619
ends up in our G M G squared so that was

2837
02:15:47,409 --> 02:15:51,970
like a super fast description which will

2838
02:15:50,619 --> 02:15:53,500
probably only make sense if you listen

2839
02:15:51,970 --> 02:15:57,340
to a three or four times on the video

2840
02:15:53,500 --> 02:15:59,729
and then talk about it on the forum yeah

2841
02:15:57,340 --> 02:16:01,960
but if you're interested let me know and

2842
02:15:59,729 --> 02:16:08,439
we can also look at Ann Ann's code

2843
02:16:01,960 --> 02:16:11,439
that's implemented yes and you know the

2844
02:16:08,439 --> 02:16:15,699
the idea of using weight decay is it's a

2845
02:16:11,439 --> 02:16:17,409
really helpful regularizer because it's

2846
02:16:15,699 --> 02:16:23,170
basically this way that we can kind of

2847
02:16:17,409 --> 02:16:26,949
stay like you know please don't increase

2848
02:16:23,170 --> 02:16:31,020
any of the weight values unless the you

2849
02:16:26,949 --> 02:16:34,510
know improvement in the loss is worth it

2850
02:16:31,020 --> 02:16:36,280
and so generally speaking pretty much

2851
02:16:34,510 --> 02:16:40,149
all state of the art models have both

2852
02:16:36,280 --> 02:16:42,640
dropout and weight decay and I don't

2853
02:16:40,149 --> 02:16:46,539
claim to know like how to set each one

2854
02:16:42,639 --> 02:16:51,189
and how much of H to use to say like you

2855
02:16:46,540 --> 02:16:53,200
it's worth trying both to go back to the

2856
02:16:51,190 --> 02:16:55,600
idea of embeddings is there any way to

2857
02:16:53,200 --> 02:16:57,460
interpret the final to reduce it

2858
02:16:55,600 --> 02:16:59,170
embeddings like absolutely we're gonna

2859
02:16:57,459 --> 02:17:01,869
look at that next week I've it's super

2860
02:16:59,170 --> 02:17:03,129
fun it turns out that you know we'll

2861
02:17:01,870 --> 02:17:05,551
learn what some of the worst movies of

2862
02:17:03,129 --> 02:17:05,550
all time

2863
02:17:06,620 --> 02:17:11,460
it's Letham it's that John Travolta

2864
02:17:09,180 --> 02:17:12,659
Scientology once my battleship earth or

2865
02:17:11,459 --> 02:17:14,250
something I think that was like the

2866
02:17:12,659 --> 02:17:21,420
worst movie of all time according to our

2867
02:17:14,250 --> 02:17:24,780
beds to many recommendations for scaling

2868
02:17:21,420 --> 02:17:27,030
the l2 penalty or is that kind of based

2869
02:17:24,780 --> 02:17:29,970
on how how wide the notes are how many

2870
02:17:27,030 --> 02:17:32,640
notes about III have no suggestion at

2871
02:17:29,969 --> 02:17:35,279
all like I I kind of look for like

2872
02:17:32,639 --> 02:17:37,289
papers or cackle competitions or

2873
02:17:35,280 --> 02:17:40,350
whatever similar and try to set up

2874
02:17:37,290 --> 02:17:43,110
frankly the same it seems like in a

2875
02:17:40,350 --> 02:17:46,079
particular area like computer vision

2876
02:17:43,110 --> 02:17:48,090
object recognition it's like somewhere

2877
02:17:46,079 --> 02:17:50,340
between one in neck four or one in egg

2878
02:17:48,090 --> 02:17:55,110
five seems to work you know

2879
02:17:50,340 --> 02:17:57,120
actually in the Adam W paper the authors

2880
02:17:55,110 --> 02:17:58,590
point out that with this new approach it

2881
02:17:57,120 --> 02:18:00,480
actually becomes like it seems to be

2882
02:17:58,590 --> 02:18:02,460
much more stable as to what the right

2883
02:18:00,479 --> 02:18:03,779
way to K amounts are so hopefully now

2884
02:18:02,459 --> 02:18:06,149
when we start playing with it

2885
02:18:03,780 --> 02:18:07,739
we'll be able to have some definitive

2886
02:18:06,149 --> 02:18:10,680
recommendations by the time we get to

2887
02:18:07,739 --> 02:18:16,020
part two all right well that's nine

2888
02:18:10,680 --> 02:18:17,280
o'clock so this week you know practice

2889
02:18:16,020 --> 02:18:19,680
the thing that you're least familiar

2890
02:18:17,280 --> 02:18:21,060
with so if it's like jacobians and

2891
02:18:19,680 --> 02:18:23,309
Hessians read about those if it's

2892
02:18:21,059 --> 02:18:25,319
broadcasting read about those if it's

2893
02:18:23,309 --> 02:18:26,969
understanding python ooo read about that

2894
02:18:25,319 --> 02:18:29,340
you know try to implement your own

2895
02:18:26,969 --> 02:18:32,219
custom layers read the faster higher

2896
02:18:29,340 --> 02:18:35,460
layers you know and and talk on the

2897
02:18:32,219 --> 02:18:39,439
forum about anything that you find weird

2898
02:18:35,459 --> 02:18:39,439
or confusing alright see you next week

