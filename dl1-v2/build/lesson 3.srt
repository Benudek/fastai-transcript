1
00:00:00,149 --> 00:00:03,600
welcome back everybody

2
00:00:01,870 --> 00:00:07,649
[Music]

3
00:00:03,600 --> 00:00:09,599
I'm sure you've noticed but there's been

4
00:00:07,649 --> 00:00:10,710
a lot of cool activity on the forum this

5
00:00:09,599 --> 00:00:12,780
week and one of the things that's been

6
00:00:10,710 --> 00:00:15,630
really great to see is that a lot of you

7
00:00:12,779 --> 00:00:18,239
have started creating really helpful

8
00:00:15,630 --> 00:00:20,010
materials both for your classmates to

9
00:00:18,239 --> 00:00:23,278
better understand stuff and also for you

10
00:00:20,010 --> 00:00:25,920
to better understand stuff by trying to

11
00:00:23,278 --> 00:00:29,550
teach what you've learned I just wanted

12
00:00:25,920 --> 00:00:32,340
to highlight a few i've actually posted

13
00:00:29,550 --> 00:00:36,780
to the wiki thread of a few of these but

14
00:00:32,340 --> 00:00:38,579
there's there's lots more Russian has

15
00:00:36,780 --> 00:00:41,549
posted a whole bunch of nice

16
00:00:38,579 --> 00:00:42,480
introductory tutorials so for example if

17
00:00:41,549 --> 00:00:46,078
you're having any trouble getting

18
00:00:42,479 --> 00:00:49,648
connected with AWS she's got a whole

19
00:00:46,079 --> 00:00:51,628
step-by-step how to go about logging in

20
00:00:49,649 --> 00:00:53,789
and getting everything working which i

21
00:00:51,628 --> 00:00:57,750
think is a really terrific thing and so

22
00:00:53,789 --> 00:00:59,429
it's a kind of thing that if you are

23
00:00:57,750 --> 00:01:01,289
writing some notes for yourself to

24
00:00:59,429 --> 00:01:02,939
remind you how to do it

25
00:01:01,289 --> 00:01:05,460
you may as well post them for others to

26
00:01:02,939 --> 00:01:07,438
do it as well and by using a markdown

27
00:01:05,459 --> 00:01:08,789
file like this and it's actually good

28
00:01:07,438 --> 00:01:10,548
practice if you haven't used github

29
00:01:08,790 --> 00:01:13,950
before if you put it up on github

30
00:01:10,549 --> 00:01:17,130
everybody can now use it or of course

31
00:01:13,950 --> 00:01:20,009
you can just put it in the forum so more

32
00:01:17,129 --> 00:01:21,929
advanced a thing that Reshma wrote up

33
00:01:20,009 --> 00:01:25,140
about is she noticed that I like using

34
00:01:21,930 --> 00:01:30,200
Tmax which is a handy little thing which

35
00:01:25,140 --> 00:01:33,719
lets me lets me basically have a window

36
00:01:30,200 --> 00:01:38,009
I'll show you so as soon as I log into

37
00:01:33,719 --> 00:01:39,868
my computer if I run Tmax you'll see

38
00:01:38,009 --> 00:01:42,150
that all of my windows pop straight up

39
00:01:39,868 --> 00:01:44,250
basically and I can like continue

40
00:01:42,150 --> 00:01:46,020
running stuff in the background and I

41
00:01:44,250 --> 00:01:48,209
can like I've got them over here and I

42
00:01:46,019 --> 00:01:50,609
can kind of zoom into it or I can move

43
00:01:48,209 --> 00:01:53,578
over to the top which is here so Jupiter

44
00:01:50,609 --> 00:01:54,359
colonel running and so forth so if that

45
00:01:53,578 --> 00:01:57,478
sounds interesting

46
00:01:54,359 --> 00:02:00,509
Reshma has a tutorial here on how you

47
00:01:57,478 --> 00:02:03,060
can use two maps and it's actually got a

48
00:02:00,509 --> 00:02:07,109
whole bunch of stuff in her github so

49
00:02:03,060 --> 00:02:10,860
that's that's really cool I built among

50
00:02:07,109 --> 00:02:13,688
has written a very nice kind of summary

51
00:02:10,860 --> 00:02:17,319
basically of our last lesson

52
00:02:13,688 --> 00:02:19,120
which kind of covers what are the key

53
00:02:17,318 --> 00:02:21,699
things we did and why did we do them so

54
00:02:19,120 --> 00:02:23,110
if you are a kind of

55
00:02:21,699 --> 00:02:25,959
wondering like how does it fit together

56
00:02:23,110 --> 00:02:28,810
I think this is a really helpful summary

57
00:02:25,959 --> 00:02:30,430
like what if those couple of hours look

58
00:02:28,810 --> 00:02:38,469
like if we summarize it all into a page

59
00:02:30,430 --> 00:02:40,750
or two I also really like Pavel has dad

60
00:02:38,469 --> 00:02:45,068
kind of done a deep dive on the learning

61
00:02:40,750 --> 00:02:46,330
rate finder which is a topic that a lot

62
00:02:45,068 --> 00:02:49,329
of you have been interested in learning

63
00:02:46,330 --> 00:02:50,469
more about particularly those of you who

64
00:02:49,330 --> 00:02:52,719
have done deep learning before I

65
00:02:50,469 --> 00:02:54,159
realized that this is like a solution to

66
00:02:52,719 --> 00:02:55,989
a problem that you've been having for a

67
00:02:54,159 --> 00:02:57,879
long time and haven't seen before and so

68
00:02:55,989 --> 00:02:59,500
it's kind of something which hasn't

69
00:02:57,879 --> 00:03:00,939
really been blogged about before so this

70
00:02:59,500 --> 00:03:04,079
is the first I've seen it's logged about

71
00:03:00,939 --> 00:03:06,519
so when I put this on Twitter a link to

72
00:03:04,079 --> 00:03:08,739
pebbles post it's been shared you know

73
00:03:06,519 --> 00:03:10,810
hundreds of times it's been really

74
00:03:08,739 --> 00:03:13,560
really popular and viewed many thousands

75
00:03:10,810 --> 00:03:16,629
of times so that's some great content

76
00:03:13,560 --> 00:03:18,579
radec has posted lots of cool stuff I

77
00:03:16,629 --> 00:03:21,189
really like this practitioners guide to

78
00:03:18,579 --> 00:03:23,079
apply torch which again this is more for

79
00:03:21,189 --> 00:03:25,419
more advanced students but it's like

80
00:03:23,079 --> 00:03:28,650
digging into people who have never used

81
00:03:25,419 --> 00:03:30,939
hi torch before but know a bit about

82
00:03:28,650 --> 00:03:32,260
numerical programming in general and

83
00:03:30,939 --> 00:03:35,560
it's a quick introduction to how high

84
00:03:32,259 --> 00:03:36,818
torch is different and then there's been

85
00:03:35,560 --> 00:03:38,709
some interesting little bits of research

86
00:03:36,818 --> 00:03:40,958
like what's the relationship between

87
00:03:38,709 --> 00:03:42,639
learning rate and batch sites so one of

88
00:03:40,959 --> 00:03:44,620
the students actually asked me this

89
00:03:42,639 --> 00:03:46,030
before class and I said oh well one of

90
00:03:44,620 --> 00:03:49,930
the other students has written an

91
00:03:46,030 --> 00:03:51,489
analysis of exactly that so what he's

92
00:03:49,930 --> 00:03:52,930
done is basically looked through and

93
00:03:51,489 --> 00:03:54,009
tried different batch sizes and

94
00:03:52,930 --> 00:03:55,389
different learning rates and tried to

95
00:03:54,009 --> 00:03:58,509
see how they seemed to relate together

96
00:03:55,389 --> 00:04:00,299
and these are all like cool experiments

97
00:03:58,509 --> 00:04:04,060
which you know you can try yourself

98
00:04:00,299 --> 00:04:05,829
I predict again he's written something

99
00:04:04,060 --> 00:04:10,810
again a kind of a research into this

100
00:04:05,829 --> 00:04:12,099
question I made a claim that the the

101
00:04:10,810 --> 00:04:14,769
stochastic gradient descent with

102
00:04:12,098 --> 00:04:16,478
restarts finds more generalizable parts

103
00:04:14,769 --> 00:04:18,340
of the function surface because they're

104
00:04:16,478 --> 00:04:19,598
kind of flatter and he's been trying to

105
00:04:18,339 --> 00:04:22,478
figure out is there a way to measure

106
00:04:19,598 --> 00:04:23,769
that more directly not quite successful

107
00:04:22,478 --> 00:04:27,788
yet but a really interesting piece of

108
00:04:23,769 --> 00:04:32,998
research got some introductions to

109
00:04:27,788 --> 00:04:34,199
convolutional neural networks and then

110
00:04:32,999 --> 00:04:36,389
something that we'll be learning about

111
00:04:34,199 --> 00:04:37,348
towards the end of this course but I'm

112
00:04:36,389 --> 00:04:40,499
sure you've noticed we're using

113
00:04:37,348 --> 00:04:43,188
something called ResNet and a nonce aha

114
00:04:40,499 --> 00:04:45,569
actually posted a pretty impressive

115
00:04:43,189 --> 00:04:47,399
analysis of like watts arrest net and

116
00:04:45,569 --> 00:04:49,169
why is it interesting and this one's

117
00:04:47,399 --> 00:04:50,639
actually being very already shared very

118
00:04:49,168 --> 00:04:54,359
widely around the internet I've seen

119
00:04:50,639 --> 00:04:57,059
also so some more advanced students who

120
00:04:54,360 --> 00:05:00,360
are interested in jumping ahead can look

121
00:04:57,059 --> 00:05:04,349
at that and uphill Tamang also has done

122
00:05:00,360 --> 00:05:06,059
something similar so lots of yeah lots

123
00:05:04,348 --> 00:05:07,918
of stuff going on on the forums

124
00:05:06,059 --> 00:05:11,069
I'm sure you've also noticed we have a

125
00:05:07,918 --> 00:05:15,928
beginner forum now specifically for you

126
00:05:11,069 --> 00:05:17,479
know asking questions which you know it

127
00:05:15,928 --> 00:05:19,859
is always the case that there are no

128
00:05:17,478 --> 00:05:21,928
dumb questions but when there's lots of

129
00:05:19,860 --> 00:05:23,369
people around you talking about advanced

130
00:05:21,928 --> 00:05:25,278
topics it might not feel that way so

131
00:05:23,369 --> 00:05:29,819
hopefully the beginners forum is just a

132
00:05:25,278 --> 00:05:32,098
less intimidating space and if there are

133
00:05:29,819 --> 00:05:34,079
more advanced student who can help

134
00:05:32,098 --> 00:05:35,128
answer those questions please do but

135
00:05:34,079 --> 00:05:37,800
remember when you do answer those

136
00:05:35,129 --> 00:05:39,419
questions try to answer in a way that's

137
00:05:37,800 --> 00:05:41,429
friendly to people that maybe you know

138
00:05:39,418 --> 00:05:42,988
have no more than a year of programming

139
00:05:41,428 --> 00:05:50,399
experience you haven't done any machine

140
00:05:42,988 --> 00:05:52,978
learning before so you know I hope other

141
00:05:50,399 --> 00:05:54,658
people in the class feel like you can

142
00:05:52,978 --> 00:05:56,158
contribute as well and just remember all

143
00:05:54,658 --> 00:05:58,978
of the people we just looked at or many

144
00:05:56,158 --> 00:06:00,959
of them I believe have never hosted

145
00:05:58,978 --> 00:06:02,428
anything to the internet before right I

146
00:06:00,959 --> 00:06:04,769
mean you don't have to be a particular

147
00:06:02,428 --> 00:06:07,078
kind of person to be allowed to block

148
00:06:04,769 --> 00:06:11,399
something you can just jot down your

149
00:06:07,079 --> 00:06:12,959
notes throw it up there and one handy

150
00:06:11,399 --> 00:06:15,778
thing is if you just put it on the forum

151
00:06:12,959 --> 00:06:18,569
and you're not quite sure of some of the

152
00:06:15,778 --> 00:06:20,399
details then then you know you have an

153
00:06:18,569 --> 00:06:22,049
opportunity to get feedback and say like

154
00:06:20,399 --> 00:06:23,579
oh well that's not quite how that works

155
00:06:22,048 --> 00:06:25,558
you know actually it works this way

156
00:06:23,579 --> 00:06:26,819
instead or oh that's a really

157
00:06:25,559 --> 00:06:29,029
interesting insight have you thought

158
00:06:26,819 --> 00:06:32,158
about taking this further and so forth

159
00:06:29,028 --> 00:06:34,918
so what we've done so far is a kind of

160
00:06:32,158 --> 00:06:38,968
an injury an introduction as a just as a

161
00:06:34,918 --> 00:06:41,308
practitioner to convolutional neural

162
00:06:38,968 --> 00:06:43,389
networks for images and we haven't

163
00:06:41,309 --> 00:06:46,330
really talked much at all about

164
00:06:43,389 --> 00:06:48,280
the theory or why they work or the math

165
00:06:46,329 --> 00:06:52,658
of them but on the other hand what we

166
00:06:48,279 --> 00:06:55,899
have done is seen how to build a model

167
00:06:52,658 --> 00:06:59,319
which actually works exceptionally well

168
00:06:55,899 --> 00:07:01,779
compact world-class level models and

169
00:06:59,319 --> 00:07:06,669
we'll kind of review a little bit of

170
00:07:01,779 --> 00:07:08,288
that today and then also today we're

171
00:07:06,668 --> 00:07:09,818
going to dig in a little quite a lot

172
00:07:08,288 --> 00:07:12,279
more actually into the underlying theory

173
00:07:09,819 --> 00:07:14,650
of like what is a what is a CNN what's a

174
00:07:12,279 --> 00:07:16,000
convolution how does this work and then

175
00:07:14,649 --> 00:07:18,448
we're going to kind of go through this

176
00:07:16,000 --> 00:07:21,038
this cycle where we're going to dig

177
00:07:18,449 --> 00:07:23,379
we're going to do a little intro into a

178
00:07:21,038 --> 00:07:25,930
whole bunch of application areas using

179
00:07:23,379 --> 00:07:29,560
neural nets for structured data so kind

180
00:07:25,930 --> 00:07:31,449
of like logistics or forecasting or you

181
00:07:29,560 --> 00:07:34,360
know financial data or that kind of

182
00:07:31,449 --> 00:07:36,669
thing and then looking at language

183
00:07:34,360 --> 00:07:39,210
applications and LP applications using

184
00:07:36,668 --> 00:07:42,688
recurrent neural Nets and then

185
00:07:39,209 --> 00:07:45,549
collaborative filtering for

186
00:07:42,689 --> 00:07:47,740
recommendations and systems and so these

187
00:07:45,550 --> 00:07:50,199
will all be like similar to what we've

188
00:07:47,740 --> 00:07:51,009
done for cnn's for images would be like

189
00:07:50,199 --> 00:07:53,020
here's how you can get a

190
00:07:51,009 --> 00:07:55,150
state-of-the-art result without digging

191
00:07:53,019 --> 00:07:57,848
into the theory but but knowing how to

192
00:07:55,149 --> 00:08:00,399
actually make a work and then we're kind

193
00:07:57,848 --> 00:08:01,838
of going to go back through those almost

194
00:08:00,399 --> 00:08:03,668
in reverse order so then we're going to

195
00:08:01,838 --> 00:08:06,250
dig right into collaborative filtering

196
00:08:03,668 --> 00:08:08,408
in a lot of detail and see how how to

197
00:08:06,250 --> 00:08:10,478
write the code underneath and how the

198
00:08:08,408 --> 00:08:11,588
math works underneath and then we're

199
00:08:10,478 --> 00:08:13,718
going to do the same thing for the

200
00:08:11,588 --> 00:08:15,399
structured data analysis we're going to

201
00:08:13,718 --> 00:08:18,069
do the same thing for comp nets for

202
00:08:15,399 --> 00:08:20,620
images and finally an in depth deep dive

203
00:08:18,069 --> 00:08:24,610
into apparent neural networks so that's

204
00:08:20,620 --> 00:08:29,019
kind of where we're okay so let's start

205
00:08:24,610 --> 00:08:32,620
by doing a little bit of a review and I

206
00:08:29,019 --> 00:08:35,049
want to also provide a bit more detail

207
00:08:32,620 --> 00:08:36,940
on some on some steps that we only

208
00:08:35,049 --> 00:08:39,208
briefly slipped over so I want to make

209
00:08:36,940 --> 00:08:42,039
sure that we're all able to complete

210
00:08:39,208 --> 00:08:45,309
kind of last week's assignment which was

211
00:08:42,038 --> 00:08:47,049
that the dog breeze I mean to basically

212
00:08:45,309 --> 00:08:48,789
apply what you've learned with another

213
00:08:47,049 --> 00:08:50,319
data set and I thought the easiest one

214
00:08:48,789 --> 00:08:52,088
to do with me the dog breeds cattle

215
00:08:50,320 --> 00:08:53,709
competition and so I want to make sure

216
00:08:52,089 --> 00:08:55,889
everybody has everything you need to do

217
00:08:53,708 --> 00:08:57,268
this right now so and the

218
00:08:55,889 --> 00:09:01,230
first thing is to make sure that you

219
00:08:57,269 --> 00:09:03,058
know how to download data and so there's

220
00:09:01,230 --> 00:09:04,798
there's two main places at the moment

221
00:09:03,058 --> 00:09:07,438
we're kind of downloading data from one

222
00:09:04,798 --> 00:09:11,338
is from cattle and the other is from

223
00:09:07,438 --> 00:09:15,298
like anywhere else and so I'll first of

224
00:09:11,339 --> 00:09:18,569
all do the the casual version so to

225
00:09:15,298 --> 00:09:24,808
download from cattle we use something

226
00:09:18,568 --> 00:09:28,378
called cattle CLI which is gear and to

227
00:09:24,808 --> 00:09:34,980
install what I think it's already in the

228
00:09:28,379 --> 00:09:39,149
system will shake yeah so it should

229
00:09:34,980 --> 00:09:41,039
already be in your environment but to

230
00:09:39,149 --> 00:09:42,448
make sure one thing that happens is

231
00:09:41,039 --> 00:09:44,368
because this is downloading from the

232
00:09:42,448 --> 00:09:45,539
cattle website through experience rating

233
00:09:44,369 --> 00:09:48,149
every time cap will change us the

234
00:09:45,539 --> 00:09:51,568
website it breaks so anytime you try to

235
00:09:48,149 --> 00:09:53,278
use it and if the cattles websites

236
00:09:51,568 --> 00:09:55,708
changed recent when you'll need to make

237
00:09:53,278 --> 00:10:00,379
sure you get the most recent version so

238
00:09:55,708 --> 00:10:06,418
you can always go to pip install cable -

239
00:10:00,379 --> 00:10:07,829
CL I - - upgrade and so that'll just

240
00:10:06,418 --> 00:10:10,289
make sure that you've got the latest

241
00:10:07,828 --> 00:10:14,008
version of of it and everything that it

242
00:10:10,289 --> 00:10:16,678
depends on okay and so then having done

243
00:10:14,009 --> 00:10:18,749
that you can follow the instructions

244
00:10:16,678 --> 00:10:21,720
actually I think Reshma was kind enough

245
00:10:18,749 --> 00:10:22,918
to they go there's a cable CLI you know

246
00:10:21,720 --> 00:10:29,910
like everything you need to know can be

247
00:10:22,918 --> 00:10:35,088
under Reshma's github so basically to do

248
00:10:29,909 --> 00:10:37,678
that at the next step you go kg download

249
00:10:35,089 --> 00:10:41,100
and then you provide your username with

250
00:10:37,678 --> 00:10:44,399
- you you provide your password with - P

251
00:10:41,100 --> 00:10:46,019
and then - see it the competition name

252
00:10:44,399 --> 00:10:47,730
and a lot of people in the forum is

253
00:10:46,019 --> 00:10:49,528
being confused about what to enter here

254
00:10:47,730 --> 00:10:51,438
and so the key thing to note is that

255
00:10:49,528 --> 00:10:55,558
when you're at a capital competition

256
00:10:51,438 --> 00:10:58,168
after the /c there's a specific name

257
00:10:55,558 --> 00:11:02,100
planet - understanding - etcetera right

258
00:10:58,168 --> 00:11:03,749
that's the name you need okay the other

259
00:11:02,100 --> 00:11:06,808
thing you'll need to make sure is that

260
00:11:03,749 --> 00:11:08,489
you've on your own computer have

261
00:11:06,808 --> 00:11:09,669
attempted to click download at least

262
00:11:08,489 --> 00:11:12,910
once because when you do

263
00:11:09,669 --> 00:11:15,729
ask you to accept the rules if you've

264
00:11:12,909 --> 00:11:17,169
forgotten to do that kg download will

265
00:11:15,730 --> 00:11:18,850
give you a hint it'll say oh it looks

266
00:11:17,169 --> 00:11:21,699
like you might have forgotten the rules

267
00:11:18,850 --> 00:11:24,009
if you log into cattle with like a

268
00:11:21,700 --> 00:11:26,800
Google account like anything other than

269
00:11:24,009 --> 00:11:28,720
a username password this won't work so

270
00:11:26,799 --> 00:11:30,729
you'll need to click forgot password on

271
00:11:28,720 --> 00:11:33,550
Kaggle and get them to send you a normal

272
00:11:30,730 --> 00:11:36,220
password so that's the cattle version

273
00:11:33,549 --> 00:11:38,409
right and so when you do that you end up

274
00:11:36,220 --> 00:11:41,430
with a whole folder created for you with

275
00:11:38,409 --> 00:11:44,079
all of that competition and data in it

276
00:11:41,429 --> 00:11:46,479
so a couple of reasons you might want to

277
00:11:44,080 --> 00:11:48,220
not use that the first years that you're

278
00:11:46,480 --> 00:11:50,320
using a data set that's not on cattle

279
00:11:48,220 --> 00:11:52,600
the second is that you don't want all of

280
00:11:50,320 --> 00:11:55,330
the data sets in a cattle competition

281
00:11:52,600 --> 00:11:56,800
for example the planet competition that

282
00:11:55,330 --> 00:12:00,009
we've been looking at a little bit we'll

283
00:11:56,799 --> 00:12:03,609
look at again today has data in two

284
00:12:00,009 --> 00:12:06,370
formats TIFF and JPEG the TIFF is 19

285
00:12:03,610 --> 00:12:08,379
gigabytes and the JPEG is 600 megabytes

286
00:12:06,370 --> 00:12:11,889
so you probably don't want to download

287
00:12:08,379 --> 00:12:13,509
both so I'll show you a really cool kit

288
00:12:11,889 --> 00:12:14,919
which actually somebody on the forum

289
00:12:13,509 --> 00:12:18,899
taught me I think was one of the young

290
00:12:14,919 --> 00:12:24,159
MSN students here at USF there's a

291
00:12:18,899 --> 00:12:27,519
Chrome extension cord curl W get so you

292
00:12:24,159 --> 00:12:29,589
can just search for a curl W get and

293
00:12:27,519 --> 00:12:31,419
then you install it by just clicking on

294
00:12:29,590 --> 00:12:35,470
installed and having an extension before

295
00:12:31,419 --> 00:12:36,969
and then from now on every time you try

296
00:12:35,470 --> 00:12:39,840
to download something so I'll try and

297
00:12:36,970 --> 00:12:39,840
download this file

298
00:12:40,480 --> 00:12:45,220
and I'll just go ahead and cancel it

299
00:12:42,698 --> 00:12:47,620
right and now you see this little yellow

300
00:12:45,220 --> 00:12:51,310
button that's added up here there's a

301
00:12:47,620 --> 00:12:59,948
whole command here right so I can copy

302
00:12:51,309 --> 00:13:03,669
that and paste it into my window and hit

303
00:12:59,948 --> 00:13:06,039
go and it's there cuz okay so what that

304
00:13:03,669 --> 00:13:08,139
does is like all of your cookies and

305
00:13:06,039 --> 00:13:10,448
headers and everything else needed to

306
00:13:08,139 --> 00:13:13,959
download that file is like say so this

307
00:13:10,448 --> 00:13:16,028
is not just useful for downloading data

308
00:13:13,958 --> 00:13:18,129
it's also useful if you like trying to

309
00:13:16,028 --> 00:13:19,419
download some I don't know TV show or

310
00:13:18,129 --> 00:13:23,439
something anything where you're it's

311
00:13:19,419 --> 00:13:25,120
hidden behind a login or something you

312
00:13:23,440 --> 00:13:26,800
can you can grab it and actually that is

313
00:13:25,120 --> 00:13:28,240
very useful for data science because

314
00:13:26,799 --> 00:13:31,629
quite often we want to analyze things

315
00:13:28,240 --> 00:13:33,610
like videos on our on our consoles so

316
00:13:31,629 --> 00:13:40,028
this is a good trick alright so there's

317
00:13:33,610 --> 00:13:44,019
two ways to get the data so then having

318
00:13:40,028 --> 00:13:47,409
got the data you then need to build your

319
00:13:44,019 --> 00:13:49,209
model right so what I tend to do like

320
00:13:47,409 --> 00:13:51,879
you'll notice that I tend to assume that

321
00:13:49,208 --> 00:13:54,250
the data is in a directory called data

322
00:13:51,879 --> 00:13:57,669
that's a subdirectory of wherever your

323
00:13:54,250 --> 00:14:00,399
notebook is right now you don't

324
00:13:57,669 --> 00:14:01,599
necessarily actually want to put your

325
00:14:00,399 --> 00:14:03,639
data there you might want to put it

326
00:14:01,600 --> 00:14:04,990
directly in your home directory or you

327
00:14:03,639 --> 00:14:07,959
might wanna put it on another drive or

328
00:14:04,990 --> 00:14:11,620
whatever so what I do is if you look

329
00:14:07,958 --> 00:14:14,379
inside my courses do one folder you'll

330
00:14:11,620 --> 00:14:17,948
see that data is actually a symbolic

331
00:14:14,379 --> 00:14:19,600
link to a different drive alright so you

332
00:14:17,948 --> 00:14:22,269
can put it anywhere you like and then

333
00:14:19,600 --> 00:14:24,430
you can just add a symbolic link or you

334
00:14:22,269 --> 00:14:26,049
can just put it there directly it's up

335
00:14:24,429 --> 00:14:28,989
to you if you haven't used symlinks

336
00:14:26,049 --> 00:14:32,019
before they're like aliases or shortcuts

337
00:14:28,990 --> 00:14:33,759
on the mac or windows very handy and

338
00:14:32,019 --> 00:14:35,828
there's some threads on the forum about

339
00:14:33,759 --> 00:14:38,528
how to use them if you want help with

340
00:14:35,828 --> 00:14:41,289
that that for example is also how we

341
00:14:38,528 --> 00:14:43,659
actually have the fast AI modules

342
00:14:41,289 --> 00:14:45,490
available from the same place as our

343
00:14:43,659 --> 00:14:48,698
notebooks it's just a symlink

344
00:14:45,490 --> 00:14:51,789
to where they come from anytime you want

345
00:14:48,698 --> 00:14:54,250
to see like where things actually point

346
00:14:51,789 --> 00:14:57,490
to in Linux you can just use the

347
00:14:54,250 --> 00:14:59,490
- L flag - listing a directory and

348
00:14:57,490 --> 00:15:01,450
that'll show you where the symlinks

349
00:14:59,490 --> 00:15:05,169
exist still lost I'll show you which

350
00:15:01,450 --> 00:15:12,340
scenes the directories so forth okay so

351
00:15:05,169 --> 00:15:14,740
one thing which may be a little unclear

352
00:15:12,340 --> 00:15:17,860
based on what we've done so far is like

353
00:15:14,740 --> 00:15:20,409
how little code you actually need to do

354
00:15:17,860 --> 00:15:22,870
this end to it so what I've got here is

355
00:15:20,409 --> 00:15:25,269
is in a single window is an entire

356
00:15:22,870 --> 00:15:27,220
end-to-end process to get a

357
00:15:25,269 --> 00:15:30,309
state-of-the-art result put cats versus

358
00:15:27,220 --> 00:15:31,810
dogs all right I've only step I've

359
00:15:30,309 --> 00:15:33,969
skipped is the bit where we've

360
00:15:31,809 --> 00:15:37,899
downloaded it from title and then where

361
00:15:33,970 --> 00:15:43,180
we unzip it all right so these are

362
00:15:37,899 --> 00:15:45,730
literally all the steps and so we import

363
00:15:43,179 --> 00:15:47,889
our libraries and actually if you import

364
00:15:45,730 --> 00:15:51,129
this one Kampf loner that basically

365
00:15:47,889 --> 00:15:53,019
imports everything else so that's that

366
00:15:51,129 --> 00:15:55,629
we need to tell at the path of where

367
00:15:53,019 --> 00:15:59,230
things are the size that we want the

368
00:15:55,629 --> 00:16:00,189
batch size that we want right so then

369
00:15:59,230 --> 00:16:02,500
and we're going to learn a lot more

370
00:16:00,190 --> 00:16:04,000
about what these do very shortly but

371
00:16:02,500 --> 00:16:06,669
basically we say how do we want to

372
00:16:04,000 --> 00:16:08,529
transform our data so we want to

373
00:16:06,669 --> 00:16:10,779
transform it in a way that's suitable to

374
00:16:08,529 --> 00:16:13,449
this particular kind of model and it

375
00:16:10,779 --> 00:16:15,250
assumes that the photos aside on photos

376
00:16:13,450 --> 00:16:18,490
and that we're going to zoom in up to

377
00:16:15,250 --> 00:16:21,490
ten percent each time we say that we

378
00:16:18,490 --> 00:16:23,019
want to get some data based on paths and

379
00:16:21,490 --> 00:16:24,610
so remember this is this idea that

380
00:16:23,019 --> 00:16:26,860
there's a path called cats and the path

381
00:16:24,610 --> 00:16:28,470
called dogs and they're inside a path

382
00:16:26,860 --> 00:16:34,360
called train and a path called valid

383
00:16:28,470 --> 00:16:36,279
note that you can always overwrite these

384
00:16:34,360 --> 00:16:37,360
with other things so if your things are

385
00:16:36,279 --> 00:16:39,850
in different folders

386
00:16:37,360 --> 00:16:42,159
you could either rename them or you can

387
00:16:39,850 --> 00:16:44,620
see here there's like a train name and a

388
00:16:42,159 --> 00:16:48,279
bowel name you can always pick something

389
00:16:44,620 --> 00:16:51,070
else here also notice there's a test

390
00:16:48,279 --> 00:16:52,838
name so if you want to submit some into

391
00:16:51,070 --> 00:16:54,430
cattle you'll need to fill in the name

392
00:16:52,839 --> 00:16:56,380
the name of the folder where the test

393
00:16:54,429 --> 00:16:58,889
sentence and obviously those those won't

394
00:16:56,379 --> 00:16:58,889
be labeled

395
00:16:59,769 --> 00:17:04,910
so then we create a model from a

396
00:17:02,750 --> 00:17:08,150
pre-training model it's from a resonant

397
00:17:04,910 --> 00:17:11,600
50 model using this data and then we

398
00:17:08,150 --> 00:17:13,640
call fit and remember by default that

399
00:17:11,599 --> 00:17:15,559
has all of the layers but the last few

400
00:17:13,640 --> 00:17:17,420
frozen and again we'll learn a lot more

401
00:17:15,559 --> 00:17:19,369
about what that means

402
00:17:17,420 --> 00:17:23,110
and so that's that's what that does so

403
00:17:19,369 --> 00:17:26,119
that that took two and a half minutes

404
00:17:23,109 --> 00:17:27,889
notice here I didn't say pre-compute

405
00:17:26,119 --> 00:17:29,509
equals true again there's been some

406
00:17:27,890 --> 00:17:33,320
confusion on the forums about like what

407
00:17:29,509 --> 00:17:35,390
that means it's only a is only something

408
00:17:33,319 --> 00:17:37,490
that makes it a little faster for this

409
00:17:35,390 --> 00:17:39,710
first step right so you can always skip

410
00:17:37,490 --> 00:17:41,480
it and if you're at all confused about

411
00:17:39,710 --> 00:17:45,039
it or it's causing you any problems just

412
00:17:41,480 --> 00:17:48,950
leave it off right because it's just a

413
00:17:45,039 --> 00:17:50,659
it's just a shortcut which caches some

414
00:17:48,950 --> 00:17:52,880
of the intermediate steps that don't

415
00:17:50,660 --> 00:17:54,710
have to be recalculating each time and

416
00:17:52,880 --> 00:17:56,930
remember that when we are using pre

417
00:17:54,710 --> 00:17:59,390
computed activations data or

418
00:17:56,930 --> 00:18:01,460
augmentation doesn't work right so even

419
00:17:59,390 --> 00:18:04,070
if you ask for a data augmentation if

420
00:18:01,460 --> 00:18:05,090
you've got pre compute equals true it

421
00:18:04,069 --> 00:18:06,710
doesn't actually do any data

422
00:18:05,089 --> 00:18:11,179
augmentation because it's using the

423
00:18:06,710 --> 00:18:12,799
cached non augmented activations so in

424
00:18:11,180 --> 00:18:14,779
this case to keep this as simple as

425
00:18:12,799 --> 00:18:17,869
possible I have no pre computed anything

426
00:18:14,779 --> 00:18:24,109
going on so I do three cycles of length

427
00:18:17,869 --> 00:18:25,869
one and then I can then unfreeze so it's

428
00:18:24,109 --> 00:18:27,769
now going to train the whole thing

429
00:18:25,869 --> 00:18:30,069
something we haven't seen before and

430
00:18:27,769 --> 00:18:33,259
we'll learn about in the second half is

431
00:18:30,069 --> 00:18:36,049
called B and freeze for now all you need

432
00:18:33,259 --> 00:18:39,170
to know is that if you're using a model

433
00:18:36,049 --> 00:18:42,829
like a bigger deeper model like ResNet

434
00:18:39,170 --> 00:18:46,250
50 or rez next 101 on a data set that's

435
00:18:42,829 --> 00:18:48,259
very very similar to imagenet like these

436
00:18:46,250 --> 00:18:51,859
cat sandbox data set sort of words it's

437
00:18:48,259 --> 00:18:54,589
like sidon photos of standard objects

438
00:18:51,859 --> 00:18:56,719
you know of a similar size to image turn

439
00:18:54,589 --> 00:18:57,970
and money somewhere between 200 and 500

440
00:18:56,720 --> 00:19:01,400
pixels

441
00:18:57,970 --> 00:19:03,380
you should probably add this line when

442
00:19:01,400 --> 00:19:06,080
you unfreeze for those of you that are

443
00:19:03,380 --> 00:19:09,290
more advanced what it's doing is it's

444
00:19:06,079 --> 00:19:10,839
causing the batch normalization moving

445
00:19:09,289 --> 00:19:12,069
averages to not be updated

446
00:19:10,839 --> 00:19:14,199
but in the second half of this course

447
00:19:12,069 --> 00:19:15,609
we're gonna learn all about why we do

448
00:19:14,200 --> 00:19:18,129
that it's something that's not supported

449
00:19:15,609 --> 00:19:20,408
by any other library but it turns out to

450
00:19:18,128 --> 00:19:21,189
be super important anyway so we do one

451
00:19:20,409 --> 00:19:26,039
more

452
00:19:21,190 --> 00:19:28,808
epoch with training the whole network

453
00:19:26,038 --> 00:19:31,990
and then at the end we use test time

454
00:19:28,808 --> 00:19:34,869
augmentation to ensure that we get the

455
00:19:31,990 --> 00:19:37,620
best predictions we can and that gives

456
00:19:34,869 --> 00:19:41,048
us ninety nine point four five percent

457
00:19:37,619 --> 00:19:44,648
so that's that's it right so when you

458
00:19:41,048 --> 00:19:47,138
try a new data set they're basically the

459
00:19:44,648 --> 00:19:49,808
minimum set of steps that you would need

460
00:19:47,138 --> 00:19:51,819
to follow you'll notice this is assuming

461
00:19:49,808 --> 00:19:53,408
I already know what learning wrote to

462
00:19:51,819 --> 00:19:56,019
use so you'd use the learning rate

463
00:19:53,409 --> 00:19:59,590
finder for that it's assuming that I

464
00:19:56,019 --> 00:20:02,798
know that the directory layout and so

465
00:19:59,589 --> 00:20:04,599
forth so that's kind of a minimum set

466
00:20:02,798 --> 00:20:07,210
now one of the things that I wanted to

467
00:20:04,599 --> 00:20:10,058
make sure you had an understanding of

468
00:20:07,210 --> 00:20:12,999
how to do is how to use other libraries

469
00:20:10,058 --> 00:20:15,038
other than fast AI and so I feel like

470
00:20:12,999 --> 00:20:18,100
the best thing to to look at is to look

471
00:20:15,038 --> 00:20:20,589
at care us because care us is a library

472
00:20:18,099 --> 00:20:23,589
just like fast AI sits on top of pi

473
00:20:20,589 --> 00:20:25,449
torch care us sits on top of actually a

474
00:20:23,589 --> 00:20:28,028
whole variety of different backends it

475
00:20:25,450 --> 00:20:28,769
fits mainly people nowadays use it with

476
00:20:28,028 --> 00:20:32,259
tensorflow

477
00:20:28,769 --> 00:20:37,450
there's also an MX net version there's

478
00:20:32,259 --> 00:20:39,788
also a Microsoft CNT K version so what

479
00:20:37,450 --> 00:20:43,480
I've got if you do a git pull you'll see

480
00:20:39,788 --> 00:20:46,058
that there's a something called care us

481
00:20:43,480 --> 00:20:48,429
less than one where I've attempted to

482
00:20:46,058 --> 00:20:52,089
replicate at least parts of lesson one

483
00:20:48,429 --> 00:20:59,110
in care us just to give you a sense of

484
00:20:52,089 --> 00:21:00,849
how that works I'm not going to talk

485
00:20:59,109 --> 00:21:04,138
more about batch two norm freeze now

486
00:21:00,849 --> 00:21:07,658
other than to say if you're using

487
00:21:04,138 --> 00:21:09,038
something which has got a number larger

488
00:21:07,659 --> 00:21:11,950
than 34 at the end

489
00:21:09,038 --> 00:21:16,089
so like ResNet 50 or res next 101 and

490
00:21:11,950 --> 00:21:18,129
you're trading a data set that has that

491
00:21:16,089 --> 00:21:20,528
is very similar to image net so it's

492
00:21:18,128 --> 00:21:22,480
like normal photos of normal sizes where

493
00:21:20,528 --> 00:21:24,519
the thing of interest takes up most of

494
00:21:22,480 --> 00:21:27,880
the frame then you probably should

495
00:21:24,519 --> 00:21:30,400
at the end fries true after unfreeze

496
00:21:27,880 --> 00:21:33,580
if in doubt try trading it with and then

497
00:21:30,400 --> 00:21:35,679
try trading it without more advanced

498
00:21:33,579 --> 00:21:37,419
students will certainly talk about it on

499
00:21:35,679 --> 00:21:39,910
the forums this week and we will be

500
00:21:37,420 --> 00:21:41,710
talking about the details of it in the

501
00:21:39,910 --> 00:21:46,720
second half of the course when we come

502
00:21:41,710 --> 00:21:55,660
back to our CNN in death section in the

503
00:21:46,720 --> 00:22:01,480
second last lesson so with care us again

504
00:21:55,660 --> 00:22:03,130
we import a bunch of stuff and remember

505
00:22:01,480 --> 00:22:04,630
I mentioned that this idea that you've

506
00:22:03,130 --> 00:22:06,190
got a thing called train and a thing

507
00:22:04,630 --> 00:22:07,540
called valid and inside that you've got

508
00:22:06,190 --> 00:22:10,679
a thing called dogs and things called

509
00:22:07,539 --> 00:22:14,678
cats is a standard way of providing

510
00:22:10,679 --> 00:22:16,870
image labelled images so Karis does that

511
00:22:14,679 --> 00:22:18,610
too right so it's going to tell it where

512
00:22:16,869 --> 00:22:24,308
the training set and the validation set

513
00:22:18,609 --> 00:22:27,099
are twice what batch size to used now

514
00:22:24,308 --> 00:22:32,019
you'll notice in Karis we need much much

515
00:22:27,099 --> 00:22:34,599
much more code to do the same thing more

516
00:22:32,019 --> 00:22:36,460
importantly each part of that code has

517
00:22:34,599 --> 00:22:38,649
many many many more things you have to

518
00:22:36,460 --> 00:22:43,840
set and if you set them wrong everything

519
00:22:38,650 --> 00:22:44,650
breaks right so I'll give you a summary

520
00:22:43,839 --> 00:22:47,049
of what they are

521
00:22:44,650 --> 00:22:51,250
so you're basically rather than creating

522
00:22:47,049 --> 00:22:52,659
a single data object in chaos we first

523
00:22:51,250 --> 00:22:55,029
of all have to define something called a

524
00:22:52,660 --> 00:22:57,550
data generator to say kind of generate

525
00:22:55,029 --> 00:23:00,009
the data and so a data generator we

526
00:22:57,549 --> 00:23:04,779
basically have to say what kind of data

527
00:23:00,009 --> 00:23:08,190
augmentation we want to do and we also

528
00:23:04,779 --> 00:23:11,170
we actually have to say what kind of

529
00:23:08,190 --> 00:23:13,179
normalization do we want to do

530
00:23:11,170 --> 00:23:16,240
so we're else with fast AI we just say

531
00:23:13,179 --> 00:23:18,160
whatever ResNet 50 requires just do that

532
00:23:16,240 --> 00:23:19,929
for me please we actually have to kind

533
00:23:18,160 --> 00:23:21,700
of know a little bit about what's

534
00:23:19,929 --> 00:23:23,320
expected of us

535
00:23:21,700 --> 00:23:25,299
generally speaking copying and pasting

536
00:23:23,319 --> 00:23:27,159
cos code from the internet is a good way

537
00:23:25,299 --> 00:23:30,220
to make sure you've got the right the

538
00:23:27,160 --> 00:23:32,110
right stuff to make that work and again

539
00:23:30,220 --> 00:23:34,600
it doesn't have a kind of a standard set

540
00:23:32,109 --> 00:23:37,279
of like here the best data augmentation

541
00:23:34,599 --> 00:23:39,019
parameters to use for photos so you know

542
00:23:37,279 --> 00:23:43,549
I've copied and pasted all of this from

543
00:23:39,019 --> 00:23:45,319
the Kaos documentation so I don't know

544
00:23:43,549 --> 00:23:46,909
if it's I don't think it's the best set

545
00:23:45,319 --> 00:23:49,220
to use it all but it's the set that

546
00:23:46,910 --> 00:23:51,470
they're using in their docks so having

547
00:23:49,220 --> 00:23:53,839
said this is how I want to generate data

548
00:23:51,470 --> 00:23:56,539
so horizontally fit sometimes you know

549
00:23:53,839 --> 00:23:59,359
zoom sometimes sheer sometimes we then

550
00:23:56,539 --> 00:24:01,879
create a generator from that by taking

551
00:23:59,359 --> 00:24:04,759
that data generator and saying I want to

552
00:24:01,880 --> 00:24:07,070
generate images by looking from a

553
00:24:04,759 --> 00:24:09,440
directory we pass in the directory which

554
00:24:07,069 --> 00:24:12,470
is of the same directory structure that

555
00:24:09,440 --> 00:24:14,090
fast AI users and you'll see there's

556
00:24:12,470 --> 00:24:16,220
some overlaps with kind of how fast AI

557
00:24:14,089 --> 00:24:18,289
works here you tell it what size images

558
00:24:16,220 --> 00:24:19,850
you want to create you tell at what

559
00:24:18,289 --> 00:24:22,639
batch size you want in your mini batches

560
00:24:19,849 --> 00:24:24,139
and then there's something you not to

561
00:24:22,640 --> 00:24:26,390
worry about too much but basically if

562
00:24:24,140 --> 00:24:28,759
you're just got two possible outcomes

563
00:24:26,390 --> 00:24:29,960
you would generally say binary here if

564
00:24:28,759 --> 00:24:32,390
you've got multiple possible outcomes

565
00:24:29,960 --> 00:24:33,440
would say categorical yeah so we've only

566
00:24:32,390 --> 00:24:37,970
got cats or dogs

567
00:24:33,440 --> 00:24:39,710
so it's binary so an example of like

568
00:24:37,970 --> 00:24:41,509
where things get a little more complex

569
00:24:39,710 --> 00:24:43,579
is you have to do the same thing for the

570
00:24:41,509 --> 00:24:45,619
validation set so it's up to you to

571
00:24:43,579 --> 00:24:47,569
create a data generator that doesn't

572
00:24:45,619 --> 00:24:49,279
have data augmentation because obviously

573
00:24:47,569 --> 00:24:51,889
for the validation set unless you're

574
00:24:49,279 --> 00:24:57,440
using t/ta that's going to start things

575
00:24:51,890 --> 00:24:59,090
up you also when you train you randomly

576
00:24:57,440 --> 00:25:01,190
reorder the images so that they're

577
00:24:59,089 --> 00:25:03,169
always shown in different orders to make

578
00:25:01,190 --> 00:25:06,080
it more random but with a validation

579
00:25:03,170 --> 00:25:07,519
it's vital that you don't do that

580
00:25:06,079 --> 00:25:09,769
because if you shuffle the validation

581
00:25:07,519 --> 00:25:11,269
set you then can't track how well you're

582
00:25:09,769 --> 00:25:11,809
doing it's in a different order for the

583
00:25:11,269 --> 00:25:14,869
labels

584
00:25:11,809 --> 00:25:17,179
that's a basically these are the kind of

585
00:25:14,869 --> 00:25:22,159
steps you have to do every time with

586
00:25:17,180 --> 00:25:24,320
care us so again the reason I was using

587
00:25:22,160 --> 00:25:26,960
ResNet 50 before is chaos doesn't have

588
00:25:24,319 --> 00:25:28,369
ResNet 34 unfortunately so I just wanted

589
00:25:26,960 --> 00:25:32,750
to compare like with Mike so we're going

590
00:25:28,369 --> 00:25:34,939
to use resident 50 here there isn't the

591
00:25:32,750 --> 00:25:37,519
same idea with chaos of saying like

592
00:25:34,940 --> 00:25:39,860
constructor model that is suitable for

593
00:25:37,519 --> 00:25:42,289
this data set for me so you have to do

594
00:25:39,859 --> 00:25:44,359
it by hand right so the way you do it is

595
00:25:42,289 --> 00:25:47,329
to basically say this is my base model

596
00:25:44,359 --> 00:25:50,000
and then you have to construct on top of

597
00:25:47,329 --> 00:25:50,599
that manually the layers that you want

598
00:25:50,000 --> 00:25:52,220
to add

599
00:25:50,599 --> 00:25:53,808
and so by the end of this course you'll

600
00:25:52,220 --> 00:25:56,630
understand a way it is that these

601
00:25:53,808 --> 00:26:01,369
particular three layers other layers

602
00:25:56,630 --> 00:26:03,230
that we add so having done that in chaos

603
00:26:01,369 --> 00:26:06,069
you basically say okay this is my model

604
00:26:03,230 --> 00:26:08,450
and then again there isn't like a

605
00:26:06,069 --> 00:26:10,879
concept to it like automatically

606
00:26:08,450 --> 00:26:13,279
freezing things or an API for that so

607
00:26:10,880 --> 00:26:16,190
you just have to allow look through the

608
00:26:13,279 --> 00:26:20,629
layers that you want to freeze and call

609
00:26:16,190 --> 00:26:22,940
dot trainable equals false on them in

610
00:26:20,630 --> 00:26:25,370
Karis there's a concept we don't have in

611
00:26:22,940 --> 00:26:27,110
fast AI or play a torch of compiling a

612
00:26:25,369 --> 00:26:28,959
model so basically once your model is

613
00:26:27,109 --> 00:26:32,000
ready to use you have to compile it

614
00:26:28,960 --> 00:26:33,860
passing in what kind of optimizer to use

615
00:26:32,000 --> 00:26:37,038
what kind of loss to look for about

616
00:26:33,859 --> 00:26:38,990
metric so again with fast AI you don't

617
00:26:37,038 --> 00:26:41,720
have to pass this in because we know

618
00:26:38,990 --> 00:26:43,038
what loss is the write loss to use you

619
00:26:41,720 --> 00:26:44,569
can always override it but for a

620
00:26:43,038 --> 00:26:48,069
particular model we give you good

621
00:26:44,569 --> 00:26:50,178
defaults okay so having done all that

622
00:26:48,069 --> 00:26:53,119
rather than calling fit you call

623
00:26:50,179 --> 00:26:54,679
generator passing in those two

624
00:26:53,119 --> 00:26:55,788
generators that you saw earlier the

625
00:26:54,679 --> 00:26:59,809
Train generator in the validation

626
00:26:55,788 --> 00:27:01,819
generator for reasons I don't quite

627
00:26:59,808 --> 00:27:03,649
understand chaos expects you to also

628
00:27:01,819 --> 00:27:06,408
tell it how many batches there are per

629
00:27:03,650 --> 00:27:08,620
epoch so the number of batches is a

630
00:27:06,409 --> 00:27:11,780
quarter the size of the generator

631
00:27:08,619 --> 00:27:16,099
divided by the batch size you can tell

632
00:27:11,779 --> 00:27:18,950
it how many epochs just like in fast AI

633
00:27:16,099 --> 00:27:21,839
you can say how many processes or how

634
00:27:18,950 --> 00:27:23,200
many workers to use for pre-processing

635
00:27:21,839 --> 00:27:26,329
[Music]

636
00:27:23,200 --> 00:27:30,830
unlike fast AI the default in chaos is

637
00:27:26,329 --> 00:27:32,089
basically not to use any so to get good

638
00:27:30,829 --> 00:27:35,869
speed you're going to make sure you

639
00:27:32,089 --> 00:27:39,288
include this and so that's basically

640
00:27:35,869 --> 00:27:44,298
enough to start fine tuning the last

641
00:27:39,288 --> 00:27:47,750
layers so as you can see I got to a

642
00:27:44,298 --> 00:27:48,918
validation accuracy of 95% but as you

643
00:27:47,750 --> 00:27:50,960
can also see something really weird

644
00:27:48,919 --> 00:27:53,840
happened we're after one it was like 49

645
00:27:50,960 --> 00:27:56,750
and then it was 69 and then 95 I don't

646
00:27:53,839 --> 00:27:58,158
know why these are so low that's not

647
00:27:56,750 --> 00:28:00,140
normal

648
00:27:58,159 --> 00:28:03,290
I may have there may be a bug and chaos

649
00:28:00,140 --> 00:28:04,340
they may be a bug in my code I reached

650
00:28:03,289 --> 00:28:05,629
out on Twitter to see if

651
00:28:04,339 --> 00:28:07,129
anybody could figure it out but they

652
00:28:05,630 --> 00:28:09,409
couldn't I guess this is one of the

653
00:28:07,130 --> 00:28:11,299
challenges with using something like

654
00:28:09,409 --> 00:28:13,700
this is one of the reasons I wanted to

655
00:28:11,298 --> 00:28:16,339
use fast AI for this course is it's much

656
00:28:13,700 --> 00:28:17,419
harder to screw things up so I don't

657
00:28:16,339 --> 00:28:18,168
know if I screwed something up or

658
00:28:17,419 --> 00:28:23,270
somebody else did

659
00:28:18,169 --> 00:28:25,400
yes you know this is you've seen the

660
00:28:23,269 --> 00:28:28,339
chance to float back end yeah yeah and

661
00:28:25,400 --> 00:28:31,850
if you want to run this to try it out

662
00:28:28,339 --> 00:28:39,019
yourself you just can just go pip

663
00:28:31,849 --> 00:28:40,699
install tensorflow - GPU Kerris okay

664
00:28:39,019 --> 00:28:45,048
because it's not part of the faster I

665
00:28:40,700 --> 00:28:47,149
environment by default but that should

666
00:28:45,048 --> 00:28:55,609
be all you need to do to get that

667
00:28:47,148 --> 00:28:58,129
working so then there isn't a concept of

668
00:28:55,609 --> 00:29:00,229
like layer groups or differential

669
00:28:58,130 --> 00:29:02,120
learning rates or partial unfreezing or

670
00:29:00,230 --> 00:29:03,470
whatever so you have to decide like I

671
00:29:02,119 --> 00:29:06,379
had to print out all of the layers and

672
00:29:03,470 --> 00:29:08,538
decide manually how many I wanted to

673
00:29:06,380 --> 00:29:10,640
fine-tune so I decided to fine-tune

674
00:29:08,538 --> 00:29:11,808
everything from a layer 140 onwards so

675
00:29:10,640 --> 00:29:14,509
that's why I just looked through like

676
00:29:11,808 --> 00:29:17,210
this after you change that you have to

677
00:29:14,509 --> 00:29:19,609
recompile the model and then after that

678
00:29:17,210 --> 00:29:20,750
I then ran another step and again I

679
00:29:19,609 --> 00:29:22,428
don't know what happened here the

680
00:29:20,750 --> 00:29:24,169
accuracy of the training set stayed

681
00:29:22,429 --> 00:29:27,830
about the same but the validation set

682
00:29:24,169 --> 00:29:29,778
totally fill in the hole and I mean the

683
00:29:27,829 --> 00:29:33,408
main thing to notice even if we put

684
00:29:29,778 --> 00:29:35,298
aside the validation set we're getting I

685
00:29:33,409 --> 00:29:36,740
mean I guess the main thing is there's a

686
00:29:35,298 --> 00:29:39,230
hell of a lot more code here which is

687
00:29:36,740 --> 00:29:40,940
kind of annoying but also the

688
00:29:39,230 --> 00:29:42,860
performance is very different so where

689
00:29:40,940 --> 00:29:46,070
else is here even on the training set

690
00:29:42,859 --> 00:29:48,528
we're getting like 97% after four epochs

691
00:29:46,069 --> 00:29:53,898
that took a total of about eight minutes

692
00:29:48,528 --> 00:29:57,169
you know over here we had 99.5% on the

693
00:29:53,898 --> 00:30:02,379
validation set and it ran a lot faster

694
00:29:57,169 --> 00:30:05,380
so I was like four or five minutes right

695
00:30:02,380 --> 00:30:05,380
so

696
00:30:06,869 --> 00:30:10,808
depending on what you do particularly if

697
00:30:09,190 --> 00:30:14,830
you end up wanting to deploy stuff to

698
00:30:10,808 --> 00:30:17,619
mobile devices at the moment the kind of

699
00:30:14,829 --> 00:30:19,689
PI torch on mobile situation is very

700
00:30:17,619 --> 00:30:21,759
early so you may find yourself wanting

701
00:30:19,690 --> 00:30:23,769
to use tensorflow or you may work for a

702
00:30:21,759 --> 00:30:27,069
company that's kind of settled on

703
00:30:23,769 --> 00:30:29,079
tensorflow so if you need to convert

704
00:30:27,069 --> 00:30:31,928
something like redo something you've

705
00:30:29,079 --> 00:30:34,509
learnt here intensive flow you probably

706
00:30:31,929 --> 00:30:37,000
want to do it with care us but just

707
00:30:34,509 --> 00:30:40,629
recognize you know it's got to take a

708
00:30:37,000 --> 00:30:43,000
bit more work to get there and by

709
00:30:40,630 --> 00:30:45,460
default it's much harder to get I mean I

710
00:30:43,000 --> 00:30:46,929
to get the same state of the out results

711
00:30:45,460 --> 00:30:49,690
you get the faster I you'd have to like

712
00:30:46,929 --> 00:30:52,090
replicate all of the state-of-the-art

713
00:30:49,690 --> 00:30:54,669
algorithms that are in first a nice so

714
00:30:52,089 --> 00:30:57,129
it's hard to get the same level of

715
00:30:54,669 --> 00:31:01,600
results but you can see the basic ideas

716
00:30:57,130 --> 00:31:03,970
are similar okay and it's certainly it's

717
00:31:01,599 --> 00:31:05,859
certainly possible you know like there's

718
00:31:03,970 --> 00:31:08,110
nothing I'm doing in fast AI that like

719
00:31:05,859 --> 00:31:09,909
would be impossible but like you'd have

720
00:31:08,109 --> 00:31:11,639
to implement stochastic gradient percent

721
00:31:09,910 --> 00:31:14,169
with restarts you would have to

722
00:31:11,640 --> 00:31:16,360
implement differential learning rates

723
00:31:14,169 --> 00:31:18,700
you would have to implement batch norm

724
00:31:16,359 --> 00:31:20,798
freezing which you probably don't want

725
00:31:18,700 --> 00:31:22,179
to do I know and well that's not quite

726
00:31:20,798 --> 00:31:24,160
true I think somewhat one person at

727
00:31:22,179 --> 00:31:26,559
least on the forum is attempting to

728
00:31:24,160 --> 00:31:28,179
create a chaos compatible version of or

729
00:31:26,558 --> 00:31:30,668
a tensorflow compatible version of fast

730
00:31:28,179 --> 00:31:32,559
AI which I think I hope will get there I

731
00:31:30,669 --> 00:31:33,820
actually spoke to Google about this a

732
00:31:32,558 --> 00:31:36,039
few weeks ago and they're very

733
00:31:33,819 --> 00:31:38,589
interested in getting faster i ported to

734
00:31:36,039 --> 00:31:40,058
tensorflow so maybe by the time you're

735
00:31:38,589 --> 00:31:43,269
looking at this on the mooc maybe that

736
00:31:40,058 --> 00:31:46,389
will exist I certainly hope so we will

737
00:31:43,269 --> 00:31:49,389
see hey wait

738
00:31:46,390 --> 00:31:53,370
so Karis is Karis intensive flow was

739
00:31:49,390 --> 00:31:55,590
certainly not you know

740
00:31:53,369 --> 00:31:56,819
that difficult to handle and so I don't

741
00:31:55,589 --> 00:31:59,009
think you should worry if you're told

742
00:31:56,819 --> 00:32:00,779
you have to learn them after this course

743
00:31:59,009 --> 00:32:08,308
for some reason even let me take you a

744
00:32:00,779 --> 00:32:11,119
couple of days I'm sure so that's kind

745
00:32:08,308 --> 00:32:13,379
of most of the stuff you would need to

746
00:32:11,119 --> 00:32:15,029
kind of complete this this kind of

747
00:32:13,380 --> 00:32:17,040
assignment from last week which was like

748
00:32:15,029 --> 00:32:19,889
try to do everything you've seen already

749
00:32:17,039 --> 00:32:23,308
but on the dog of reinstated said just

750
00:32:19,890 --> 00:32:26,600
to remind you that kind of last few

751
00:32:23,308 --> 00:32:30,869
minutes of last week's lesson I show you

752
00:32:26,599 --> 00:32:32,730
how to do much of that including like

753
00:32:30,869 --> 00:32:35,909
how I actually explored the data to find

754
00:32:32,730 --> 00:32:38,099
out like what the classes were and how

755
00:32:35,910 --> 00:32:39,960
big the images were and stuff like that

756
00:32:38,099 --> 00:32:42,089
right so if you've forgotten that or

757
00:32:39,960 --> 00:32:44,100
didn't quite follow at all last week

758
00:32:42,089 --> 00:32:47,039
check out the video from last week to

759
00:32:44,099 --> 00:32:49,529
see one thing that we didn't talk about

760
00:32:47,039 --> 00:32:51,259
is how do you actually submit to Carol

761
00:32:49,529 --> 00:32:53,519
so how do you actually get predictions

762
00:32:51,259 --> 00:32:56,429
so I just wanted to show you that last

763
00:32:53,519 --> 00:32:58,230
piece as well and on the wiki thread

764
00:32:56,429 --> 00:33:01,500
this week I've already put a little

765
00:32:58,230 --> 00:33:04,860
image of this to show you these days but

766
00:33:01,500 --> 00:33:06,210
if you go to the kaggle website for

767
00:33:04,859 --> 00:33:08,219
every competition there's a section

768
00:33:06,210 --> 00:33:10,230
called evaluation and they tell you what

769
00:33:08,220 --> 00:33:13,589
it's a bit and so I just copied and

770
00:33:10,230 --> 00:33:15,779
pasted these two lines from from there

771
00:33:13,589 --> 00:33:18,558
and so it says we're expected to submit

772
00:33:15,779 --> 00:33:22,139
a file where the first line contains the

773
00:33:18,558 --> 00:33:23,910
the work the word ID and then a comma

774
00:33:22,140 --> 00:33:26,190
separated list of all of the possible

775
00:33:23,910 --> 00:33:29,090
dog breeds and then every line after

776
00:33:26,190 --> 00:33:31,860
that will contain the idea itself

777
00:33:29,089 --> 00:33:35,609
followed by all the probabilities of all

778
00:33:31,859 --> 00:33:39,659
the different dog breeds so how do you

779
00:33:35,609 --> 00:33:42,959
create that so recognize that inside our

780
00:33:39,660 --> 00:33:46,140
data object there's a dot classes which

781
00:33:42,960 --> 00:33:52,200
has got in alphabetical order all of the

782
00:33:46,140 --> 00:33:53,850
four other classes and then so it's got

783
00:33:52,200 --> 00:33:58,380
all of the different classes and then

784
00:33:53,849 --> 00:34:00,240
inside data dot test data set just yes

785
00:33:58,380 --> 00:34:05,010
you can also see there's all the file

786
00:34:00,240 --> 00:34:05,490
names so and just to remind you dogs and

787
00:34:05,009 --> 00:34:06,690
cats

788
00:34:05,490 --> 00:34:11,579
sorry

789
00:34:06,690 --> 00:34:13,679
cats dog breeds was not provided in the

790
00:34:11,579 --> 00:34:15,929
kind of care our style format where the

791
00:34:13,679 --> 00:34:18,269
dogs and cats from different folders but

792
00:34:15,929 --> 00:34:21,059
instead it was provided as a CSV file of

793
00:34:18,269 --> 00:34:25,108
labels right so when you get a CSV file

794
00:34:21,059 --> 00:34:27,859
of labels you use image classifier data

795
00:34:25,108 --> 00:34:31,798
from CSV rather than image classifier

796
00:34:27,858 --> 00:34:33,719
data from parts there isn't an

797
00:34:31,798 --> 00:34:36,148
equivalent in care us so you'll see like

798
00:34:33,719 --> 00:34:38,128
on the cattle forums people share

799
00:34:36,148 --> 00:34:40,138
scripts for how to convert it to a care

800
00:34:38,128 --> 00:34:41,250
our style folders but in our case we

801
00:34:40,139 --> 00:34:44,039
don't have to we just go image

802
00:34:41,250 --> 00:34:49,019
classifier data from CSV passing in that

803
00:34:44,039 --> 00:34:51,210
CSV file and so the CSV file will you

804
00:34:49,019 --> 00:34:54,568
know has automatically told the data you

805
00:34:51,210 --> 00:34:57,750
know what the masses are and then also

806
00:34:54,568 --> 00:35:00,199
we can see from the folder of test

807
00:34:57,750 --> 00:35:03,739
images what the file names of those are

808
00:35:00,199 --> 00:35:07,019
so with those two pieces of information

809
00:35:03,739 --> 00:35:09,659
we're ready to go so I always think it's

810
00:35:07,019 --> 00:35:11,548
a good idea to use TTA as you saw with

811
00:35:09,659 --> 00:35:13,500
that dogs and cats example just now it

812
00:35:11,548 --> 00:35:17,099
can really improve things particularly

813
00:35:13,500 --> 00:35:22,130
when your model is less good so I can

814
00:35:17,099 --> 00:35:22,130
say learn dot t ta and if you pass in

815
00:35:25,548 --> 00:35:32,130
yeah if you pass in is test equals true

816
00:35:29,630 --> 00:35:33,599
then it's going to give you predictions

817
00:35:32,130 --> 00:35:35,369
on the test set rather than the

818
00:35:33,599 --> 00:35:39,480
validation set okay

819
00:35:35,369 --> 00:35:40,980
and now obviously we can't now get an

820
00:35:39,480 --> 00:35:42,929
accuracy or anything because by

821
00:35:40,980 --> 00:35:48,650
definition we don't know the labels for

822
00:35:42,929 --> 00:35:51,239
the test set right so by default most

823
00:35:48,650 --> 00:35:54,539
high-touch models give you back the log

824
00:35:51,239 --> 00:35:57,149
of the predictions so then we just have

825
00:35:54,539 --> 00:35:59,760
to go X of that to get back out

826
00:35:57,150 --> 00:36:01,139
probabilities so in this case the test

827
00:35:59,760 --> 00:36:03,809
set had ten thousand three hundred and

828
00:36:01,139 --> 00:36:06,538
fifty seven images in it and there are

829
00:36:03,809 --> 00:36:10,528
120 possible breeds all right so we get

830
00:36:06,539 --> 00:36:12,890
back a matrix of of that size and so we

831
00:36:10,528 --> 00:36:16,019
now need to turn that into something

832
00:36:12,889 --> 00:36:17,460
that looks like this

833
00:36:16,019 --> 00:36:19,469
and so the easiest way to do that is

834
00:36:17,460 --> 00:36:21,360
with pandas if you're not familiar with

835
00:36:19,469 --> 00:36:23,339
pandas there's lots of information

836
00:36:21,360 --> 00:36:24,630
online about it or check out the machine

837
00:36:23,340 --> 00:36:26,400
learning course intro to machine

838
00:36:24,630 --> 00:36:28,320
learning that we have where we do lots

839
00:36:26,400 --> 00:36:30,990
of stuff with pandas but basically we

840
00:36:28,320 --> 00:36:33,630
can describe PD data frame and pass in

841
00:36:30,989 --> 00:36:36,059
that matrix and then we can say the

842
00:36:33,630 --> 00:36:38,640
names of the columns are equal to data

843
00:36:36,059 --> 00:36:41,670
duck classes and then finally we can

844
00:36:38,639 --> 00:36:45,539
insert a new column at position 0 called

845
00:36:41,670 --> 00:36:47,300
ID that contains the file names but

846
00:36:45,539 --> 00:36:51,210
you'll notice that the file names

847
00:36:47,300 --> 00:36:52,800
contain five letters at the end I start

848
00:36:51,210 --> 00:36:56,849
we don't want and four letters at the

849
00:36:52,800 --> 00:37:02,789
end we don't want so I just subset in

850
00:36:56,849 --> 00:37:06,509
like so right so at that point I've got

851
00:37:02,789 --> 00:37:10,409
a data frame that looks like this which

852
00:37:06,510 --> 00:37:13,470
is what we want so you can now call a

853
00:37:10,409 --> 00:37:24,000
data frame data so social cues dated DF

854
00:37:13,469 --> 00:37:26,879
not des let's fix it now data frame okay

855
00:37:24,000 --> 00:37:30,900
so you can now call data frame to CSV

856
00:37:26,880 --> 00:37:33,180
and quite often you'll find these files

857
00:37:30,900 --> 00:37:35,280
actually get quite big so it's a good

858
00:37:33,179 --> 00:37:37,079
idea to say compression equals gzip and

859
00:37:35,280 --> 00:37:41,180
that'll zip it up on the server for you

860
00:37:37,079 --> 00:37:44,279
and that's going to create a zipped up

861
00:37:41,179 --> 00:37:46,230
CSV file on the server on wherever

862
00:37:44,280 --> 00:37:47,790
you're running is Jupiter notebook so

863
00:37:46,230 --> 00:37:49,349
you need apps that you now need to get

864
00:37:47,789 --> 00:37:53,250
that back to your computer so you can

865
00:37:49,349 --> 00:37:55,619
upload it or you can use carol CLA so

866
00:37:53,250 --> 00:37:55,980
you can type kgs submit and do it that

867
00:37:55,619 --> 00:37:58,949
way

868
00:37:55,980 --> 00:38:00,659
I generally download it to my computer

869
00:37:58,949 --> 00:38:04,169
so like how often back to this lab

870
00:38:00,659 --> 00:38:05,609
double check it all looks ok so to do

871
00:38:04,170 --> 00:38:09,300
that there's a cool little theme called

872
00:38:05,610 --> 00:38:11,880
file link and if you run file link with

873
00:38:09,300 --> 00:38:14,970
a path on your server it gives you back

874
00:38:11,880 --> 00:38:18,390
a URL which you can click on and it will

875
00:38:14,969 --> 00:38:21,389
download that file from the server onto

876
00:38:18,389 --> 00:38:25,289
your computer so if I click on that now

877
00:38:21,389 --> 00:38:31,429
I can go ahead and save it

878
00:38:25,289 --> 00:38:31,429
and then I can see in my downloads

879
00:38:33,489 --> 00:38:38,589
there it is here's my submission file

880
00:38:40,329 --> 00:38:45,819
they want to open their yeah and as you

881
00:38:44,559 --> 00:38:49,059
can see it's exactly what I asked for

882
00:38:45,820 --> 00:38:51,820
there's my ID and 120 different dog

883
00:38:49,059 --> 00:38:54,039
breeds and then here's my first row

884
00:38:51,820 --> 00:38:56,470
containing the file name and the 120

885
00:38:54,039 --> 00:38:58,119
different probabilities okay so then you

886
00:38:56,469 --> 00:39:00,699
can go ahead and submit that to cattle

887
00:38:58,119 --> 00:39:03,369
through their through their regular form

888
00:39:00,699 --> 00:39:05,909
and so this is also a good way you can

889
00:39:03,369 --> 00:39:08,380
see we've now got a good way of both

890
00:39:05,909 --> 00:39:10,989
grabbing any file off the internet and

891
00:39:08,380 --> 00:39:15,160
getting a to our AWS instance or paper

892
00:39:10,989 --> 00:39:17,169
space or whatever by using the cool

893
00:39:15,159 --> 00:39:18,849
little extension in chrome and we've

894
00:39:17,170 --> 00:39:22,200
also got a way of grabbing stuff off our

895
00:39:18,849 --> 00:39:24,940
server easily those of you that are more

896
00:39:22,199 --> 00:39:28,029
command line oriented you can also use

897
00:39:24,940 --> 00:39:31,269
SCP of course but I kind of like doing

898
00:39:28,030 --> 00:39:32,890
everything through the notebook all

899
00:39:31,269 --> 00:39:34,960
right

900
00:39:32,889 --> 00:39:37,809
one other question I had during the week

901
00:39:34,960 --> 00:39:42,909
was like what if I want to just get a

902
00:39:37,809 --> 00:39:45,610
single a single file that I want to you

903
00:39:42,909 --> 00:39:47,289
know get a prediction for so for example

904
00:39:45,610 --> 00:39:49,300
you know maybe I want to get this first

905
00:39:47,289 --> 00:39:52,389
file from my validation set

906
00:39:49,300 --> 00:39:54,250
so there's its name so you can always

907
00:39:52,389 --> 00:39:59,349
look at a file just by calling image dot

908
00:39:54,250 --> 00:40:04,239
open that just uses regular - imaging

909
00:39:59,349 --> 00:40:05,799
library and so what you can do is

910
00:40:04,239 --> 00:40:09,399
there's actually I'll show you the

911
00:40:05,800 --> 00:40:16,230
shortest version you can just call learn

912
00:40:09,400 --> 00:40:20,010
predict array passing in your your image

913
00:40:16,230 --> 00:40:21,389
okay now the image needs to have been

914
00:40:20,010 --> 00:40:24,360
transformed

915
00:40:21,389 --> 00:40:27,759
so you've seen transform Trent

916
00:40:24,360 --> 00:40:29,890
transforms from model before normally we

917
00:40:27,760 --> 00:40:31,270
just put put it all in one variable but

918
00:40:29,889 --> 00:40:32,500
actually behind the scenes it was

919
00:40:31,269 --> 00:40:34,480
returning to things

920
00:40:32,500 --> 00:40:36,309
it was returning training transforms and

921
00:40:34,480 --> 00:40:38,769
validation transforms so I can actually

922
00:40:36,309 --> 00:40:41,110
split them apart and so here you can see

923
00:40:38,769 --> 00:40:43,179
I'm actually applying example my

924
00:40:41,110 --> 00:40:45,280
training transforms or probably more

925
00:40:43,179 --> 00:40:48,819
likely that would apply validation

926
00:40:45,280 --> 00:40:51,220
transforms that gives me back an array

927
00:40:48,820 --> 00:40:55,110
containing the image the transformed

928
00:40:51,219 --> 00:40:55,109
image which I can then past

929
00:40:55,440 --> 00:41:01,570
everything that gets passed to or

930
00:40:58,210 --> 00:41:03,460
returned from bottles is generally

931
00:41:01,570 --> 00:41:05,320
assumed to be a mini batch right it's

932
00:41:03,460 --> 00:41:07,929
generally assumed to be a bunch of

933
00:41:05,320 --> 00:41:10,690
images so we'll talk more about some

934
00:41:07,929 --> 00:41:13,329
numpy tricks later but basically in this

935
00:41:10,690 --> 00:41:15,250
case we only have one image so we have

936
00:41:13,329 --> 00:41:16,929
to turn that into a mini batch of images

937
00:41:15,250 --> 00:41:21,489
so in other words we need to create a

938
00:41:16,929 --> 00:41:24,129
tensor that basically is not just rows

939
00:41:21,489 --> 00:41:26,229
by columns by channels but it's number

940
00:41:24,130 --> 00:41:29,079
of image by rows by columns by channels

941
00:41:26,230 --> 00:41:31,329
and as one image so it's basically

942
00:41:29,079 --> 00:41:33,429
becomes a 4 dimensional tensor so

943
00:41:31,329 --> 00:41:36,610
there's a cool little trick in numpy

944
00:41:33,429 --> 00:41:39,879
that if you index into an array with

945
00:41:36,610 --> 00:41:41,349
none that basically adds additional unit

946
00:41:39,880 --> 00:41:44,079
access to the start so it turns it from

947
00:41:41,349 --> 00:41:46,480
an image into a mini batch of one images

948
00:41:44,079 --> 00:41:48,909
and so that's why we had to do that so

949
00:41:46,480 --> 00:41:52,630
if you basically find you're trying to

950
00:41:48,909 --> 00:41:55,539
do things with a single image with any

951
00:41:52,630 --> 00:41:56,980
kind of Pi torch or fast AI thing this

952
00:41:55,539 --> 00:41:59,400
is just something you might you might

953
00:41:56,980 --> 00:42:02,079
find it says like expecting four

954
00:41:59,400 --> 00:42:04,470
dimensions only got three it probably

955
00:42:02,079 --> 00:42:07,329
means that or if you get back a return

956
00:42:04,469 --> 00:42:10,119
value from something that has like some

957
00:42:07,329 --> 00:42:11,349
weird first access that's probably why

958
00:42:10,119 --> 00:42:13,690
it's probably giving you like back a

959
00:42:11,349 --> 00:42:15,039
mini batch okay and so we'll learn a lot

960
00:42:13,690 --> 00:42:22,050
more about this but it's just something

961
00:42:15,039 --> 00:42:22,050
to be aware of okay so that's kind of

962
00:42:22,079 --> 00:42:29,579
everything you need to do in practice so

963
00:42:28,139 --> 00:42:32,460
now we're going to kind of get into a

964
00:42:29,579 --> 00:42:34,500
little bit of theory what's actually

965
00:42:32,460 --> 00:42:36,990
going on behind the scenes with these

966
00:42:34,500 --> 00:42:42,409
convolutional neural networks and you

967
00:42:36,989 --> 00:42:42,409
might remember it back in Lesson one we

968
00:42:43,849 --> 00:42:51,719
actually saw our first little bit of

969
00:42:48,510 --> 00:42:53,670
theory which we stole from this

970
00:42:51,719 --> 00:42:56,730
fantastic websites a toaster dot IO

971
00:42:53,670 --> 00:42:58,889
either explained visually and we learnt

972
00:42:56,730 --> 00:43:01,320
that a that a convolution is something

973
00:42:58,889 --> 00:43:03,869
where we basically have a little matrix

974
00:43:01,320 --> 00:43:06,600
in deep learning nearly always three by

975
00:43:03,869 --> 00:43:09,029
three a little matrix that we basically

976
00:43:06,599 --> 00:43:10,769
multiply every element of that matrix by

977
00:43:09,030 --> 00:43:14,010
every element of a three by three

978
00:43:10,769 --> 00:43:16,440
section of an image add them all

979
00:43:14,010 --> 00:43:19,670
together to get the result of that

980
00:43:16,440 --> 00:43:22,260
convolution at one point all right now

981
00:43:19,670 --> 00:43:26,369
let's see how that all gets turned

982
00:43:22,260 --> 00:43:29,370
together to create these these various

983
00:43:26,369 --> 00:43:31,859
layers that we saw in the the Zeiler and

984
00:43:29,369 --> 00:43:33,299
burgers paper and to do that again I'm

985
00:43:31,860 --> 00:43:35,010
going to steal off somebody who's much

986
00:43:33,300 --> 00:43:38,370
smarter than I am

987
00:43:35,010 --> 00:43:39,570
we're going to steal from a guy called

988
00:43:38,369 --> 00:43:42,480
Ottavia good

989
00:43:39,570 --> 00:43:45,539
Ottavia oh good was the guy who created

990
00:43:42,480 --> 00:43:47,460
Word Lens which nowadays is part of

991
00:43:45,539 --> 00:43:48,869
Google Translate if I'm Google Translate

992
00:43:47,460 --> 00:43:52,740
you've ever like done that thing where

993
00:43:48,869 --> 00:43:54,599
you point your camera at something at

994
00:43:52,739 --> 00:43:56,189
something which has any kind of foreign

995
00:43:54,599 --> 00:43:58,079
language on it and in real-time it

996
00:43:56,190 --> 00:44:01,440
overlays it with the translation that

997
00:43:58,079 --> 00:44:03,630
was a views company that built that and

998
00:44:01,440 --> 00:44:05,130
so Tokyo was kind enough to share this

999
00:44:03,630 --> 00:44:09,240
fantastic video

1000
00:44:05,130 --> 00:44:10,349
he created he's at Google now and I want

1001
00:44:09,239 --> 00:44:12,029
to kind of step you through works I

1002
00:44:10,349 --> 00:44:14,309
think it explains really really well

1003
00:44:12,030 --> 00:44:15,930
what's going on and then after we look

1004
00:44:14,309 --> 00:44:18,710
at the video we're going to see how to

1005
00:44:15,929 --> 00:44:20,940
implement the whole a whole sequence of

1006
00:44:18,710 --> 00:44:23,400
kintyre set of layers of convolution on

1007
00:44:20,940 --> 00:44:26,159
your network in Microsoft Excel

1008
00:44:23,400 --> 00:44:27,720
so with you're a visual learner or a

1009
00:44:26,159 --> 00:44:29,879
spreadsheet learner hopefully you'll be

1010
00:44:27,719 --> 00:44:32,189
able to understand all this okay so

1011
00:44:29,880 --> 00:44:33,539
we're going to start with an image and

1012
00:44:32,190 --> 00:44:35,159
something that we're going to do later

1013
00:44:33,539 --> 00:44:36,989
in the course is we're going to learn to

1014
00:44:35,159 --> 00:44:39,239
nice digits so we'll do it like end to

1015
00:44:36,989 --> 00:44:41,609
end we'll do the whole thing so this is

1016
00:44:39,239 --> 00:44:44,099
pretty similar so we're going to try and

1017
00:44:41,610 --> 00:44:46,289
recognize in this case letters so here's

1018
00:44:44,099 --> 00:44:50,009
an A which obviously it's actually a

1019
00:44:46,289 --> 00:44:51,989
grid of numbers right and so there's the

1020
00:44:50,010 --> 00:44:55,650
creative numbers and so what we do is we

1021
00:44:51,989 --> 00:44:57,149
take our first convolutional filter so

1022
00:44:55,650 --> 00:44:58,769
we're assuming this is all this is

1023
00:44:57,150 --> 00:45:01,019
assuming that these are already learned

1024
00:44:58,769 --> 00:45:03,239
right and you can see this point it's

1025
00:45:01,019 --> 00:45:05,009
got wiped down the right-hand side right

1026
00:45:03,239 --> 00:45:07,409
and black down the left so it's like

1027
00:45:05,010 --> 00:45:10,350
zero zero zero maybe negative 1 negative

1028
00:45:07,409 --> 00:45:13,139
1 negative 1 0 0 0 1 1 1 and so we're

1029
00:45:10,349 --> 00:45:16,949
taking each 3x3 part of the image and

1030
00:45:13,139 --> 00:45:18,989
multiplying it by that 3x3 matrix not as

1031
00:45:16,949 --> 00:45:21,329
a matrix product that an element-wise

1032
00:45:18,989 --> 00:45:24,449
product and so you can see what happens

1033
00:45:21,329 --> 00:45:28,139
is everywhere where the the white edge

1034
00:45:24,449 --> 00:45:30,179
is matching the edge of the a and the

1035
00:45:28,139 --> 00:45:32,339
black edge isn't we're getting green

1036
00:45:30,179 --> 00:45:33,869
we're getting a positive and everywhere

1037
00:45:32,340 --> 00:45:35,970
where it's the opposite we're getting a

1038
00:45:33,869 --> 00:45:39,179
negative we're getting a red right and

1039
00:45:35,969 --> 00:45:41,849
so that's the first filter creating the

1040
00:45:39,179 --> 00:45:44,759
first that the result of the first

1041
00:45:41,849 --> 00:45:46,619
kernel right and so here's a new kernel

1042
00:45:44,760 --> 00:45:49,380
this one is it's got a white stripe

1043
00:45:46,619 --> 00:45:52,759
along the top right so we literally scan

1044
00:45:49,380 --> 00:45:55,890
through every 3x3 part of the matrix

1045
00:45:52,760 --> 00:45:57,450
multiplying those 3 bits of the a the

1046
00:45:55,889 --> 00:45:59,940
neighbors of the a by the 9 bits as a

1047
00:45:57,449 --> 00:46:02,609
filter to find out whether it's red or

1048
00:45:59,940 --> 00:46:04,860
green and how red or green it is ok and

1049
00:46:02,610 --> 00:46:07,079
so this is assuming we had two filters

1050
00:46:04,860 --> 00:46:08,970
one was a bottom edge one was a left

1051
00:46:07,079 --> 00:46:10,799
edge and you can see here the top edge

1052
00:46:08,969 --> 00:46:12,689
not surprisingly it's red here so a

1053
00:46:10,800 --> 00:46:15,570
bottom edge was red here and green here

1054
00:46:12,690 --> 00:46:17,880
the right edge right here in green here

1055
00:46:15,570 --> 00:46:20,460
and then in the next step we add a

1056
00:46:17,880 --> 00:46:23,039
non-linearity ok the rectified linear

1057
00:46:20,460 --> 00:46:26,519
unit which literally means strongly the

1058
00:46:23,039 --> 00:46:28,289
negatives so here the Reds all gone okay

1059
00:46:26,519 --> 00:46:30,780
so here's layer 1 the input

1060
00:46:28,289 --> 00:46:33,659
here's layup to the result of 2

1061
00:46:30,780 --> 00:46:36,180
convolutional filters here's layer 3

1062
00:46:33,659 --> 00:46:38,190
which is which is throw away all of the

1063
00:46:36,179 --> 00:46:41,159
red stuff and that's called a rectified

1064
00:46:38,190 --> 00:46:43,889
linear unit and then layer 4 is

1065
00:46:41,159 --> 00:46:48,089
something called a max pull on a layer 4

1066
00:46:43,889 --> 00:46:48,699
we replace every 2 by 2 part of this

1067
00:46:48,090 --> 00:46:51,338
grid

1068
00:46:48,699 --> 00:46:53,039
and we replace it with its maximum mat

1069
00:46:51,338 --> 00:46:55,358
so it basically makes it half the size

1070
00:46:53,039 --> 00:46:57,699
it's basically the same thing but half

1071
00:46:55,358 --> 00:46:59,619
the size and then we can go through and

1072
00:46:57,699 --> 00:47:02,618
do exactly the same thing we can have

1073
00:46:59,619 --> 00:47:04,510
some new filter three by three filter

1074
00:47:02,619 --> 00:47:08,680
that we put through each of the two

1075
00:47:04,510 --> 00:47:10,510
results of the previous layer okay and

1076
00:47:08,679 --> 00:47:13,029
again we can throw away the red bits

1077
00:47:10,510 --> 00:47:14,349
right so get rid of all the negatives so

1078
00:47:13,030 --> 00:47:18,298
we just keep the positives that's called

1079
00:47:14,349 --> 00:47:21,190
applying a rectified linear unit and

1080
00:47:18,298 --> 00:47:23,530
that gets us to our next layer of this

1081
00:47:21,190 --> 00:47:26,530
convolutional neural network so you can

1082
00:47:23,530 --> 00:47:29,109
see that by you know at this layer back

1083
00:47:26,530 --> 00:47:30,490
here it was kind of very interpretive

1084
00:47:29,108 --> 00:47:33,009
all it's like we've either got bottom

1085
00:47:30,489 --> 00:47:36,129
edges or left edges but then the next

1086
00:47:33,010 --> 00:47:37,660
layer was combining the results of

1087
00:47:36,130 --> 00:47:40,088
convolutions so it's starting to become

1088
00:47:37,659 --> 00:47:42,129
a lot less clear like intuitively what's

1089
00:47:40,088 --> 00:47:45,130
happening but it's doing the same thing

1090
00:47:42,130 --> 00:47:48,818
and then we do another max pull right so

1091
00:47:45,130 --> 00:47:52,180
we replace every 2x2 or 3x3 section with

1092
00:47:48,818 --> 00:47:53,798
a single digit so here this 2x2 it's all

1093
00:47:52,179 --> 00:47:56,379
black so we replaced it with a black

1094
00:47:53,798 --> 00:48:00,429
right and then we go and we take that

1095
00:47:56,380 --> 00:48:03,099
and we we compare it to basically a kind

1096
00:48:00,429 --> 00:48:05,139
of a template of what we would expect to

1097
00:48:03,099 --> 00:48:07,599
see if it was an a it was a B but the

1098
00:48:05,139 --> 00:48:10,690
see it was d give it an E and we see how

1099
00:48:07,599 --> 00:48:12,309
closely it matches and we can do it in

1100
00:48:10,690 --> 00:48:15,700
exactly the same way we can multiply

1101
00:48:12,309 --> 00:48:18,579
every one of the values in this four by

1102
00:48:15,699 --> 00:48:20,649
eight matrix with every one of the four

1103
00:48:18,579 --> 00:48:22,390
by eight in this one and this one and

1104
00:48:20,650 --> 00:48:24,579
this one and we add we just add them

1105
00:48:22,389 --> 00:48:26,858
together to say like how often does it

1106
00:48:24,579 --> 00:48:29,798
match versus how often does it not match

1107
00:48:26,858 --> 00:48:33,909
and then that could be converted to give

1108
00:48:29,798 --> 00:48:35,530
us a percentage probability that this is

1109
00:48:33,909 --> 00:48:39,460
a no so in this case this particular

1110
00:48:35,530 --> 00:48:41,619
template matched well with a so notice

1111
00:48:39,460 --> 00:48:43,539
we're not doing an each training here

1112
00:48:41,619 --> 00:48:46,119
right this is how it would work if we

1113
00:48:43,539 --> 00:48:48,039
have a pre trained model all right so

1114
00:48:46,119 --> 00:48:50,170
when we download a pre trained imagenet

1115
00:48:48,039 --> 00:48:52,690
model off the internet and isn't on an

1116
00:48:50,170 --> 00:48:54,460
image without any changing to it this is

1117
00:48:52,690 --> 00:48:56,619
what's happening or if we take a model

1118
00:48:54,460 --> 00:48:58,568
that you've trained and you're applying

1119
00:48:56,619 --> 00:49:00,789
it to some test set or for some new

1120
00:48:58,568 --> 00:49:02,500
image this is what it's doing all right

1121
00:49:00,789 --> 00:49:04,869
as it's basically taking it through it

1122
00:49:02,500 --> 00:49:08,710
buying a convolution to each layer to

1123
00:49:04,869 --> 00:49:12,789
each multiple convolutional filters to

1124
00:49:08,710 --> 00:49:14,679
each layer and then doing the rectified

1125
00:49:12,789 --> 00:49:18,369
linear unit so throw away the negatives

1126
00:49:14,679 --> 00:49:21,129
and then do the max pull and then repeat

1127
00:49:18,369 --> 00:49:24,519
that a bunch of times and so then we can

1128
00:49:21,130 --> 00:49:28,030
do it with a new letter A or letter B or

1129
00:49:24,519 --> 00:49:31,329
whatever and keep going through that

1130
00:49:28,030 --> 00:49:33,790
process right so as you can see that's

1131
00:49:31,329 --> 00:49:35,199
far nice the visualization thing and I

1132
00:49:33,789 --> 00:49:37,809
could have created because I'm not at a

1133
00:49:35,199 --> 00:49:40,419
vo so thanks to him for sharing this

1134
00:49:37,809 --> 00:49:42,519
with us because it's totally awesome

1135
00:49:40,420 --> 00:49:43,480
he actually this is not done by hand he

1136
00:49:42,519 --> 00:49:45,099
actually wrote a piece of computer

1137
00:49:43,480 --> 00:49:47,409
software to actually do these

1138
00:49:45,099 --> 00:49:49,869
convolutions this is actually being

1139
00:49:47,409 --> 00:49:52,480
actually being done dynamically which is

1140
00:49:49,869 --> 00:49:56,349
pretty cool so I'm more of a spreadsheet

1141
00:49:52,480 --> 00:49:58,059
guy personally I'm a simple person so

1142
00:49:56,349 --> 00:50:00,460
here is the same thing now in

1143
00:49:58,059 --> 00:50:03,489
spreadsheet all right and so you'll find

1144
00:50:00,460 --> 00:50:06,610
this in the github repo so you can

1145
00:50:03,489 --> 00:50:08,949
either get clone the repo to your own

1146
00:50:06,610 --> 00:50:11,670
computer open up the spreadsheet or you

1147
00:50:08,949 --> 00:50:21,069
can just go to github.com slash / ji and

1148
00:50:11,670 --> 00:50:23,829
click on this it's it's inside if you go

1149
00:50:21,070 --> 00:50:26,890
to our repo and just go to courses as

1150
00:50:23,829 --> 00:50:29,170
usual go to deal 1 as usual you'll see

1151
00:50:26,889 --> 00:50:30,819
there's an Excel section there okay and

1152
00:50:29,170 --> 00:50:32,230
so he lay all that so you can just

1153
00:50:30,820 --> 00:50:33,970
download them by clicking them or you

1154
00:50:32,230 --> 00:50:36,880
can clone the whole repo and we're

1155
00:50:33,969 --> 00:50:40,689
looking at cognitive example convolution

1156
00:50:36,880 --> 00:50:44,710
example all right so you can see I have

1157
00:50:40,690 --> 00:50:48,250
here an input right so in this case the

1158
00:50:44,710 --> 00:50:51,309
input is the number 7 so I grab this

1159
00:50:48,250 --> 00:50:52,599
from a dataset called m-must MN ist

1160
00:50:51,309 --> 00:50:55,289
which we'll be looking at in a lot of

1161
00:50:52,599 --> 00:50:58,119
detail and I just took one of those

1162
00:50:55,289 --> 00:51:01,360
digits at random and I put it into Excel

1163
00:50:58,119 --> 00:51:05,230
and so you can see every Hextall is

1164
00:51:01,360 --> 00:51:09,640
actually just a number between 9 1 okay

1165
00:51:05,230 --> 00:51:13,059
very often actually it'll be a bite

1166
00:51:09,639 --> 00:51:15,250
between Norton 255 or sometimes it might

1167
00:51:13,059 --> 00:51:16,088
be a float between naught and 1 it

1168
00:51:15,250 --> 00:51:18,789
doesn't really matter

1169
00:51:16,088 --> 00:51:22,449
by the time it gets to PI torch we're

1170
00:51:18,789 --> 00:51:24,519
generally dealing with floats so we if

1171
00:51:22,449 --> 00:51:25,900
one of the steps we often will take will

1172
00:51:24,518 --> 00:51:29,798
be to convert it to a number between

1173
00:51:25,900 --> 00:51:32,170
naught 1 so then you can see I've just

1174
00:51:29,798 --> 00:51:34,509
use conditional formatting in Excel to

1175
00:51:32,170 --> 00:51:36,489
kind of make the higher numbers more red

1176
00:51:34,509 --> 00:51:40,389
so you can clearly see that this is a

1177
00:51:36,489 --> 00:51:42,400
red this is a 7 but but it's just a

1178
00:51:40,389 --> 00:51:49,478
bunch of numbers that have been imported

1179
00:51:42,400 --> 00:51:52,298
into Excel okay so here's our input so

1180
00:51:49,478 --> 00:51:55,838
remember what at a via did was he then

1181
00:51:52,298 --> 00:51:58,298
applied two filters right with different

1182
00:51:55,838 --> 00:52:02,048
shapes so here I've created a filter

1183
00:51:58,298 --> 00:52:05,170
which is designed to detect top edges so

1184
00:52:02,048 --> 00:52:07,268
this is a 3 by 3 filter okay and I've

1185
00:52:05,170 --> 00:52:10,119
got ones along the top zeroes in the

1186
00:52:07,268 --> 00:52:11,998
middle minus ones at the bottom right so

1187
00:52:10,119 --> 00:52:16,630
let's take a look at an example that's

1188
00:52:11,998 --> 00:52:19,478
here right and so if I hit that - you

1189
00:52:16,630 --> 00:52:22,028
can see here highlighted this is the 3

1190
00:52:19,478 --> 00:52:24,518
by 3 part of the input that this

1191
00:52:22,028 --> 00:52:28,239
particular thing is calculating right so

1192
00:52:24,518 --> 00:52:32,618
here you can see it's got 1 1 1 are all

1193
00:52:28,239 --> 00:52:35,249
being multiplied by 1 and point 1 0 0

1194
00:52:32,619 --> 00:52:37,989
are all being multiplied by negative 1

1195
00:52:35,248 --> 00:52:39,639
okay so in other words all the positive

1196
00:52:37,989 --> 00:52:41,318
bits are getting a lot of positive the

1197
00:52:39,639 --> 00:52:44,098
negative bits are getting nearly nothing

1198
00:52:41,318 --> 00:52:47,048
at all so we end up with a high number

1199
00:52:44,099 --> 00:52:51,028
okay where else on the other side of

1200
00:52:47,048 --> 00:52:54,670
this bit of the 7 right you can see how

1201
00:52:51,028 --> 00:52:56,920
you know this is basically zeros here or

1202
00:52:54,670 --> 00:53:04,420
perhaps more interestingly on the top of

1203
00:52:56,920 --> 00:53:06,039
it okay here we've got high numbers at

1204
00:53:04,420 --> 00:53:08,858
the top but we've also got high numbers

1205
00:53:06,039 --> 00:53:10,719
at the bottom which are negating it ok

1206
00:53:08,858 --> 00:53:14,768
so you can see that the only place that

1207
00:53:10,719 --> 00:53:19,179
we end up activating is where we're

1208
00:53:14,768 --> 00:53:22,358
actually at an edge so in this case this

1209
00:53:19,179 --> 00:53:25,660
here this number 3 this is called an

1210
00:53:22,358 --> 00:53:28,989
activation ok so when I say an

1211
00:53:25,659 --> 00:53:33,670
activation I mean ah

1212
00:53:28,989 --> 00:53:38,500
at number a number that is calculated

1213
00:53:33,670 --> 00:53:42,880
and it is calculated by taking some

1214
00:53:38,500 --> 00:53:46,000
numbers from the input and applying some

1215
00:53:42,880 --> 00:53:48,430
kind of linear operation in this case a

1216
00:53:46,000 --> 00:53:52,239
convolutional kernel to calculate an

1217
00:53:48,429 --> 00:53:56,919
output right you'll notice that other

1218
00:53:52,239 --> 00:53:59,229
than going inputs multiplied by kernel

1219
00:53:56,920 --> 00:54:03,210
and summing it together

1220
00:53:59,230 --> 00:54:07,059
right so here's my some and here's my x

1221
00:54:03,210 --> 00:54:10,599
then take that and I go max of 0 comma

1222
00:54:07,059 --> 00:54:13,750
that and so that's my rectified linear

1223
00:54:10,599 --> 00:54:15,339
unit so it sounds very fancy rectified

1224
00:54:13,750 --> 00:54:17,710
linear unit but what they actually mean

1225
00:54:15,340 --> 00:54:22,720
is open up Excel and type equals max 0

1226
00:54:17,710 --> 00:54:25,480
comma C ok that's all about then you'll

1227
00:54:22,719 --> 00:54:28,959
see people in the biz so to say value a

1228
00:54:25,480 --> 00:54:32,920
so ral you means rectified linear unit

1229
00:54:28,960 --> 00:54:34,690
means max 0 comma thing and I'm not like

1230
00:54:32,920 --> 00:54:36,610
simplifying it I really mean it like

1231
00:54:34,690 --> 00:54:38,889
when I say like if I'm simplifying I

1232
00:54:36,610 --> 00:54:40,930
always say so I'm simplifying but if I'm

1233
00:54:38,889 --> 00:54:42,969
not saying I'm simplifying that's the

1234
00:54:40,929 --> 00:54:45,879
entirety okay so a rectified linear unit

1235
00:54:42,969 --> 00:54:50,489
in its entirety is this and a

1236
00:54:45,880 --> 00:54:54,789
convolution in its entirety is is this

1237
00:54:50,489 --> 00:54:57,369
okay so a single layer of a

1238
00:54:54,789 --> 00:55:00,769
convolutional neural network is being

1239
00:54:57,369 --> 00:55:03,559
implemented in its entirety

1240
00:55:00,769 --> 00:55:06,710
here in Excel okay and so you can see

1241
00:55:03,559 --> 00:55:09,289
what it's done is it's deleted pretty

1242
00:55:06,710 --> 00:55:12,559
much the vertical edges and highlighted

1243
00:55:09,289 --> 00:55:15,920
the horizontal edges so again this is

1244
00:55:12,559 --> 00:55:18,769
assuming that our network is trained and

1245
00:55:15,920 --> 00:55:20,750
that at the end of training it a created

1246
00:55:18,769 --> 00:55:24,500
a convolutional filter with these

1247
00:55:20,750 --> 00:55:28,670
specific line numbers in and so here is

1248
00:55:24,500 --> 00:55:32,059
a second convolutional filter it's just

1249
00:55:28,670 --> 00:55:35,210
a different line numbers now pi torch

1250
00:55:32,059 --> 00:55:38,480
doesn't store them as two separate nine

1251
00:55:35,210 --> 00:55:41,539
digit arrays it stores it as a tensor

1252
00:55:38,480 --> 00:55:46,010
right remember a tensor just means an

1253
00:55:41,539 --> 00:55:50,059
array with more dimensions okay you can

1254
00:55:46,010 --> 00:55:52,850
use the word array as well it's the same

1255
00:55:50,059 --> 00:55:54,230
thing but in pi torch they always use

1256
00:55:52,849 --> 00:55:57,440
the word tensor so I'm going to say

1257
00:55:54,230 --> 00:55:59,869
cancer okay so it's just a tensor with

1258
00:55:57,440 --> 00:56:02,539
an additional axis which allows us to

1259
00:55:59,869 --> 00:56:07,219
stack each of these filters together

1260
00:56:02,539 --> 00:56:09,349
right filter and kernel pretty much mean

1261
00:56:07,219 --> 00:56:13,489
the same thing yeah right it refers to

1262
00:56:09,349 --> 00:56:17,179
one of these three by three matrices or

1263
00:56:13,489 --> 00:56:19,699
one of these three by three slices of a

1264
00:56:17,179 --> 00:56:22,039
three dimensional tensor so if I take

1265
00:56:19,699 --> 00:56:23,989
this one and here I've literally just

1266
00:56:22,039 --> 00:56:27,349
copied the formulas in Excel from above

1267
00:56:23,989 --> 00:56:30,679
okay and so you can see this one is now

1268
00:56:27,349 --> 00:56:31,130
finding a vertebra which as we would

1269
00:56:30,679 --> 00:56:40,009
expect

1270
00:56:31,130 --> 00:56:42,559
okay so we've now created one layer

1271
00:56:40,010 --> 00:56:44,150
right this here is a layer them

1272
00:56:42,559 --> 00:56:46,519
specifically we'd say it's a hidden

1273
00:56:44,150 --> 00:56:48,680
layer which is it's not an input layer

1274
00:56:46,519 --> 00:56:50,809
and it's not an output layer so

1275
00:56:48,679 --> 00:56:55,480
everything else is a hidden layer okay

1276
00:56:50,809 --> 00:56:59,210
and this particular hidden layer has is

1277
00:56:55,480 --> 00:57:02,079
a size two on this dimension right

1278
00:56:59,210 --> 00:57:02,079
because it has two

1279
00:57:02,199 --> 00:57:09,848
filters right two kernels so what

1280
00:57:07,809 --> 00:57:14,500
happens next

1281
00:57:09,849 --> 00:57:17,710
well let's do another one okay so as we

1282
00:57:14,500 --> 00:57:20,050
kind of go along things can multiply a

1283
00:57:17,710 --> 00:57:23,588
little bit in complexity right because

1284
00:57:20,050 --> 00:57:27,010
my next filter is going to have to

1285
00:57:23,588 --> 00:57:28,929
contain two of these three by threes

1286
00:57:27,010 --> 00:57:31,690
because I'm gonna have to say how do I

1287
00:57:28,929 --> 00:57:34,539
want to bring Adam I want to write these

1288
00:57:31,690 --> 00:57:36,369
three things and at the same time how do

1289
00:57:34,539 --> 00:57:38,588
I want to wait the corresponding three

1290
00:57:36,369 --> 00:57:41,200
things down here right because in pi

1291
00:57:38,588 --> 00:57:44,108
torch this is going to be this whole

1292
00:57:41,199 --> 00:57:46,659
thing here is going to be stored as a

1293
00:57:44,108 --> 00:57:48,429
multi-dimensional tensor right so you

1294
00:57:46,659 --> 00:57:54,039
shouldn't really think of this now as

1295
00:57:48,429 --> 00:57:58,598
two 3x3 kernels but one two by three by

1296
00:57:54,039 --> 00:58:02,579
three eternal okay so to calculate this

1297
00:57:58,599 --> 00:58:09,510
value here I've got the sum product of

1298
00:58:02,579 --> 00:58:17,349
all of that plus the sum product of

1299
00:58:09,510 --> 00:58:19,119
scroll down all of that okay and so the

1300
00:58:17,349 --> 00:58:21,130
top ones are being multiplied by this

1301
00:58:19,119 --> 00:58:22,420
part of the kernel and the bottom ones

1302
00:58:21,130 --> 00:58:25,869
have been multiplied by this part of the

1303
00:58:22,420 --> 00:58:28,000
kernel and so over time you want to

1304
00:58:25,869 --> 00:58:31,170
start to get very comfortable with the

1305
00:58:28,000 --> 00:58:35,530
idea of these like higher dimensional

1306
00:58:31,170 --> 00:58:37,900
linear combinations right like it's it's

1307
00:58:35,530 --> 00:58:40,720
harder to draw it on the screen like I

1308
00:58:37,900 --> 00:58:42,670
had to put one above the other but

1309
00:58:40,719 --> 00:58:44,439
conceptually just stuck it in your mind

1310
00:58:42,670 --> 00:58:47,139
like this that's really how you want to

1311
00:58:44,440 --> 00:58:50,400
think right and actually Geoffrey Hinton

1312
00:58:47,139 --> 00:58:54,159
in his original 2012 neural Nets

1313
00:58:50,400 --> 00:58:56,289
Coursera class has a tip which is how

1314
00:58:54,159 --> 00:58:58,989
all computer scientists deal with like

1315
00:58:56,289 --> 00:59:01,000
very high dimensional spaces which is

1316
00:58:58,989 --> 00:59:01,929
that they basically just visualize the

1317
00:59:01,000 --> 00:59:04,659
two-dimensional space

1318
00:59:01,929 --> 00:59:06,389
and then say like twelve dimensions

1319
00:59:04,659 --> 00:59:08,108
really fast and they had lots of tires

1320
00:59:06,389 --> 00:59:09,759
so that's it

1321
00:59:08,108 --> 00:59:11,139
right we can see two dimensions on the

1322
00:59:09,760 --> 00:59:14,470
screen and then you're just going to try

1323
00:59:11,139 --> 00:59:15,799
to trust that you can have more

1324
00:59:14,469 --> 00:59:17,659
dimensions like the Const

1325
00:59:15,800 --> 00:59:19,550
it's just you know there's there's

1326
00:59:17,659 --> 00:59:21,829
nothing different about them and so you

1327
00:59:19,550 --> 00:59:23,090
can see in Excel you know Excel doesn't

1328
00:59:21,829 --> 00:59:25,099
have the ability to handle

1329
00:59:23,090 --> 00:59:27,730
three-dimensional tensors so I had to

1330
00:59:25,099 --> 00:59:31,219
like say okay take this two-dimensional

1331
00:59:27,730 --> 00:59:33,170
dot product add on this two-dimensional

1332
00:59:31,219 --> 00:59:35,659
dot product right but if there was some

1333
00:59:33,170 --> 00:59:38,900
kind of 3d Excel I could have to stand

1334
00:59:35,659 --> 00:59:42,889
that in a single line all right and then

1335
00:59:38,900 --> 00:59:44,720
again apply max 0 comma otherwise known

1336
00:59:42,889 --> 00:59:48,710
as rectified linear unit otherwise known

1337
00:59:44,719 --> 00:59:51,969
as value okay so here is my second layer

1338
00:59:48,710 --> 00:59:54,740
and so when people create different

1339
00:59:51,969 --> 00:59:59,299
architectures write an architecture

1340
00:59:54,739 --> 00:59:59,959
means like how big is your kernel at

1341
00:59:59,300 --> 01:00:02,539
layer 1

1342
00:59:59,960 --> 01:00:04,630
how many filters are in your kernel at

1343
01:00:02,539 --> 01:00:08,509
layer 1 so here I've got a 3 by 3

1344
01:00:04,630 --> 01:00:11,329
where's number 1 and a 3 by 3 there's

1345
01:00:08,510 --> 01:00:15,250
number 2 so like this architecture I've

1346
01:00:11,329 --> 01:00:20,480
created starts off with 2 3 by 3

1347
01:00:15,250 --> 01:00:25,099
convolutional kernels and then my second

1348
01:00:20,480 --> 01:00:27,019
layer has another two kernels of size 2

1349
01:00:25,099 --> 01:00:30,650
by 3 by 3 so there's the first one

1350
01:00:27,019 --> 01:00:34,429
and then down here here's a second 2 by

1351
01:00:30,650 --> 01:00:35,930
3 by 3 kernel okay and so remember one

1352
01:00:34,429 --> 01:00:40,639
of these specific any one of these

1353
01:00:35,929 --> 01:00:43,849
numbers is an activation okay so this

1354
01:00:40,639 --> 01:00:45,679
activation is being calculated from

1355
01:00:43,849 --> 01:00:47,779
these three things here and other 3

1356
01:00:45,679 --> 01:00:52,940
things up there and we're using these

1357
01:00:47,780 --> 01:00:55,160
this 2 by 3 by 3 kernel okay and so what

1358
01:00:52,940 --> 01:00:57,829
tends to happen is people generally give

1359
01:00:55,159 --> 01:01:01,219
names to their layers so I say okay

1360
01:00:57,829 --> 01:01:07,670
let's call this layer here con 1 and

1361
01:01:01,219 --> 01:01:11,629
this layer here and this and this layer

1362
01:01:07,670 --> 01:01:13,730
here con - all right so that's you know

1363
01:01:11,630 --> 01:01:15,920
but generally you'll just see that like

1364
01:01:13,730 --> 01:01:17,929
when you print out a summary of a

1365
01:01:15,920 --> 01:01:22,340
network every layer will have some kind

1366
01:01:17,929 --> 01:01:26,149
of name okay and so then what happens

1367
01:01:22,340 --> 01:01:28,690
next well part of the architecture is

1368
01:01:26,150 --> 01:01:30,400
like do you have some max pooling where

1369
01:01:28,690 --> 01:01:32,409
bounces up Matt spalling happen so in

1370
01:01:30,400 --> 01:01:36,519
this architecture we're inventing we're

1371
01:01:32,409 --> 01:01:40,239
going to next step is do max fully okay

1372
01:01:36,519 --> 01:01:43,568
Matt spooling is a little hard to kind

1373
01:01:40,239 --> 01:01:45,939
of show in Excel but we've got it so max

1374
01:01:43,568 --> 01:01:49,179
pooling if I do a two by two max pooling

1375
01:01:45,940 --> 01:01:51,579
it's going to have the resolution both

1376
01:01:49,179 --> 01:01:57,358
height and width so you can see here

1377
01:01:51,579 --> 01:02:00,240
that I've replaced these four numbers

1378
01:01:57,358 --> 01:02:02,559
with the maximum of those four numbers

1379
01:02:00,239 --> 01:02:04,389
right and so because I'm having the

1380
01:02:02,559 --> 01:02:06,779
resolution it only makes sense to

1381
01:02:04,389 --> 01:02:10,358
actually have something every two cells

1382
01:02:06,780 --> 01:02:13,690
okay so you can see here the way I've

1383
01:02:10,358 --> 01:02:15,940
got kind of the same looking shape as I

1384
01:02:13,690 --> 01:02:19,030
had back here okay but it's now half the

1385
01:02:15,940 --> 01:02:22,119
resolution so for placed every two by

1386
01:02:19,030 --> 01:02:24,160
two with its max and you'll notice like

1387
01:02:22,119 --> 01:02:26,318
it's not every possible two by two I

1388
01:02:24,159 --> 01:02:29,199
skip over from here so this is like

1389
01:02:26,318 --> 01:02:33,460
starting at beat Hugh and then the next

1390
01:02:29,199 --> 01:02:35,739
one starts at BS right so they're like

1391
01:02:33,460 --> 01:02:38,050
non-overlapping that's why it's

1392
01:02:35,739 --> 01:02:39,939
decreasing the resolution okay

1393
01:02:38,050 --> 01:02:43,000
so anybody who's comfortable with

1394
01:02:39,940 --> 01:02:45,369
spreadsheets you know you can open this

1395
01:02:43,000 --> 01:02:52,150
and have a look and so after our max

1396
01:02:45,369 --> 01:02:54,010
pooling there's a number of different

1397
01:02:52,150 --> 01:02:58,119
things we could do next and I'm going to

1398
01:02:54,010 --> 01:03:00,790
show you a kind of classic old style

1399
01:02:58,119 --> 01:03:02,890
approach nowadays in fact what generally

1400
01:03:00,789 --> 01:03:04,900
happens nowadays is we do a max pool

1401
01:03:02,889 --> 01:03:08,259
where we kind of like max across the

1402
01:03:04,900 --> 01:03:10,180
entire size right but on older

1403
01:03:08,260 --> 01:03:12,790
architectures and also on all the

1404
01:03:10,179 --> 01:03:14,739
structured data stuff we do we actually

1405
01:03:12,789 --> 01:03:16,929
do something called a fully connected

1406
01:03:14,739 --> 01:03:19,059
layer and so here's a fully connected

1407
01:03:16,929 --> 01:03:22,299
layer I'm going to take every single one

1408
01:03:19,059 --> 01:03:24,509
of these activations and I've got to

1409
01:03:22,300 --> 01:03:28,150
give every single one of them or weight

1410
01:03:24,510 --> 01:03:33,130
right and so then I'm going to take over

1411
01:03:28,150 --> 01:03:35,650
here here is the sum product of every

1412
01:03:33,130 --> 01:03:40,440
one of the activations by every one of

1413
01:03:35,650 --> 01:03:40,440
the weights for both of the

1414
01:03:40,670 --> 01:03:45,619
two levels of my three-dimensional

1415
01:03:43,159 --> 01:03:47,598
tensor right and so this is called a

1416
01:03:45,619 --> 01:03:49,490
fully connected layer notice it's

1417
01:03:47,599 --> 01:03:51,980
different to a convolution I'm not going

1418
01:03:49,489 --> 01:03:54,618
through a few at a time right but I'm

1419
01:03:51,980 --> 01:03:57,079
creating a really big weight matrix

1420
01:03:54,619 --> 01:03:59,930
right so rather than having a couple of

1421
01:03:57,079 --> 01:04:03,289
little 3x3 kernels my weight matrix is

1422
01:03:59,929 --> 01:04:07,759
now as big as the entire input and so as

1423
01:04:03,289 --> 01:04:09,920
you can imagine architectures that make

1424
01:04:07,760 --> 01:04:13,640
heavy use of fully convolutional layers

1425
01:04:09,920 --> 01:04:15,048
can have a lot of weights which means

1426
01:04:13,639 --> 01:04:17,838
they can have trouble with overfitting

1427
01:04:15,048 --> 01:04:20,539
and they can also be slow and so you're

1428
01:04:17,838 --> 01:04:23,298
going to see a lot an architecture

1429
01:04:20,539 --> 01:04:25,279
called vgg because it was the first kind

1430
01:04:23,298 --> 01:04:28,880
of successful deeper architecture it has

1431
01:04:25,280 --> 01:04:31,750
up to 19 layers and vgg actually

1432
01:04:28,880 --> 01:04:35,568
contains a fully connected layer with

1433
01:04:31,750 --> 01:04:39,710
4096 weights connected to a hidden layer

1434
01:04:35,568 --> 01:04:42,858
with 4,000 sorry 4096 activations

1435
01:04:39,710 --> 01:04:46,250
connected to a hidden layer with 4096

1436
01:04:42,858 --> 01:04:50,358
activations so you've got like 4096 by

1437
01:04:46,250 --> 01:04:53,179
4096 x remember or apply it by the

1438
01:04:50,358 --> 01:04:59,838
number of kind of kernels that we've

1439
01:04:53,179 --> 01:05:03,318
calculated so in vgg there's this I

1440
01:04:59,838 --> 01:05:06,318
think it's like 300 million weights of

1441
01:05:03,318 --> 01:05:09,380
which something like 250 million of them

1442
01:05:06,318 --> 01:05:11,420
are in these fully connected layers so

1443
01:05:09,380 --> 01:05:13,700
we'll learn later on in the course about

1444
01:05:11,420 --> 01:05:15,470
how we can kind of avoid using these big

1445
01:05:13,699 --> 01:05:17,449
fully connected layers and behind the

1446
01:05:15,469 --> 01:05:20,058
scenes all the stuff that you've seen us

1447
01:05:17,449 --> 01:05:22,879
using like ResNet and res next none of

1448
01:05:20,059 --> 01:05:26,048
them use very large fully connected

1449
01:05:22,880 --> 01:05:30,318
layers you know you had a question

1450
01:05:26,048 --> 01:05:32,210
sorry yeah come on um so could you tell

1451
01:05:30,318 --> 01:05:34,818
us more about for example if we had like

1452
01:05:32,210 --> 01:05:40,280
three channels for the input what would

1453
01:05:34,818 --> 01:05:42,619
be the shape yeah these filters right so

1454
01:05:40,280 --> 01:05:44,960
that's a great question so if we have 3

1455
01:05:42,619 --> 01:05:48,950
channels of input it would look exactly

1456
01:05:44,960 --> 01:05:52,159
like conv one right cons one kind of has

1457
01:05:48,949 --> 01:05:54,588
two channels right and so you can see

1458
01:05:52,159 --> 01:05:57,440
with cons one we had two channels so

1459
01:05:54,588 --> 01:06:00,288
therefore our filters had to have like

1460
01:05:57,440 --> 01:06:02,989
two channels per filter and so you could

1461
01:06:00,289 --> 01:06:05,210
like imagine that this input didn't

1462
01:06:02,989 --> 01:06:06,949
exist you know and actually this was the

1463
01:06:05,210 --> 01:06:09,349
airport alright so when you have a

1464
01:06:06,949 --> 01:06:11,748
multi-channel input it just means that

1465
01:06:09,349 --> 01:06:15,410
your filters look like this and so

1466
01:06:11,748 --> 01:06:17,778
images often full color they have three

1467
01:06:15,409 --> 01:06:20,179
red green and blue sometimes they also

1468
01:06:17,778 --> 01:06:23,059
have an alpha Channel so however many

1469
01:06:20,179 --> 01:06:25,639
you have that's how many inputs you need

1470
01:06:23,059 --> 01:06:27,528
and so something which I know Jeanette

1471
01:06:25,639 --> 01:06:32,118
was playing with recently was like using

1472
01:06:27,528 --> 01:06:34,099
a full color image net model in medical

1473
01:06:32,119 --> 01:06:36,170
imaging for something called bone age

1474
01:06:34,099 --> 01:06:38,499
calculations which has a single channel

1475
01:06:36,170 --> 01:06:42,559
and so what she did was basically take

1476
01:06:38,498 --> 01:06:45,709
the the input the the single channel

1477
01:06:42,559 --> 01:06:48,619
input and make three copies of it so you

1478
01:06:45,710 --> 01:06:52,838
end up with basically like one two three

1479
01:06:48,619 --> 01:06:55,460
versions of the same thing which is like

1480
01:06:52,838 --> 01:06:57,679
it's kind of a small idea like it's kind

1481
01:06:55,460 --> 01:06:59,630
of redundant information that we don't

1482
01:06:57,679 --> 01:07:01,969
quite want but it does mean that then if

1483
01:06:59,630 --> 01:07:05,960
you had a something that expected a

1484
01:07:01,969 --> 01:07:08,659
three channel convolutional filter you

1485
01:07:05,960 --> 01:07:11,420
can use it right and so at the moment

1486
01:07:08,659 --> 01:07:15,679
there's a cable competition for iceberg

1487
01:07:11,420 --> 01:07:17,659
detection using a some funky satellite

1488
01:07:15,679 --> 01:07:20,899
specific data format that has two

1489
01:07:17,659 --> 01:07:23,719
channels so here's how you could do that

1490
01:07:20,900 --> 01:07:25,369
you could either copy one of those two

1491
01:07:23,719 --> 01:07:27,380
channels into the third channel or I

1492
01:07:25,369 --> 01:07:30,920
think what people in Carroll are doing

1493
01:07:27,380 --> 01:07:32,778
is to take the average of the two again

1494
01:07:30,920 --> 01:07:37,460
it's not ideal but it's a way that you

1495
01:07:32,778 --> 01:07:40,909
can use pre-trained networks yeah I've

1496
01:07:37,460 --> 01:07:42,650
done a lot of fiddling around like that

1497
01:07:40,909 --> 01:07:45,440
you can also actually I've actually done

1498
01:07:42,650 --> 01:07:47,568
things where I wanted to use a three

1499
01:07:45,440 --> 01:07:48,409
channel image net Network on four

1500
01:07:47,568 --> 01:07:50,329
channel data

1501
01:07:48,409 --> 01:07:53,028
I had a satellite data where the fourth

1502
01:07:50,329 --> 01:07:59,269
channel was near-infrared and so

1503
01:07:53,028 --> 01:08:02,239
basically I added an extra kind of level

1504
01:07:59,268 --> 01:08:04,909
to my convolutional kernels that were

1505
01:08:02,239 --> 01:08:06,889
all zeros and so basically like started

1506
01:08:04,909 --> 01:08:08,319
off by ignoring the near-infrared band

1507
01:08:06,889 --> 01:08:10,299
and

1508
01:08:08,320 --> 01:08:13,480
so what happens it basically and you'll

1509
01:08:10,300 --> 01:08:15,940
see this next week is that rather than

1510
01:08:13,480 --> 01:08:17,770
having these like carefully trained

1511
01:08:15,940 --> 01:08:19,869
filters when you're actually training

1512
01:08:17,770 --> 01:08:21,450
something from scratch we're actually

1513
01:08:19,869 --> 01:08:23,229
going to start with random numbers

1514
01:08:21,449 --> 01:08:25,179
that's actually what we do we actually

1515
01:08:23,229 --> 01:08:26,469
start with random numbers and then we

1516
01:08:25,180 --> 01:08:27,909
use this thing called stochastic

1517
01:08:26,470 --> 01:08:29,050
gradient descent which we've kind of

1518
01:08:27,909 --> 01:08:31,479
seen conceptually

1519
01:08:29,050 --> 01:08:33,070
to slightly improve those random numbers

1520
01:08:31,479 --> 01:08:35,349
to make them less random and we

1521
01:08:33,069 --> 01:08:37,599
basically do that again and again and

1522
01:08:35,350 --> 01:08:39,760
again okay great

1523
01:08:37,600 --> 01:08:48,700
let's take a seven minute break and

1524
01:08:39,760 --> 01:08:50,310
we'll come back at 7:50 all right so

1525
01:08:48,699 --> 01:08:57,579
what happens next

1526
01:08:50,310 --> 01:09:00,339
so we've got as far as doing a fully

1527
01:08:57,579 --> 01:09:02,289
connected layer right so we had our the

1528
01:09:00,338 --> 01:09:04,239
results of our max pooling layer got fed

1529
01:09:02,289 --> 01:09:07,180
to a fully connected layer and he might

1530
01:09:04,239 --> 01:09:09,338
notice those of you that remember your

1531
01:09:07,180 --> 01:09:11,770
linear algebra the fully connected layer

1532
01:09:09,338 --> 01:09:14,949
is actually doing a classic traditional

1533
01:09:11,770 --> 01:09:17,920
matrix product okay so it's basically

1534
01:09:14,949 --> 01:09:19,479
just going through each pair in turn

1535
01:09:17,920 --> 01:09:23,970
multiplying them together and then

1536
01:09:19,479 --> 01:09:33,838
adding them up to do a matrix product

1537
01:09:23,970 --> 01:09:36,460
now in practice if we want to calculate

1538
01:09:33,838 --> 01:09:40,869
which one of the ten digits we're

1539
01:09:36,460 --> 01:09:44,739
looking at their single number we've

1540
01:09:40,869 --> 01:09:48,130
calculated isn't enough we would

1541
01:09:44,739 --> 01:09:50,369
actually calculate ten numbers so what

1542
01:09:48,130 --> 01:09:55,239
we will have is rather than just having

1543
01:09:50,369 --> 01:09:57,130
one set of fully connected weights like

1544
01:09:55,239 --> 01:10:00,369
this and I say set because remember

1545
01:09:57,130 --> 01:10:04,029
there's like a whole 3d kind of tensor

1546
01:10:00,369 --> 01:10:06,789
of them we would actually need ten of

1547
01:10:04,029 --> 01:10:09,789
those right so you can see that these

1548
01:10:06,789 --> 01:10:12,250
tensors start to get a little bit high

1549
01:10:09,789 --> 01:10:14,680
dimensional right and so this is where

1550
01:10:12,250 --> 01:10:17,140
my patients we're doing it next cell ran

1551
01:10:14,680 --> 01:10:19,810
out but imagine that I had done this ten

1552
01:10:17,140 --> 01:10:21,499
times I could now have ten different

1553
01:10:19,810 --> 01:10:23,150
numbers or being calculated

1554
01:10:21,498 --> 01:10:27,109
yeah using exactly the same process

1555
01:10:23,149 --> 01:10:35,679
right we'll just be ten of these fully

1556
01:10:27,109 --> 01:10:40,189
connected to by m-by-n erased basically

1557
01:10:35,679 --> 01:10:44,359
and so then we would have ten numbers

1558
01:10:40,189 --> 01:10:48,189
being spat out so what happens next so

1559
01:10:44,359 --> 01:10:52,069
next up we can open up a different Excel

1560
01:10:48,189 --> 01:10:56,538
worksheet entropy example dot XLS that's

1561
01:10:52,069 --> 01:10:59,210
got two different worksheets one of them

1562
01:10:56,538 --> 01:11:02,029
is called soft mass and what happens

1563
01:10:59,210 --> 01:11:04,399
here sorry I've changed domains rather

1564
01:11:02,029 --> 01:11:06,198
than predicting whether it's the number

1565
01:11:04,399 --> 01:11:07,638
from one not to nine I'm going to

1566
01:11:06,198 --> 01:11:10,819
predict whether something is a cat a dog

1567
01:11:07,639 --> 01:11:14,179
a plane of Fisher Building okay so out

1568
01:11:10,819 --> 01:11:16,988
of our that fully connected layer we've

1569
01:11:14,179 --> 01:11:20,389
got this case we'd have five numbers and

1570
01:11:16,988 --> 01:11:23,149
notice at this point there's no rail you

1571
01:11:20,389 --> 01:11:27,498
okay in the last layer there's no rail

1572
01:11:23,149 --> 01:11:32,299
you okay so I can have negatives so I

1573
01:11:27,498 --> 01:11:34,429
want to turn these five numbers H into a

1574
01:11:32,300 --> 01:11:37,038
probability I want to turn it into a

1575
01:11:34,429 --> 01:11:37,699
probability from naught to one that it's

1576
01:11:37,038 --> 01:11:40,340
a cat

1577
01:11:37,698 --> 01:11:42,948
that's a dog there's a plane that it's a

1578
01:11:40,340 --> 01:11:44,479
fish that it's a building and I want

1579
01:11:42,948 --> 01:11:45,978
those probabilities to have a couple of

1580
01:11:44,479 --> 01:11:48,249
characteristics first is that each of

1581
01:11:45,979 --> 01:11:50,510
them should be between zero and one and

1582
01:11:48,248 --> 01:11:52,550
the second is that this state together

1583
01:11:50,510 --> 01:11:55,969
should add up to one right it's

1584
01:11:52,550 --> 01:11:58,219
definitely one of these five things okay

1585
01:11:55,969 --> 01:12:01,399
so to do that we use a different kind of

1586
01:11:58,219 --> 01:12:04,460
activation function what's an activation

1587
01:12:01,399 --> 01:12:07,448
function an activation function is a

1588
01:12:04,460 --> 01:12:13,399
function that is applied to activations

1589
01:12:07,448 --> 01:12:16,339
so for example max 0 comma something is

1590
01:12:13,399 --> 01:12:18,888
a function that I applied to an

1591
01:12:16,340 --> 01:12:23,630
activation so an activation function

1592
01:12:18,889 --> 01:12:27,800
always takes in one number and spits out

1593
01:12:23,630 --> 01:12:29,779
one number so max of 0 comma X takes in

1594
01:12:27,800 --> 01:12:32,920
a number X and spits out some different

1595
01:12:29,779 --> 01:12:32,920
number value of s

1596
01:12:33,069 --> 01:12:39,259
that's all an activation function is and

1597
01:12:35,779 --> 01:12:44,229
if you remember back to that PowerPoint

1598
01:12:39,260 --> 01:12:44,230
we saw in Lesson one

1599
01:12:45,590 --> 01:12:55,230
each of our layers was just a linear

1600
01:12:50,539 --> 01:12:58,618
function and then after every layer we

1601
01:12:55,229 --> 01:13:01,559
said we needed some non-linearity act as

1602
01:12:58,618 --> 01:13:04,500
if you stack a bunch of linear layers

1603
01:13:01,560 --> 01:13:06,810
together right then all you end up with

1604
01:13:04,500 --> 01:13:09,840
is a linear layer okay

1605
01:13:06,810 --> 01:13:13,340
so somebody's talking can can you not a

1606
01:13:09,840 --> 01:13:13,340
slow just acting thank you

1607
01:13:13,828 --> 01:13:18,210
if you stack a number of linear

1608
01:13:15,979 --> 01:13:20,729
functions together you just end up with

1609
01:13:18,210 --> 01:13:22,139
a linear function and nobody does any

1610
01:13:20,729 --> 01:13:24,119
cool deep learning with displaying your

1611
01:13:22,139 --> 01:13:29,179
functions right but remember we also

1612
01:13:24,118 --> 01:13:33,089
learnt that by stacking linear functions

1613
01:13:29,179 --> 01:13:34,769
with between each one a non-linearity we

1614
01:13:33,090 --> 01:13:37,349
could create like arbitrarily complex

1615
01:13:34,769 --> 01:13:39,599
shapes and so the non-linearity that

1616
01:13:37,349 --> 01:13:42,979
we're using after every hidden layer is

1617
01:13:39,599 --> 01:13:45,569
a rally rectified linear unit a

1618
01:13:42,979 --> 01:13:48,510
non-linearity is an activation function

1619
01:13:45,569 --> 01:13:51,268
an activation function is a

1620
01:13:48,510 --> 01:13:52,619
non-linearity in with in deep way

1621
01:13:51,269 --> 01:13:54,748
obviously there's lots of other

1622
01:13:52,618 --> 01:13:58,109
nonlinearities and in the world but in

1623
01:13:54,748 --> 01:14:00,238
deep learning this is what we mean so an

1624
01:13:58,109 --> 01:14:02,998
activation function is any function that

1625
01:14:00,238 --> 01:14:04,978
takes some activation in as a single

1626
01:14:02,998 --> 01:14:08,969
number and spits out some new activation

1627
01:14:04,979 --> 01:14:10,320
like max of 0 comma so I'm now going to

1628
01:14:08,969 --> 01:14:12,300
tell you about a different activation

1629
01:14:10,319 --> 01:14:16,170
function it's slightly more complicated

1630
01:14:12,300 --> 01:14:19,288
than value but not too much it's called

1631
01:14:16,170 --> 01:14:22,139
soft max soft max only ever occurs in

1632
01:14:19,288 --> 01:14:24,868
the final layer at the very end and the

1633
01:14:22,139 --> 01:14:27,900
reason why is that soft max always spits

1634
01:14:24,868 --> 01:14:29,549
out numbers as an activation function

1635
01:14:27,899 --> 01:14:32,848
that always spits out a number between

1636
01:14:29,550 --> 01:14:35,729
Norton 1 and it always spits out a bunch

1637
01:14:32,849 --> 01:14:39,889
of numbers that add to 1 so a soft max

1638
01:14:35,729 --> 01:14:43,289
gives us what we want right in theory

1639
01:14:39,889 --> 01:14:46,139
this isn't strictly necessary right like

1640
01:14:43,288 --> 01:14:50,578
we could ask our neural net to learn a

1641
01:14:46,139 --> 01:14:53,489
set of kernels which have you know which

1642
01:14:50,578 --> 01:14:54,960
which give probabilities that line up as

1643
01:14:53,488 --> 01:14:57,598
closely as possible with what we want

1644
01:14:54,960 --> 01:14:59,939
but in general with deep learning if you

1645
01:14:57,599 --> 01:15:03,090
can construct your architecture so that

1646
01:14:59,939 --> 01:15:06,089
the desired characteristics are as easy

1647
01:15:03,090 --> 01:15:07,739
to express as possible you'll end up

1648
01:15:06,090 --> 01:15:10,170
with better models like they'll learn

1649
01:15:07,738 --> 01:15:13,138
more quickly with less parameters so in

1650
01:15:10,170 --> 01:15:16,199
this case we know that our probabilities

1651
01:15:13,139 --> 01:15:18,630
should end up being between 9 1 we know

1652
01:15:16,198 --> 01:15:20,189
that they should end up adding to 1 so

1653
01:15:18,630 --> 01:15:23,279
if we construct an activation function

1654
01:15:20,189 --> 01:15:24,868
which always has those features then

1655
01:15:23,279 --> 01:15:26,670
we're going to make our neural network

1656
01:15:24,868 --> 01:15:27,929
do a better job

1657
01:15:26,670 --> 01:15:29,609
it's gonna make it easier for it it

1658
01:15:27,930 --> 01:15:32,400
doesn't have to learn to do those things

1659
01:15:29,609 --> 01:15:37,469
because it all happen automatically okay

1660
01:15:32,399 --> 01:15:39,000
so in order to make this work we first

1661
01:15:37,470 --> 01:15:41,760
of all have to get rid of all of the

1662
01:15:39,000 --> 01:15:44,670
negatives right like we can't have

1663
01:15:41,760 --> 01:15:46,500
negative probabilities so to make things

1664
01:15:44,670 --> 01:15:49,680
not being negative one way we could do

1665
01:15:46,500 --> 01:15:51,420
it is just go into the pair of right so

1666
01:15:49,680 --> 01:15:55,950
here you can see my first step is to go

1667
01:15:51,420 --> 01:15:58,649
X of the previous one right and I think

1668
01:15:55,949 --> 01:16:01,079
I've mentioned this before but of all

1669
01:15:58,649 --> 01:16:03,449
the math that you just need to be super

1670
01:16:01,079 --> 01:16:05,489
familiar with to do deep learning the

1671
01:16:03,449 --> 01:16:08,670
one you really need is logarithms and

1672
01:16:05,489 --> 01:16:10,710
asks write all of deep learning and all

1673
01:16:08,670 --> 01:16:19,529
of machine learning they appear all the

1674
01:16:10,710 --> 01:16:24,930
time right so for example you absolutely

1675
01:16:19,529 --> 01:16:33,090
need to know that log of x times y

1676
01:16:24,930 --> 01:16:35,070
equals log of X plus log of Y all right

1677
01:16:33,090 --> 01:16:37,260
and like not just know that that's a

1678
01:16:35,069 --> 01:16:39,239
formula that exists but have a sense of

1679
01:16:37,260 --> 01:16:41,010
like what does that mean why is that

1680
01:16:39,239 --> 01:16:42,929
interesting oh I can turn

1681
01:16:41,010 --> 01:16:45,360
multiplications into additions that

1682
01:16:42,930 --> 01:16:50,820
could be really handy right and

1683
01:16:45,359 --> 01:16:56,159
therefore log of x over y equals log of

1684
01:16:50,819 --> 01:16:57,479
X minus log of Y again that's going to

1685
01:16:56,159 --> 01:16:59,849
come in pretty handy you know rather

1686
01:16:57,479 --> 01:17:03,389
than dividing I can just subtract things

1687
01:16:59,850 --> 01:17:08,039
right and also remember that if I've got

1688
01:17:03,390 --> 01:17:14,369
log of x equals y then that means a to

1689
01:17:08,039 --> 01:17:17,250
the y equals x in other words log log

1690
01:17:14,369 --> 01:17:21,449
and E to the for the inverse of each

1691
01:17:17,250 --> 01:17:23,550
other okay again you just you need to

1692
01:17:21,449 --> 01:17:25,139
really really understand these things

1693
01:17:23,550 --> 01:17:27,829
and like so if you if you haven't spent

1694
01:17:25,140 --> 01:17:31,650
much time with logs and X for a while

1695
01:17:27,829 --> 01:17:33,779
try plotting them in Excel or a notebook

1696
01:17:31,649 --> 01:17:35,670
have a sense of what shape they are how

1697
01:17:33,779 --> 01:17:38,259
they combine together just make sure

1698
01:17:35,670 --> 01:17:42,640
you're really comfortable with them so

1699
01:17:38,260 --> 01:17:44,590
we're using it here right we're using it

1700
01:17:42,640 --> 01:17:46,750
here so one of the things that we know

1701
01:17:44,590 --> 01:17:49,449
is a to the power of something is

1702
01:17:46,750 --> 01:17:50,890
positive okay so that's great

1703
01:17:49,449 --> 01:17:52,630
the other thing you'll notice about e to

1704
01:17:50,890 --> 01:17:56,289
the power of something is because it's a

1705
01:17:52,630 --> 01:17:58,210
power numbers that are slightly bigger

1706
01:17:56,289 --> 01:18:00,640
than other numbers like four is a little

1707
01:17:58,210 --> 01:18:02,710
bit bigger than 2.8 when you go e to the

1708
01:18:00,640 --> 01:18:04,119
power of it really accentuates that

1709
01:18:02,710 --> 01:18:05,319
difference okay

1710
01:18:04,119 --> 01:18:06,849
so we're going to take advantage of both

1711
01:18:05,319 --> 01:18:10,479
of these features for the purpose of

1712
01:18:06,850 --> 01:18:13,329
deep learning okay so we take our the

1713
01:18:10,479 --> 01:18:15,849
results of this fully connected layer we

1714
01:18:13,329 --> 01:18:24,130
go e to the power of for each of them

1715
01:18:15,850 --> 01:18:28,120
and then we're gonna yeah and then we're

1716
01:18:24,130 --> 01:18:32,340
going to add them up okay so here is the

1717
01:18:28,119 --> 01:18:35,590
sum of e to the power of so then here

1718
01:18:32,340 --> 01:18:37,829
we're going to take e to the power of

1719
01:18:35,590 --> 01:18:41,079
divided by the sum of e to the power of

1720
01:18:37,829 --> 01:18:43,859
so if you take all of these things

1721
01:18:41,079 --> 01:18:49,140
divided by their sum then by definition

1722
01:18:43,859 --> 01:18:51,460
all of those things must add up to 1 and

1723
01:18:49,140 --> 01:18:54,579
furthermore since we're dividing by

1724
01:18:51,460 --> 01:18:56,890
their sum they must always vary between

1725
01:18:54,579 --> 01:18:59,590
0 and 1 because they were always

1726
01:18:56,890 --> 01:19:04,840
positive alright and that's it so that's

1727
01:18:59,590 --> 01:19:08,470
what softmax is ok so I've got this kind

1728
01:19:04,840 --> 01:19:11,050
of doing random numbers each time right

1729
01:19:08,470 --> 01:19:14,770
and so you can see like as I as I look

1730
01:19:11,050 --> 01:19:16,510
through my softmax generally has quite a

1731
01:19:14,770 --> 01:19:19,090
few things that are so close to zero

1732
01:19:16,510 --> 01:19:20,739
that they round down to zero and you

1733
01:19:19,090 --> 01:19:23,110
know maybe one thing that's nearly 1

1734
01:19:20,738 --> 01:19:25,319
right and the reason for that is what we

1735
01:19:23,109 --> 01:19:28,420
just talked about that is with the X

1736
01:19:25,319 --> 01:19:30,309
just having one number a bit bigger than

1737
01:19:28,420 --> 01:19:33,100
the others tends to like push it out

1738
01:19:30,310 --> 01:19:35,440
further right so even though my inputs

1739
01:19:33,100 --> 01:19:39,190
here around on numbers between negative

1740
01:19:35,439 --> 01:19:41,139
5 and 5 right my outputs from the

1741
01:19:39,189 --> 01:19:44,769
softmax don't really look that random at

1742
01:19:41,140 --> 01:19:47,140
all in the sense that they tend to have

1743
01:19:44,770 --> 01:19:50,650
one big number and a bunch of small

1744
01:19:47,140 --> 01:19:53,680
numbers and now that's what we want

1745
01:19:50,649 --> 01:19:55,599
right we want to say like in terms of

1746
01:19:53,680 --> 01:19:57,700
like is this a cat a dog a plane a fish

1747
01:19:55,600 --> 01:20:00,100
or a building we really want it to say

1748
01:19:57,699 --> 01:20:03,880
like it's it's that you know it's it's a

1749
01:20:00,100 --> 01:20:07,510
dog or it's a plane not like I don't

1750
01:20:03,880 --> 01:20:10,029
know okay so softmax has lots of these

1751
01:20:07,510 --> 01:20:12,610
cool properties right it's going to

1752
01:20:10,029 --> 01:20:14,979
return a probability that adds up to 1

1753
01:20:12,609 --> 01:20:20,259
and it's going to tend to want to pick

1754
01:20:14,979 --> 01:20:22,599
one thing particularly strongly okay so

1755
01:20:20,260 --> 01:20:29,230
that's soft mess your net could you pass

1756
01:20:22,600 --> 01:20:31,180
actually bust me up we how would we do

1757
01:20:29,229 --> 01:20:32,439
something that as let's say you have any

1758
01:20:31,180 --> 01:20:34,780
imaging you want to count in categorize

1759
01:20:32,439 --> 01:20:37,599
I was like cat and the dog or like has

1760
01:20:34,779 --> 01:20:40,149
multiple things but what kind of

1761
01:20:37,600 --> 01:20:43,230
function will we try to use so happens

1762
01:20:40,149 --> 01:20:43,229
we're going to do that right now so

1763
01:20:45,418 --> 01:20:48,840
so hope you think about why we might

1764
01:20:47,099 --> 01:20:50,069
want to do that and so runways where you

1765
01:20:48,840 --> 01:20:53,010
might want to do that is to do

1766
01:20:50,069 --> 01:20:54,988
multi-label classification so we're

1767
01:20:53,010 --> 01:20:57,119
looking now at listen to image models

1768
01:20:54,988 --> 01:21:00,418
and specifically we're going to take a

1769
01:20:57,118 --> 01:21:04,139
look at the planet competition satellite

1770
01:21:00,418 --> 01:21:08,189
imaging competition now the satellite

1771
01:21:04,139 --> 01:21:09,840
imaging competition has some

1772
01:21:08,189 --> 01:21:13,229
similarities to stuff we've seen before

1773
01:21:09,840 --> 01:21:15,860
right so before we've seen cat versus

1774
01:21:13,229 --> 01:21:19,769
dog and these images are a cat or a dog

1775
01:21:15,859 --> 01:21:22,549
they're not Maya they're not both right

1776
01:21:19,769 --> 01:21:25,559
but the satellite imaging competition

1777
01:21:22,550 --> 01:21:27,869
has stayed as images that look like this

1778
01:21:25,559 --> 01:21:30,389
and in fact every single one of the

1779
01:21:27,868 --> 01:21:31,529
images is classified by whether there's

1780
01:21:30,389 --> 01:21:34,260
four kinds of weather

1781
01:21:31,529 --> 01:21:37,288
one of which is haze and another of

1782
01:21:34,260 --> 01:21:39,719
which is clear in addition to which

1783
01:21:37,288 --> 01:21:42,478
there is a list of features that may be

1784
01:21:39,719 --> 01:21:44,880
present including agriculture which is

1785
01:21:42,479 --> 01:21:48,269
like some some cleared area used for

1786
01:21:44,880 --> 01:21:50,998
agriculture primary which means primary

1787
01:21:48,269 --> 01:21:54,019
rainforest and water which means a river

1788
01:21:50,998 --> 01:21:56,969
or a creek so here is a clear day

1789
01:21:54,019 --> 01:21:59,579
satellite image showing some agriculture

1790
01:21:56,969 --> 01:22:02,550
some primary rainforest and some water

1791
01:21:59,578 --> 01:22:07,078
features and here's one which is in haze

1792
01:22:02,550 --> 01:22:09,840
and is entirely primary rainforest so in

1793
01:22:07,078 --> 01:22:13,228
this case we're going to want to be able

1794
01:22:09,840 --> 01:22:15,689
to show we're going to predict multiple

1795
01:22:13,229 --> 01:22:19,439
things and so softmax wouldn't be good

1796
01:22:15,689 --> 01:22:21,749
because softmax doesn't like predicting

1797
01:22:19,439 --> 01:22:23,610
multiple things and like I would

1798
01:22:21,748 --> 01:22:25,859
definitely recommend anthropomorphizing

1799
01:22:23,609 --> 01:22:28,078
your activation functions right they

1800
01:22:25,859 --> 01:22:30,868
have personalities okay and the

1801
01:22:28,078 --> 01:22:34,558
personality of the softmax is it wants

1802
01:22:30,868 --> 01:22:36,679
to pick a thing and people forget this

1803
01:22:34,559 --> 01:22:39,418
all the time I've seen many people even

1804
01:22:36,679 --> 01:22:44,189
well-regarded researchers in famous

1805
01:22:39,418 --> 01:22:46,229
academic papers using like soft maps for

1806
01:22:44,189 --> 01:22:46,860
multi-label classification it happens

1807
01:22:46,229 --> 01:22:49,590
all the time

1808
01:22:46,859 --> 01:22:52,408
right and it's kind of ridiculous

1809
01:22:49,590 --> 01:22:55,248
because they're not understanding the

1810
01:22:52,408 --> 01:22:58,000
personality of their activation function

1811
01:22:55,248 --> 01:23:00,550
so for multi

1812
01:22:58,000 --> 01:23:03,189
classification where each sample can

1813
01:23:00,550 --> 01:23:05,590
belong to one or more classes we have to

1814
01:23:03,189 --> 01:23:05,919
change a few things but here's the good

1815
01:23:05,590 --> 01:23:08,560
news

1816
01:23:05,920 --> 01:23:12,279
in fast AI we don't have to change

1817
01:23:08,560 --> 01:23:15,940
anything right so fast AI will look at

1818
01:23:12,279 --> 01:23:21,880
the labels in the CSV and if there is

1819
01:23:15,939 --> 01:23:23,710
more than one label ever for any item it

1820
01:23:21,880 --> 01:23:25,600
will automatically switch into like

1821
01:23:23,710 --> 01:23:27,670
multi-label mode so I'm going to show

1822
01:23:25,600 --> 01:23:29,770
you how it works behind the scenes but

1823
01:23:27,670 --> 01:23:35,949
the good news is you don't actually have

1824
01:23:29,770 --> 01:23:40,690
to care it happens anywhere so if you

1825
01:23:35,949 --> 01:23:43,149
have multi label images multi label

1826
01:23:40,689 --> 01:23:46,000
objects you obviously can't use the

1827
01:23:43,149 --> 01:23:48,369
classic Kerris style approach where

1828
01:23:46,000 --> 01:23:51,430
things are in folders because something

1829
01:23:48,369 --> 01:23:55,180
can't conveniently be in multiple

1830
01:23:51,430 --> 01:23:57,610
folders at the same time right so that's

1831
01:23:55,180 --> 01:24:04,050
why we you basically have to use the

1832
01:23:57,609 --> 01:24:06,179
from CSV approach right so if we look at

1833
01:24:04,050 --> 01:24:12,159
[Music]

1834
01:24:06,180 --> 01:24:14,350
an example actually I'll show you I tend

1835
01:24:12,159 --> 01:24:16,420
to take you through it right so we can

1836
01:24:14,350 --> 01:24:18,970
say okay this is the CSV file containing

1837
01:24:16,420 --> 01:24:21,609
our labels this looks exactly the same

1838
01:24:18,970 --> 01:24:24,699
as I did before but rather than side on

1839
01:24:21,609 --> 01:24:27,219
its top down alright and top down I've

1840
01:24:24,699 --> 01:24:28,809
mentioned before that can do our

1841
01:24:27,220 --> 01:24:29,800
vertical flips it actually does more

1842
01:24:28,810 --> 01:24:32,110
than that there's actually eight

1843
01:24:29,800 --> 01:24:35,140
possible symmetries for a square which

1844
01:24:32,109 --> 01:24:37,659
is it can be rotated through 90 180 270

1845
01:24:35,140 --> 01:24:39,670
or 0 degrees and for each of those it

1846
01:24:37,659 --> 01:24:42,130
can be flipped and if you think about it

1847
01:24:39,670 --> 01:24:45,069
for awhile you'll realize that that's a

1848
01:24:42,130 --> 01:24:48,430
complete enumeration of everything that

1849
01:24:45,069 --> 01:24:50,619
you can do in terms of symmetries to a

1850
01:24:48,430 --> 01:24:53,530
square so they're called it's called the

1851
01:24:50,619 --> 01:24:55,630
dihedral group of eight so if you see in

1852
01:24:53,529 --> 01:24:59,619
the code there's actually a transform or

1853
01:24:55,630 --> 01:25:01,690
dihedral that's why it's called that so

1854
01:24:59,619 --> 01:25:05,670
this transforms will basically do the

1855
01:25:01,689 --> 01:25:09,279
full set of eight symmetric dihedral

1856
01:25:05,670 --> 01:25:11,130
rotations and flips plus everything

1857
01:25:09,279 --> 01:25:13,859
which we can do to dogs and cats

1858
01:25:11,130 --> 01:25:16,289
you know small clinical rotations a

1859
01:25:13,859 --> 01:25:18,809
little bit of zooming a little bit of

1860
01:25:16,289 --> 01:25:23,340
contrast and brightness adjustment so

1861
01:25:18,810 --> 01:25:24,870
these images are a size 256 by 256 so I

1862
01:25:23,340 --> 01:25:28,440
just create a little function here to

1863
01:25:24,869 --> 01:25:32,429
let me quickly grab you know data loader

1864
01:25:28,439 --> 01:25:36,178
of any size so here's a 256 by 256 once

1865
01:25:32,429 --> 01:25:37,649
you've got a data object inside it we've

1866
01:25:36,179 --> 01:25:41,399
already seen that there's things called

1867
01:25:37,649 --> 01:25:43,229
Val D s test D s train D s there are

1868
01:25:41,399 --> 01:25:45,509
things that you can just index into and

1869
01:25:43,229 --> 01:25:48,359
grab a particular image so you just use

1870
01:25:45,510 --> 01:25:50,429
square brackets 0 you'll also see that

1871
01:25:48,359 --> 01:25:53,309
all of those things have a DL that's a

1872
01:25:50,429 --> 01:25:55,440
data loader so des is data set DL is

1873
01:25:53,310 --> 01:25:58,080
data motor these are concepts from PI

1874
01:25:55,439 --> 01:26:00,299
watch so if you Google PI torch data set

1875
01:25:58,079 --> 01:26:02,488
or pipe watch data loader you can

1876
01:26:00,300 --> 01:26:04,650
basically see what it means but the

1877
01:26:02,488 --> 01:26:08,428
basic idea is a data set gives you a

1878
01:26:04,649 --> 01:26:10,349
single image or a single object back a

1879
01:26:08,429 --> 01:26:12,899
data loader gives you back a mini batch

1880
01:26:10,350 --> 01:26:15,989
and specifically it gives you back a

1881
01:26:12,899 --> 01:26:20,219
transformed mini - so that's why when we

1882
01:26:15,988 --> 01:26:24,029
create our data object we can pass in

1883
01:26:20,219 --> 01:26:25,560
num workers and transforms it's like how

1884
01:26:24,029 --> 01:26:28,198
many processes do you want to use what

1885
01:26:25,560 --> 01:26:30,480
transforms do you want and so with with

1886
01:26:28,198 --> 01:26:32,158
a data loader you can't ask for an

1887
01:26:30,479 --> 01:26:35,158
individual image you can only get back

1888
01:26:32,158 --> 01:26:37,259
at a mini batch and you can't get back a

1889
01:26:35,158 --> 01:26:39,750
particular mini batch you can only get

1890
01:26:37,260 --> 01:26:42,719
back the next mini - so something

1891
01:26:39,750 --> 01:26:46,560
reverses look through grabbing a mini

1892
01:26:42,719 --> 01:26:47,880
batch at a time and so in Python the

1893
01:26:46,560 --> 01:26:50,760
thing that does that is called a

1894
01:26:47,880 --> 01:26:51,989
generator right or an iterator this

1895
01:26:50,760 --> 01:26:54,480
slightly different versions are the same

1896
01:26:51,988 --> 01:26:56,488
thing so to turn a data loader into an

1897
01:26:54,479 --> 01:26:57,479
iterator you use the standard Python

1898
01:26:56,488 --> 01:27:00,029
function cordetta

1899
01:26:57,479 --> 01:27:02,549
that's a Python function just a regular

1900
01:27:00,029 --> 01:27:04,710
part of the Python basic language that

1901
01:27:02,550 --> 01:27:07,350
returns you an iterator and an iterator

1902
01:27:04,710 --> 01:27:08,908
is something that takes you can pass the

1903
01:27:07,350 --> 01:27:13,289
static give pass it to the standard

1904
01:27:08,908 --> 01:27:15,839
Python function or statement next and

1905
01:27:13,289 --> 01:27:18,800
that just says give me another batch

1906
01:27:15,840 --> 01:27:21,000
from this iterator

1907
01:27:18,800 --> 01:27:23,070
so we're basically this is one of the

1908
01:27:21,000 --> 01:27:24,828
things I really like about PI torch is

1909
01:27:23,069 --> 01:27:28,759
it really leverages

1910
01:27:24,828 --> 01:27:31,488
modern pythons kind of stuff you know in

1911
01:27:28,760 --> 01:27:35,090
in tensorflow they invent their whole

1912
01:27:31,488 --> 01:27:38,328
new world earth ways of doing things and

1913
01:27:35,090 --> 01:27:40,039
so it's kind of more in a sense it's

1914
01:27:38,328 --> 01:27:42,349
more like cross-platform but in another

1915
01:27:40,039 --> 01:27:46,429
sense like it's not a good fit to any

1916
01:27:42,349 --> 01:27:49,880
platform so it's nice if you if you know

1917
01:27:46,429 --> 01:27:52,069
Python well PI torch comes very

1918
01:27:49,880 --> 01:27:53,960
naturally if you don't know Python well

1919
01:27:52,069 --> 01:27:58,368
PI torches are good reason to learn

1920
01:27:53,960 --> 01:28:00,559
Python well a PI torch your module

1921
01:27:58,368 --> 01:28:03,738
neural network module is a standard

1922
01:28:00,559 --> 01:28:05,690
Python bus for example so any work you

1923
01:28:03,738 --> 01:28:08,089
put into learning Python better will pay

1924
01:28:05,689 --> 01:28:12,049
off with paid watch so here I am using

1925
01:28:08,090 --> 01:28:15,619
standard Python iterators and next to

1926
01:28:12,050 --> 01:28:17,779
grab my next mini batch from the

1927
01:28:15,618 --> 01:28:19,158
validation sets data loader and that's

1928
01:28:17,779 --> 01:28:20,960
going to return two things it's going to

1929
01:28:19,158 --> 01:28:23,779
return the images in the mini batch and

1930
01:28:20,960 --> 01:28:25,359
the labels of the mini - so standard

1931
01:28:23,779 --> 01:28:30,738
Python approach I can pull them apart

1932
01:28:25,359 --> 01:28:34,098
like so and so here is one mini batch of

1933
01:28:30,738 --> 01:28:37,669
labels and so not surprisingly since I

1934
01:28:34,099 --> 01:28:40,239
said that my batch size let's go ahead

1935
01:28:37,670 --> 01:28:40,239
and find it

1936
01:28:41,569 --> 01:28:47,840
Oh actually it's the batch size by

1937
01:28:44,389 --> 01:28:50,000
default is 64 so I didn't pass in a

1938
01:28:47,840 --> 01:28:52,099
batch size and so just remember shift

1939
01:28:50,000 --> 01:28:54,439
tab to see like what are the things you

1940
01:28:52,099 --> 01:28:57,710
can pass and what are the defaults so by

1941
01:28:54,439 --> 01:29:01,819
default my batch size is 64 so I've got

1942
01:28:57,710 --> 01:29:07,039
that something of size 64 by 17 so there

1943
01:29:01,819 --> 01:29:12,500
are 17 of the possible classes right so

1944
01:29:07,039 --> 01:29:15,828
let's take a look at the zeroth set of

1945
01:29:12,500 --> 01:29:18,349
labels so the zeroth images labels so I

1946
01:29:15,828 --> 01:29:21,558
can zip again standard Python things it

1947
01:29:18,349 --> 01:29:23,150
takes two lists and combines it so you

1948
01:29:21,559 --> 01:29:25,489
get the zero theme from the first list

1949
01:29:23,149 --> 01:29:27,439
as you're asking for the second list and

1950
01:29:25,488 --> 01:29:29,209
the first thing for the first first this

1951
01:29:27,439 --> 01:29:31,189
first thing from the second list and so

1952
01:29:29,210 --> 01:29:34,250
forth so I can zip them together and

1953
01:29:31,189 --> 01:29:36,789
that way I can find out for the zeroth

1954
01:29:34,250 --> 01:29:38,368
image and the validation set is

1955
01:29:36,789 --> 01:29:42,599
agriculture

1956
01:29:38,368 --> 01:29:47,279
it's clear its primary rainforest its

1957
01:29:42,599 --> 01:29:51,869
slash-and-burn its water okay so as you

1958
01:29:47,279 --> 01:29:53,219
can see here this is a MOLLE label you

1959
01:29:51,868 --> 01:29:57,559
see here's a way to do multi-label

1960
01:29:53,219 --> 01:30:01,079
classification so by the same token

1961
01:29:57,560 --> 01:30:03,179
right if we go back to our single label

1962
01:30:01,079 --> 01:30:07,229
classification it's a cat dog playing

1963
01:30:03,179 --> 01:30:08,579
official building behind the scenes we

1964
01:30:07,229 --> 01:30:12,598
haven't actually looked at it but behind

1965
01:30:08,579 --> 01:30:15,389
the scenes fast AI imply torch are

1966
01:30:12,599 --> 01:30:18,869
turning our labels into something called

1967
01:30:15,389 --> 01:30:22,940
one hot encoded labels and so if it was

1968
01:30:18,868 --> 01:30:25,979
actually a dog than the actual values

1969
01:30:22,939 --> 01:30:29,698
would be like that right so these are

1970
01:30:25,979 --> 01:30:32,218
like the actuals okay so do you remember

1971
01:30:29,698 --> 01:30:34,138
at the very end of at AV o--'s video he

1972
01:30:32,219 --> 01:30:36,929
showed how like the template had to

1973
01:30:34,139 --> 01:30:39,118
match to one of the like five ABCDE

1974
01:30:36,929 --> 01:30:42,989
templates and so what it's actually

1975
01:30:39,118 --> 01:30:44,488
doing is it's comparing when I said it's

1976
01:30:42,988 --> 01:30:46,079
basically doing a dot product it's

1977
01:30:44,488 --> 01:30:50,039
actually a fully connected layer at the

1978
01:30:46,079 --> 01:30:52,889
end right that calculates an output

1979
01:30:50,039 --> 01:30:56,939
activation that goes through a soft Max

1980
01:30:52,889 --> 01:30:59,730
and then the soft max is compared to the

1981
01:30:56,939 --> 01:31:03,539
one hot encoded label right so if it was

1982
01:30:59,729 --> 01:31:05,250
a dog there would be a one here and then

1983
01:31:03,539 --> 01:31:08,670
we take the difference between the

1984
01:31:05,250 --> 01:31:10,170
actuals and the softmax activation is to

1985
01:31:08,670 --> 01:31:12,179
say and add those add up those

1986
01:31:10,170 --> 01:31:14,489
differences to say how much error is

1987
01:31:12,179 --> 01:31:16,109
there essentially we're skipping over

1988
01:31:14,488 --> 01:31:17,488
something called a loss function that

1989
01:31:16,109 --> 01:31:21,319
we'll learn about next week but

1990
01:31:17,488 --> 01:31:21,319
essentially we're basically doing that

1991
01:31:21,680 --> 01:31:28,250
now if it's one hot encoded like there's

1992
01:31:25,609 --> 01:31:32,409
only one thing which have a 1 in it then

1993
01:31:28,250 --> 01:31:36,529
actually storing it as 0 1 0 0 0 is

1994
01:31:32,409 --> 01:31:38,269
terribly inefficient right like we could

1995
01:31:36,529 --> 01:31:40,699
basically say what are the index of each

1996
01:31:38,270 --> 01:31:44,890
of these things right so we can say it's

1997
01:31:40,699 --> 01:31:48,710
like 0 1 2 3 4 like so right and so

1998
01:31:44,890 --> 01:31:52,360
rather than storing it is 0 1 0 0 0

1999
01:31:48,710 --> 01:31:56,420
we actually just store the index value

2000
01:31:52,359 --> 01:31:58,849
right so if you look at the the Y values

2001
01:31:56,420 --> 01:32:01,159
for the cats and dogs competition or the

2002
01:31:58,850 --> 01:32:03,140
dog breeds competition you won't

2003
01:32:01,159 --> 01:32:04,849
actually see a big lists of ones and

2004
01:32:03,140 --> 01:32:07,520
zeros like this you'll see a single

2005
01:32:04,850 --> 01:32:11,680
integer right which is like what what

2006
01:32:07,520 --> 01:32:15,140
class index is it right and internally

2007
01:32:11,680 --> 01:32:17,750
inside pipe arch it will actually turn

2008
01:32:15,140 --> 01:32:19,360
that into a one hot encoded vector but

2009
01:32:17,750 --> 01:32:23,270
like you will literally never see it

2010
01:32:19,359 --> 01:32:26,089
okay and and pi torch has different loss

2011
01:32:23,270 --> 01:32:27,860
functions where you basically say this

2012
01:32:26,090 --> 01:32:30,050
thing's won this thing is one hot

2013
01:32:27,859 --> 01:32:31,809
encoder door this thing is not and it

2014
01:32:30,050 --> 01:32:34,159
uses different bus functions

2015
01:32:31,810 --> 01:32:36,500
that's all hidden by the faster I

2016
01:32:34,159 --> 01:32:40,039
library right so like you don't have to

2017
01:32:36,500 --> 01:32:42,640
worry about it but is but the the cool

2018
01:32:40,039 --> 01:32:45,350
thing to realize is that this approach

2019
01:32:42,640 --> 01:32:48,619
for multi-label encoding with these ones

2020
01:32:45,350 --> 01:32:51,079
and zeros behind the scenes the exact

2021
01:32:48,619 --> 01:32:56,210
same thing happens for single level

2022
01:32:51,079 --> 01:32:59,000
classification does it make sense to

2023
01:32:56,210 --> 01:33:00,920
change the beginners of the sigmoid of

2024
01:32:59,000 --> 01:33:07,939
the softmax function by changing the

2025
01:33:00,920 --> 01:33:19,449
base no because when you change the more

2026
01:33:07,939 --> 01:33:24,199
math log base a of B equals log B over

2027
01:33:19,449 --> 01:33:27,079
log a so changing the base is just a

2028
01:33:24,199 --> 01:33:29,670
linear scaling and linear scaling is

2029
01:33:27,079 --> 01:33:33,109
something which the neural net can

2030
01:33:29,670 --> 01:33:33,109
with very easily

2031
01:33:35,449 --> 01:33:44,250
good question okay so here is that image

2032
01:33:42,140 --> 01:33:47,850
right here is the image with

2033
01:33:44,250 --> 01:33:49,649
slash-and-burn water etc etc one of the

2034
01:33:47,850 --> 01:33:54,150
things to notice here is like when I

2035
01:33:49,649 --> 01:33:56,069
first displayed this image it was so

2036
01:33:54,149 --> 01:34:00,420
washed out I really couldn't see it

2037
01:33:56,069 --> 01:34:02,969
right but remember images now you know

2038
01:34:00,420 --> 01:34:04,890
we know images are just matrices of

2039
01:34:02,970 --> 01:34:08,670
numbers and so you can see here I just

2040
01:34:04,890 --> 01:34:11,579
said times 1.4 just to make it more

2041
01:34:08,670 --> 01:34:12,930
visible right so like now that you kind

2042
01:34:11,579 --> 01:34:14,819
of it's the kind of thing I want you to

2043
01:34:12,930 --> 01:34:15,840
get familiar with is the idea that this

2044
01:34:14,819 --> 01:34:18,059
stuff you're dealing with

2045
01:34:15,840 --> 01:34:19,710
they're just matrices of numbers and you

2046
01:34:18,060 --> 01:34:21,120
can fiddle around with them so if you're

2047
01:34:19,710 --> 01:34:22,829
looking at something like guys a bit

2048
01:34:21,119 --> 01:34:25,920
washed out you can just multiply it by

2049
01:34:22,829 --> 01:34:27,989
something to brighten it up a bit okay

2050
01:34:25,920 --> 01:34:30,510
so here we can see I guess this is the

2051
01:34:27,989 --> 01:34:33,300
slash-and-burn here's the river that's

2052
01:34:30,510 --> 01:34:35,400
the water here's the primary rainforest

2053
01:34:33,300 --> 01:34:41,010
maybe that's the agriculture so forth

2054
01:34:35,399 --> 01:34:44,299
okay so so you know with all that

2055
01:34:41,010 --> 01:34:46,350
background how do we actually use this

2056
01:34:44,300 --> 01:34:49,529
exactly the same way as everything we've

2057
01:34:46,350 --> 01:34:52,170
done before right so you know size and

2058
01:34:49,529 --> 01:34:54,269
and the interesting thing about playing

2059
01:34:52,170 --> 01:34:56,369
around with this planet competition is

2060
01:34:54,270 --> 01:35:00,480
that these images are not at all like

2061
01:34:56,369 --> 01:35:02,579
image there and I would guess that the

2062
01:35:00,479 --> 01:35:05,039
vast majority is of stuff that the vast

2063
01:35:02,579 --> 01:35:07,769
majority of you do involving

2064
01:35:05,039 --> 01:35:10,109
convolutional neural Nets won't actually

2065
01:35:07,770 --> 01:35:14,130
be anything like image net you know

2066
01:35:10,109 --> 01:35:15,869
it'll be it'll be medical imaging it'll

2067
01:35:14,130 --> 01:35:18,529
be like classifying different kinds of

2068
01:35:15,869 --> 01:35:21,300
steel tube or figuring out whether a

2069
01:35:18,529 --> 01:35:25,649
world you know is going to break or not

2070
01:35:21,300 --> 01:35:30,539
or or looking at satellite images or you

2071
01:35:25,649 --> 01:35:32,689
know whatever right so it's it's good to

2072
01:35:30,539 --> 01:35:35,189
experiment with stuff like this planet

2073
01:35:32,689 --> 01:35:37,169
competition to get a sense of kind of

2074
01:35:35,189 --> 01:35:40,439
what you want to do and so you'll see

2075
01:35:37,170 --> 01:35:44,909
here I start out by resizing my data to

2076
01:35:40,439 --> 01:35:46,409
64 by 64 it starts out at 256 by 256

2077
01:35:44,909 --> 01:35:48,269
right now

2078
01:35:46,409 --> 01:35:50,279
I wouldn't want to do this for the cats

2079
01:35:48,270 --> 01:35:52,620
and dogs competition because it cats end

2080
01:35:50,279 --> 01:35:54,359
on competition we start with a pre

2081
01:35:52,619 --> 01:35:57,029
trained imagenet Network it's it's

2082
01:35:54,359 --> 01:36:00,059
nearly isn't it starts off nearly

2083
01:35:57,029 --> 01:36:02,069
perfect right so if we resized

2084
01:36:00,060 --> 01:36:04,980
everything to 64 by 64 and then

2085
01:36:02,069 --> 01:36:07,439
retrained the whole set regular it we'd

2086
01:36:04,979 --> 01:36:09,149
basically destroy the weights that are

2087
01:36:07,439 --> 01:36:12,000
already pre trained to be very good

2088
01:36:09,149 --> 01:36:14,399
remember imagenet most imagenet models

2089
01:36:12,000 --> 01:36:17,760
are trained at either 224 by 224 or

2090
01:36:14,399 --> 01:36:20,219
$2.99 by 299 all right so if we like

2091
01:36:17,760 --> 01:36:23,480
retrain them at 64 by 64 we're going to

2092
01:36:20,220 --> 01:36:25,920
we're going to kill it on the other hand

2093
01:36:23,479 --> 01:36:28,049
there's nothing in image net that looks

2094
01:36:25,920 --> 01:36:31,260
anything like this you know there's no

2095
01:36:28,050 --> 01:36:36,029
satellite images so the only useful bits

2096
01:36:31,260 --> 01:36:39,869
of the image net Network for us are kind

2097
01:36:36,029 --> 01:36:42,809
of layers like this one you know finding

2098
01:36:39,869 --> 01:36:44,460
edges and gradients and this one you

2099
01:36:42,810 --> 01:36:48,090
know finding kind of textures and

2100
01:36:44,460 --> 01:36:49,890
repeating patterns and maybe these ones

2101
01:36:48,090 --> 01:36:52,550
are kind of finding more complex

2102
01:36:49,890 --> 01:36:57,530
textures but that's probably about it

2103
01:36:52,550 --> 01:37:00,210
right so so in other words you know

2104
01:36:57,529 --> 01:37:02,849
starting out by training very small

2105
01:37:00,210 --> 01:37:04,920
images works pretty well when you're

2106
01:37:02,850 --> 01:37:07,460
using stuff like satellites so in this

2107
01:37:04,920 --> 01:37:13,409
case I started right back at 64 by 64

2108
01:37:07,460 --> 01:37:15,779
grab some data built my model found out

2109
01:37:13,409 --> 01:37:17,279
what learning rate to use and

2110
01:37:15,779 --> 01:37:22,109
interestingly it turned out to be quite

2111
01:37:17,279 --> 01:37:25,920
high it seems that because like it's so

2112
01:37:22,109 --> 01:37:28,139
unlike imagenet I needed to do quite a

2113
01:37:25,920 --> 01:37:30,390
bit more fitting with just that last

2114
01:37:28,140 --> 01:37:33,510
layer before it started to flatten out

2115
01:37:30,390 --> 01:37:38,090
then I unfreeze dit and again this is

2116
01:37:33,510 --> 01:37:41,070
the difference to image net like

2117
01:37:38,090 --> 01:37:44,190
datasets is my learning rate in the

2118
01:37:41,069 --> 01:37:47,189
initial layer i set 2/9 the middle

2119
01:37:44,189 --> 01:37:48,929
layers I said 2/3 where else for stuff

2120
01:37:47,189 --> 01:37:52,409
like it's like image net I had a

2121
01:37:48,930 --> 01:37:54,780
multiple of 10 each of those you know

2122
01:37:52,409 --> 01:37:58,139
again the idea being that that earlier

2123
01:37:54,779 --> 01:38:00,500
layers probably and not as close to what

2124
01:37:58,140 --> 01:38:04,619
they need to be compared to the

2125
01:38:00,500 --> 01:38:08,159
like dances again

2126
01:38:04,619 --> 01:38:09,119
unfreeze train for a while and you can

2127
01:38:08,159 --> 01:38:11,789
kind of see here

2128
01:38:09,119 --> 01:38:14,819
you know there's cycle one there's cycle

2129
01:38:11,789 --> 01:38:18,439
- there's cycle three and then I kind of

2130
01:38:14,819 --> 01:38:21,840
increased double the size with my images

2131
01:38:18,439 --> 01:38:23,219
fit for a while and freeze fit for a

2132
01:38:21,840 --> 01:38:25,949
while double the size of the images

2133
01:38:23,220 --> 01:38:28,740
again fit for a while I'm freeze for a

2134
01:38:25,949 --> 01:38:30,090
while and then add TTA and so as I

2135
01:38:28,739 --> 01:38:32,939
mentioned last time we looked at this

2136
01:38:30,090 --> 01:38:35,300
this process ends up you know getting us

2137
01:38:32,939 --> 01:38:38,099
about 30th place in this competition

2138
01:38:35,300 --> 01:38:39,440
which is really cool because people you

2139
01:38:38,100 --> 01:38:42,150
know a lot of very very smart people

2140
01:38:39,439 --> 01:38:46,829
just a few months ago worked very very

2141
01:38:42,149 --> 01:38:51,139
hard on this competition a couple of

2142
01:38:46,829 --> 01:38:51,140
things people have asked about one is

2143
01:38:51,199 --> 01:38:59,159
what is this data dot resize do so a

2144
01:38:57,420 --> 01:39:05,520
couple of different pieces here the

2145
01:38:59,159 --> 01:39:07,649
first is that when we say back here what

2146
01:39:05,520 --> 01:39:10,080
transforms do we apply and here's our

2147
01:39:07,649 --> 01:39:12,809
transforms we actually pass in a size

2148
01:39:10,079 --> 01:39:14,519
right so one of the things that that one

2149
01:39:12,810 --> 01:39:17,130
of the things that data loaded does is

2150
01:39:14,520 --> 01:39:20,300
to resize the images like on-demand

2151
01:39:17,130 --> 01:39:20,300
every time it sees them

2152
01:39:21,510 --> 01:39:25,710
it's got nothing to do with that dot

2153
01:39:22,979 --> 01:39:27,238
resize method right so this is this is

2154
01:39:25,710 --> 01:39:29,698
the thing that happens at the end like

2155
01:39:27,238 --> 01:39:31,379
whatever's passed in before it hits out

2156
01:39:29,698 --> 01:39:35,098
that before our data loader spits it out

2157
01:39:31,380 --> 01:39:38,550
it's going to resize it to this size if

2158
01:39:35,099 --> 01:39:41,659
the initial input is like a thousand by

2159
01:39:38,550 --> 01:39:45,989
a thousand reading that JPEG and

2160
01:39:41,658 --> 01:39:48,420
resizing it to 64 by 64 turns out to

2161
01:39:45,988 --> 01:39:51,299
actually take more time than training

2162
01:39:48,420 --> 01:39:55,078
the content that's for each batch all

2163
01:39:51,300 --> 01:39:57,150
right so basically all resize does is it

2164
01:39:55,078 --> 01:40:01,319
says hey I'm not going to be using any

2165
01:39:57,149 --> 01:40:03,388
images bigger than size times 1.3 so

2166
01:40:01,319 --> 01:40:08,399
just grow through once and create new

2167
01:40:03,389 --> 01:40:10,590
JPEGs of this size right and they're

2168
01:40:08,399 --> 01:40:14,069
rectangular right so new JPEGs where the

2169
01:40:10,590 --> 01:40:16,170
smallest edges of this size and again

2170
01:40:14,069 --> 01:40:19,078
it's like you never have to do this

2171
01:40:16,170 --> 01:40:21,389
there's no reason to ever use it if you

2172
01:40:19,078 --> 01:40:23,279
don't want to it's just a speed-up okay

2173
01:40:21,389 --> 01:40:25,288
but if you've got really big images

2174
01:40:23,279 --> 01:40:27,300
coming in it saves you a lot of time and

2175
01:40:25,288 --> 01:40:30,389
you'll often see on like Carol kernels

2176
01:40:27,300 --> 01:40:34,409
or forum posts or whatever people will

2177
01:40:30,389 --> 01:40:36,809
have like bash script stuff like that -

2178
01:40:34,408 --> 01:40:38,518
like loop through and resize images to

2179
01:40:36,809 --> 01:40:40,440
save time you never have to do that

2180
01:40:38,519 --> 01:40:44,219
right just you can just say dot resize

2181
01:40:40,439 --> 01:40:46,138
and it'll just create you know once-off

2182
01:40:44,219 --> 01:40:48,389
it'll go through and create that if it's

2183
01:40:46,139 --> 01:40:51,000
already there and it'll use the

2184
01:40:48,389 --> 01:40:53,880
criticized ones for you okay so it's

2185
01:40:51,000 --> 01:41:00,420
just it's just a speed up convenience

2186
01:40:53,880 --> 01:41:05,118
function no more okay so for those of

2187
01:41:00,420 --> 01:41:09,210
you that are kind of past dog breeds I

2188
01:41:05,118 --> 01:41:13,979
would be looking at planet next you know

2189
01:41:09,210 --> 01:41:15,840
like try it like play around with with

2190
01:41:13,979 --> 01:41:19,169
trying to get a sense of like how can

2191
01:41:15,840 --> 01:41:20,429
you get this as an accurate model one

2192
01:41:19,170 --> 01:41:21,840
thing to mention and I'm not really

2193
01:41:20,429 --> 01:41:22,800
going to go into it in details there's

2194
01:41:21,840 --> 01:41:24,900
nothing to do with deep learning

2195
01:41:22,800 --> 01:41:26,880
particularly is that I'm using a

2196
01:41:24,899 --> 01:41:29,939
different metric I didn't use metrics

2197
01:41:26,880 --> 01:41:32,569
equals accuracy but I said metrics

2198
01:41:29,939 --> 01:41:32,569
equals f2

2199
01:41:33,979 --> 01:41:39,399
remember from last week that confusion

2200
01:41:37,159 --> 01:41:41,930
matrix that like two by two you know

2201
01:41:39,399 --> 01:41:47,089
correct incorrect for each of dogs and

2202
01:41:41,930 --> 01:41:48,770
cats there's a lot of different ways you

2203
01:41:47,090 --> 01:41:51,140
could turn that confusion matrix into a

2204
01:41:48,770 --> 01:41:52,730
score you know do you care more about

2205
01:41:51,140 --> 01:41:54,260
false negatives or do you care more

2206
01:41:52,729 --> 01:41:55,729
about false positives and how do you

2207
01:41:54,260 --> 01:41:59,390
weight them and how do you combine them

2208
01:41:55,729 --> 01:42:02,719
together right there's a basic there's

2209
01:41:59,390 --> 01:42:04,369
basically a function called F beta where

2210
01:42:02,720 --> 01:42:06,170
the beta says how much do you weight

2211
01:42:04,369 --> 01:42:11,539
false negatives versus false positives

2212
01:42:06,170 --> 01:42:13,730
and so f 2 is f beta with beta equals 2

2213
01:42:11,539 --> 01:42:15,140
and it's basically as particular way of

2214
01:42:13,729 --> 01:42:17,238
weighting false negatives and false

2215
01:42:15,140 --> 01:42:19,700
positives and the reason we use it is

2216
01:42:17,238 --> 01:42:21,738
because cattle told us that planet who

2217
01:42:19,699 --> 01:42:26,090
are running this competition wanted to

2218
01:42:21,738 --> 01:42:28,729
use this particular F beta metric the

2219
01:42:26,090 --> 01:42:32,060
important thing for you to know is that

2220
01:42:28,729 --> 01:42:33,109
you can create custom metrics so in this

2221
01:42:32,060 --> 01:42:35,930
case you can see here it says from

2222
01:42:33,109 --> 01:42:38,119
Planet import f2 and really I've got

2223
01:42:35,930 --> 01:42:43,789
this here so that you can see how to do

2224
01:42:38,119 --> 01:42:47,269
it right so if you look inside courses

2225
01:42:43,789 --> 01:42:51,800
DL 1 you can see there's something

2226
01:42:47,270 --> 01:42:55,540
called planet py right and so if I look

2227
01:42:51,800 --> 01:42:57,320
at planet py you'll see there's a

2228
01:42:55,539 --> 01:43:03,800
function there called

2229
01:42:57,319 --> 01:43:08,689
f2 right and so f2 simply calls F beta

2230
01:43:03,800 --> 01:43:12,289
score from psychic or side PI and patent

2231
01:43:08,689 --> 01:43:13,399
where it came from and does a couple

2232
01:43:12,289 --> 01:43:17,238
little tweets that are particularly

2233
01:43:13,399 --> 01:43:19,339
important but the important thing is

2234
01:43:17,238 --> 01:43:23,029
like you can write any metric you like

2235
01:43:19,340 --> 01:43:25,970
right as long as it takes in set of

2236
01:43:23,029 --> 01:43:27,979
predictions and a set of targets and

2237
01:43:25,970 --> 01:43:30,530
they're both going to be numpy arrays

2238
01:43:27,979 --> 01:43:33,379
one dimensional non pyros and then you

2239
01:43:30,529 --> 01:43:35,259
return back a number okay and so as long

2240
01:43:33,380 --> 01:43:38,930
as you put a function that takes two

2241
01:43:35,260 --> 01:43:41,810
vectors and returns at number you can

2242
01:43:38,930 --> 01:43:44,289
call it as a metric and so then when we

2243
01:43:41,810 --> 01:43:44,289
said

2244
01:43:46,710 --> 01:43:54,489
see here learn metrics equals and then

2245
01:43:52,510 --> 01:43:57,070
past in that array which just contains a

2246
01:43:54,489 --> 01:44:00,579
single function f2 then it's just going

2247
01:43:57,069 --> 01:44:03,429
to be printed out after every for you

2248
01:44:00,579 --> 01:44:06,970
okay so in general like the the faster I

2249
01:44:03,430 --> 01:44:11,579
library everything is customizable so

2250
01:44:06,970 --> 01:44:15,940
kind of the idea is that everything is

2251
01:44:11,579 --> 01:44:18,519
everything is kind of gives you what you

2252
01:44:15,939 --> 01:44:23,169
might want by default but also

2253
01:44:18,520 --> 01:44:26,470
everything can be changed as well yes

2254
01:44:23,170 --> 01:44:30,010
you know um we have a little confusion

2255
01:44:26,470 --> 01:44:32,890
about the difference between multi-label

2256
01:44:30,010 --> 01:44:34,930
and a single label uh-huh

2257
01:44:32,890 --> 01:44:38,140
the vanish as an example in which

2258
01:44:34,930 --> 01:44:42,970
compared like similarly the example they

2259
01:44:38,140 --> 01:44:47,079
just show us ah activation function yeah

2260
01:44:42,970 --> 01:44:49,690
so so I'm so sorry I said I'd do that

2261
01:44:47,079 --> 01:44:52,180
then I didn't so the activation the

2262
01:44:49,689 --> 01:44:55,329
output activation function for a single

2263
01:44:52,180 --> 01:44:58,390
label classification is softmax but all

2264
01:44:55,329 --> 01:45:00,100
the reasons that we talked today but if

2265
01:44:58,390 --> 01:45:05,350
we were trying to predict something that

2266
01:45:00,100 --> 01:45:07,060
was like 0 0 1 1 0 then softmax would be

2267
01:45:05,350 --> 01:45:09,100
a terrible choice because it's very hard

2268
01:45:07,060 --> 01:45:11,470
to come up with something where both of

2269
01:45:09,100 --> 01:45:13,510
these are high in fact it's impossible

2270
01:45:11,470 --> 01:45:15,449
because they have to add up to 1 so the

2271
01:45:13,510 --> 01:45:19,690
closest they could be would be point 5

2272
01:45:15,449 --> 01:45:23,649
so for multi-label classification our

2273
01:45:19,689 --> 01:45:26,529
activation function is called sigmoid ok

2274
01:45:23,649 --> 01:45:28,689
and again the faster library does this

2275
01:45:26,529 --> 01:45:32,170
automatically for you if it notices you

2276
01:45:28,689 --> 01:45:34,210
have a multi label problem and it does

2277
01:45:32,170 --> 01:45:36,430
that by checking your data tip to see if

2278
01:45:34,210 --> 01:45:40,869
anything has more than one label applied

2279
01:45:36,430 --> 01:45:44,350
to it and so sigmoid is a function which

2280
01:45:40,869 --> 01:45:48,239
is equal to it's basically the same

2281
01:45:44,350 --> 01:45:52,060
thing except rather than we never add up

2282
01:45:48,239 --> 01:45:55,170
all of these X but instead we just take

2283
01:45:52,060 --> 01:45:59,829
this X when we say it's just equal to it

2284
01:45:55,170 --> 01:46:05,118
divided by one plus

2285
01:45:59,828 --> 01:46:09,188
it and so the nice thing about that is

2286
01:46:05,118 --> 01:46:15,438
that now like multiple things can be

2287
01:46:09,189 --> 01:46:18,588
high at once right and so generally then

2288
01:46:15,439 --> 01:46:21,229
if something is less than zero its

2289
01:46:18,588 --> 01:46:23,088
sigmoid is going to be less than 0.5 if

2290
01:46:21,229 --> 01:46:26,300
it's greater than zero is signal it's

2291
01:46:23,088 --> 01:46:27,948
going to be greater than 0.5 and so the

2292
01:46:26,300 --> 01:46:34,570
important thing to know about a sigmoid

2293
01:46:27,948 --> 01:46:34,569
function is that its shape is

2294
01:46:36,050 --> 01:46:44,560
something which asymptotes at the top to

2295
01:46:39,710 --> 01:46:44,560
one and asymptotes drew

2296
01:46:45,868 --> 01:46:53,439
asymptotes at the bottom to zero and so

2297
01:46:52,210 --> 01:46:57,069
therefore it's a good thing to model a

2298
01:46:53,439 --> 01:47:01,960
probability with anybody who has done

2299
01:46:57,069 --> 01:47:03,219
any logistic regression will be familiar

2300
01:47:01,960 --> 01:47:05,439
with this is what we do in logistic

2301
01:47:03,219 --> 01:47:06,819
regression so it kind of appears

2302
01:47:05,439 --> 01:47:08,948
everywhere in machine learning and

2303
01:47:06,819 --> 01:47:12,840
you'll see that kind of a sigmoid and a

2304
01:47:08,948 --> 01:47:15,879
softmax they're very close to each other

2305
01:47:12,840 --> 01:47:18,279
conceptually but this is what we want is

2306
01:47:15,880 --> 01:47:19,868
our activation function for multi-label

2307
01:47:18,279 --> 01:47:22,300
and this is what we want the single

2308
01:47:19,868 --> 01:47:22,658
label and again first AI does it all for

2309
01:47:22,300 --> 01:47:31,119
you

2310
01:47:22,658 --> 01:47:34,058
there was a question over here yes I

2311
01:47:31,118 --> 01:47:35,978
have a question about the initial

2312
01:47:34,059 --> 01:47:39,659
training that you do if I understand

2313
01:47:35,979 --> 01:47:42,989
correctly you have we have frozen the

2314
01:47:39,658 --> 01:47:46,679
the premium model and you only need

2315
01:47:42,988 --> 01:47:50,919
initially try to train the latest

2316
01:47:46,679 --> 01:47:53,889
playwright right but from the other hand

2317
01:47:50,920 --> 01:47:55,960
we said that only the initial layer so

2318
01:47:53,889 --> 01:47:58,809
let's last probably the first layer is

2319
01:47:55,960 --> 01:48:01,599
like important to us and the other two

2320
01:47:58,809 --> 01:48:03,400
are more like features that are you must

2321
01:48:01,599 --> 01:48:07,119
not related and we then apply in this

2322
01:48:03,399 --> 01:48:10,839
case what that they the lie is a very

2323
01:48:07,118 --> 01:48:13,420
important but the pre-trained weights in

2324
01:48:10,840 --> 01:48:16,210
them aren't so it's the later layers

2325
01:48:13,420 --> 01:48:20,158
that we really want to train the most so

2326
01:48:16,210 --> 01:48:23,769
earlier layers likely to be like already

2327
01:48:20,158 --> 01:48:25,448
closer to what we want okay so you

2328
01:48:23,769 --> 01:48:27,519
started with the latest one and then you

2329
01:48:25,448 --> 01:48:31,629
go right so if you go back to our quick

2330
01:48:27,519 --> 01:48:33,820
dogs and cats right when we create a

2331
01:48:31,630 --> 01:48:36,190
model from pre train from a pre train

2332
01:48:33,819 --> 01:48:38,279
model it returns something where all of

2333
01:48:36,189 --> 01:48:42,969
the convolutional layers are frozen and

2334
01:48:38,279 --> 01:48:46,328
some randomly set fully connected layers

2335
01:48:42,969 --> 01:48:50,309
we add to the end our unfrozen and so

2336
01:48:46,328 --> 01:48:53,920
when we go fit at first it just trains

2337
01:48:50,309 --> 01:48:56,920
the randomly set a randomly initialized

2338
01:48:53,920 --> 01:48:59,739
fully connected letters right

2339
01:48:56,920 --> 01:49:02,529
and if something is like really close to

2340
01:48:59,738 --> 01:49:04,329
imagenet that's often all we need

2341
01:49:02,529 --> 01:49:08,380
but because the early early layers are

2342
01:49:04,329 --> 01:49:11,948
already good at finding edges gradients

2343
01:49:08,380 --> 01:49:16,829
repeating patterns for ears and dogs

2344
01:49:11,948 --> 01:49:19,569
heads you know so then when we unfreeze

2345
01:49:16,829 --> 01:49:22,960
we set the learning rates for the early

2346
01:49:19,569 --> 01:49:25,029
layers to be really low because we don't

2347
01:49:22,960 --> 01:49:28,270
want to change the mesh for us the later

2348
01:49:25,029 --> 01:49:32,829
ones we set them to be higher where else

2349
01:49:28,270 --> 01:49:34,690
for satellite data right this is no

2350
01:49:32,829 --> 01:49:37,420
longer true you know the early layers

2351
01:49:34,689 --> 01:49:40,419
are still like better than the later

2352
01:49:37,420 --> 01:49:42,730
layers but we still probably need to

2353
01:49:40,420 --> 01:49:45,460
change them quite a bit so that's right

2354
01:49:42,729 --> 01:49:48,279
this learning rate is nine times smaller

2355
01:49:45,460 --> 01:49:52,149
than the final learning rate rather than

2356
01:49:48,279 --> 01:49:55,869
a thousand times smaller the final loan

2357
01:49:52,149 --> 01:50:00,219
rate okay you play with with the weights

2358
01:49:55,869 --> 01:50:01,809
of the layers yeah normally most of the

2359
01:50:00,219 --> 01:50:03,989
stuff you see online if they talk about

2360
01:50:01,810 --> 01:50:07,300
this at all they'll talk about

2361
01:50:03,988 --> 01:50:10,779
unfreezing different subsets of layers

2362
01:50:07,300 --> 01:50:13,960
and indeed we do unfreeze our randomly

2363
01:50:10,779 --> 01:50:16,029
generated runs but what I found is

2364
01:50:13,960 --> 01:50:18,250
although the first layer library you can

2365
01:50:16,029 --> 01:50:20,948
type learn dot freeze too and just

2366
01:50:18,250 --> 01:50:22,448
freeze a subset of layers this approach

2367
01:50:20,948 --> 01:50:26,349
of using differential learning rates

2368
01:50:22,448 --> 01:50:28,178
seems to be like more flexible to the

2369
01:50:26,350 --> 01:50:32,380
point that I never find myself I'm

2370
01:50:28,179 --> 01:50:34,960
freezing subsets of layers that I would

2371
01:50:32,380 --> 01:50:37,900
expect you to start with that with a

2372
01:50:34,960 --> 01:50:40,270
different cell the different learning

2373
01:50:37,899 --> 01:50:43,089
rates rather than trying to learn the

2374
01:50:40,270 --> 01:50:48,190
last layer so the reason okay so you

2375
01:50:43,090 --> 01:50:49,719
could skip this training just the last

2376
01:50:48,189 --> 01:50:52,089
layers and just go straight to

2377
01:50:49,719 --> 01:50:53,890
differential learning rates but you

2378
01:50:52,090 --> 01:50:55,510
probably don't want to and the reason

2379
01:50:53,890 --> 01:50:57,610
you probably don't want to is that

2380
01:50:55,510 --> 01:51:00,909
there's a difference the convolutional

2381
01:50:57,609 --> 01:51:03,909
layers all contain pre-trained weights

2382
01:51:00,909 --> 01:51:05,289
so they're like they're not random for

2383
01:51:03,909 --> 01:51:07,059
things that are close to imagenet

2384
01:51:05,289 --> 01:51:09,189
they're actually really good for things

2385
01:51:07,060 --> 01:51:10,840
that are not close to imagenet they're

2386
01:51:09,189 --> 01:51:13,250
better than that

2387
01:51:10,840 --> 01:51:17,420
all of our fully connected layers

2388
01:51:13,250 --> 01:51:19,969
however are totally random so therefore

2389
01:51:17,420 --> 01:51:21,980
you would always want to make the fully

2390
01:51:19,969 --> 01:51:24,050
connected weights better than random by

2391
01:51:21,979 --> 01:51:26,649
training them a bit first because

2392
01:51:24,050 --> 01:51:28,840
otherwise if you go straight to unfreeze

2393
01:51:26,649 --> 01:51:31,698
then you're actually going to be like

2394
01:51:28,840 --> 01:51:34,369
fiddling around of those early early can

2395
01:51:31,698 --> 01:51:35,808
early layer weights when the later ones

2396
01:51:34,368 --> 01:51:37,848
are still random that's probably not

2397
01:51:35,809 --> 01:51:41,900
what you want I think that's another

2398
01:51:37,849 --> 01:51:46,909
question here any possible so when we

2399
01:51:41,899 --> 01:51:50,118
unfreeze what are the things we're

2400
01:51:46,908 --> 01:51:54,348
trying to change there will it change

2401
01:51:50,118 --> 01:51:57,859
the Colonel's themselves that that's

2402
01:51:54,349 --> 01:52:02,869
always what SGD does yeah so the only

2403
01:51:57,859 --> 01:52:09,618
thing what training means is setting

2404
01:52:02,868 --> 01:52:18,139
these numbers right and these numbers

2405
01:52:09,618 --> 01:52:19,698
and these numbers the weights so the

2406
01:52:18,139 --> 01:52:22,460
weights are the weights of the fully

2407
01:52:19,698 --> 01:52:24,319
connected layers and the weights in

2408
01:52:22,460 --> 01:52:27,469
those kernels and the convolutions so

2409
01:52:24,319 --> 01:52:29,750
that's what training means it's and

2410
01:52:27,469 --> 01:52:32,059
we'll learn about how to do it with SGD

2411
01:52:29,750 --> 01:52:34,729
but training literally is setting those

2412
01:52:32,059 --> 01:52:38,059
numbers these numbers on the other hand

2413
01:52:34,729 --> 01:52:41,328
are activations they're calculated

2414
01:52:38,059 --> 01:52:44,449
they're calculated from the weights and

2415
01:52:41,328 --> 01:52:49,250
the previous layers activations or

2416
01:52:44,448 --> 01:52:51,229
amounts of questions so you can lift it

2417
01:52:49,250 --> 01:52:52,670
up higher and speak badly so in your

2418
01:52:51,229 --> 01:52:55,879
example of a cheerleader set of that

2419
01:52:52,670 --> 01:52:58,609
English example so you start with very

2420
01:52:55,880 --> 01:53:00,739
small size existed for yeah so does it

2421
01:52:58,609 --> 01:53:03,558
literally mean you know the model takes

2422
01:53:00,738 --> 01:53:08,149
a small area from the entire image that

2423
01:53:03,559 --> 01:53:13,489
is 64 bytes so how do we get that 64 by

2424
01:53:08,149 --> 01:53:18,018
64 depends on the transforms by default

2425
01:53:13,488 --> 01:53:22,268
our transform takes the smallest edge

2426
01:53:18,019 --> 01:53:24,070
and recites the whole thing out

2427
01:53:22,269 --> 01:53:27,010
samples it so the smallest edge is

2428
01:53:24,069 --> 01:53:29,828
societics t4 and then it takes a Center

2429
01:53:27,010 --> 01:53:33,099
crop of that okay

2430
01:53:29,828 --> 01:53:35,558
although when we're using data

2431
01:53:33,099 --> 01:53:40,538
augmentation it actually takes a

2432
01:53:35,559 --> 01:53:43,029
randomly chosen prop ie the case where

2433
01:53:40,538 --> 01:53:46,779
the image ties to multiple objects don't

2434
01:53:43,029 --> 01:53:48,158
in this case like would it be possible

2435
01:53:46,779 --> 01:53:50,170
that you would just lose the other

2436
01:53:48,158 --> 01:53:51,969
things that they try to predict yeah

2437
01:53:50,170 --> 01:53:54,069
which is why data augmentation is

2438
01:53:51,969 --> 01:53:57,130
important so by by and particularly

2439
01:53:54,069 --> 01:53:59,288
their test time augmentation is going to

2440
01:53:57,130 --> 01:54:00,940
be particularly important because you

2441
01:53:59,288 --> 01:54:03,880
would you wouldn't want to you know that

2442
01:54:00,939 --> 01:54:06,129
there may be a artisanal mine out in the

2443
01:54:03,880 --> 01:54:08,859
corner which if you take a center crop

2444
01:54:06,130 --> 01:54:15,819
you you don't see so data augmentation

2445
01:54:08,859 --> 01:54:17,498
becomes very important yeah so when we

2446
01:54:15,819 --> 01:54:21,038
talk on their tributaries are he

2447
01:54:17,498 --> 01:54:23,229
receiver up to that's not really what a

2448
01:54:21,038 --> 01:54:25,179
model choice Delton that's a great point

2449
01:54:23,229 --> 01:54:27,038
that's not the loss function yeah right

2450
01:54:25,179 --> 01:54:30,248
the loss function is something we'll be

2451
01:54:27,038 --> 01:54:33,429
learning about next week and it uses

2452
01:54:30,248 --> 01:54:37,179
cross entropy or otherwise known as like

2453
01:54:33,429 --> 01:54:39,248
negative log likelihood the metric is

2454
01:54:37,179 --> 01:54:43,960
just this thing that's printed so we can

2455
01:54:39,248 --> 01:54:47,078
see what's going on just next to that so

2456
01:54:43,960 --> 01:54:49,149
in the context of my deep pass modeling

2457
01:54:47,078 --> 01:54:51,279
cannot change data does it trading it

2458
01:54:49,149 --> 01:54:53,109
also have to be multiplied so can I

2459
01:54:51,279 --> 01:54:54,908
train on just like images of pure cats

2460
01:54:53,109 --> 01:54:57,609
and dogs and expect it at prediction

2461
01:54:54,908 --> 01:55:04,029
time to predict if I give it a picture

2462
01:54:57,609 --> 01:55:06,308
of both having cat eye on it over I've

2463
01:55:04,029 --> 01:55:08,408
never tried that and I've never seen an

2464
01:55:06,309 --> 01:55:11,800
example of something that needed it

2465
01:55:08,408 --> 01:55:14,948
I guess conceptually there's no reason

2466
01:55:11,800 --> 01:55:18,219
it wouldn't work but it's kind of out

2467
01:55:14,948 --> 01:55:19,388
there and you still use a sigmoid you

2468
01:55:18,219 --> 01:55:20,859
would have to make sure you're using a

2469
01:55:19,389 --> 01:55:22,659
sigmoid loss function so in this case

2470
01:55:20,859 --> 01:55:24,578
faster eyes default would not work

2471
01:55:22,658 --> 01:55:26,618
because by default first day I would say

2472
01:55:24,578 --> 01:55:28,149
your training data knitter has both a

2473
01:55:26,618 --> 01:55:31,229
cat and the dog so you would have to

2474
01:55:28,149 --> 01:55:31,229
override the loss function

2475
01:55:35,349 --> 01:55:40,729
when you use the differential learning

2476
01:55:37,849 --> 01:55:42,918
rates those three learning rates do they

2477
01:55:40,729 --> 01:55:46,550
just kind of spread evenly across the

2478
01:55:42,918 --> 01:55:48,288
layers yeah we'll talk more about this

2479
01:55:46,550 --> 01:55:50,689
later in the course but I mean the

2480
01:55:48,288 --> 01:55:53,569
faster I library there's a concept of

2481
01:55:50,689 --> 01:55:55,579
layer groups so in something like a

2482
01:55:53,569 --> 01:55:57,889
resonant 50 you know there's hundreds of

2483
01:55:55,578 --> 01:56:00,708
layers and I think it you don't want to

2484
01:55:57,889 --> 01:56:03,679
write down hundreds of learning rates so

2485
01:56:00,708 --> 01:56:07,488
I've basically decided for you how to

2486
01:56:03,679 --> 01:56:10,219
split them and the the last one always

2487
01:56:07,488 --> 01:56:12,049
refers just to the fully connected

2488
01:56:10,219 --> 01:56:14,929
layers that we've randomly initialized

2489
01:56:12,050 --> 01:56:16,579
and edit to the end and then these ones

2490
01:56:14,929 --> 01:56:19,668
are split generally about halfway

2491
01:56:16,578 --> 01:56:22,429
through basically I've tried to make it

2492
01:56:19,668 --> 01:56:24,019
so that these you know these ones are

2493
01:56:22,429 --> 01:56:25,399
kind of the ones which you hardly want

2494
01:56:24,019 --> 01:56:26,630
to change at all and these are the ones

2495
01:56:25,399 --> 01:56:29,238
you might want to change a little bit

2496
01:56:26,630 --> 01:56:30,469
and I don't think we're covered in the

2497
01:56:29,238 --> 01:56:31,938
course but if you're interested we can

2498
01:56:30,469 --> 01:56:33,859
talk about in the forum there are ways

2499
01:56:31,939 --> 01:56:36,159
you can override this behavior to define

2500
01:56:33,859 --> 01:56:38,868
your own layer groups if you want to and

2501
01:56:36,158 --> 01:56:41,418
is there any way to visualize the model

2502
01:56:38,868 --> 01:56:43,998
easily or like don't dump the layers of

2503
01:56:41,418 --> 01:56:47,238
the model yeah absolutely

2504
01:56:43,998 --> 01:56:54,228
you can let's make sure we've got one

2505
01:56:47,238 --> 01:56:55,518
here okay so if you just type learn it

2506
01:56:54,229 --> 01:57:02,479
doesn't tell you much at all but what

2507
01:56:55,519 --> 01:57:07,579
you can do is go learn summary and that

2508
01:57:02,479 --> 01:57:09,590
spits out basically everything there's

2509
01:57:07,578 --> 01:57:13,099
all the letters and so you can see in

2510
01:57:09,590 --> 01:57:14,418
this case these are the names I

2511
01:57:13,099 --> 01:57:17,469
mentioned how they look up names right

2512
01:57:14,418 --> 01:57:21,918
so the first layer is called con 2 d 1

2513
01:57:17,469 --> 01:57:24,050
and it's going to take as input this is

2514
01:57:21,918 --> 01:57:27,109
useful to actually look at it's taking

2515
01:57:24,050 --> 01:57:28,998
64 by 64 images which is what we told it

2516
01:57:27,109 --> 01:57:33,259
we're going to transform things to this

2517
01:57:28,998 --> 01:57:35,208
is three channels high torch like most

2518
01:57:33,260 --> 01:57:38,449
things have channels at the end would

2519
01:57:35,208 --> 01:57:41,868
say 64 by 64 by 3 ply torch music to the

2520
01:57:38,448 --> 01:57:44,029
front so it's 3 by 64 by 64 that's

2521
01:57:41,868 --> 01:57:46,819
because it turns out that some of the

2522
01:57:44,029 --> 01:57:49,069
GPU computations run faster when it

2523
01:57:46,819 --> 01:57:50,899
in that order okay but that happens all

2524
01:57:49,069 --> 01:57:53,509
behind-the-scenes automatic plays a part

2525
01:57:50,899 --> 01:57:56,259
of that transformation stuff that's kind

2526
01:57:53,510 --> 01:58:00,829
of all done automatically is to do that

2527
01:57:56,260 --> 01:58:04,280
minus one means however however big the

2528
01:58:00,829 --> 01:58:07,579
batch size is in care us they use the

2529
01:58:04,279 --> 01:58:09,739
number they use a special number none in

2530
01:58:07,579 --> 01:58:13,130
pile types that used minus one so this

2531
01:58:09,739 --> 01:58:15,920
is a four dimensional mini batch the

2532
01:58:13,130 --> 01:58:18,289
number of elements in the amount of

2533
01:58:15,920 --> 01:58:20,270
images in the mini batch is dynamic you

2534
01:58:18,289 --> 01:58:23,659
can change that the number of channels

2535
01:58:20,270 --> 01:58:26,360
is three number which is a 64 by 64 okay

2536
01:58:23,659 --> 01:58:29,199
and so then you can basically see that

2537
01:58:26,359 --> 01:58:32,960
this particular convolutional kernel

2538
01:58:29,199 --> 01:58:35,239
apparently has 64 kernels in it and it's

2539
01:58:32,960 --> 01:58:36,500
also having we haven't talked about this

2540
01:58:35,239 --> 01:58:39,199
but convolutions can have something

2541
01:58:36,500 --> 01:58:41,390
called a stride that is like Matt pullin

2542
01:58:39,199 --> 01:58:47,869
it changes the size so it's returning a

2543
01:58:41,390 --> 01:58:51,950
32 by 32 564 kernel tenza and so on and

2544
01:58:47,869 --> 01:58:54,079
so forth so that's summary and we'll

2545
01:58:51,949 --> 01:58:57,619
learn all about that's doing in detail

2546
01:58:54,079 --> 01:59:00,769
on in the second half of the course one

2547
01:58:57,619 --> 01:59:02,720
where I clicked in my own data set and I

2548
01:59:00,770 --> 01:59:06,260
try to use the in as a really small data

2549
01:59:02,720 --> 01:59:09,470
set these currencies from Google Images

2550
01:59:06,260 --> 01:59:12,650
and I tried to do a learning rate find

2551
01:59:09,470 --> 01:59:14,030
and then the plot and it just it gave me

2552
01:59:12,649 --> 01:59:15,979
some numbers which I didn't understand

2553
01:59:14,029 --> 01:59:18,259
and the learning rate font yeah and then

2554
01:59:15,979 --> 01:59:20,029
the plot was empty so yeah I mean let's

2555
01:59:18,260 --> 01:59:22,909
let's talk about that on the forum but

2556
01:59:20,029 --> 01:59:24,289
basically the learning rate finder is

2557
01:59:22,909 --> 01:59:26,449
going to go through a mini batch at a

2558
01:59:24,289 --> 01:59:28,369
time if you've got a tiny data set

2559
01:59:26,449 --> 01:59:30,559
there's just not enough mini batches so

2560
01:59:28,369 --> 01:59:32,720
the trick is to make your mini bit make

2561
01:59:30,560 --> 01:59:35,530
your batch size really small like try

2562
01:59:32,720 --> 01:59:35,530
making it like four

2563
01:59:38,250 --> 01:59:44,439
okay they were great questions it's not

2564
01:59:40,479 --> 01:59:45,579
nothing online to add you know they were

2565
01:59:44,439 --> 01:59:48,159
great questions we've got a little bit

2566
01:59:45,579 --> 01:59:51,430
past where I hope to but let's let's

2567
01:59:48,159 --> 01:59:52,689
quickly talk about structured data so we

2568
01:59:51,430 --> 01:59:59,770
can start thinking about it for next

2569
01:59:52,689 --> 02:00:02,500
week so this is really weird right to me

2570
01:59:59,770 --> 02:00:04,090
there's basically two types of data set

2571
02:00:02,500 --> 02:00:10,300
we use in machine learning there's a

2572
02:00:04,090 --> 02:00:14,319
type of data like audio images natural

2573
02:00:10,300 --> 02:00:16,600
language text where all of the all of

2574
02:00:14,319 --> 02:00:18,939
the things inside an object like all of

2575
02:00:16,600 --> 02:00:21,850
the pixels inside an image are all the

2576
02:00:18,939 --> 02:00:26,409
same kind of thing they're all pixels or

2577
02:00:21,850 --> 02:00:30,130
they're all apertures of a waveform or

2578
02:00:26,409 --> 02:00:33,039
they're all words I call this kind of

2579
02:00:30,130 --> 02:00:37,199
data unstructured and then there's data

2580
02:00:33,039 --> 02:00:39,960
sets like a profit and loss statement or

2581
02:00:37,198 --> 02:00:43,960
the information about a Facebook user

2582
02:00:39,960 --> 02:00:45,520
where each column is like structurally

2583
02:00:43,960 --> 02:00:47,260
quite different you know one thing is

2584
02:00:45,520 --> 02:00:49,449
representing like how many page views

2585
02:00:47,260 --> 02:00:51,550
last month another one is their sex

2586
02:00:49,448 --> 02:00:55,719
another one is what zip code they're in

2587
02:00:51,550 --> 02:00:57,940
and I call this structure there that

2588
02:00:55,719 --> 02:01:00,480
particular terminology is not unusual

2589
02:00:57,939 --> 02:01:02,409
like lots of people use that terminology

2590
02:01:00,479 --> 02:01:05,488
but lots of people don't

2591
02:01:02,409 --> 02:01:08,800
there's no particularly agreed-upon

2592
02:01:05,488 --> 02:01:11,769
terminology so when I say structured

2593
02:01:08,800 --> 02:01:14,619
data I'm referring to kind of columnar

2594
02:01:11,770 --> 02:01:16,210
data as you might find in a database or

2595
02:01:14,619 --> 02:01:19,000
a spreadsheet where different columns

2596
02:01:16,210 --> 02:01:21,840
represent different kinds of things and

2597
02:01:19,000 --> 02:01:25,630
each row represents an observation and

2598
02:01:21,840 --> 02:01:31,199
so structured data is probably what most

2599
02:01:25,630 --> 02:01:34,810
of you are analyzing most of the time

2600
02:01:31,198 --> 02:01:37,179
funnily enough you know academics in the

2601
02:01:34,810 --> 02:01:39,730
deep learning world don't really give a

2602
02:01:37,180 --> 02:01:41,590
about structured data because it's

2603
02:01:39,729 --> 02:01:44,259
pretty hard to get published in fancy

2604
02:01:41,590 --> 02:01:46,179
conference proceedings if you're like if

2605
02:01:44,260 --> 02:01:47,829
you've got a better logistics model you

2606
02:01:46,179 --> 02:01:49,719
know it's the thing that makes the world

2607
02:01:47,829 --> 02:01:51,139
goes round it's a thing that makes

2608
02:01:49,719 --> 02:01:55,880
everybody you know

2609
02:01:51,139 --> 02:02:00,319
and efficiency and make stuff work but

2610
02:01:55,880 --> 02:02:01,309
it's largely ignored sadly so we're not

2611
02:02:00,319 --> 02:02:04,188
going to ignore it because we're a

2612
02:02:01,309 --> 02:02:05,748
practical deep learning and cackled

2613
02:02:04,189 --> 02:02:08,090
doesn't ignore it either because people

2614
02:02:05,748 --> 02:02:10,069
put prize money up on Cagle to solve

2615
02:02:08,090 --> 02:02:11,899
real-world problems so there are some

2616
02:02:10,069 --> 02:02:14,779
great capable competitions we can look

2617
02:02:11,899 --> 02:02:16,519
at there's one running right now which

2618
02:02:14,779 --> 02:02:21,130
is the grocery sales forecasting

2619
02:02:16,519 --> 02:02:21,130
competition for Ecuador's largest chain

2620
02:02:21,399 --> 02:02:26,360
it's always a little I've got to be a

2621
02:02:24,559 --> 02:02:28,099
little careful about how much I show you

2622
02:02:26,359 --> 02:02:29,839
about currently running competitions

2623
02:02:28,099 --> 02:02:32,659
because I don't want to you know help

2624
02:02:29,840 --> 02:02:35,989
you cheat but it so happens there was a

2625
02:02:32,658 --> 02:02:38,268
competition a year or two ago for one of

2626
02:02:35,988 --> 02:02:40,098
Germany's magistrate chains which is

2627
02:02:38,269 --> 02:02:45,380
almost identical so I'm going to show

2628
02:02:40,099 --> 02:02:50,538
you how to do that so that was called

2629
02:02:45,380 --> 02:02:52,489
the Rossman stores data and so I would

2630
02:02:50,538 --> 02:02:53,779
suggest you know first of all try

2631
02:02:52,488 --> 02:02:56,118
practicing what we're learning on

2632
02:02:53,779 --> 02:02:58,578
Russman right but then see if you can

2633
02:02:56,118 --> 02:03:02,118
get it working on on grocery because

2634
02:02:58,578 --> 02:03:03,319
currently on the leaderboard no one

2635
02:03:02,118 --> 02:03:05,148
seems to basically know what they're

2636
02:03:03,319 --> 02:03:10,130
doing in the groceries competition if

2637
02:03:05,149 --> 02:03:13,280
you look at the leaderboard the let's

2638
02:03:10,130 --> 02:03:15,769
see here yeah these ones around 5 to 9 v

2639
02:03:13,279 --> 02:03:18,018
3o are people that are literally finding

2640
02:03:15,769 --> 02:03:19,849
like group averages and submitting those

2641
02:03:18,019 --> 02:03:22,369
I know because they're kernels that

2642
02:03:19,849 --> 02:03:24,369
they're using so you know the basically

2643
02:03:22,368 --> 02:03:29,058
the people around 20th place are not

2644
02:03:24,368 --> 02:03:31,748
actually doing any machine learning so

2645
02:03:29,059 --> 02:03:34,639
yeah let's see if we can improve things

2646
02:03:31,748 --> 02:03:38,029
so you'll see there's a less than 3

2647
02:03:34,639 --> 02:03:40,189
rossmann notebook sure you get pull ok

2648
02:03:38,029 --> 02:03:42,800
in fact you know just reminder you know

2649
02:03:40,189 --> 02:03:45,289
before you start working get pull in

2650
02:03:42,800 --> 02:03:48,889
you're faster a repo and from time to

2651
02:03:45,288 --> 02:03:51,018
time Condor and update for you guys

2652
02:03:48,889 --> 02:03:52,849
during the in-person course the Condor

2653
02:03:51,019 --> 02:03:55,309
and update you should do it more often

2654
02:03:52,849 --> 02:03:57,769
because we're kind of changing things a

2655
02:03:55,309 --> 02:04:01,670
little bit um folks in the MOOC you know

2656
02:03:57,769 --> 02:04:04,010
more like once a month should be fine

2657
02:04:01,670 --> 02:04:05,210
so anyway I just just changed this a

2658
02:04:04,010 --> 02:04:09,560
little bit so make sure you get Paul to

2659
02:04:05,210 --> 02:04:11,390
get lesson 3 Rossman and there's a

2660
02:04:09,560 --> 02:04:14,600
couple of new libraries here one is fast

2661
02:04:11,390 --> 02:04:16,369
AI dot structure faster guided

2662
02:04:14,600 --> 02:04:18,950
structured contain stuff which is

2663
02:04:16,369 --> 02:04:20,599
actually not at all high torch specific

2664
02:04:18,949 --> 02:04:23,210
and we actually use that in the machine

2665
02:04:20,600 --> 02:04:25,000
learning course as well for doing random

2666
02:04:23,210 --> 02:04:27,439
forests with no tie torch at all I

2667
02:04:25,000 --> 02:04:30,229
mentioned that because you can use that

2668
02:04:27,439 --> 02:04:33,529
particular library without any of the

2669
02:04:30,229 --> 02:04:36,079
other parts of fast AI so that can be

2670
02:04:33,529 --> 02:04:39,019
handy and then we're also going to use

2671
02:04:36,079 --> 02:04:41,630
faster column data which is basically

2672
02:04:39,020 --> 02:04:44,540
some stuff that allows us to do fast a

2673
02:04:41,630 --> 02:04:49,100
type a torch stuff with columnar

2674
02:04:44,539 --> 02:04:52,729
structured data for structured data we

2675
02:04:49,100 --> 02:04:54,710
need to use pandas a lot anybody who's

2676
02:04:52,729 --> 02:04:56,719
used our data frames will be very

2677
02:04:54,710 --> 02:04:59,239
familiar with pandas pandas is basically

2678
02:04:56,720 --> 02:05:03,199
an attempt to kind of replicate data

2679
02:04:59,239 --> 02:05:08,239
friends in Python you know and a bit

2680
02:05:03,199 --> 02:05:13,689
more if you're not entirely familiar

2681
02:05:08,239 --> 02:05:13,689
with pandas there's a great book

2682
02:05:14,729 --> 02:05:18,019
[Music]

2683
02:05:16,849 --> 02:05:23,150
which I think I might have mentioned

2684
02:05:18,020 --> 02:05:24,920
before - for data analysis by Wes

2685
02:05:23,149 --> 02:05:27,848
McKinney there's a new addition that

2686
02:05:24,920 --> 02:05:30,618
just came out a couple of weeks ago

2687
02:05:27,849 --> 02:05:32,810
obviously being by the pandas author its

2688
02:05:30,618 --> 02:05:37,339
coverage of pandas is excellent but it

2689
02:05:32,810 --> 02:05:42,199
also covers numpy scipy matplotlib

2690
02:05:37,340 --> 02:05:45,940
scikit-learn - and Jupiter really well

2691
02:05:42,198 --> 02:05:48,948
okay and so I'm kind of going to assume

2692
02:05:45,939 --> 02:05:52,819
that you know your way around these

2693
02:05:48,948 --> 02:05:54,979
libraries to some extent also there was

2694
02:05:52,819 --> 02:05:56,630
the workshop we did before they started

2695
02:05:54,979 --> 02:05:59,029
and there's a video of that online where

2696
02:05:56,630 --> 02:06:03,980
we kind of have a brief mention of all

2697
02:05:59,029 --> 02:06:06,710
of those tools structured data is

2698
02:06:03,979 --> 02:06:08,899
generally shared as CSV files it was no

2699
02:06:06,710 --> 02:06:10,609
different in this competition as you'll

2700
02:06:08,899 --> 02:06:14,210
see there's a hyperlink to the rustman

2701
02:06:10,609 --> 02:06:15,500
data set here right now if you look at

2702
02:06:14,210 --> 02:06:17,899
the bottom of my screen you'll see this

2703
02:06:15,500 --> 02:06:19,578
goes to file start faster day.i because

2704
02:06:17,899 --> 02:06:21,979
this doesn't require any login or

2705
02:06:19,578 --> 02:06:24,408
anything to grab this data set it's as

2706
02:06:21,979 --> 02:06:26,959
simple as right clicking copy link

2707
02:06:24,408 --> 02:06:36,158
address head over to wherever you want

2708
02:06:26,960 --> 02:06:40,039
it and just type W get and the URL okay

2709
02:06:36,158 --> 02:06:44,149
so that's because you know it's it's not

2710
02:06:40,039 --> 02:06:47,779
behind a login or anything so you can

2711
02:06:44,149 --> 02:06:49,819
grab the grab it from there and you can

2712
02:06:47,779 --> 02:06:52,039
always read a CSV file with just pandas

2713
02:06:49,819 --> 02:06:55,639
don't read CSV now in this particular

2714
02:06:52,039 --> 02:06:57,920
case there's a lot of pre-processing

2715
02:06:55,639 --> 02:07:03,710
that we do and what I've actually done

2716
02:06:57,920 --> 02:07:06,408
here is I've I've actually stolen the

2717
02:07:03,710 --> 02:07:08,779
entire pipeline from the third place

2718
02:07:06,408 --> 02:07:11,118
winner roster okay so they made all

2719
02:07:08,779 --> 02:07:12,439
their data they're really great you know

2720
02:07:11,118 --> 02:07:14,420
they better get hub available with

2721
02:07:12,439 --> 02:07:16,879
everything that we need and I've ported

2722
02:07:14,420 --> 02:07:18,828
it all across and simplified it and

2723
02:07:16,880 --> 02:07:23,029
tried to make it pretty easy to

2724
02:07:18,828 --> 02:07:25,819
understand this course is about deep

2725
02:07:23,029 --> 02:07:28,460
learning not about data processing so

2726
02:07:25,819 --> 02:07:29,929
I'm not going to go through it but we

2727
02:07:28,460 --> 02:07:30,840
will be going through it in the machine

2728
02:07:29,929 --> 02:07:32,579
learning course

2729
02:07:30,840 --> 02:07:34,469
in some detail because feature

2730
02:07:32,579 --> 02:07:37,109
engineering is really important so if

2731
02:07:34,469 --> 02:07:41,520
you're interested you know check out the

2732
02:07:37,109 --> 02:07:44,189
machine learning course for that I will

2733
02:07:41,520 --> 02:07:48,780
however show you kind of what it looks

2734
02:07:44,189 --> 02:07:50,908
like so once we read the CSV Xin you can

2735
02:07:48,779 --> 02:08:01,738
see basically what's there so the key

2736
02:07:50,908 --> 02:08:04,189
one is for a particular store we have

2737
02:08:01,738 --> 02:08:04,189
the

2738
02:08:05,399 --> 02:08:12,269
we have the date and we have the sales

2739
02:08:09,170 --> 02:08:18,239
for that particular store we know

2740
02:08:12,270 --> 02:08:19,860
whether that thing is on promo or not we

2741
02:08:18,238 --> 02:08:23,428
know the number of customers at that

2742
02:08:19,859 --> 02:08:26,509
particular store had we know whether

2743
02:08:23,429 --> 02:08:26,510
that date was a school holiday

2744
02:08:29,939 --> 02:08:38,319
we also know what kind of store it is so

2745
02:08:36,520 --> 02:08:41,530
this is pretty common right you'll often

2746
02:08:38,319 --> 02:08:43,359
get datasets where there's some column

2747
02:08:41,529 --> 02:08:44,170
with like just some kind of code we

2748
02:08:43,359 --> 02:08:47,409
don't really know what the code means

2749
02:08:44,170 --> 02:08:49,449
and most of the time I find it doesn't

2750
02:08:47,409 --> 02:08:51,760
matter what it means like normally you

2751
02:08:49,449 --> 02:08:53,170
get given a data dictionary when you

2752
02:08:51,760 --> 02:08:54,550
start on a project and obviously if

2753
02:08:53,170 --> 02:08:56,380
you're working on an internal project

2754
02:08:54,550 --> 02:08:59,739
you can ask the people at your company

2755
02:08:56,380 --> 02:09:01,810
what does this column mean I kind of

2756
02:08:59,739 --> 02:09:03,399
stay away from learning too much about

2757
02:09:01,810 --> 02:09:08,080
it I prefer to like to see what the data

2758
02:09:03,399 --> 02:09:10,210
says first there's something about what

2759
02:09:08,079 --> 02:09:12,930
kind of product are we selling in this

2760
02:09:10,210 --> 02:09:12,930
particular row

2761
02:09:13,988 --> 02:09:19,238
and then there's information about like

2762
02:09:16,229 --> 02:09:24,819
how far away is the nearest competitor

2763
02:09:19,238 --> 02:09:31,119
how long have they been open for how

2764
02:09:24,819 --> 02:09:32,920
long is the promo being on for for each

2765
02:09:31,119 --> 02:09:35,380
store we can find out what state it's in

2766
02:09:32,920 --> 02:09:38,670
for each state we can find at the name

2767
02:09:35,380 --> 02:09:40,539
of the state this is in Germany and

2768
02:09:38,670 --> 02:09:42,640
interestingly they were allowed to

2769
02:09:40,539 --> 02:09:44,380
download any data external data they

2770
02:09:42,640 --> 02:09:46,150
wanted in this competition just very

2771
02:09:44,380 --> 02:09:48,520
common as long as you share it with

2772
02:09:46,149 --> 02:09:53,829
everybody else and so some folks tried

2773
02:09:48,520 --> 02:09:55,180
downloading data from Google Trends I'm

2774
02:09:53,829 --> 02:09:56,949
not sure exactly what it was that they

2775
02:09:55,180 --> 02:09:59,430
would check in the trend off but we have

2776
02:09:56,949 --> 02:10:01,809
this information from Google Trends

2777
02:09:59,430 --> 02:10:05,760
somebody downloaded the weather for

2778
02:10:01,810 --> 02:10:05,760
every day in Germany every state

2779
02:10:09,250 --> 02:10:20,710
and yeah that's about it right so you

2780
02:10:17,319 --> 02:10:22,689
can get a data frame summary with pandas

2781
02:10:20,710 --> 02:10:24,430
which kind of lets you see how many

2782
02:10:22,689 --> 02:10:27,189
observations and means and standard

2783
02:10:24,430 --> 02:10:30,130
deviations again I don't do a hell of a

2784
02:10:27,189 --> 02:10:34,000
lot with that early on but it's nice to

2785
02:10:30,130 --> 02:10:35,529
note there so what we do you know this

2786
02:10:34,000 --> 02:10:37,420
is called a relational data set a

2787
02:10:35,529 --> 02:10:38,920
relational data set is one where there's

2788
02:10:37,420 --> 02:10:41,409
quite a few tables we have to join

2789
02:10:38,920 --> 02:10:43,779
together it's very easy to do that in

2790
02:10:41,409 --> 02:10:45,279
pandas there's a thing called merge so

2791
02:10:43,779 --> 02:10:46,359
great little function to do that and so

2792
02:10:45,279 --> 02:10:46,750
I just started joining everything

2793
02:10:46,359 --> 02:10:49,380
together

2794
02:10:46,750 --> 02:10:57,279
joining the weather or the Google Trends

2795
02:10:49,380 --> 02:10:59,529
stores yeah that's about everything I

2796
02:10:57,279 --> 02:11:02,649
guess you'll see there's one thing that

2797
02:10:59,529 --> 02:11:04,300
I'm using from the FASTA a library which

2798
02:11:02,649 --> 02:11:06,099
is called add date part we talked about

2799
02:11:04,300 --> 02:11:07,630
this a lot in the machine learning

2800
02:11:06,100 --> 02:11:10,570
course but basically this is going to

2801
02:11:07,630 --> 02:11:12,970
take a date and pull out of a bunch of

2802
02:11:10,569 --> 02:11:16,000
columns day of week is at the start of a

2803
02:11:12,970 --> 02:11:18,460
quarter month of year so on and so forth

2804
02:11:16,000 --> 02:11:22,020
and add them all in to the dataset okay

2805
02:11:18,460 --> 02:11:22,020
so this is all standard pre-processing

2806
02:11:22,739 --> 02:11:26,800
all right so we join everything together

2807
02:11:24,640 --> 02:11:28,329
we fiddle around with some of the dates

2808
02:11:26,800 --> 02:11:29,949
a little bit some of them are in month

2809
02:11:28,329 --> 02:11:35,399
in year format we turn it into date

2810
02:11:29,949 --> 02:11:37,929
format we spend a lot of time trying to

2811
02:11:35,399 --> 02:11:40,689
take information about for example

2812
02:11:37,930 --> 02:11:42,940
holidays and add a column for like how

2813
02:11:40,689 --> 02:11:45,129
long until the next holiday how long has

2814
02:11:42,939 --> 02:11:49,869
it been since the last holiday ditto for

2815
02:11:45,130 --> 02:11:52,180
promos so on and so forth okay so we do

2816
02:11:49,869 --> 02:11:54,609
all that and at the very end we

2817
02:11:52,180 --> 02:11:57,630
basically save a big structured data

2818
02:11:54,609 --> 02:11:59,589
file that contains all that stuff

2819
02:11:57,630 --> 02:12:01,300
something that those of you that use

2820
02:11:59,590 --> 02:12:03,100
pandas may not be aware of is that

2821
02:12:01,300 --> 02:12:05,800
there's a very cool new format called

2822
02:12:03,100 --> 02:12:08,890
feather which you can save a panda's

2823
02:12:05,800 --> 02:12:11,170
data frame into this feather format it's

2824
02:12:08,890 --> 02:12:13,960
kind of pretty much takes it as it sits

2825
02:12:11,170 --> 02:12:16,420
in RAM and dumps it to the disk and so

2826
02:12:13,960 --> 02:12:18,130
it's like really really really fast the

2827
02:12:16,420 --> 02:12:20,770
reason that you need to know this is

2828
02:12:18,130 --> 02:12:22,720
because the ecuadorian grocery

2829
02:12:20,770 --> 02:12:26,410
competition is on now has

2830
02:12:22,720 --> 02:12:28,420
350 million records so you will care

2831
02:12:26,409 --> 02:12:30,849
about how long things take a talk I

2832
02:12:28,420 --> 02:12:32,409
believe about six seconds for me to save

2833
02:12:30,850 --> 02:12:35,010
three hundred and fifty million records

2834
02:12:32,409 --> 02:12:37,539
to feather format so that's pretty cool

2835
02:12:35,010 --> 02:12:39,070
so at the end of all that I'd save it as

2836
02:12:37,539 --> 02:12:40,539
a feather format and for the rest of

2837
02:12:39,069 --> 02:12:43,170
this discussion I'm just going to take

2838
02:12:40,539 --> 02:12:45,399
it as given that we've got this nicely

2839
02:12:43,170 --> 02:12:48,789
pre-processed feature engineered file

2840
02:12:45,399 --> 02:12:50,649
and I can just go read better okay but

2841
02:12:48,789 --> 02:12:53,220
for you to play along at home you will

2842
02:12:50,649 --> 02:12:57,579
have to run those previous cells oh

2843
02:12:53,220 --> 02:12:59,949
except the see these ones have commented

2844
02:12:57,579 --> 02:13:01,689
out you don't have to run those because

2845
02:12:59,949 --> 02:13:03,729
the file that you download from files

2846
02:13:01,689 --> 02:13:09,399
doc bastard AI has already done that for

2847
02:13:03,729 --> 02:13:14,739
you okay all right so we basically have

2848
02:13:09,399 --> 02:13:18,509
all these columns so it basically is

2849
02:13:14,739 --> 02:13:23,829
going to tell us you know how many of

2850
02:13:18,510 --> 02:13:25,780
this thing was sold on this date at this

2851
02:13:23,829 --> 02:13:28,989
store and so the goal of this

2852
02:13:25,779 --> 02:13:32,590
competition is to find out how many

2853
02:13:28,989 --> 02:13:36,159
things will be sold for each store for

2854
02:13:32,590 --> 02:13:38,199
each type of thing in the future okay

2855
02:13:36,159 --> 02:13:40,899
and so that's basically what we're going

2856
02:13:38,199 --> 02:13:42,340
to be trying to do and so here's an

2857
02:13:40,899 --> 02:13:48,639
example of what some of the data looks

2858
02:13:42,340 --> 02:13:50,619
like and so next week we're going to see

2859
02:13:48,640 --> 02:13:52,300
how to go through these steps but

2860
02:13:50,619 --> 02:13:55,149
basically what we're going to learn is

2861
02:13:52,300 --> 02:13:58,480
we're going to learn to split the

2862
02:13:55,149 --> 02:14:01,389
columns into two types some columns were

2863
02:13:58,479 --> 02:14:06,309
going to treat as categorical which is

2864
02:14:01,390 --> 02:14:08,050
to say store ID one and store ID - I'm

2865
02:14:06,310 --> 02:14:11,050
not numerically related to each other

2866
02:14:08,050 --> 02:14:13,150
they're categories right we're going to

2867
02:14:11,050 --> 02:14:15,489
treat day of week like that - Monday and

2868
02:14:13,149 --> 02:14:17,799
Tuesday day zero and day one not

2869
02:14:15,489 --> 02:14:20,619
numerically related to each other where

2870
02:14:17,800 --> 02:14:24,100
else distance in kilometers to the

2871
02:14:20,619 --> 02:14:26,170
nearest competitor that's a number that

2872
02:14:24,100 --> 02:14:28,090
we're going to treat numerically right

2873
02:14:26,170 --> 02:14:29,770
so in other words the categorical

2874
02:14:28,090 --> 02:14:32,409
variables we basically are going to one

2875
02:14:29,770 --> 02:14:34,039
how to encode them you can think of it

2876
02:14:32,409 --> 02:14:35,838
as one hot encoding on where

2877
02:14:34,038 --> 02:14:38,198
the continuous variables we're going to

2878
02:14:35,838 --> 02:14:43,609
be feeding into fully connected layers

2879
02:14:38,198 --> 02:14:47,198
just as is okay so what we'll be doing

2880
02:14:43,609 --> 02:14:49,849
is we'll be basically creating a

2881
02:14:47,198 --> 02:14:51,108
validation set and you'll see like a lot

2882
02:14:49,849 --> 02:14:53,418
of these are start to look familiar this

2883
02:14:51,109 --> 02:14:54,739
is the same function we used on planet

2884
02:14:53,418 --> 02:14:58,668
and dog breeds to create a validation

2885
02:14:54,738 --> 02:14:59,658
set there's some stuff that you haven't

2886
02:14:58,668 --> 02:15:02,898
seen before

2887
02:14:59,658 --> 02:15:06,018
where we're going to basically rather

2888
02:15:02,899 --> 02:15:09,109
than saying image data dot from CSV

2889
02:15:06,019 --> 02:15:11,030
we're going to say columnar data from

2890
02:15:09,109 --> 02:15:13,780
data frame right so you can see like the

2891
02:15:11,029 --> 02:15:17,688
basic API concepts will be the same but

2892
02:15:13,779 --> 02:15:18,918
they're a little different right but

2893
02:15:17,689 --> 02:15:23,659
just like before we're going to get a

2894
02:15:18,918 --> 02:15:26,358
learner and we're going to go lr find to

2895
02:15:23,658 --> 02:15:28,748
find our best learning rate and then

2896
02:15:26,359 --> 02:15:33,019
we're going to go dot fit with a metric

2897
02:15:28,748 --> 02:15:35,469
with a cycle length okay so the basic

2898
02:15:33,019 --> 02:15:39,079
sequence who's going to end up looking

2899
02:15:35,469 --> 02:15:40,338
hopefully very familiar okay so we're

2900
02:15:39,078 --> 02:15:42,408
out of time

2901
02:15:40,338 --> 02:15:47,479
so what I suggest you do this week is

2902
02:15:42,408 --> 02:15:49,969
like try to enter as many capital image

2903
02:15:47,479 --> 02:15:53,030
competitions as possible like like try

2904
02:15:49,969 --> 02:15:59,029
to really get this feel for like cycling

2905
02:15:53,029 --> 02:16:03,948
learning rates plotting things you know

2906
02:15:59,029 --> 02:16:05,868
that that post I showed you at the start

2907
02:16:03,948 --> 02:16:08,928
of class today that kind of took you

2908
02:16:05,868 --> 02:16:11,029
through lesson one like really go

2909
02:16:08,929 --> 02:16:13,788
through that on as many image datasets

2910
02:16:11,029 --> 02:16:17,059
as you can you just feel really

2911
02:16:13,788 --> 02:16:18,408
comfortable with it right because you

2912
02:16:17,059 --> 02:16:19,878
want to get to the point where next week

2913
02:16:18,408 --> 02:16:22,308
when we start talking about structured

2914
02:16:19,878 --> 02:16:24,618
data that this idea of like how learners

2915
02:16:22,309 --> 02:16:26,449
kind of work and data works and data

2916
02:16:24,618 --> 02:16:28,868
loaders and data sets and looking at

2917
02:16:26,448 --> 02:16:31,818
pictures should be really you know

2918
02:16:28,868 --> 02:16:34,118
intuitive all right good luck see you

2919
02:16:31,819 --> 02:16:34,119
next week

2920
02:16:35,510 --> 02:16:39,040
[Applause]

