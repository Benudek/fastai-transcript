<p><strong>Lesson 7</strong></p>
<ul>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=3m4s">00:03:04</a>  Review of last week lesson on RNNs,<br>
Part 1, what to expect in Part 2 (start date: 19/03/2018)</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=8m48s">00:08:48</a> Building the RNN model with ‘self.init_hidden(bs)’ and ‘self.h’, the “back prop through time (BPTT)” approach</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=17m50s">00:17:50</a> Creating mini-batches, “split in 64 equal size chunks” not “split in chunks of size 64”, questions on data augmentation and choosing a BPTT size, PyTorch QRNN</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=23m41s">00:23:41</a> Using the data formats for your API, changing your data format vs creating a new dataset class, ‘data.Field()’</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=24m45s">00:24:45</a> How to create Nietzsche training/validation data</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=35m43s">00:35:43</a> Dealing with PyTorch not accepting a “Rank 3 Tensor”, only Rank 2 or 4, ‘F.log_softmax()’</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=44m5s">00:44:05</a> Question on ‘F.tanh()’, tanh activation function,<br>
replacing the ‘RNNCell’ by ‘GRUCell’</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=47m15s">00:47:15</a> Intro to GRU cell (RNNCell has gradient explosion problem - i.e. you need to use low learning rate and small BPTT)</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=53m40s">00:53:40</a> Long Short Term Memory (LSTM), ‘LayerOptimizer()’, Cosine Annealing ‘CosAnneal()’</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=1h1m47s">01:01:47</a> Pause</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=1h1m57s">01:01:57</a> Back to Computer Vision with CIFAR 10 and ‘lesson7-cifar10.ipynb’ notebook, Why study research on CIFAR 10 vs ImageNet vs MNIST ?</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=1h8m54s">01:08:54</a> Looking at a Fully Connected Model, based on a notebook from student ‘Kerem Turgutlu’, then a CNN model (with Excel demo)</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=1h21m54s">01:21:54</a> Refactored the model with new class ‘ConvLayer()’ and ‘padding’</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=1h25m40s">01:25:40</a> Using Batch Normalization (BatchNorm) to make the model more resilient, ‘BnLayer()’ and ‘ConvBnNet()’</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=1h36m2s">01:36:02</a> Previous bug in ‘Mini net’ in ‘lesson5-movielens.ipynb’, and many questions on BatchNorm, Lesson 7 Cifar10, AI/DL researchers vs practioners, ‘Yann Lecun’ &amp; ‘Ali Rahimi talk at NIPS 2017’ rigor/rigueur/theory/experiment.</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=1h50m51s">01:50:51</a> ‘Deep BatchNorm’</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=1h52m43s">01:52:43</a> Replace the model with ResNet, class ‘ResnetLayer()’, using ‘boosting’</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=1h58m38s">01:58:38</a> ‘Bottleneck’ layer with ‘BnLayer()’, ‘ResNet 2’ with ‘Resnet2()’, Skipping Connections.</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=2h2m1s">02:02:01</a> ‘lesson7-CAM.ipynb’ notebook, an intro to Part #2 using ‘Dogs v Cats’.</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=2h8m55s">02:08:55</a> Class Activation Maps (CAM) of ‘Dogs v Cats’.</p>
</li>
<li>
<p><a href="https://youtu.be/H3g26EVADgY?t=2h14m27s">02:14:27</a> Questions to Jeremy: “Your journey into Deep Learning” and “How to keep up with important research for practioners”,<br>
“If you intend to come to Part 2, you are expected to master all the techniques in Part 1”, Jeremy’s advice to master Part 1 and help new students in the incoming MOOC version to be released in January 2018.</p></li></ul>

